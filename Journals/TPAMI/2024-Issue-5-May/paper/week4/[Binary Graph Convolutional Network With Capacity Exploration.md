# [Binary Graph Convolutional Network With Capacity Exploration](https://ieeexplore.ieee.org/document/10356827/)
## 题目：具有容量探索的二进制图卷积网络
**作者：Junfu Wang; Yuanfang Guo; Liang Yang; Yunhong Wang**  
**源码：https://github.com/bywmm/Bi-GCN**  
****

# 摘要
当前图神经网络（GNNs）的成功通常依赖于加载整个属性图进行处理，这在有限的内存资源下可能无法满足，特别是当属性图很大时。本文首次提出了一种二进制图卷积网络（BiGCN），它将网络参数和输入节点属性二值化，并利用二进制运算代替浮点数矩阵乘法，以实现网络压缩和加速。同时，我们还提出了一种新的基于梯度近似的反向传播方法来适当训练我们的Bi-GCN。根据理论分析，我们的Bi-GCN可以平均减少网络参数和输入数据的内存消耗约31倍，并在三个引文网络上平均加速推理速度约51倍，即Cora、PubMed和CiteSeer。此外，我们介绍了一种通用方法，将我们的二值化方法推广到其他GNN变体，并实现了类似的效率。尽管所提出的Bi-GCN和Bi-GNNs简单而高效，但这些压缩网络也可能存在潜在的容量问题，即它们可能没有足够的存储容量来学习特定任务的适当表示。为了解决这个问题，提出了一个熵覆盖假设，以预测Bi-GNN隐藏层宽度的下限。广泛的实验表明，我们的Bi-GCN和Bi-GNNs可以在七个节点分类数据集上提供与相应的全精度基线相当的性能，并验证了我们的熵覆盖假设在解决容量问题方面的有效性。
# 关键词
- 二进制图神经网络
- 图表示学习
- 信息存储容量

# I. 引言
图（Graph）是一种广泛用于现实世界应用的数据表示方法，它能够表示具有复杂关系的数据。近年来，由于图神经网络（GNNs）在不规则图数据上的卓越表示能力，它们在各种任务上都取得了令人印象深刻的性能，例如生物预测[1]、[2]、社交分析[3]、[4]、交通预测[5]、[6]等。

然而，当前的GNNs设计基于一个隐含的假设，即GNNs的输入包含了整个属性图[7]、[8]、[9]。如果整个图太大而无法输入GNNs（由于内存资源有限），在训练和推理过程中，GNNs的性能可能会急剧下降。

为了解决这个问题，一个直观的解决方案是采样，即采样合适大小的子图分别加载到GNNs中。然后，可以通过小批量方案训练GNNs。基于采样的方法可以分为三类：邻居采样[10]、[11]、[12]，层采样[13]、[14]和图采样[15]、[16]、[17]。邻居采样为下一层中的每个节点选择固定数量的邻居，以确保每个节点都能被采样。因此，它可以在训练和推理过程中使用。然而，当层数增加时，会出现邻居爆炸问题[17]，即训练和推理时间将呈指数增长。与邻居采样不同，层采样通常在每一层采样一个子图，而图采样构建一组子图并在每个子图上构建完整的GNN。这两种方法直接在训练过程中采样子图，因此它们可以避免邻居爆炸问题。然而，它们不能保证在整个训练/推理过程中至少对每个节点进行一次采样。因此，它们只适用于训练过程，因为测试过程通常要求GNNs处理图中的每个节点。

另一个可行的解决方案是压缩，即压缩输入图数据和GNN模型的大小，以更好地利用有限的内存和计算资源。已经提出了一些压缩卷积神经网络（CNNs）的方法，如设计浅层网络[18]、剪枝[19]、设计紧凑层[20]和量化参数[21]。在这些方法中，量化由于其在减少内存消耗和计算复杂性方面的出色性能而在实践中得到广泛应用。二值化[21]、[22]、[23]，作为一种基于量化的方法，当需要更快的速度和更低的内存消耗时，在许多基于CNN的视觉任务中取得了巨大成功。

然而，与CNNs的压缩相比，GNNs的压缩具有独特的挑战。首先，由于输入图数据通常比GNN模型大得多，因此加载数据的压缩需要更多的关注。其次，节点在高层语义空间上往往与其邻居相似，而在低层特征空间上则不同。这与像图像这样的网格状数据不同。它要求压缩的GNNs具有足够的参数来进行表示学习。最后，GNNs通常是浅层的，例如标准GCN[7]只有两层，包含的冗余较少。因此，实现GNNs的压缩更具挑战性。

为了解决内存和复杂性问题，SGC[24]，一个1层的GNN，通过移除其非线性和折叠连续层之间的权重矩阵来压缩GCN[7]。这个1层GNN可以加速训练和推理过程，并具有可比的性能。尽管SGC压缩了网络参数，但它没有压缩加载的数据，这是处理图时GNNs的主要内存消耗。

在本文中，为了缓解内存和复杂性问题，我们首次提出了一个二值化的GCN，称为二进制图卷积网络（Bi-GCN），这是GCN[7]的一个简单而高效的近似，通过二值化参数和节点属性表示。具体来说，权重的二值化是通过将它们分割成多个特征选择器，并为每个选择器保持一个标量来进一步减少量化误差来执行的。类似地，节点特征的二值化可以通过分割节点特征并为每个节点分配一个注意力权重来执行。通过使用这些额外的标量，可以更有效地学习和保留更多有效的信息。在二值化权重和节点特征之后，由网络参数和输入数据引起的计算复杂性和内存消耗可以大大减少。由于现有的二进制反向传播方法[22]没有考虑二进制权重之间的关系，我们还设计了一种新的反向传播方法来解决这个问题。我们的Bi-GCN与基线方法之间的直观比较如图1所示，它表明我们的Bi-GCN在保持与标准全精度GNNs相当的准确性的同时，可以实现最快的推理速度和最低的内存消耗。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/11c199bf35f94e4ea35cab55d86021e3.png" width="70%" /> </div>

总的来说，我们提出的Bi-GCN可以减少节点表示中的冗余，同时保持主要信息。当层数增加时，Bi-GCN还可以更明显地减少参数的内存消耗，并有效缓解过拟合问题。此外，Bi-GCN的二值化方法可以很容易地应用于其他GNN。我们介绍了一种通用的二值化方法，将二值化应用于其他GNN变体。通过将二值化与其他高效技术（如注意力和采样）相结合，我们给出了三个最流行的GNN的详细二值化版本，即GAT、GraphSAGE和GraphSAINT，然后是通用的二值化方法。实验验证了这些Bi-GNNs也是有效的。

类似于其他压缩神经网络，我们的Bi-GCN和其他Bi-GNNs也可能遇到容量问题。例如，我们假设一个浮点GCN为半监督分类任务实现了峰值性能，恰好有足够的参数，这表明这个GCN的信息存储容量对于这个任务来说是恰当的。在这种情况下，对这个特定的浮点GCN进行直接二值化可能无法很好地工作，因为二值化后存储容量会减少。为了解决这个问题，一个实用的解决方案是增加隐藏层的宽度，以经验性地搜索合适的容量。然而，仍然不清楚为什么搜索到的宽度适合于近似浮点GCN的存储容量。

为了理论上解决这个容量问题，我们提出了一个熵覆盖假设，以估计Bi-GNN隐藏层的适当宽度的下限，与浮点GNNs相比。根据信息论，熵和最大离散熵可以代表实际信息量和存储容量[25]。熵覆盖假设假设Bi-GNN隐藏层的存储容量不小于训练良好的GNN隐藏层的信息量。然后，我们可以进一步得出结论，Bi-GNN隐藏层的宽度的下限，等价于存储容量，应该不小于训练良好的相应浮点GNN隐藏层的最大熵。我们提出的假设的有效性可以通过实验来证明。

贡献总结如下：
- 我们首次提出了一个二值化的GCN，称为二进制图卷积网络（Bi-GCN），可以显著减少网络参数和输入节点属性的内存消耗约31倍，并在三个引文网络上平均加速推理约51倍。
- 我们设计了一种新的反向传播方法来有效地训练我们的Bi-GCN，通过在反向传播过程中考虑二进制权重之间的关系。
- 我们介绍了一种通用方法，将我们的二值化方法推广到其他GNNs。关于显著的内存减少和加速，我们的二值化GNNs在七个节点分类任务上也可以提供与浮点GNNs相当的性能。
- 我们提出了一个熵覆盖假设，理论上分析了容量问题，以描述Bi-GNNs的二进制隐藏层的适当宽度的下限。

本文的初步版本发表在[26]。本文在以下方面显著改进了[26]：
（i）我们介绍了一种通用的二值化方法，用于其他GNN变体，以结合二值化与其他高效技术的优势，而[26]主要关注Bi-GCN的设计。
（ii）提出了熵覆盖假设，为容量问题提供了理论上的解释，根据训练良好的浮点GNN描述了Bi-GNN二进制隐藏层的适当宽度的下限。
（iii）我们在Raspberry Pi 4B板上评估了我们的Bi-GCN，以验证其现实世界的效率，而[26]只提供了理论分析。
（iv）使用新提出的基准测试OGB[27]进行了更多评估，进一步证明了所提出的Bi-GCN和Bi-GNNs的有效性。

本文的其余部分组织如下。第二节回顾了GNNs的采样技术以及CNNs的二值化方法的相关文献。第三节定义了数学符号，并介绍了流行的图卷积网络[7]。第四节提出了所提出的Bi-GCN和通用二值化方法。第五节分析了通过我们的二值化方法获得的Bi-GNNs的效率。第六节介绍了所提出的熵覆盖假设。第七节讨论了七个节点分类数据集上的实验结果。最后，第八节总结了本文。

# III. 预备知识

## A. 符号定义
本文中使用的符号定义如下。我们将无向属性图表示为 $G = \{V, E, X\}$ ，其中 $V = \{v_i\}_ {i=1}^{N}$ 是顶点集合， $E = \{e_i\}_ {i=1}^{E}$ 是边集合。每个顶点 $v_i$ 包含一个特征 $X_i \in \mathbb{R}^d$ 。 $X \in \mathbb{R}^{N \times d}$ 是所有节点中所有特征的集合。 $A = [a_{ij}] \in \mathbb{R}^{N \times N}$ 表示邻接矩阵，它揭示了每一对顶点之间的关系，即图 $G$ 的拓扑信息。 $d_i = \sum_{j} a_{ij}$ 表示顶点 $v_i$ 的度， $D = \text{diag}(d_1, d_2, ..., d_n)$ 对应于邻接矩阵 $A$ 的度矩阵。然后 $\hat{A} = A + I$ 是原始拓扑的邻接矩阵加上自环， $\hat{D}$ 是它对应的度矩阵，其中 $\hat{D}_ {ii} = \sum_{j} \hat{a}_{ij}$ 。注意我们使用上标 “(\(l\))” 来表示第 $l$ 层，例如 $H^{(l)}$ 是输入到第 $l$ 层的节点特征。

## B. 图卷积网络
图卷积网络 (GCN) [7] 已经成为过去几年中最流行的图神经网络。由于我们的二值化方法以GCN作为基础GNN，我们在此简要回顾GCN。

给定一个无向图 $G$ ，图卷积操作可以描述为

$$
H^{(l+1)} = \sigma( \tilde{A} H^{(l)} W^{(l)}), \quad (1)
$$

其中 $\tilde{A} = \hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1} {2}}$ 是一个稀疏矩阵， $W^{(l)} \in \mathbb{R}^{d^{(l)}_ {\text{in}} \times d^{(l)}_{\text{out}}}$ 包含可学习参数。注意 $H^{(l+1)}$ 是第 $l$ 层的输出以及第 $(l + 1)$ 层的输入， $H^{(0)} = X$ 。 $\sigma$ 是非线性激活函数，例如ReLU。

从空间方法的角度来看，GCN中的图卷积层可以分解为两步，其中 $\tilde{A} H^{(l)}$ 是聚合步骤， $H^{(l)} W^{(l)}$ 是特征提取步骤。聚合步骤倾向于约束局部邻域内的节点属性彼此相似。聚合之后，特征提取步骤可以轻松提取相邻节点之间的共同点。

GCN通常使用任务依赖的损失函数，例如节点分类任务的交叉熵损失，定义为

$$
L = -\sum_{v_i \in V_{\text{label}}} \sum_{c=1}^{C} Y_ {i,c} \log( \hat{Y}_ {i,c}), \quad (2)
$$

其中 $V_{\text{label}}$ 表示标记节点的集合， $C$ 表示类别数量， $Y$ 表示真实标签， $\hat{Y} = \text{softmax}(H^{(L)})$ 是 $L$ 层GCN的预测。


# IV. 二进制图卷积网络

本节介绍所提出的二进制图卷积网络(Bi-GCN)，这是标准GCN[7]的二进制版本。如前一节所述，图卷积层可以分解为两个步骤：聚合和特征提取。在Bi-GCN中，我们只关注二值化特征提取步骤，因为聚合步骤没有可学习的参数（这导致可以忽略不计的内存消耗），并且与特征提取步骤相比，它只需要少量的计算。因此，保持了原始GCN的聚合步骤。对于特征提取步骤，我们二值化了网络参数和节点特征，以减少内存消耗。为了降低计算复杂度并加速推理过程，采用了XNOR（非异或）和位计数操作，而不是传统的浮点乘法。然后，我们为训练我们的二进制图卷积层设计了一种有效的反向传播算法。最后，我们介绍了一种通用方法，将我们的二值化方法应用于其他GNNs，并给出了三种常用的GNNs的示例，即GAT、GraphSAGE和GraphSAINT。

## A. 向量二值化

首先，我们介绍向量二值化方法，这是我们二值化过程的重要组成部分。考虑存在一个向量 $V = (V_1, V_2, ..., V_t)$ ，我们的目标是获得其二进制近似值，用一个二进制向量 $V_B = \{-1, 1\}^t$ 和一个实数标量 $\alpha$ 表示，使得 $V \approx \alpha V_B$ 。这个近似可以表述为：

$$
J_v(V_B, \alpha) = ||V - \alpha V_B||_2^2.
$$

通过最小化上述优化问题，最优解[22]可以通过以下方式计算：

$$
V_B^* = \text{sign}(V),
$$

$$
\alpha^* = \frac{1}{t ||V||_1},
$$

其中sign(·)是符号函数，用于提取实数的符号。然后，如果存在另一个向量 $I = (I_1, I_2, ..., I_t)$ ，I和V的内积可以通过以下方式近似：

$$
I \cdot V \approx \alpha^* I \cdot V_B^*,
$$

其中·表示向量内积。如果需要进一步二值化向量I以压缩此内积，可以通过以下方式实现：

$$
I \cdot V \approx \alpha \beta I_B \cdot V_B,
$$

其中 $I_B$ 是二进制向量， $\beta$ 是标量。如果我们打算最小化直接近似误差 $|I \cdot V - \alpha \beta I_B \cdot V_B|$ 并计算此优化问题的最优解，一个最优解， $|I_B \cdot V_B| = 1$ 和 $\alpha \beta = \text{sign}(I_B \cdot V_B) I \cdot V$ ，可以容易地计算出来。不幸的是，这个解对 $I \cdot V$ 的值有很强的依赖性，并且倾向于丢失原始向量的大量信息。为了缓解这个问题，我们定义了内积 $I \cdot V$ 的近似问题为：

$$
J_{ip}(\alpha, \beta, I_B, V_B) = ||I \odot V - \alpha \beta I_B \odot V_B||_2^2,
$$

其中 $\odot$ 表示逐元素乘积。与(3)类似，最优解可以计算为：

$$
\alpha^* = \frac{1}{t ||V||_1},
$$

$$
\beta^* = \frac{1}{t ||I||_1},
$$

$$
V_B^* = \text{sign}(V),
$$

$$
I_B^* = \text{sign}(I).
$$

然后，(6)可以被重新表述为：

$$
I \cdot V \approx \alpha^* \beta^* I_B^* \cdot V_B^*.
$$

在(12)中本质上是按照我们的向量二值化算法对I和V进行二值化的结果。

## B. 特征提取步骤的二值化

基于向量二值化算法，我们可以对图卷积中的(1)式中的特征提取步骤 $Z^{(l)} = H^{(l)}W^{(l)}$ 进行二值化。注意，对于这个特征提取（矩阵乘法）步骤，我们采用分桶[40]方法将二进制内积操作推广到二进制矩阵乘法操作。具体来说，我们将矩阵分割成具有固定大小的多个连续值的桶，并分别进行缩放操作。

1) 参数的二值化：由于第l层参数矩阵的每个列 $W^{(l)}$ 在计算 $Z^{(l)}$ 时充当特征选择器，因此将 $W^{(l)}$ 的每个列分割为一个桶。设 $\alpha^{(l)} = (\alpha^{(l)}_ 1, \alpha^{(l)}_ 2, ..., \alpha^{(l)}_ {d^{(l)}_{\text{out}}})$ ，它们是每个桶对应的标量。然后，基于桶和它们对应的标量，可以实现 $W^{(l)}$ 的二值化。注意，标量 $\alpha^{(l)}$ 的值实际上决定了每个特征的重要性，因此可以被视为特征注意力。

设 $B^{(l)} = (B^{(l)}_ 1, B^{(l)}_ 2, ..., B^{(l)}_ {d^{(l)}_ {\text{out}}}) \in \{-1, 1\}^ {d^{(l)}_ {\text{in}} \times d^{(l)}_{\text{out}}}$ 是 $W^{(l)}$ 的二进制桶。那么，基于向量二值化算法，可以通过以下方式轻松计算出最优的 $B^{(l)}$ 和 $\alpha^{(l)}$ ：

$$
B^{(l)}_ j = \text{sign}(W^{(l)}_{:,j}),
$$

$$
\alpha^{(l)}_ j = \frac{1}{d^{(l)}_ {\text{out}}} ||W^{(l)}_{:,j}||_1,
$$

其中 $W^{(l)}_{:,j}$ 表示 $W^{(l)}$ 的第j列。它可以通过以下方式近似：

$$
W^{(l)}_ {:,j} \approx \tilde{W}^{(l)}_{:,j} = \alpha^{(l)}_j B^{(l)}_j.
$$

基于(15)，具有二进制权重的图卷积操作可以描述为：

$$
H^{(l+1)} \approx H^{(l+1)}_p = \tilde{A} H^{(l)} \tilde{W}^{(l)},
$$

其中 $H^{(l+1)}_p$ 是具有二进制权重 $\tilde{W}^{(l)}$ 的 $H^{(l+)}$ 的二进制近似。参数的二值化可以将内存消耗减少约30倍，与具有全精度的参数相比。

2) 节点特征的二值化：由于当前图卷积操作引起的过平滑问题[34]，当前的GNNs通常较浅，例如，普通的GCN只包含2个图卷积层。尽管未来的GNNs可能拥有更大的模型，但常用的属性图的数据大小通常比当前模型大小大得多。为了减少GNNs处理数据集时输入数据的内存消耗，我们对将由图卷积层处理的节点特征也进行了二值化。

要二值化节点特征，我们将 $H^{(l)}$ 按行桶分割，基于计算 $Z^{(l)}$ 的矩阵乘法约束，即 $H^{(l)}$ 的每一行将与 $W^{(l)}$ 的每一列进行内积。设 $\beta^{(l)} = (\beta^{(l)}_ 1, \beta^{(l)}_ 2, ..., \beta^{(l)}_ N)$ 分别表示 $H^{(l)}$ 中每个桶的标量。设 $F^{(l)} = (F^{(l)}_ 1; F^{(l)}_ 2; ...; F^{(l)}_ N) \in \{-1, 1\}^{N \times d^{(l)}_{\text{in}}}$ 是二进制桶。然后，借助向量二值化算法，可以通过以下方式计算出最优的 $\beta$ 和 $F$ ：

$$
\beta^{(l)}_i = \frac{1}{N} ||H^{(l)}_i,:||_1,
$$

$$
F^{(l)}_i = \text{sign}(H^{(l)}_i,:),
$$

其中 $H^{(l)}_i,:$ 表示 $H^{(l)}$ 的第i行。然后，可以通过以下方式获得 $H(l)$ 的二值近似：

$$
H(l)_i,: \approx \tilde{H}(l)_i,: = \beta(l)_i F(l)_i
$$

直观上， $\beta$ 可以被视为特征表示的节点权重。最后，带有二值化权重和节点特征的图卷积操作可以表述为：

$$
H(l+1) \approx H(l+1)_{ip} = \tilde{A} \tilde{H}(l) \tilde{W}(l)
$$

注意，节点特征的这种二值化，即图卷积层的输入，也具有激活的能力。因此我们不采用特定的激活函数（如 ReLU）。类似于权重的二值化，加载的属性图数据的内存消耗可以减少约 30 倍，与原始 GCN 相比。

3) 二值运算：有了二值化图卷积层，我们可以通过使用 XNOR 和位计数操作代替浮点加法和乘法来加速计算。设 $\zeta^{(l)}$ 表示 $Z^{(l)}$ 的近似值。然后，

$$
Z^{(l)}_ {ij} \approx \zeta^{(l)}_ {ij} = \beta^{(l)}_ i \alpha^{(l)}_ j F^{(l)}_ i,: \cdot B^{(l)}_{:,j}
$$

由于 $F^{(l)}$ 和 $B^{(l)}$ 的每个元素是 -1 或 1，这两个二值向量的内积可以被二值运算替换，即 XNOR 和位计数操作。然后， $21$ 可以重写为：

$$
\zeta^{(l)}_ {ij} = \beta^{(l)}_ i \alpha^{(l)}_ j F^{(l)}_ i,: \oplus B^{(l)}_{:,j}
$$

其中 $\oplus$ 表示使用 XNOR 和位计数操作的二值乘法操作。详细过程如图 2 所示。因此，普通 GCN 中的图卷积操作可以通过以下方式近似：

$$
H^{(l+1)} \approx H^{(l+1)}_b = \tilde{A}\zeta^{(l)}
$$

其中 $\zeta^{(l)}$ 通过 (22) 计算， $H^{(l+)}_b$ 是第 $l$ 层最终输出，具有二值化参数和输入。通过采用这种二值乘法操作，原始的浮点计算可以被相同数量的二值运算和一些额外的浮点计算替换。这将显著加快图卷积层的处理速度。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c2df71ba6f814222b978cd617c5fa712.png" width="70%" /> </div>

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0358eb71893b4deb8e9154888e61fd53.png" width="70%" /> </div>


## C. 基于二值梯度近似的反向传播

我们训练过程中的关键部分包括损失函数的选择以及用于训练二值化图卷积层的反向传播方法。在Bi-GCN中采用的损失函数与普通的GCN相同，如公式(2)所示。由于现有的反向传播方法[22]没有考虑二值权重之间的关系，为了对二值化图卷积层执行反向传播，需要新设计梯度计算方法。

为了计算第 $l$ 层的实际传播梯度，使用二值近似梯度 $\frac{\partial L}{\partial \tilde{H}^{(l)}}$ 来近似原始梯度，如[21]、[22]所示：

$$
\frac{\partial L}{\partial H^{(l)}} \approx \frac{\partial L}{\partial \tilde{H}^{(l)}} \cdot 1\{|r| < 1\}
$$

注意， $1\{|r| < 1\}$ 是指示函数，其值在 $|r| < 1$ 时为1，否则为0。这个指示函数作为硬双曲正切函数，保留了梯度信息。如果梯度的绝对值变得太大，性能会降低。因此，指示函数也用来“杀死”那些绝对值变得太大的梯度。

网络参数的梯度通过另一种梯度计算方法获得。这里，采用全精度梯度以保留更多的梯度信息。如果获得了二值化权重的梯度 $\frac{\partial L}{\partial \tilde{W}^{(l)}}$，那么 $\frac{\partial L}{\partial W^{(l)}_ {ij}}$ 可以计算为：

$$
\frac{\partial L}{\partial W^{(l)}_ {ij}} = \frac{\partial L}{\partial \tilde{W}^{(l)}_ {:,j}} \cdot \frac{\partial \tilde{W}^{(l)}_ {:,j}}{\partial W^{(l)}_ {ij}} = \frac{1}{d^{(l)}_ {in}} \sum_k \frac{\partial L}{\partial \tilde{W}^{(l)}_ {kj}} \cdot B^{(l)}_ {kj} + \alpha^{(l)}_ j \cdot \frac{\partial L}{\partial \tilde{W}^{(l)}_ {ij}} \cdot \frac{\partial B^{(l)}_ {ij}}{\partial W^{(l)}_{ij}}
$$

注意，在训练过程中，全精度梯度引入的内存大小受到所采用的GNN模型大小的限制，这只要求整体训练过程中非常小一部分的内存消耗。

计算符号函数 $\text{sign}(r)$ 的梯度时，采用直通估计器(STE)函数[41]，其中 $\frac{\partial \text{sign}(r)}{\partial r} = 1\{|r| < 1\}$ 。反向传播过程在算法1中进行了总结。


## D. 推广至其他二值图神经网络（Bi-GNNs）

这里，我们介绍了将我们的二值化方法推广到其他流行的图神经网络变体。一般来讲，二值化一个图神经网络层包括三个步骤。

首先，对于层的输入，使用标准批量归一化[42]（均值为零，方差为一），以在二值化后保持-1和1的平衡。然后，可以尽可能多地利用有限的存储容量。

其次，输入 $H^{(l)}$ 和层参数 $\Theta^{(l)}$ 被二值化，其中二值运算代替了矩阵乘法。然后，可以通过我们基于二值梯度近似的反向传播来训练二值化参数。

第三，移除原始的非线性函数，例如ReLU，原因有二。1) 我们二值化方法中的符号函数已经充当了非线性激活。2) 类似于ReLU的激活函数倾向于保留正值并大幅抑制负值，这明显改变了正负值的分布，并与采用的符号函数冲突。

具体来说，我们将展示三个详细的例子，即二值化三种最著名的图神经网络变体，包括GAT、GraphSAGE和GraphSAINT。通过二值化这些基础而实用的技术，例如图神经网络中的注意力和采样，可以为不同的实际场景构建不同的Bi-GNNs。

1) Bi-GAT: 图注意力网络[43]，作为图神经网络的一个流行变体，通过将自注意力策略应用于节点特征来学习加权聚合函数。典型的GAT卷积层定义为：
2) 
$$
h^{(l+1)}_ i = \sigma \left( \sum_{j \in N_i} \alpha^{(l)}_{ij} W^{(l)}_a h^{(l)}_j \right),
$$

其中 $W^{(l)}_ a$ 是可学习的参数矩阵， $h^{(l)}_ j$ 是第 $l$ 层的节点表示，满足 $h^{(0)}_ j = X_j$ 。注意 $\alpha^{(l)}_ {ij}$ 是通过以下方式计算的注意力分数：

$$
\alpha^{(l)}_{ij} = \text{softmax} \left( \left( a^{(l)} \right)^T \left( W^{(l)}_a h^{(l)}_i || W^{(l)}_a h^{(l)}_j \right) \right) \cdot \text{LeakyReLU} \left( \left( a^{(l)} \right)^T \left( W^{(l)}_a h^{(l)}_i + W^{(l)}_a h^{(l)}_j \right) \right)
$$

然后，利用我们的二值化方法，Bi-GAT的卷积层可以表示为：

$$
h^{(l+1)}_ i \approx \sum_{j \in N_i} \alpha^{(l)}_{ij} \cdot \tilde{W}^{(l)}_a \odot \tilde{h}^{(l)}_j,
$$

其中 $\alpha^{(l)}_{ij}$ 是通过将(27)中的 $W^{(l)}_a h^{(l)}$ 替换为其二值化版本 $\tilde{W}^{(l)}_a \odot \tilde{h}^{(l)}$ 来计算的。注意，我们没有二值化注意力参数 $a^{(l)}$，因为它也是高效的，即 $a^{(l)}$ 引入的计算和内存消耗接近于我们二值化版本的 $W^{(l)}_a$ 和 $h^{(l)}$ 的消耗。

2) Bi-GraphSAGE: GraphSAGE[10]提出了一种归纳学习方案，即邻居采样和聚合，用于图神经网络模型。在其聚合过程中使用了四种聚合器，即均值、LSTM、池化和GCN中的聚合器，它们在评估中倾向于给出类似的性能。在后续文献中，均值聚合器通常在GraphSAGE中使用，构建如下：

$$
h^{(l+1)}_ i = \text{ReLU} \left( W^{(l)}_ \theta \cdot h^{(l)}_ i + \frac{1}{|N_i|} \sum_{j \in N_i} W^{(l)}_n \cdot h^{(l)}_j \right),
$$

其中 $N_i$ 表示节点 $v_i$ 的邻居集合， $|N_i|$ 表示邻居的总数。最后，对于 $L$ 层的GraphSAGE， $h^{(L)}_i$ 被用来生成节点 $v_i$ 的预测。根据我们的二值化方法，Bi-GraphSAGE的卷积层形成为：

$$
h^{(l+1)}_ i \approx \tilde{W}^{(l)}_ \theta \odot \tilde{h}^{(l)}_ i + \frac{1}{|N_i|} \sum_{j \in N_i} \tilde{W}^{(l)}_n \odot \tilde{h}^{(l)}_j,
$$

其中 $\tilde{h}^{(l)}$ 是二值化的节点表示， $\tilde{W}^{(l)}_\theta$ 和 $\tilde{W}^{(l)}_n$ 是要学习的参数， $\odot$ 表示第4.2节中的二值运算。类似于我们的Bi-GCN，我们没有保留原始的非线性函数。

3) Bi-GraphSAINT: GraphSAINT[17]提出了一个基于图采样的图神经网络训练框架。它构建由一组子图组成的小批量，并在每个小批量上构建完整的图神经网络模型。这种图采样框架可以有效地应用于各种图神经网络模型，例如GraphSAGE[10]、GAT[43]、JK-Net[44]等。由于GraphSAGE和GAT的二值化版本已经介绍过了，JK-Net被用作GraphSAINT的基线图神经网络模型，以增加模型选择的多样性。

跳跃知识网络(JK-Net)[44]提出了一种层聚合操作，用于混合局部邻域中不同跳数的表示。对于 $L$ 层的JK-Net，

$$
Z_i = \text{LA}(h^{(1)}_i, h^{(2)}_i, ..., h^{(L)}_i),
$$

这有助于预测节点 $v_i$ 。注意 $h^{(l)}_ i$ 是第 $l$ 层图神经网络的表示， $\text{LA}(\cdot)$ 是层聚合函数，可以作为串联、池化、LSTM等实现。这里，使用常用的串联操作，即：

$$
Z_i = W_{\text{LA}} [h^{(1)}_i || h^{(2)}_i || ... || h^{(L)}_i],
$$

其中 $||$ 表示串联操作。类似于特征提取步骤，节点特征和权重参数通过以下方式二值化：

$$
Z_i = \tilde{W}_{\text{LA}} \odot [ \tilde{h}^{(1)}_i || \tilde{h}^{(2)}_i || ... || \tilde{h}^{(L)}_i ].
$$

在采用的GraphSAINT中，上述JK-Net的版本被使用，其中 $h^{(l)}_i$ 是通过在(29)中引入的GraphSAGE层计算的。

# V. 效率分析

本节以我们的Bi-GCN为例，提供对我们Bi-GNNs效率的理论分析，即与全精度(32位浮点数)GCN相比，模型大小和加载数据大小的压缩比，以及加速比。其他Bi-GNNs的压缩和加速比也可以以类似方式进行分析。

## A. 模型大小压缩
设全精度GCN中每层的参数表示为  $W^{(l)} \in \mathbb{R}^{d^{(l)}_ {in} \times d^{(l)}_ {out}}$ ，包含  $(d^{(l)}_ {in} \times d^{(l)}_ {out})$  个浮点数参数。相反，我们Bi-GCN中的第  $l$  层只包含  $(d^{(l)}_ {in} \times d^{(l)}_ {out})$  个二进制参数和  $d^{(l)}_{out}$  个浮点数参数。因此，参数大小可以减少一个因子：

$$
PC^{(l)} = \frac{32 d^{(l)}_ {in} d^{(l)}_ {out}}{d^{(l)}_ {in} d^{(l)}_ {out} + 32 d^{(l)}_ {out}} = \frac{32 d^{(l)}_ {in}}{d^{(l)}_{in} + 32}
$$

根据  $PC^{(l)}$  ，第  $l$  层参数的压缩比取决于输入节点特征的维度。例如，如表I所示，在Cora数据集上，具有64个神经元的2层Bi-GCN与全精度GCN相比，可以实现约31倍的模型大小压缩比。尽管对于普通GCN，网络参数的内存消耗小于输入数据，我们的二值化方法仍然有贡献。目前，已经有许多努力构建更深的GNNs [35] [45] [46]。随着层数的增加，内存消耗的减少将变得更大，这种贡献将变得更加重要。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/03beebb84dc744ef89207fc13ff17c6d.png" width="70%" /> </div>


## B. 数据大小压缩
目前，加载的数据往往是内存消耗的主要部分。在常用的数据集中，节点特征往往是加载数据的主要部分。因此，加载的节点特征的二值化可以大大减少GNNs处理数据集时的内存消耗。注意，本文中，节点特征的数据大小被用作整个加载数据大小的近似，因为在常用的属性图中，边通常是稀疏的，分割掩码的大小也很小。

设加载的节点特征表示为  $X \in \mathbb{R}^{N \times d}$ ，其中  $N$  是节点数， $d$  是每个节点的特征数。那么，全精度的  $X$  包含  $N \times d$  个浮点数值。在Bi-GCN中，加载的数据  $X$  可以被二值化。然后，可以获得  $N \times d$  个二进制值和  $N$  个浮点数值。因此，加载数据  $X$  的大小可以减少一个因子：

$$
DC = \frac{32 N d}{N d + 32 N} = \frac{32 d}{d + 32}
$$

根据  $DC$  ，加载数据大小的压缩比取决于节点特征的维度。在实践中，Bi-GCN可以实现平均约31倍的内存消耗减少，这表明可以完全加载更大的属性图。对于一些归纳数据集，我们就可以成功地加载整个图或比全精度GCN更大的子图。数据大小压缩的结果可以在表III和表IV中找到。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/810d3dc01b9346668d1b534e3fed9f2d.png" width="70%" /> </div>


## C. 加速
在分析了内存消耗之后，接下来对Bi-GCN与GCN相比的加速进行分析。设第  $l$  层的输入矩阵和参数的维度分别为  $N \times d^{(l)}_ {in}$  和  $d^{(l)}_ {in} \times d^{(l)}_ {out}$ 。原始GCN中的特征提取步骤需要  $N d^{(l)}_ {in} d^{(l)}_ {out}$  次加法和  $N d^{(l)}_ {in} d^{(l)}_ {out}$  次乘法运算。相反，我们Bi-GCN中第  $l$  层的特征提取步骤只需要  $N d^{(l)}_ {in} d^{(l)}_ {out}$  次二进制运算和  $2N d^{(l)}_ {out}$  次浮点乘法运算来进行二进制矩阵乘法。对于二值化特征提取步骤中的转换操作，需要通过(17)和(18)将中间节点特征二值化。具体来说，当  $l < L$  时，大约需要  $N d^{(l)}_{out}$  次浮点加法运算来转换(17)中的浮点输出，这将在下一层中使用。此外，由于(18)中的符号函数通常可以高效实现[47]，我们简单地忽略它，遵循[22]中的分析。

根据[22]，执行一个周期操作（包含一次乘法和一次加法）的处理时间可以用来执行64次二进制运算。然后，第  $l$  层特征提取步骤的加速比可以计算为：

$$
S^{(l)}_ {fe} = \frac{N d^{(l)}_ {in} d^{(l)}_ {out} + 2N d^{(l)}_ {out}}{64 N d^{(l)}_ {in} d^{(l)}_ {out} + 2N d^{(l)}_ {out}} = \frac{64 d^{(l)}_ {in}}{d^{(l)}_ {in} + 128}
$$

从  $S^{(l)}_ {fe}$  可以观察到，节点特征的维度  $d^{(l)}_{in}$  决定了特征提取步骤的加速效率。

对于聚合步骤，稀疏矩阵乘法包含  $|E| d^{(l)}_ {out}$  次浮点加法和  $|E| d^{(l)}_{out}$  次浮点乘法运算。如果我们设节点的平均度数为  $\text{deg}$  ，那么  $|E| = \frac{N \text{deg}}{2}$  。

根据上述分析，可以近似计算第  $l$  层图卷积层的完整加速比：

$$
S^{(l)}_ {full} = \frac{N d^{(l)}_ {in} d^{(l)}_ {out} + |E| d^{(l)}_ {out}}{64 N d^{(l)}_ {in} d^{(l)}_ {out} + 2N d^{(l)}_ {out} + |E| d^{(l)}_ {out}} = \frac{64 d^{(l)}_ {in} + 32 \text{deg} / d^{(l)}_ {in} + 128 + 32 \text{deg}}{d^{(l)}_{in} + 32 \text{deg}}
$$

注意，平均度数  $\text{deg}$  通常在基准数据集中较小，例如Cora数据集中  $\text{deg} \approx 2.0$  。当处理具有较低平均节点度数的图时，聚合步骤的计算成本，即  $32 \text{deg}$  ，通常对加速比的影响可以忽略不计。因此，第  $l$  层的加速比可以通过以下方式近似计算：

$$
S^{(l)}_ {full} \approx S^{(l)}_{fe}
$$

因此，当  $\text{deg}$  较小时，加速比主要取决于二值化图卷积层的输入维度，根据  $S^{(l)}_ {fe}$  和  $S^{(l)}_{full}$  。第一层图卷积层的输入维度等于输入图中节点特征的维度。其他图卷积层的输入维度等于隐藏层的维度。由于输入节点特征的维度通常较大，第一层的加速比往往较高，例如在Cora数据集上约为59倍。一般来说，具有较大输入维度的层需要更多的计算，因此可以通过我们的二值化节省更多的计算。例如，在Cora数据集上，两层Bi-GCN的第一层可以实现约59倍的加速比，第二层约为21倍。总体而言，两层Bi-GCN在Cora数据集上可以实现约53倍的加速比。

# VI. 容量探索

提出的Bi-GNNs旨在使用比浮点数GNNs更小的网络来提取关键信息。因此，它们在计算复杂性和内存消耗方面更为高效。不幸的是，与其他简单网络类似，当处理特定任务时，如果存储容量小于需求，Bi-GNNs很容易遇到容量问题。例如，如果一个浮点数GCN恰好有足够的参数进行良好训练，直接二值化原始结构可能会遇到容量问题，即它可能没有足够的表示能力来适当地学习任务。为了理论上解决这个问题，我们提出了一个简单而有效的熵覆盖假设，用于探索Bi-GNNs二进制隐藏层相对于特定任务的浮点数GNNs的适当宽度的下限。注意，我们仍然以Bi-GCN为例进行本节的讨论。

## A. 熵覆盖假设
首先，考虑一个在节点分类任务中表现良好的2层浮点数GCN，它在一个典型的数据集上进行了良好的训练，例如PubMed[48]。假设每个隐藏神经元的分布是可访问的。设 $d_{in}, d_{fp}, d_{out}$ 分别为输入层、隐藏层和输出层的维度，且满足 $d_{in} \geq d_{fp} \geq d_{out}$ 。设 $H(x), H(h), H(\hat{y})$ 分别表示输入层、隐藏层和输出层的熵。直观地说，神经网络扮演着语义提取器的角色，逐步从输入中提取与语义相关的信息。显然，总信息量预期会减少，即 $H(x) \geq H(h) \geq H(\hat{y})$ 。

然后，如果我们直接二值化上述GCN，可以得到一个具有相同输入维度 $d_{in}$ 和隐藏维度 $d_{fp}$ 的直接2层Bi-GCN，以及一个近似相同的输出 $\hat{y}$ ，因为期望与GCN具有相同的表示能力。类似地，在Bi-GCN中，也期望理想情况下信息量的减少量，即

$$
H(x_b) \geq H(h_b) \geq H(\hat{y}),
$$

其中 $x_b$ 表示二进制输入特征， $h_b$ 是具有 $d_{fp}$ 个二进制神经元的二进制中间特征。然而，由于期望的输出 $\hat{y}$ 是一个浮点数输出向量， $H(h_b) \geq H(\hat{y})$ 并不总是得到保证。因此，如果我们直接使用与GCN中浮点数隐藏层相同维度的二进制隐藏层，可能会引起容量问题。因此，为了实现与浮点数GCN相当的性能，需要探索合适的二进制隐藏神经元数量 $d_{bin}$ 。

为解决这个问题，一个直观的假设是二进制隐藏层的存储容量应该能够包含（不小于）浮点数隐藏层中的信息量 $H(h)$ 。最大熵被用来表示存储容量[25]，记作 $C(h_b)$ 。然后，上述假设可以描述为

$$
C(h_b) = d_{bin} \cdot \log_2 2 = d_{bin} \geq H(h).
$$

在这种情况下，二进制隐藏神经元的数量应该不小于 $H(h)$ 。然后，我们的熵覆盖假设定义如下。

熵覆盖假设：如果一个L层的GCN在每个隐藏层具有 $d_{fp}$ 个神经元时产生峰值性能，那么一个L层的Bi-GCN在二进制隐藏神经元数量满足

$$
d_{bin} \geq \max\{H(h^{(l)}) | l = 1, ..., L - 1\}
$$

时可以获得峰值性能。

具体来说，对于一个2层GCN，2层Bi-GCN的二进制隐藏层的宽度应满足

$$
d_{bin} \geq H(h^{(1)}).
$$

提出的熵覆盖假设引入了Bi-GCN隐藏层宽度的下限。注意，其有效性将在后面的章节中通过实验验证。然后，熵覆盖假设可以用来指导相应Bi-GCN的构建，通过估计存储容量的下限。

## B. 隐藏层熵估计
本小节中，我们将介绍采用的隐藏层熵估计方法。

首先，对于隐藏层中的一个浮点数神经元，这里采用分箱方法[49]、[50]来估计其熵，该方法将浮点数值神经元的连续空间划分为特定的间隔，并将其实现为有限数量M。然后，给定N个输入样本，任何浮点数值神经元的实证分布，例如 $h_i$ ，可以表述为

$$
\phi_{i,m} \equiv \frac{1}{N} \sum_{j=1}^N \delta_m(h_{ij}),
$$

其中 $\delta_m$ 表示集中在m上的概率测量[51]。然后，可以估计浮点数值神经元的熵[49]、[52]，即

$$
\hat{H}(h_i) = -\sum_{m=1}^M \phi_{i,m} \log \phi_{i,m}.
$$

根据条件概率的链式法则，隐藏层的联合熵 $H(h) = H(h_1, h_2, ..., h_{d_{fp}})$ 可以分解为

$$
H(h) = H(h_1 | h_2, ..., h_{d_{fp}}) + H(h_2 | h_3, ..., h_{d_{fp}}) + \cdots + H(h_{d_{fp}}),
$$

其中 $h_1, h_2, ..., h_{d_{fp}}$ 是隐藏层 $h$ 中的 $d_{fp}$ 个神经元。(45)是隐藏层熵 $H(h)$ 的确切计算公式。然而，由于隐藏神经元之间复杂的依赖关系，直接计算 $H(h)$ 是困难的。

考虑到GCN包含的参数恰好足够，其隐藏表示可能具有很少的冗余。在这种情况下，隐藏神经元倾向于彼此独立，以充分利用有限的存储容量。

$$
H_{\text{ind}}(h) = H(h_1) + H(h_2) + \cdots + H(h_{d_{fp}})
$$

被用来估计联合熵 $H(h)$ ，可以通过分箱[52]方法计算。然后，可以根据我们提出的熵覆盖假设估计Bi-GNN隐藏层宽度的下限。
# VII. 评估

在本节中，我们在基准数据集上评估了我们提出的二值化方法和我们的Bi-GNNs，对节点分类任务进行了验证，并验证了我们熵覆盖假设的有效性。注意，基于方法和数据集的特定设置，内存消耗和周期操作的数量是理论上估计的。我们的代码可在 https://github.com/bywmm/Bi-GCN 上获得。

## A. 数据集
我们使用七个常用的数据集进行了实验，这些数据集包括传递式和归纳式学习设置。数据集总结在表II中。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/335fa9c3a9004081bc55ef6833412b8f.png" width="70%" /> </div>


对于传递式学习任务，我们使用了四个引用网络，即Cora、PubMed、CiteSeer[48]和OGBN-Arxiv[27]。在这些引用网络中，节点和边分别代表研究论文（具有词袋特征）和引用（作为无向链接）。文章根据学科被分类为不同的类别。对于OGBN-Arxiv，我们采用了OGB基准[27]的数据划分策略；对于Cora、PubMed和CiteSeer，我们采用了Planetoid[53]的数据划分策略。

三个数据集，即Flickr、Reddit和OGBN-Products，被用于归纳学习任务。Flickr是一个由SNAP网站从四个不同来源收集的图像网络。每个节点代表上传到Flickr的图像，具有500维的词袋特征。无向边在捕获于同一位置的每一对图像之间、共享公共标签等的每一对图像之间形成。我们采用了GraphSAINT[53]的数据划分策略。Reddit是一个在[10]中构建的帖子网络。每个节点代表一个帖子，具有GloVe 300维词向量[54]。如果同一用户对两个帖子发表评论，则形成帖子到帖子的连接。标签表示帖子所属的社区。我们采用了GraphSAGE[10]中的数据划分策略。OGBN-Products[27]是在亚马逊上构建的一个共同购买网络。节点和边分别代表产品和产品之间的共同购买关系。这里，节点特征是产品描述的词袋特征[16]。标签是产品类别。我们采用了OGB基准[27]中相同的数据划分和评估策略。

根据采用的数据划分策略，Cora、CiteSeer和PubMed上的任务是半监督节点分类；Flickr、Reddit、OGBN-Arxiv和OGBN-Products上的任务是监督节点分类。

## B. 设置
对于三个常用的引用网络，即Cora、PubMed和CiteSeer，我们选择了一个具有64个隐藏神经元的2层GCN[7]作为基线。通过二值化这个GCN，我们得到了Bi-GCN。应用于Bi-GCN的评估协议是[7]中的。在训练过程中，GCN和Bi-GCN都使用Adam[55]优化器以0.001的学习率训练最多1000个周期，并在100个周期后采用早停条件，训练过程中使用dropout层，dropout率为0.4，在二值化中间层的输入后。我们使用Xavier初始化[56]初始化全精度权重。在Bi-GCN中，将标准批量归一化[42]（均值为零，方差为一）应用于输入特征向量。注意，我们还研究了不同模型深度对分类性能的影响。所有超参数都设置为与2层情况相同。

对于Reddit和Flickr，我们选择了GCN[10]、GraphSAGE[10]和GraphSAINT[17]的归纳版本作为我们的基线。注意，这里使用了2层GraphSAINT模型以进行公平比较。采用了文献中的设置。我们将二值化所有特征提取步骤以获得它们相应的二值化版本。二值化模型中的超参数与它们的全精度版本相同。

对于OGBN-Products和OGBN-Arxiv，选择了GCN、GraphSAGE和GAT作为基线。所有这些GNN都堆叠了3层。在OGBN-Arxiv上，我们为GCN和GraphSAGE使用了256个隐藏神经元，为GAT使用了128个隐藏神经元和2个头。在OGBN-Products上，我们为GCN和GraphSAGE设置了512个隐藏神经元，为GAT设置了256个隐藏神经元和2个头。所有模型都使用Xavier初始化[56]。它们在OGBNArxiv上训练了500个周期，在OGBN-Products上训练了20个周期，采用Adam优化器[55]，学习率为0.001。训练过程中使用了dropout层，dropout率为0.5。在每个全精度GNN层之后使用了批量归一化层，而在每个Bi-GNN层之前使用了批量归一化层。我们采用了OGB基准评估中常用的传递式学习设置用于OGBN-Arxiv，归纳式学习设置用于OGBN-Products。在OGBN-Products的训练过程中，使用了邻居采样策略[10]来生成它们的归纳版本，对于GCN和GraphSAGE，采样的邻居大小设置为20，对于GAT设置为10。采用了与二值化版本相同的超参数。

## C. 结果
1) 比较：在Cora、PubMed和CiteSeer上的结果展示在表III中。可以观察到，我们的Bi-GCN与全精度GCN和其他基线相比具有可比的性能。同时，GCN(Float16)和GCN(BFloat16)分别使用半精度浮点和brain浮点数据格式，可以实现与全精度GCN大致相同的结果。然而，它们只能提供比普通GCN快2倍的推理速度和低2倍的内存消耗。相反，我们的Bi-GCN在三个引用数据集上可以实现平均约51倍的推理速度和约30倍的内存消耗低于普通GCN、FastGCN和GAT。此外，所提出的Bi-GCN在加载数据大小时比SGC更有效。注意，在CiteSeer数据集上的预测精度下降比其他两个数据集更差。这可能是由于其较小的平均节点度数，即\( \frac{|V|}{|E|} \)，使得节点特征在输入数据中占有更大的信息比例。

表IV和表V展示了我们的Bi-GNNs，即Bi-GraphSAGE、Bi-GraphSAINT和Bi-GAT在四个基准上的结果。与我们的Bi-GCN类似，我们的二值化GNNs也可以显著减轻加载数据和模型参数的内存消耗，并减少计算量，具有可比的性能。Reddit和OGBN-Products数据集的原始数据大小分别为534.99 M和934.23 M。相比之下，我们的二值化GNNs分别只需要17.61 M和38.54 M来加载数据，证明了我们二值化方法的重要性。注意，Reddit数据集上二值化GNNs的加速比仅为约10倍，因为平均节点度数较大，如第5.3节所讨论。总的来说，这些结果证明了我们的二值化方法是有效的，并且可以成功地推广到各种GNNs。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/91303c78b302489280e1f938c415c080.png" width="70%" /> </div>


2) 消融研究：这里进行了消融研究，以验证网络参数和节点特征二值化以及二值梯度近似的有效性。

表III显示，当仅对节点特征进行二值化时，预测性能的变化较小。这表明全精度特征中存在许多冗余，我们的二值化可以保留用于节点分类的大部分有效信息。同时，对网络参数进行二值化的结果表明，二值化参数不能表示与全精度参数一样多的信息。然而，如果同时对节点属性和参数进行二值化，可以实现与GCN相当的性能。这表明二值化网络参数可以有效地通过二值化特征进行训练，即Bi-GCN可以成功地减少节点表示中的冗余，从而通过轻量级的二值化网络学习到有用的线索。此外，单独对参数和特征进行二值化都可以减少内存消耗，而推理加速只有在同时对参数和特征进行二值化时才会出现。

为了验证我们的二值梯度近似的有效性，构建了一个名为Bi-GCN-G的Bi-GCN变体，它采用了[22]中的梯度近似方法，即

$$
\frac{\partial L}{\partial W^{(l)}_ {ij}} = \frac{\partial L}{\partial \tilde{W}^ {(l)}_ {ij}} \left( 1 + \frac{d^{(l)}_ {in}} {d^{(l)}_ {in} + \alpha^{(l)}_ j} \cdot \frac{\partial B^{(l)}_ {ij}}{\partial W^{(l)}_{ij}} \right)
$$

。如表III所示，我们的Bi-GCN在所有三个数据集上的性能都优于Bi-GCN-G。这表明我们的二值梯度近似可以使Bi-GCN学习到合适的权重，并与浮点数对应物相比获得可比的性能。

3) 在硬件上的性能：第V节分析了Bi-GCN的理论效率，即模型大小和加载数据大小的压缩比以及加速比。这里，我们研究了Bi-GCN与全精度GCN相比在真实硬件平台上的推理时间和内存消耗。我们采用了一个具有64个隐藏神经元的2层全精度GCN及其对应的Bi-GCN。注意，所采用的Bi-GCN仅对第一层图卷积层进行了二值化，因为第二层图卷积层只包含少量参数和浮点运算。表VI中所实现Bi-GCN的理想和实际理论效率是基于第V节中的计算方法计算的。

为了方便起见，我们在一台Raspberry Pi 4B开发板上进行了评估，该开发板配备了4GB RAM和一个Broadcom BCM2711四核Cortex-A72（ARM v8）64位SoC，时钟频率为1.5 GHz，运行Ubuntu Server 23.04（64位操作系统）。我们使用Larq框架[47]将GCN和Bi-GCN转换为TensorFlow Lite（TFLite），Larq框架是一个用于二值化神经网络（BNNs）的开源Python库。然后，我们使用Larq Convert Engine（LCE）工具评估这两个模型，这是用于BNNs的高度优化的推理引擎。值得注意的是，目前LCE工具尚未针对我们的Bi-GCN所需的GNN操作进行充分优化，即Larq框架中的二值密集层（包括二值矩阵乘法）尚未优化。因此，我们使用了一些等效的优化运算符，包括内核大小为1×1的二值卷积层作为替代。不幸的是，Larq框架中的二值卷积层尚未针对1×1内核大小进行充分优化。具体来说，二值卷积层可以同时处理四个输入像素，而这种等效的1×1卷积的内核只有一个输入像素。因此，这种二值1×1卷积层的实现速度比完全优化的版本慢4倍[57]。因此，考虑到这一点，我们可以通过第V节中的方法计算出所实现Bi-GCN的实际理论加速比，考虑到一个周期运算只能执行16次二进制运算，即原始的64次二进制运算减少了4倍。

如表VI所示，在真实的Raspberry Pi平台上，我们的Bi-GCN与全精度GCN相比仍然可以实现显著更快的推理速度和更少的内存消耗。在推理过程中，我们的Bi-GCN在三个引用网络上平均加速了13.39倍，与GCN相比。理想情况下，我们实现的Bi-GCN平均可以实现43.75倍的理想理论加速比。如果我们考虑到Larq框架中未优化的操作，我们的实现的Bi-GCN平均可以实现14.27倍的实际理论加速比。可以观察到，Bi-GCN在现实世界中的加速比非常接近实现的Bi-GCN的实际理论加速比。然后，我们可以预期，如果二值矩阵乘法操作可以在Larq框架中完全优化，Bi-GCN在现实世界中的加速比将趋向于接近它们的理想理论加速比，例如平均为43.75倍。


<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/7eac45b12f9f4095802bf5644da921c6.png" width="70%" /> </div>


同时，我们的Bi-GCN在推理过程中平均减少了10.41倍的总体内存消耗，与全精度GCN相比，在三个引用网络上。对于模型大小压缩，我们的Bi-GCN在三个引用网络上平均减少了17.35倍的总体模型大小，与全精度GCN相比。注意，TFLite需要一些非数据缓冲区来存储操作符、子图等。当三个网络在Raspberry Pi 4B平台上通过模型处理时，实现的GCN的非数据缓冲区大小约为7K，而实现的Bi-GCN的非数据缓冲区大小约为7.5K。通过排除非数据缓冲区，我们实现的Bi-GCN几乎可以达到模型大小的理论压缩比。这些结果表明，Bi-GCN可以在真实平台上处理比GCN更大的图，该平台具有有限的内存资源。

4) 不同模型深度的影响：本小节分析了Bi-GCN中不同模型深度的影响。图3(a)显示了在Cora数据集上具有不同模型深度的GCN和Bi-GCN的传递结果。可以观察到，Bi-GCN比原始GCN更适合构建更深的GNN。当GCN由三个或更多图卷积层组成时，其准确率急剧下降。相反，我们提出的Bi-GCN的性能下降缓慢。根据图3(b)，随着层数的增加，GCN将很快遇到过拟合问题。然而，我们提出的Bi-GCN可以有效缓解这个过拟合问题。图4展示了内存消耗和推理速度的比较。由于SGC只包含一层，其内存消耗不会随着聚合次数的增加而改变。随着层数的增加，Bi-GCN可以节省更多的内存。对于加速结果，GCN和Bi-GCN之间的比率随着层数的增加而略有下降，而实际减少的计算成本却增加了。请注意，SGC中所需的操作不会明显增加，因为它只包含一个特征提取层。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/193e638a1d6f43c48680c16fbee8136e.png" width="70%" /> </div>


5) 容量分析：在这一小节中，我们使用了三个不同规模和类型的节点分类数据集，即 PubMed、Reddit 和 OGBN-Products，来验证所提出的熵覆盖假设的有效性。图 5 展示了在这三个数据集上，具有不同隐藏神经元数量的 GNNs 和 Bi-GNNs 的性能。可以观察到，峰值点，即最小适当隐藏神经元数量，对于 PubMed 上的 GCN 是 16，对于 Reddit 上的 GCN 是 32，对于 OGBN-Products 上的 GCN 是 128，对于 OGBN-Products 上的 GraphSAGE 是 512。注意，适当的隐藏神经元数量是指，(Bi-)GNN 可以实现其最佳性能至少 99% 的性能。根据我们的熵覆盖假设，边界显示了相应 Bi-GNN 隐藏层宽度的下限。详细的熵估计结果展示在表 VII 中。例如，98 是 PubMed 数据集上 Bi-GCN 隐藏层宽度的下限，而 Bi-GCN 的峰值性能是使用 128 个隐藏神经元实现的，这略大于估计的下界。在其他三个案例中也可以发现类似的观察结果。根据第 V 节的分析，与具有相同隐藏神经元数量的浮点数 GCN 相比，Bi-GCN 可以实现约 31 倍的模型参数压缩比。考虑到极端情况，即 GCN 和 Bi-GCN 都使用最小适当数量的隐藏神经元实现峰值性能，Bi-GCN 在 PubMed 和 Reddit 上的模型大小上也可以分别提高约 4 倍和 8 倍，在 OGBN-Products 上也是类似的。此外，加载数据的压缩效率不会随着隐藏层宽度的变化而变化。由于典型数据（图）的大小通常比模型大小大得多，我们的 Bi-GNN 在这三个数据集上仍然可以实现约 30 倍的整体压缩比。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d7997d93f83b49ee96b0d2b190335e0b.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/940c7158ad3b4d4bb79a1bb0d69fe169.png" width="70%" /> </div>


# VIII. 结论

本文提出了一种图卷积网络（GCN）的二值化版本，名为Bi-GCN，通过二值化网络参数和节点属性（输入数据）来实现。浮点运算已被二值运算替代，以加速推理。此外，我们设计了一种新的基于梯度近似的反向传播方法来训练二值化图卷积层。根据我们的理论分析，Bi-GCN可以在三个引用网络上，即Cora、PubMed和CiteSeer上，平均减少约31倍的网络参数和节点属性的内存消耗，并加速推理速度约51倍。此外，我们引入了一种通用的二值化方法，应用于其他图神经网络（GNNs），并且二值化后的GNNs（Bi-GNNs）也能获得类似的显著内存减少和加速。最后，提出了一个直观的熵覆盖假设，通过估计Bi-GNN隐藏层宽度的下限来解决Bi-GNN的容量问题。广泛的实验已经证明，我们的Bi-GCN和Bi-GNN在传递式和归纳式任务中都能提供与相应图网络相当的表现。此外，我们已经验证了我们的Bi-GCN在真实硬件平台（即Raspberry Pi 4B板）上仍然比全精度GCN更有效，它可以在三个引用网络上平均减少约17倍的模型大小，平均减少约13倍的整体内存消耗，并加速推理速度约10倍。此外，我们还通过实验验证了我们的熵覆盖假设解决容量问题的有效性。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
