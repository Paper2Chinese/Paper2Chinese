# 题目：[Low-Rank Tensor Function Representation for Multi-Dimensional Data Recovery](https://ieeexplore.ieee.org/document/10354352/)  
## 多维数据恢复的低阶张量函数表示
**作者：Yisi Luo; Xile Zhao; Zhemin Li; Michael K. Ng; Deyu Meng** 

****

# 摘要

由于高阶张量天然适合于表示现实世界中的多维数据，例如彩色图像和视频，低秩张量表示已成为机器学习和计算机视觉中的新兴领域之一。然而，传统的低秩张量表示仅能表示网格上的多维离散数据，这限制了它们在网格之外许多场景中的潜在应用性。为了打破这一障碍，我们提出了一种由多层感知器（MLPs）参数化的低秩张量函数表示（LRTFR），它可以连续地表示超出网格的数据，并具有强大的表示能力。具体来说，所提出的张量函数将任意坐标映射到相应的值，能够连续地在无限的实数空间中表示数据。与离散张量并行，我们为张量函数开发了两个基本概念，即张量函数秩和低秩张量函数分解，并利用MLPs来参数化张量函数分解中的因子函数。我们理论上证明了LRTFR中低秩和平滑正则化是和谐统一的，这使得数据连续表示具有高效性和有效性。从图像处理（图像修复和去噪）、机器学习（超参数优化）和计算机图形学（点云上采样）中产生的广泛的多维数据恢复应用，证明了我们方法的优越性和多功能性，与最先进的方法相比。特别是，超出原始网格分辨率（超参数优化）或甚至超出网格（点云上采样）的实验验证了我们方法在连续表示方面的优越性能。

# 关键词

数据恢复
多维数据
张量分解

# I. 引言

近年来，由于技术的进步，各种类型的多维数据（例如彩色图像、多光谱图像、点云、交通流量数据、用户-项目数据等）不断涌现。从数学角度来看，高阶张量天然适合于多维数据建模和处理，这是机器学习、计算机视觉和科学计算中的一个重点领域。

大多数现实世界数据本质上表现出低维结构，例如图像、视频、点云等。因此，矩阵/张量的低秩建模已广泛用于数据处理和表示。不同于矩阵，高阶张量的秩定义不是唯一的。最经典的张量秩是Tucker秩（由展开矩阵的秩定义）和CANDECOMP/PARAFAC（CP）秩（定义为最小的秩一张量分解数）。研究表明，求解低Tucker/CP秩规划可以获得有效的多维数据的低秩表示。同时，低Tucker/CP秩模型已应用于提高现代深度学习算法的效率，这显示了它们广泛而有前途的应用性。另一种基于所谓张量网络分解的张量秩，例如张量列车秩、张量环秩和张量树秩，已被证明对多维建模具有高度的代表性。最近，基于张量奇异值分解（t-SVD）的张量管秩引起了极大关注，因为它与矩阵秩的定义有强烈的关系。

总之，低秩张量建模已成为多维数据表示和处理的日益流行的选择。除了低秩性，平滑性是多维数据表示中另一个经常考虑的正则化项。许多文献将平滑性纳入低秩表示中。在这种混合模型中，低秩性通常由低秩分解或替代函数揭示。除了这些，平滑正则化的两种主要类别是显式的和隐式的。显式的平滑正则化主要基于总变差（TV）及其变体。隐式的平滑正则化是通过使用基函数来参数化模型，这自然地将模型扩展到与本工作相关的函数表示。例如，先驱工作利用非负矩阵分解通过基函数参数化来揭示隐式平滑性，并进一步扩展到高阶张量模型中使用低Tucker秩正则化。最近，CP分解通过使用傅里叶级数被巧妙地推广到多变量函数。这些隐式平滑表示的主要目的是使用一些基函数（例如高斯基或傅里叶基）来表示低秩矩阵/张量，这隐式地诱导了矩阵/张量中相邻元素之间的平滑性。尽管如此，这些浅层基函数有时可能不适合捕捉现实世界数据的复杂细节。与此同时，这些基于基函数的表示仍然依赖于在原始网格上定义的低秩正则化来处理简单的数据分析或回归问题，这可能不适用于更具挑战性的多维数据恢复问题。

总之，低秩张量表示适用于在离散网格上表示数据。然而，如何将低秩张量模型扩展为连续数据表示超出网格，是一个由现实世界应用驱动的紧迫挑战。例如，在点云表示中，传统的低秩表示无法表示超出网格的此类信号。一个重要的问题自然出现了：我们能否开发一种多维数据表示，不仅可以保留低秩结构，还可以无限分辨率地连续表示超出网格的数据？

为了应对这一挑战，本工作提出了低秩张量函数表示（LRTFR），用于多维数据的连续表示。具体来说，我们考虑用一个张量函数来表示数据，该函数将多维坐标映射到相应的值，以连续地表示数据。与经典的低秩张量表示并行，我们为张量函数开发了两个基本概念，即张量函数秩（定义3）和低秩张量函数分解（定理2），这本质上将低秩性编码到连续表示中。我们使用多层感知器（MLPs）来参数化张量函数分解中的因子函数，这使得模型对现实世界数据表示具有高表达性。此外，我们理论上证明了LRTFR中隐含的Lipschitz平滑正则化。在LRTFR中，低秩性和平滑性和谐统一，使其对数据的连续表示既有效又高效。与传统的低秩张量表示相比，LRTFR更通用，能够表示网格上或网格之外的现实世界离散数据；见图1。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/2ec81d50c5be425eafe182f703411181.png#pic_ center" width="70%" />
</div>

总之，本工作做出了以下贡献：

i) 为了在网格上或网格之外连续表示多维数据，我们提出了由MLPs参数化的低秩张量函数表示。由于函数表示的连续性质和MLPs的表达能力，我们的LRTFR在表示网格上或网格之外的各种现实世界多维数据方面既有效又强大。

ii) 我们提出了张量函数的两个概念，即张量函数秩和低秩张量函数分解，这为多维数据的离散和连续表示建立了联系。我们理论上证明了在MLP参数化的LRTFR中，低秩和平滑正则化是和谐统一的，这揭示了其在多维数据恢复中的潜在有效性。

iii) 所提出的LRTFR适用于多个多维数据恢复任务，无论是在网格上还是在网格之外，包括多维图像修复和去噪（在原始分辨率的网格上）、超参数优化（在超出原始分辨率的网格上）以及点云上采样（超出网格）。广泛的实验验证了我们方法的广泛适用性和优越性，与最先进的方法相比。

# III：提出的方法

## A. 预备知识

本文中使用的一些常用符号总结在表I中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/050aa941cfec49fea3564d462bd0fa65.png#pic_ center" width="70%" />
</div>

此外，我们引入张量Tucker秩和Tucker分解如下。


**定义1. (Tucker秩[11])**：张量 $X \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ 的Tucker秩是一个向量，定义为

$$
\text{rank}_ T(X) := (\text{rank}(X^{(1)}), \text{rank}(X^{(2)}), \text{rank}(X^{(3)})), (1)
$$

其中 $X^{(i)}$ ( $i = 1, 2, 3$ )表示 $X$的第 $i$ 模式展开矩阵， $\text{rank}(\cdot)$ 表示矩阵秩。为简化，我们有时使用符号 $(\text{rank}_ T(X))(i) := \text{rank}(X^{(i)})$ 。

**定理1. (Tucker分解[11])**：设 $X \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ 。

i) 若 $\text{rank}_ T(X) = (r_ 1, r_ 2, r_ 3)$，则存在核心张量 $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 和三个因子矩阵 $U \in \mathbb{R}^{n_ 1 \times r_ 1}, V \in \mathbb{R}^{n_ 2 \times r_ 2}, W \in \mathbb{R}^{n_ 3 \times r_ 3}$ ，使得 $X = C \times_ 1 U \times_ 2 V \times_ 3 W$ 。

ii) 设 $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 为任意张量， $U \in \mathbb{R}^{n_ 1 \times r_ 1}, V \in \mathbb{R}^{n_ 2 \times r_ 2}, W \in \mathbb{R}^{n_ 3 \times r_ 3}$ 为任意矩阵( $r_ i \leq n_ i$ ，对于 $i = 1, 2, 3$ )。则有

$$
\text{rank}_ T(C \times_ 1 U \times_ 2 V \times_ 3 W) \leq r_ i, (i = 1, 2, 3)。
$$

## B. 低秩张量函数表示

在本节中，我们详细介绍了为多维数据提出的低秩张量函数表示(LRTFR)。不失一般性，我们考虑三维情况，尽管它可以很容易地推广到更高维情况。设 $f(\cdot): X_ f \times Y_ f \times Z_ f \rightarrow \mathbb{R}$ 为一个有界实函数，其中 $X_ f, Y_ f, Z_ f \subset \mathbb{R}$ 是在三个维度中的定义域。函数 $f(\cdot)$ 在任何坐标 $D_ f := X_ f \times Y_ f \times Z_ f$ 给出数据的值。我们将 $f(\cdot)$ 解释为张量函数，因为它将一个三维坐标映射到相应的值，隐式地表示三阶张量数据。与传统的张量表述相比，张量函数本质上允许我们在超出原始分辨率甚至超出网格的网格上处理和分析多维数据。当 $D_ f$ 是一些常数的离散集合时， $f(\cdot)$ 的输出形式退化为离散情况(即张量)。

基于张量函数，我们可以自然地定义以下采样张量集，它涵盖了所有可以从张量函数中通过不同的采样坐标采样得到的张量。

**定义2**：对于张量函数 $f(\cdot): D_ f \rightarrow \mathbb{R}$ ，其中 $D_ f \subset \mathbb{R}^3$ ，我们定义采样张量集 $S[f]$ 为

$$
S[f] := \{T | T(i,j,k) = f(x(i), y(j), z(k)), x \in X_ {n_ 1}^f, y \in Y_ {n_ 2}^f, z \in Z_ {n_ 3}^f, n_ 1, n_ 2, n_ 3 \in \mathbb{N}^+\}, (2)
$$

其中 $x, y, z$ 表示坐标向量变量， $n_ 1, n_ 2, n_ 3$ 是正整数变量，决定了采样张量 $T$ 的大小。

在定义2中，采样张量集 $S[f]$ 定义为包含从 $f(\cdot)$ 中采样得到的所有离散张量的集合。具体来说，给定采样坐标向量 $x \in X_ {n_ 1}^f, y \in Y_ {n_ 2}^f, z \in Z_ {n_ 3}^f$ ，相应的采样张量 $T \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ 定义为 $T(i,j,k) = f(x(i), y(j), z(k))$ ，对于所有的 $i, j, k$ ，其中 $T(i,j,k)$ 表示 $T$ 的第 $(i, j, k)$ 个元素， $x(i)$ 表示向量 $x$ 的第 $i$ 个元素， $y(j)$ 表示向量 $y$ 的第 $j$ 个元素， $z(k)$ 表示向量 $z$ 的第 $k$ 个元素。给定不同的坐标向量 $x, y, z$ ，我们可以获得张量集合 $S[f]$ 中的不同离散张量。

张量函数是处理多维数据的一种有前景和潜在的工具。一个有趣且基本的问题是，我们是否可以类似地为张量函数定义“秩”并开发“张量分解”。关于秩的定义，一个合理的期望是，如果 $f(\cdot)$ 是低秩的，那么从 $S[f]$ 中采样的任何张量都是低秩张量。因此，我们可以自然地定义 $f(\cdot)$ 的函数秩(F-秩)为 $S[f]$ 中张量秩的最大值。

**定义3**：给定张量函数 $f: D_ f = X_ f \times Y_ f \times Z_ f \rightarrow \mathbb{R}$ ，我们定义其复杂性度量，记为 $F\text{-rank}[f]$ ( $f(\cdot)$ 的函数秩)，为采样张量集 $S[f]$ 中Tucker秩的最大值：

$$
F\text{-rank}[f] := (r_ 1, r_ 2, r_ 3), \text{ where } r_ i = \sup_ {T \in S[f]} \text{rank}(T^{(i)}). (3)
$$

这里，给定采样张量集 $S[f]$ 中的张量 $T$ ， $T^{(i)}$ ( $i = 1, 2, 3$ )表示 $T$ 的第 $i$ 模式展开矩阵。

我们称 $F\text{-rank}[f] = (r_ 1, r_ 2, r_ 3)$ (对于 $i = 1, 2, 3$ ， $r_ i < \infty$ )的张量函数 $f(\cdot)$ 为低秩张量函数，因为任何 $T \in S[f]$ 的Tucker秩都受到 $(r_ 1, r_ 2, r_ 3)$ 的限制。当 $f(\cdot)$ 在某些离散集合上定义时，F-秩退化为离散情况，即经典的Tucker秩，如命题1.1所述。

**命题1**：设 $X \in \mathbb{R}^{m_ 1 \times m_ 2 \times m_ 3}$ 为任意张量。设 $X_ f = \{1, 2, . . . , m_ 1\}, Y_ f = \{1, 2, . . . , m_ 2\}, Z_ f = \{1, 2, . . . , m_ 3\}$ 为三个离散集合，记 $D_ f = X_ f \times Y_ f \times Z_ f$ 。定义张量函数 $f(\cdot): D_ f \rightarrow \mathbb{R}$ 为 $f(v_ 1, v_ 2, v_ 3) = X(v_ 1,v_ 2,v_ 3)$ 对于任何 $(v_ 1, v_ 2, v_ 3) \in D_ f$ 。那么我们有 $F\text{-rank}[f] = \text{rank}_ T(X)$ 。

命题1在离散情况下建立了F-秩和经典张量秩之间的联系。否则，如果定义域是连续的，例如 $X_ f = [1, n_ 1], Y_ f = [1, n_ 2], Z_ f = [1, n_ 3]$ 对于一些常数 $n_ i s$ ( $i = 1, 2, 3$ )， $f(\cdot)$ 可以表示超出网格的无限分辨率数据。因此，F-秩是从离散张量到张量函数的Tucker秩的扩展，用于连续表示。

类似地，传统张量表示，考虑一个低秩张量函数 $f(\cdot)$ 是否可以有某些张量分解策略来编码低秩性是有意义的。我们给出了一个积极的答案，即具有 $F\text{-rank}[f] = (r_ 1, r_ 2, r_ 3)$ 的张量函数 $f(\cdot)$ 可以分解为核心张量 $C$ 和三个因子函数 $f_ x(\cdot), f_ y(\cdot), f_ z(\cdot)$ 的乘积，它们的输出维度与F-秩 $r_ i$ ( $i = 1, 2, 3$ )有关。相反，核心张量 $C$ 和三个因子函数 $g_ x(\cdot), g_ y(\cdot), g_ z(\cdot)$ 的乘积形成一个低秩表示 $g(\cdot)$ ，其中 $F\text{-rank}[g]$ 受到 $g_ x(\cdot), g_ y(\cdot), g_ z(\cdot)$ 的输出维度的限制。理论形式如下。

**定理2. (低秩张量函数分解)**：设 $f(\cdot): D_ f = X_ f \times Y_ f \times Z_ f \rightarrow \mathbb{R}$ 为一个有界张量函数，其中 $X_ f, Y_ f, Z_ f \subset \mathbb{R}$ 。

i) 若 $F\text{-rank}[f] = (r_ 1, r_ 2, r_ 3)$ ，则存在一个张量 $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 和三个有界函数 $f_ x(\cdot): X_ f \rightarrow \mathbb{R}^{r_ 1}, f_ y(\cdot): Y_ f \rightarrow \mathbb{R}^{r_ 2}, f_ z(\cdot): Z_ f \rightarrow \mathbb{R}^{r_ 3}$ ，使得对于任何 $(v_ 1, v_ 2, v_ 3) \in D_ f$ ，有 $f(v_ 1, v_ 2, v_ 3) = C \times_ 1 f_ x(v_ 1) \times_ 2 f_ y(v_ 2) \times_ 3 f_ z(v_ 3)$ 。

ii) 另一方面，设 $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 为任意张量， $g_ x(\cdot): X_ g \rightarrow \mathbb{R}^{r_ 1}, g_ y(\cdot): Y_ g \rightarrow \mathbb{R}^{r_ 2}, g_ z(\cdot): Z_ g \rightarrow \mathbb{R}^{r_ 3}$ 为任意定义在 $X_ g, Y_ g, Z_ g \subset \mathbb{R}$ 上的有界函数。那么我们有 $(F\text{-rank}[g])(i) \leq r_ i$  ( $i = 1, 2, 3$ )，其中 $g(\cdot): D_ g = X_ g \times Y_ g \times Z_ g \rightarrow \mathbb{R}$  定义为 $g(v_ 1, v_ 2, v_ 3) = C \times_ 1 g_ x(v_ 1) \times_ 2 g_ y(v_ 2) \times_ 3 g_ z(v_ 3)$ 对于任何 $(v_ 1, v_ 2, v_ 3) \in D_ g$ 。

**定理2**是从离散网格到连续域自然扩展的Tucker分解(**定理1**)。它继承了Tucker分解的优良性质，即可将低秩张量函数 $f(\cdot)$ 分解为核心张量和三个因子函数。

**注解1**：Tucker分解(**定理1**)是**定理2**的特殊情况，当 $D_ f$  （或 $D_ g$ ）是表示网格的某个离散集合时。这可以通过将命题1合并到**定理2**中轻松推导出来。

**注解1**建立了离散张量分解和我们连续张量函数分解之间的联系。基于低秩张量函数分解，我们可以用低秩张量函数表示多维数据，该函数表示为

$$
C;f_x,f_y,f_z(v):=C\times_ 1 f_ x(v^{(1)}) \times_ 2f_ y(v^{(2)})\times_ 3f_ z(v^{(3)})
$$

由核心张量 $C$ 和因子函数 $f_ x(\cdot), f_ y(\cdot), f_ z(\cdot)$ 参数化。该表示隐式地通过低秩函数分解编码张量函数的低秩性，即从张量函数表示中采样的任何张量必须是低秩张量，如**定理2**所述。

在LRTFR(4)中，我们进一步建议使用MLP来参数化因子函数，因为其强大的通用逼近能力[60]。具体来说，我们采用三个MLP $f_ {\theta_ x}(\cdot), f_ {\theta_ y}(\cdot), f_ {\theta_ z}(\cdot)$ ，其参数为 $\theta_ x, \theta_ y, \theta_ z$ 来参数化因子函数 $f_ x(\cdot), f_ y(\cdot), f_ z(\cdot)$ 。以 $f_ {\theta_ x}(\cdot)$ 为例，其公式为

$$
f_{\theta_x}(x) := H_ d(\sigma(H_ {d-1} \cdots \sigma(H_ 1 x))) : X_ f \rightarrow \mathbb{R}^{r_ 1}, (5)
$$

其中 $X_ f \subset \mathbb{R}$ 是第一维的定义域， $\sigma(\cdot)$ 是具有Lipschitz连续性的非线性激活函数， $\theta_ x := \{H_ i\}_ d^1$ 是MLP的可学习权重矩阵。考虑到这些，MLP参数化的LRTFR公式化为

$$
C; f_{\theta_x}, f_{\theta_y}, f_{\theta_z}(v) := C\times_1f_{\theta_ x}(v^{(1)}) \times_ 2f_ {\theta_ y}(v^{(2)}) \times_ 3f_ {\theta_ z}(v^{(3)})  
$$

由核心张量 $C$ 和MLP权重 $\theta_ x, \theta_ y, \theta_ z$ 参数化。

## C. LRTFR的隐式平滑正则化

由于平滑性是多维数据的另一个常见属性，例如视频的时间平滑性和高光谱图像的光谱平滑性[61]，除了低秩性之外，在LRTFR中探索LRTFR的平滑属性是有意义的。接下来，我们从MLP的特定结构中隐式编码的平滑正则化进行理论证明。

**定理3**：设 $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ ，以及 $f_ {\theta_ x}(\cdot): X_ f \rightarrow \mathbb{R}^{r_ 1}, f_ {\theta_ y}(\cdot): Y_ f \rightarrow \mathbb{R}^{r_ 2}, f_ {\theta_ z}(\cdot): Z_ f \rightarrow \mathbb{R}^{r_ 3}$ 是三个结构如(5)所述的MLP，其参数为 $\theta_ x, \theta_ y, \theta_ z$ ，其中 $X_ f, Y_ f, Z_ f \subset \mathbb{R}$ 。假设MLP共享相同的激活函数 $\sigma(\cdot)$ 和深度 $d$ 。此外，我们假设 $r \sigma(\cdot)$ 是Lipschitz连续的，其Lipschitz常数为 $\kappa$ 。 $\ell_ 1$ -范数的 $C$ 受到 $\eta_ 1 > 0$ 的限制。 $\ell_ 1$ -范数的每个权重矩阵 $H_ i$ 在三个MLP中受到 $\eta_ 2 > 0$ 的限制。设 $\eta = \max\{\eta_ 1, \eta_ 2\}$ 。

定义一个张量函数 $f(\cdot): D_ f = X_ f \times Y_ f \times Z_ f \rightarrow \mathbb{R}$ 为 $f(\cdot) := C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}(\cdot)$ 。那么，对于任何 $x_ 1, x_ 2 \in X_ f, y_ 1, y_ 2 \in Y_ f, z_ 1, z_ 2 \in Z_ f$ ，以下不等式成立：

$$
\begin{cases}
|f(x_ 1, y_ 1, z_ 1) − f(x_ 2, y_ 1, z_ 1)| \leq \delta |x_ 1 − x_ 2| \\
|f(x_ 1, y_ 1, z_ 1) − f(x_ 1, y_ 2, z_ 1)| \leq \delta |y_ 1 − y_ 2| \\
|f(x_ 1, y_ 1, z_ 1) − f(x_ 1, y_ 1, z_ 2)| \leq \delta |z_ 1 − z_ 2|,
\end{cases} (6)
$$

其中 $\delta = \eta^3 d^+1 \kappa^{3d-3} \zeta^2$ ， $\zeta = \max\{|x_ 1|, |y_ 1|, |z_ 1|\}$ 。

**定理3**提供了MLP参数化的LRTFR的Lipschitz类型平滑性保证。平滑性是在非线性激活函数和权重矩阵的温和假设下隐式编码的，这些假设在实际实现中很容易实现。例如，大多数广泛使用的激活函数都是Lipschitz连续的，如ReLU、LeakyReLU、Sine和Tanh。此外，我们可以通过控制MLP的结构来方便地控制平滑度的程度，如下所述。

**注解2**：在**定理3**中，我们可以看到平滑度的程度，即 $\delta$ ，与Lipschitz常数 $\kappa$ 和权重矩阵及核心张量的最大值 $\eta$ 有关。因此，在实践中，我们可以控制两个变量来平衡隐式平滑度：

1. 首先，我们在MLP中使用正弦函数 $\sigma(\cdot) = \sin(\omega_ 0 \cdot)$ 作为非线性激活函数。正弦函数是Lipschitz连续的。调整其Lipschitz常数 $\kappa$ 的有效方法是改变 $\omega_ 0$ 的值，即 $\omega_ 0$ 越小，可以获得越小的Lipschitz常数 $\kappa$ ，从而获得更平滑的结果。

2. 其次，为了控制权重矩阵和核心张量的最大值 $\eta$ ，我们可以调整MLP权重的能耗正则化项的权衡参数，即现代深度学习优化器中的权重衰减。这一策略控制了 $\eta$ 的强度。

基于**定理3**，我们可以得出以下推论，得出连续表示 $f(\cdot)$ 中任何采样张量的平滑性。

**推论1**：假设**定理3**中的条件成立。定义 $f(\cdot) := C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}(\cdot)$ 。那么，对于任何由坐标向量 $x \in X_ f^{n_ 1}, y \in Y_ f^{n_ 2}, z \in Z_ f^{n_ 3}$ 采样的 $T \in S[f] \cap \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ （其中 $n_ i s$ 是任何正数），

$$
\begin{cases}
|T(x(i),y(j),z(k)) − T(x(i-1),y(j),z(k))| \leq \delta |x(i) − x(i-1)| \\
|T(x(i),y(j),z(k)) − T(x(i),y(j-1),z(k))| \leq \delta |y(j) − y(j-1)| \\
|T(x(i),y(j),z(k)) − T(x(i),y(j),z(k-1))| \leq \delta |z(k) − z(k-1)|,
\end{cases}
$$

其中 $\delta = \eta^3 d^+1 \kappa^{3d-3} \tilde{\zeta}^2$ ， $\tilde{\zeta} = \max\{\|x\|_ \infty, \|y\|_ \infty, \|z\|_ \infty\}$ 。

**推论1**声称，对于任何采样张量 $T \in S[f]$ ，其相邻元素之间的差异受到相邻坐标之间距离的常数倍的限制。因此，我们的LRTFR隐式地在所有三个维度上统一了低秩性和平滑性，使其对连续数据表示有效且高效。

## D. 与隐式神经表示的比较

接下来，我们讨论我们方法与经典隐式神经表示(INR)[44]的关系和优势。经典INR学习一个由深度神经网络参数化的隐式函数 $f_ \theta(\cdot)$ ，将向量形式的坐标 $(x, y, z) \in \mathbb{R}^3$ 映射到感兴趣的值，即 $f_ \theta(x, y, z)$ 。我们的LRTFR（见图1(a)中的说明）通过将连续表示分解为三个更简单的因子函数，隐式地将一些独立的坐标 $x, y, z$ 映射到相应的值，隐式地将低秩性编码到表示中。INR和我们的方法都在无限实数空间中学习数据的连续表示。然而，我们的方法在LRTFR中额外引入了低秩领域知识，而经典INR[44]忽略了数据的内在结构。通过引入领域知识，我们的方法具有以下内在优势，优于经典INR。

首先，我们的方法在多维数据恢复方面更有效，这归因于我们LRTFR中编码的低秩性。例如，我们直接将INR[44]和我们的LRTFR应用于图像修复任务；见图4。具体来说，我们直接使用具有相同MLP结构的INR和LRTFR来拟合图像的观察条目，并使用学习到的连续表示来预测未观察到的条目。我们已经将原始图像从 $512 \times 512 \times 3$ 裁剪到 $300 \times 300 \times 3$ ，原因是INR的巨大内存成本。为了公平起见，我们测试了不同超参数值的INR和LRTFR（即MLP的深度、MLP的隐藏单元数和 $\omega_ 0$ ，其中 $\omega_ 0$ 是正弦激活函数 $\sin(\omega_ 0 \cdot)$ 的超参数），并为INR和LRTFR选择了最佳结果进行比较。图4中的结果显示，我们的方法在多维数据恢复方面比经典INR更有效，这可以归因于我们LRTFR中编码的低秩性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0825444807ac4be79c52064ce9825f68.png#pic_ center" width="70%" />
</div>

与此同时，通过利用多维数据的低秩性，我们的方法具有更稳定的数值收敛行为。为了更好地证明这一点，我们在图2中通过计算两个连续解之间的相对误差(RE)曲线来比较我们的方法和经典INR，即RE =  $\|T_ {t+1} - T_ t\|_ F^2 / \|T_ t\|_ F^2$ ，其中 $T_ t$ 表示第 $t$ 次迭代时恢复的图像。我们可以观察到，我们的方法具有稳定的数值收敛行为，而INR的RE曲线有波动。这应该合理地归因于我们LRTFR中编码的低秩性，它将解空间限制在低秩流形中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e4a051b075db45a992b1a37009f620be.png#pic_ center" width="70%" />
</div>

此外，通过利用多维数据的低秩性，我们的方法倾向于缓解经典INR的过拟合问题。为了直观地说明这一点，我们在图3中绘制了我们的方法和INR在图像修复问题上的训练损失和测试损失与迭代次数的关系图。这里，训练损失指的是在观察到的条目上损失，即训练损失 =  $\|P_ \Omega(O - T)\|_ F^2$ ，其中 $O$ 表示观察到的图像，  $T$ 表示恢复的图像， $\Omega$ 是观察到的索引集， $P_ \Omega$ 是保持 $\Omega$ 中的元素并将其他元素置为零的投影算子。相应地，测试损失定义为测试损失 =  $\|P_ {\Omega_ C}(G - T)\|_ F^2$ ，其中 $G$ 表示真实图像， $\Omega_ C$ 表示  $\Omega$ 的补集。我们可以观察到，我们的方法和INR的训练损失都在单调递减。然而，INR的测试损失并没有随着迭代次数保持这样的一致递减趋势，而我们的方法仍然保持一致递减的测试损失。这些观察表明，我们的方法倾向于缓解INR的过拟合问题，这归因于我们LRTFR中编码的低秩性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fbc19c3b5b294ee49d728c4b19bd018d.png#pic_ center" width="70%" />
</div>

最后，由于张量函数分解，我们LRTFR的参数可以以比经典INR更低的计算成本进行有效训练。以图像表示为例。给定观察到的图像 $O \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ ，其中 $n_ 1, n_ 2, n_ 3$ 分别表示图像的第一、第二和第三维度的长度，INR[44]需要一个大小为 $n_ 1 n_ 2 n_ 3 \times 3$ 的输入坐标矩阵来训练网络，每次前向传播的计算成本为 $O(m^2 d n_ 1 n_ 2 n_ 3)$ ，其中 $m$ 表示MLP的隐藏单元数， $d$ 表示深度。在我们的LRTFR中，连续表示（张量函数）被分解为三个更简单的因子函数，可以更有效地表示数据。更具体地说，我们使用核心张量 $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 和三个因子矩阵 $U \in \mathbb{R}^{n_ 1 \times r_ 1}, V \in \mathbb{R}^{n_ 2 \times r_ 2}, W \in \mathbb{R}^{n_ 3 \times r_ 3}$ ，其中 $r_ 1, r_ 2, r_ 3$ 是预设的F-秩，以Tucker格式表示图像大小为 $n_ 1 \times n_ 2 \times n_ 3$ 。在我们的LRTFR中，我们用因子函数 $f_ {\theta_ x}(\cdot): \mathbb{R} \rightarrow \mathbb{R}^{r_ 1}$  表示因子矩阵U， $f_ {\theta_ x}(\cdot)$ 以x坐标标量作为输入，输出大小为 $1 \times r_ 1$ 的向量。我们分别将函数 $f_ {\theta_ x}(\cdot)$ 应用于 $n_ 1$ 个x坐标标量，并输出相应的 $n_ 1$ 个大小为 $1 \times r_ 1$ 的向量。这些输出向量形成因子矩阵U $\in \mathbb{R}^{n_ 1 \times r_ 1}$ 。类似地，我们分别对 $n_ 2$ 个y坐标应用 $f_ {\theta_ y}(\cdot)$ 以获得因子矩阵V，对 $n_ 3$ 个z坐标应用 $f_ {\theta_ z}(\cdot)$ 以获得因子矩阵W。因此，我们方法的图像表示的输入大小为 $n_ 1 + n_ 2 + n_ 3$ ，其中 $n_ 1, n_ 2, n_ 3$ 分别是图像的第一、第二和第三维度的长度。我们方法的每次前向传播的计算成本为 $O(\hat{r}^d(n_ 1 + n_ 2 + n_ 3) + \hat{r} n_ 1 n_ 2 n_ 3)$ ，其中 $\hat{r} = \max\{r_ 1, r_ 2, r_ 3\}$ 。这个成本比INR的成本低得多，即 $O(m^2 d n_ 1 n_ 2 n_ 3)$ ，因为实践中 $\hat{r}$ 比 $m^2 \tilde{d}$ 小得多。我们总结了我们的方法和经典INR在图像修复问题上的计算复杂性、浮点运算次数(FLOPs)、参数数量（即模型大小）和平均运行时间；见表II。2由于低秩张量函数分解，即使用三个MLP和一个核心张量来表示图像，我们的计算效率（在计算复杂性、FLOPs和运行时间方面）优于INR，明显高一个数量级，除了模型大小（我们方法的模型大小比INR大，但具有类似的数量级）。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5604e0379d224c23bd6b06052c19cf5c.png#pic_ center" width="70%" />
</div>

接下来，我们讨论我们的方法与神经辐射场(NeRF)[45]的关系和区别。INR的理念是使用神经网络连续表示离散数据。相比之下，NeRF[45]旨在使用神经网络连续表示3D场景。具体来说，NeRF使用一个MLP处理输入坐标 $(x, y, z)$ ，并输出体积密度 $\sigma$ 和一个特征向量。然后，这个特征向量与相机射线的视向量连接，并传递给一个额外的全连接层，输出依赖于视图的RGB颜色。因此，NeRF和我们的方法本质上都属于INR家族，但仍存在实质性差异。具体来说，NeRF的想法是直接使用神经网络连续表示3D场景。然而，我们的想法是整合领域知识（即低秩性）和神经网络的优势，连续表示多维离散数据。通过利用多维数据的低秩性，我们的方法倾向于具有更好的稳定性和多维数据恢复任务的有效性。我们的想法可以扩展到NeRF用于3D场景表示，但这不是这项工作的范围，我们将尝试在未来的工作中扩展我们的方法到NeRF。

## E. LRTFR用于多维数据恢复

由于LRTFR可以连续表示数据并捕获低秩结构，它适用于网格内外的数据处理和分析。在这项工作中，我们将LRTFR部署在多维数据恢复问题上，以检验其有效性。具体来说，我们首先建立了一个使用LRTFR的通用数据恢复模型，然后更详细地介绍了四个数据恢复问题，包括图像修复和图像去噪（在原始分辨率的网格上），超参数优化（在超出原始分辨率的网格上），以及点云上采样（超出网格）。

1)通用数据恢复模型：在这一部分，我们介绍了一个使用LRTFR进行多维数据恢复的通用模型。假设观察到的多维数据定义为函数形式 $h(\cdot): D_ h \rightarrow \mathbb{R}$ ，其中 $D_ h \subset \mathbb{R}^3$ 是观察到的集合。观察到的多维数据通常以离散方式在网格上（例如图像）或甚至不在网格上（例如点云）。在这两种情况下，观察到的函数 $h(\cdot)$ 定义在某个离散集合 $D_ h$上。我们假设真实的低秩张量函数具有有界F-秩 $(r_ 1, r_ 2, r_ 3)$ 。使用MLP参数化的LRTFR，我们可以制定以下多维数据恢复模型：

$$
\min_ {C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}, \theta_ x, \theta_ y, \theta_ z} \sum_ {v \in D_ h} \left(h(v) - C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}(v)\right)^2  
$$

其中 $f_ {\theta_ x}(\cdot): X_ f \rightarrow \mathbb{R}^{r_ 1}, f_ {\theta_ y}(\cdot): Y_ f \rightarrow \mathbb{R}^{r_ 2}, f_ {\theta_ z}(\cdot): Z_ f \rightarrow \mathbb{R}^{r_ 3}$ 是三个因子MLP， $C$ 是核心张量。恢复的低秩张量函数是 $f(\cdot) := C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}(\cdot)$ 。根据**定理2**，真实的低秩连续张量函数必须位于优化空间(7)中，并且恢复的张量函数 $f(\cdot)$ 必须是一个具有 $(F\text{-rank}[f])(i) \leq r_ i$ 的低秩张量函数。在经典Tucker分解中，通常要求因子矩阵是正交的[11]，以限制解空间。与离散设置不同，在连续因子函数上实施正交约束是困难的。多变量函数近似的先驱Tucker分解方法[59]也没有考虑正交约束。因此，在我们的LRTFR中，我们不考虑正交约束。

在模型(7)中，优化变量是核心张量$C$和MLP权重，目标函数是平方误差项，它对所有MLP权重和核心张量都是可微的。因此，我们可以使用易于附加的基于梯度下降的深度学习优化器来解决模型(7)。在这项工作中，我们一致使用高效的自适应矩估计(Adam)算法[62]。接下来，我们更详细地介绍四个应用在多维数据恢复中的应用，这些是通用模型(7)的一些特定示例。

2)多维图像修复：多维图像修复[21], [63]，作为原始网格上的经典问题，旨在从未观察到的图像中恢复底层图像。给定一个观察到的图像 $O \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ ，观察集合 $\Omega \subset \Psi$ ，其中 $\Psi := \{(i, j, k) | i = 1, 2, ..., n_ 1, j = 1, 2, ..., n_ 2, k = 1, 2, ..., n_ 3\}$ ，我们的LRTFR用于多维图像修复的优化模型公式化为：

$$
\min_ {C, \theta_ x, \theta_ y, \theta_ z} \|P_ \Omega(O - T)\|_ F^2,
$$

$$
T(i,j,k) = [C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}](i, j, k), \forall (i, j, k) \in \Psi. (8)
$$

在这里， $f_ {\theta_ x}(\cdot): X_ f \rightarrow \mathbb{R}^{r_ 1}, f_ {\theta_ y}(\cdot): Y_ f \rightarrow \mathbb{R}^{r_ 2}, f_ {\theta_ z}(\cdot): Z_ f \rightarrow \mathbb{R}^{r_ 3}$ 是三个因子MLP， $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 是核心张量， $r_ i s$ ( $i = 1, 2, 3$ )是预设的F-秩。 $P_ \Omega(\cdot)$ 表示保持 $\Omega$ 中的元素并将其他元素置为零的投影算子。恢复的结果是 $P_ \Omega(O) + P_ {\Omega_ C}(T)$ ，其中 $\Omega_ C$ 表示 $\Omega$ 在 $\Psi$ 中的补集。

我们采用Adam优化器来解决图像修复模型(8)，通过优化MLP参数和核心张量。为了更好地说明提出的方法，我们总结了我们的多维图像修复方法的步骤在算法1中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d9a72fd9eaa1498ab2c108700fd09c6a.png#pic_ center" width="70%" />
</div>

3)多光谱图像去噪：多光谱图像(MSI)去噪[10], [13]旨在从未观察到的噪声图像中恢复干净图像。这是另一个在原始网格上的经典问题。在实践中，MSI被混合噪声污染，如高斯噪声、稀疏噪声、条纹噪声和截止线（缺失列）。给定观察到的噪声MSI $O \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ ，LRTFR用于MSI去噪的优化模型

$$
\min_ {C,S,\theta_ x,\theta_ y,\theta_ z} \|O - T - S\|_ F^2 + \gamma_ 1\|S\|_ {\ell_ 1} + \gamma_ 2\|T\|_ {TV},
$$

$$
T(i,j,k) = [C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}](i, j, k), \forall (i, j, k) \in \Psi. (9)
$$

在这个模型中， $f_ {\theta_ x}(\cdot): X_ f \rightarrow \mathbb{R}^{r_ 1}, f_ {\theta_ y}(\cdot): Y_ f \rightarrow \mathbb{R}^{r_ 2}, f_ {\theta_ z}(\cdot): Z_ f \rightarrow \mathbb{R}^{r_ 3}$ 是三个因子MLP， $C \in \mathbb{R}^{r_ 1 \times r_ 2 \times r_ 3}$ 是核心张量， $S \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ 表示稀疏噪声， $T \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ 表示恢复结果， $\gamma_ i s$ ( $i = 1, 2$ )是权衡参数。我们引入了一个简单的空间TV正则化 $\|T\|_ {TV} := \sum_ {k=1}^{n_ 3}(\sum_ {i=1}^{n_ 1-1}\sum_ {j=1}^{n_ 2} |T(i+1,j,k) - T(i,j,k)| + \sum_ {i=1}^{n_ 1}\sum_ {j=1}^{n_ 2-1} |T(i,j+1,k) - T(i,j,k)|)$ ，以更忠实地去除噪声。在这里，**定理3**中的Lipschitz平滑性和TV正则化揭示了不同类型的平滑性：Lipschitz平滑性提供了梯度在任何地方都有界的全局平滑结构，而TV考虑了相邻像素的局部平滑性。全局和局部平滑正则化相互补充，以产生更有希望的去噪结果。

我们使用交替最小化算法来解决去噪模型。具体来说，在第 $t$ 次迭代中，我们解决以下子问题：

$$
\min_ {C,\theta_ x,\theta_ y,\theta_ z} \|O - T - S_ t\|_ F^2 + \gamma_ 2\|T\|_ {TV},
$$

$$
\min_ S \|O_ t - T_ t - S\|_ F^2 + \gamma_ 1\|S\|_ {\ell_ 1},
$$

$$
T(i,j,k) = [C; f_ {\theta_ x}, f_ {\theta_ y}, f_ {\theta_ z}](i, j, k), \forall (i, j, k) \in \Psi. (10)
$$

我们使用Adam算法来解决 $\{C, \theta_ x, \theta_ y, \theta_ z\}$ 子问题。在交替最小化的每次迭代中，我们使用Adam算法的一步来更新 $\{C, \theta_ x, \theta_ y, \theta_ z\}$ 。S子问题可以通过 $S = \text{Soft}_ {\gamma_ 1/2}(O_ t - T_ t)$ 精确解决，其中 $\text{Soft}_ v(\cdot) := \text{sgn}(\cdot) \max\{| \cdot | - v, 0\}$ 表示应用于输入的每个元素的软阈值算子。

4)超参数优化：超参数优化(HPO)[64], [65]，或超参数搜索，是机器学习中的关键步骤。最近的研究[64]巧妙地将HPO建模为经典的低秩张量补全问题。使用我们的LRTFR，在连续域中进行HPO，探索连续表示相对于经典的离散张量补全方法在HPO中的优越性，这是非常有趣的。

具体来说，基于张量补全的HPO[64]将超参数搜索建模为高阶张量的补全问题，其元素表示不同超参数值下算法的性能。在实践中，一些部分观察结果已经给出，即存在一个不完整的张量 $O \in \mathbb{R}^{n_ 1 \times n_ 2 \times n_ 3}$ ，它给出了某些配置下的真实性能。我们的目标是补全张量以预测所有配置中的性能。这个问题可以等价地表述为张量补全问题(8)。有了补全结果 $T$ ，我们选择对应于预测性能最佳（即 $T$ 中的最大值）的配置作为推荐的超参数值。

由于我们的方法预测了一个连续域上的张量函数，探索在原始网格分辨率之外寻找超参数值的好处是有趣的。直观地说，在连续域中寻找超参数值有望获得更好的结果，因为最优配置可能不位于固定的网格中。为了说明这一点，我们使用学习到的张量函数进行了×2和×4超分辨率采样，均匀间隔采样，这提供了更多的候选配置，并给出了相应的预测。结果表明，这种策略在大多数情况下比网格方法更能推荐更好的配置；见第IV-C节。

5)点云上采样：我们进一步将我们的LRTFR应用于点云表示，以测试我们方法在网格之外的有效性。点云表示是一个具有挑战性的任务，因为点云的无序和无组织性质。由于这些无序点云定义在网格之外，所以很难使用基于网格的经典低秩表示进行点云表示。相比之下，我们的LRTFR可以使用连续表示来表示点云，这证明了它与经典低秩表示相比的多功能性。

具体来说，我们考虑点云上采样任务[66], [67]，它指的是将稀疏点云上采样成密集点云，这将有利于后续应用[68]。假设我们有一个稀疏点云 $P \in \mathbb{R}^{p \times 3}$ ，其中 $p$ 表示点的数量。我们用 $\Omega = \{P(m,:)\}_ {m=1}^p$ 来表示观察集合。我们借鉴了有符号距离函数(SDF)[42]来从未观察到的点云中学习连续表示。学习SDF的损失函数公式化为：

$$
\min_ {C,\theta_ x,\theta_ y,\theta_ z} \sum_ {v \in \Omega} |s(v)| + \gamma_ 1 \int_ {\mathbb{R}^3} \|\nabla s(v)\|_ F^2 - 1 \, dv + \gamma_ 2 \int_ {\mathbb{R}^3 \backslash \Omega} e^{-|s(v)|} \, dv,
$$

$$
s(v) := C \times_ 1 f_ {\theta_ x}(v^{(1)}) \times_ 2 f_ {\theta_ y}(v^{(2)}) \times_ 3 f_ {\theta_ z}(v^{(3)}), (11)
$$

其中 $s(\cdot): \mathbb{R}^3 \rightarrow \mathbb{R}$ 表示由LRTFR表示的SDF， $\gamma_ i s$ ( $i = 1, 2$ )是权衡参数。第一项强制SDF在观察点处为零。第二项强制SDF梯度在任何地方都为一。第三项限制SDF值在观察集合外远离零[44]。在实践中，我们通过在空间中随机采样大量点来近似积分。损失函数可以通过使用Adam算法来最小化。 $s(v) = 0$ 的解形成了一个表面，表示点云的底层形状。我们通过均匀间隔采样在空间中采样密集点 $v$ ，使得 $|s(v)| < \tau$ ，其中 $\tau$ 是预定义的阈值。这些点表示所需的上采样结果。

# IV. 实验

在实验中，我们对所有介绍的任务进行了比较实验和分析。我们首先介绍一些重要的实验设置。然后，我们为不同任务介绍基线、数据集和结果。我们的方法在Pytorch 1.7.0上实现，使用i5-9400f CPU和RTX 3060 GPU（12 GB GPU内存）。

评估指标：对于修复和去噪，我们使用峰值信噪比(PSNR)、结构相似性(SSIM)和归一化均方根误差(NRMSE)进行评估。对于HPO，我们报告不同方法的平均分类准确率(ACA)和平均推荐准确率(ARA)[64], [69]。对于点云上采样，我们采用广泛使用的Chamfer距离(CD)[70]和F-Score[71]作为评估指标。

超参数设置：对于所有任务，我们在以下集合中搜索F-秩 $(r_ 1, r_ 2, r_ 3)$ 的值

$$
\{(\lfloor n_ 1/s \rfloor, \lfloor n_ 2/s \rfloor, \lfloor n_ 3/s_ 3 \rfloor) | s, s_ 3 = 1, 2, 4, 8, 16, 32\}, (12)
$$

其中 $n_ i s$ ( $i = 1, 2, 3$ )表示观察数据的大小。同时，我们在MLP中采用正弦激活函数 $\sigma(\cdot) = \sin(\omega_ 0 \cdot)$ 来学习LRTFR，其中 $\omega_ 0$ 是一个超参数。文献[44], [48], [72]彻底证明了正弦函数比其他激活函数（例如ReLU和RBF）具有更好的连续表示能力。我们在 $\{1, 2, 4, 8, 16, 32\}$ 中搜索超参数 $\omega_ 0$ 。上述超参数搜索的目标是为不同的样本获得最佳的PSNR（对于修复和去噪）、ACA（对于HPO）和CD（对于点云上采样）值。Adam的权重衰减设置为1、0.1、0.5和0.5，分别用于修复、去噪、HPO和点云上采样。在去噪模型(9)中，我们将 $\gamma_ 2$ 设置为 $10^{-4}$ 用于MSIs和 $10^{-5}$ 用于高光谱图像。 $\gamma_ 1$ 设置为0.1（带稀疏噪声）或10（不带稀疏噪声）。在点云上采样模型(11)中，我们将 $\gamma_ 1$ 设置为 $10^{-5}$ ， $\gamma_ 2$ 设置为 $10^{-2}$ ，对于所有样本。阈值  $\tau$ 调整，使得恢复的密集点云至少有 $10^4$ 个点。MLP的隐藏单元数设置为图像修复、去噪和超参数优化任务中观察到的多维数据的维度的最大值，其中 $n_ 1, n_ 2, n_ 3$ 表示观察到的多维数据的维度。点云上采样中隐藏单元的数量设置为观察到的点的数量。MLP的深度设置为图像修复、去噪和超参数优化的3，以及点云上采样的4。对于所有比较方法，我们通过遵循作者提供的推荐配置来选择它们的超参数值，以使它们可能具有最佳性能。

## A. 多维图像修复结果

多维图像修复是原始网格上典型的数据恢复问题。为了验证LRTFR在原始网格上的有效性，我们将其与最先进的基于低秩张量的修复方法TRLRF[73]、FTNN[63]、FCTN[21]和HLRTF[31]以及最近的基于INR的方法INRR[74]进行了比较。测试数据包括彩色图像、CAVE数据集[77]中的多光谱图像(MSIs)以及视频。我们考虑了随机缺失的采样率(SRs)0.1、0.15、0.2、0.25和0.3。

多维图像修复的定量和定性结果分别显示在表III和图5-6中。可以看出，我们的LRTFR在定量和定性结果上都取得了最佳结果，这揭示了我们LRTFR相对于经典低秩张量表示的优越性。我们方法的有希望的结果可以归因于我们LRTFR同时编码了低秩性和平滑性。同时，我们可以观察到，LRTFR恢复的图像比其他比较方法更干净、更平滑，这主要是因为我们的LRTFR隐式地将Lipschitz平滑性编码到连续表示中，使恢复的图像具有更好的视觉质量。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9e9547fed37543918ca6b500df7e0747.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b01e4c5ef3ff4a6cb10954994ed24e72.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/06dd23193ad54d87ad4618801cf4fad2.png#pic_ center" width="70%" />
</div>

## B. 多光谱图像去噪结果

MSI去噪是另一个在原始网格上的具有挑战性的数据恢复问题。我们将我们的LRTFR与基于低秩矩阵/张量的去噪方法LRTDTV[13]、E3DTV[37]和HLRTF[31]进行了比较。同时，我们还包括了两种基于监督的深度学习方法HSID-CNN[75]和SDeCNN[76]进行比较。我们使用了HSID-CNN和SDeCNN的最佳预训练模型进行测试。测试数据包括四个MSIs（Balloons、Fruits[77]、Pool和Doll[31]）和两个高光谱图像(HSIs)。我们考虑了不同的噪声情况。Case 1包含标准差为0.2的高斯噪声。Case 2包含标准差为0.1的高斯噪声和SR为0.1的稀疏噪声。Case 3在所有光谱波段中包含与Case 2相同的噪声加上截止线[78]。Case 4包含与Case 2相同的噪声加上40%的光谱波段中的条纹噪声[78]。Case 5包含与Case 3相同的噪声，加上40%的光谱波段中的条纹噪声。

MSI去噪的结果显示在表IV和图7-8中。从表IV中，我们可以看到LRTFR是在不同噪声情况和不同数据上测试的算法中最稳定的。特别是，LRTFR优于精心设计的基于TV的方法LRTDTV和E3DTV，这证明了我们结合的全局-局部平滑正则化的优越性。在图7-8中，我们可以观察到我们的方法可以很好地去除复杂噪声。相比之下，其他去噪方法有时不能完全去除混合噪声。此外，从图7中，我们可以看到其他基于模型的方法(LRTDTV和E3DTV)可能会产生过度平滑。相比之下，我们的方法更好地保留了图像细节。深度学习方法HSID-CNN和SDeCNN在高斯噪声(Case 1)下表现相对较好，但在处理混合噪声时会受到训练和测试样本之间域差异的影响。相比之下，我们的方法是一种基于模型的方法，隐式编码了低秩性和平滑性，为不同类型的噪声提供了更稳定的性能。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ae48498cbefb4e3ebe7e642cb9d11992.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/47ebbfc8a5c54cbcbb2db17a081429a2.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1f5083daa6e54f94b2b5f9a54e0c7b0e.png#pic_ center" width="70%" />
</div>

## C. 超参数优化结果

HPO可以优雅地建模为低秩张量补全问题[64]。如前所述，使用我们的LRTFR在连续域中进行HPO，探索连续表示相对于经典离散张量补全方法在HPO中的优越性，这是非常有趣的。

按照先前SOTA工作[64]的实验设置，我们考虑使用高斯核支持向量机(SVM)进行分类问题，它有两个超参数——正则化参数C和核参数σ。搜索集分别是 $\{3^i | i = -15 : 1 : 15\}$ 用于C和 $\{2^i | i = -15 : 1 : 15\}$ 用于σ，因此共有 $31 \times 31 = 961$ 个候选配置。我们使用高斯分布生成分类数据集。高斯分布的方差在 $\{0.05, 0.1, 0.15, ..., 0.4\}$ 中遍历，构建了八个具有不同难度级别的分类任务。每个任务包含16个数据集。这些数据集是通过具有相同方差的高斯分布生成的，并将生成的点分为两组以形成二元分类数据集。每个数据集包含100个训练点和500个测试点。因此，通过对所有配置和数据集进行网格搜索(GS)，我们为每个任务（方差）形成了一个大小为 $31 \times 31 \times 16$ 的张量。前两个维度( $31 \times 31$ )表示候选配置的数量，第三个维度(16)表示数据集的数量。按照[64]，我们使用0.01的低SR采样与新数据集对应的张量切片，并使用0.1的SR采样与历史数据集对应的张量切片（详见[64]），形成不完整的张量。我们重复采样过程16次，每个数据集轮流作为新数据集一次，其他作为历史数据集。这导致16个不完整的张量，我们报告在新数据集上的平均HPO结果。

我们报告了我们方法的三个结果，包括使用LRTFR的标准张量补全，以及使用学习到的连续表示和均匀间隔采样的超分辨率结果，称为LRTFR (×2)/(×4)。超分辨率结果提供了更多的候选配置，并给出了预测的准确性，即使我们的方法只在原始网格上观察到的数据。我们将我们的方法与随机搜索(RS)[65]进行了比较，它对应于使用观察到的张量进行推荐的结果。同时，我们使用了最先进的张量补全方法DCTNN[29]、TRLRF[73]、FTNN[63]、FCTN[21]和HLRTF[31]作为基线。

HPO的结果显示在表V中。与经典张量补全方法相比，我们的LRTFR在推荐合适的配置方面更有效。此外，LRTFR (×2)/(×4)倾向于比LRTFR获得更好的性能，这表明在原始网格分辨率之外搜索超参数值有助于获得更好的推荐结果。同时，我们观察到LRTFR (×2)和(×4)获得了类似的平均性能。LRTFR (×2)和(×4)的类似性能可以通过它们使用相同的给定信息，并且可能已经达到了上采样方法的性能极限来合理解释。然而，考虑到LRTFR (×2)/(×4)与LRTFR相比一贯的性能提升，我们仍然有理由说，在原始网格分辨率之外搜索超参数值有助于获得更好的推荐结果，这揭示了连续表示的优越性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9222680163df4099910a496eb3ea604f.png#pic_ center" width="70%" />
</div>

## D. 点云上采样结果

然后，我们考虑点云上采样问题，以展示我们方法在网格之外的有效性。标准基于低秩张量的方法不能应用于点云上采样，因为它们不适合表示无序的点云超出网格。相比之下，我们的LRTFR适合表示点云，因为它学习了数据的连续表示。

我们考虑了五个基于深度学习的方法作为基线，包括MSN[79]、SnowflakeNet[80]、SMOG[81]、NeuralPoints[82]和SAPCU[83]。这里，SAPCU[83]是基于INR的方法。我们使用了作者提供的最佳预训练模型。我们采用了不同的数据集，包括ShapeNet基准[84]中的Table、Sofa、Vessel和Lamp，Stanford Bunny，以及三个手工制作的形状(Doughnut、Sphere和Heart)。我们使用随机采样将原始点云下采样到观察点云，点数少于500。

点云上采样的结果显示在表VI和图9-10中。这里，相对改进是通过 $|m_ 1 - m_ 0|/m_ 0$ 计算的，其中 $m_ 0$ 和 $m_ 1$ 分别表示比较方法和我们方法的度量值（即DC/F-Score值）。从表VI中，我们可以观察到，我们的方法在平均度量值和平均度量值的相对改进方面都获得了显著的优势。SnowflakeNet在Table和Sofa上表现良好，这些被包含在ShapeNet数据集[84]中。这是因为SnowflakeNet是在同一ShapeNet数据集上训练的。我们的方法在Table和Sofa上，这些被包含在ShapeNet数据集中，具有可比的性能，并且在其他数据集上比其他方法表现更好，特别是在野外数据集上，例如Bunny、Doughnut、Sphere和Heart。这种现象可以合理地归因于我们的无监督方法，它不需要任何训练数据集，将低秩性编码到连续表示中，因此在不同数据集上获得了良好的泛化能力。从图9-10中，我们可以观察到，我们的LRTFR通常可以获得比其他方法更好的恢复点云，这验证了LRTFR用于连续表示的有效性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/bd3113f842d94532b3ca1532bc571869.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/7ba8fcaf18cc4ba3af287da0cc4229bf.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/071042d173eb4c3dad734be99df1a3fd.png#pic_ center" width="70%" />
</div>

# V. 讨论

## A. 张量分解的影响

张量Tucker分解是我们LRTFR的核心构建块。然而，我们的方法可以方便地扩展到不同的张量函数分解。这里，我们比较了我们的多维图像修复中的CP分解[11]和Tucker分解；见图11(d)-(e)。可以看出，Tucker函数分解显示出某些优越的性能。这可能归因于前者Tucker表示方式更好的张量结构保持能力。我们将在未来的研究中更深入地探讨这个问题。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b23532c582664774a3f1d6240df35d1a.png#pic_ center" width="70%" />
</div>

## B. 激活函数的影响

由于我们使用MLP来参数化LRTFR的因子函数，MLP中激活函数的选择值得讨论。受到最近INR研究[44]的启发，该研究表明周期性正弦激活函数可以捕获自然信号的复杂结构和细节，我们采用正弦激活函数在MLP中学习LRTFR，以帮助获得更现实的连续表示。为了验证正弦激活函数的有效性，我们将其与其他激活函数ReLU、LeakyReLU和Tanh进行了比较；见图11(a)-(c), (e)。结果表明，正弦激活函数可以帮助比其他激活函数更好地恢复信号，这与现有文献[44], [72]的结果一致。

## C. 超参数的影响

选择适当的超参数值是我们方法的必要步骤。这些超参数包括Adam优化器的权重衰减（记为w）、正弦激活的超参数ω0、F-秩(r1, r2, r3)和MLP的深度（记为d)。同时，在去噪模型(9)和点云上采样模型(11)中还有两个正则化参数γ1, γ2。为了全面分析不同超参数对我们方法性能的影响，我们改变每个超参数的值，固定其他值，并报告相应的结果。结果如图12和13所示。我们注意到，不同的任务需要不同的超参数值，因此一个超参数的测试范围对于不同的任务是不一致的。从结果中我们可以看到，我们的方法对这些超参数相对稳健，因为它可以在广泛的值范围内获得令人满意的性能。这使得我们的方法在现实场景中相对容易应用。此外，从图13中我们可以看到，我们采用的正则化（例如，(9)中的TV正则化）在适当的超参数值下是有效的，可以提高我们方法的性能，这揭示了我们方法与其他成熟技术提高性能的兼容性。无论如何，如何构建易于使用和自动化的超参数调整策略，使我们的方法更加灵活和适应不同的场景，仍然是我们未来研究需要更多努力的课题。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d51a792717404496a0f2282f6baf2fd6.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1683ed17c918465884537d10ee82d9a1.png#pic_ center" width="70%" />
</div>

# VI. 结论

在本文中，我们提出了用于多维数据连续表示的LRTFR。我们将连续表示制定为低秩张量函数，并发展了包括F-秩和张量函数分解在内的基本概念。我们从理论上证明了LRTFR中隐藏的低秩和平滑正则化，使其对连续表示有效且高效。在包括多维图像修复、去噪、HPO和点云上采样在内的多维数据恢复问题上的广泛实验验证了我们方法与现有最先进方法相比的广泛适用性和优越性。建议的连续表示是多维数据处理和分析的潜在工具，可以在未来应用于更多任务，例如高光谱融合和盲图像超分辨率。

声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
