# 题目：[Interpretable Rotation-Equivariant Quaternion Neural Networks for 3D Point Cloud Processing](https://ieeexplore.ieee.org/document/10384563)  
## 用于三维点云处理的可解释旋转等变四元数神经网络
**作者：Wen Shen; Zhihua Wei; Qihan Ren; Binbin Zhang; Shikun Huang; Jiaqi Fan; Quanshi Zhang** 

****

# 摘要

本研究提出了一套通用规则，用于修改现有的3D点云处理神经网络，使其成为旋转等变的四元数神经网络（REQNNs），以实现神经网络的特征表示具有旋转等变性和排列不变性。特征的旋转等变性意味着，在旋转输入点云上计算的特征与在原始输入点云上计算的特征应用相同的旋转变换是相同的。我们发现，如果神经网络使用四元数特征，那么特征的旋转等变性自然得到满足。有趣的是，我们证明了这样的网络修改还使得REQNN中特征的梯度相对于输入具有旋转等变性，并且REQNN的训练相对于输入具有旋转不变性。此外，排列不变性检查了当我们重新排列输入点时，中间层特征是否不变。我们还评估了REQNN的知识表示稳定性以及REQNN对对抗性旋转攻击的鲁棒性。实验表明，REQNN在分类准确性和旋转测试样本的鲁棒性方面优于传统神经网络。

# 关键词

- 旋转等变性
- 排列不变性
- 3D点云处理
- 四元数

# I. 引言

3D点云处理近年来受到了越来越多的关注，这归功于它在机器人技术和自动驾驶等应用中的重要作用。图像处理中的神经网络通常从丰富的颜色信息中提取特征。相比之下，3D形状分类的神经网络应该从3D结构中提取特征，而不使用输入点的方向或顺序信息。换句话说，人们期望3D形状分类的神经网络满足旋转不变性和排列不变性的特性。

在旋转不变性和排列不变性之外，本研究同时追求四个属性：1. 特征的旋转等变性，2. 特征梯度的旋转等变性，3. 网络训练的旋转不变性，以及4. 特征的排列不变性。

**特征的旋转等变性**是旋转不变性的一个扩展属性，它将旋转信息与旋转不变的结构信息分离。给定一个3D点云，旋转等变性属性指的是神经网络中间层计算特征的两种方式的等变性[9]。第一种方式是先将点云按特定角度旋转，然后在旋转后的样本上计算特征。第二种方式是在点云上计算特征，然后对特征应用相同的旋转变换。如果上述两种方式产生相同的特征，即encoder(rotate(sample))=rotate(encoder(sample)) ，那么我们认为这个神经网络满足特征的旋转等变性，如图1所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fb40775094d1410386d8cfbe7ff0df30.png#pic_ center" width="70%" />
</div>

注意，特征的旋转不变性和旋转等变性都防止旋转影响推理。一些神经网络通过简单地从特征中移除旋转信息来实现特征的旋转不变性[5]，[8]。相比之下，我们希望神经网络追求特征的旋转等变性，这通过分离旋转特征和旋转独立结构特征来保持中间层的旋转特征。特征的旋转等变性在许多应用中具有重要价值。例如，我们可以使用解耦的结构信息进行推理，并使用旋转信息控制对象合成中的对象方向。

**特征梯度的旋转等变性**类似于特征的旋转等变性，我们期望神经网络的特征梯度也是旋转等变的。这意味着无论我们如何旋转输入的3D点云，所有维度在特征梯度中始终与特征的维度一致。更重要的是，特征梯度的旋转等变性进一步导致网络训练的旋转不变性，这保证了训练神经网络的稳定性。

**网络训练的旋转不变性**指的是神经网络可以学习相同的参数，无论我们如何旋转训练样本。

**特征的排列不变性**指的是给定点云x，神经网络中间层的特征g(x)不会改变，无论人们如何重新排序x中的3D点，即 $g(\text{x}_ {\text{reorder}}) = g(\text{x})$ 。这里，g表示从输入x到中间层的四元数特征的信号处理函数。

本研究提出了一套通用规则来修改任何任意的3D点云处理神经网络，并使修改后的神经网络满足上述四个属性。具体来说，我们修改神经网络使用四元数特征，而不是使用实数值特征。因此，修改后的神经网络被称为旋转等变四元数神经网络（REQNN）。

关键是，这样的规则可以广泛用于修改大多数现有的3D点云处理神经网络。我们在四个经典的3D形状分类和重建神经网络上验证了所提出规则的有效性和广泛的适用性，包括PointNet[29]，PointNet++ [30]，DGCNN [45]和PointConv [49]。

具体来说，为了满足上述四个属性，REQNN使用四元数特征。REQNN的输入3D点云和中间层特征都使用四元数表示。四元数 $q = q_ 0 + q_ 1i + q_ 2j + q_ 3k \in \mathbb{H}$ 是一个超复数，有三个虚部 (i, j, 和 k) [17]。点云中的每个3D点都被表示为纯四元数 $x = 0 + x_ 1i + x_ 2j + x_ 3k$ 。四元数特征可以是向量/矩阵/张量，每个元素都是纯四元数。对于每个四元数特征，我们可以围绕四元数轴 $o = 0 + o_ 1i + o_ 2j + o_ 3k \in \mathbb{H}$ ( $o_ 1, o_ 2, o_ 3 \in \mathbb{R}$ ) 旋转一个角度 $\theta \in [0, 2\pi)$ 。这种旋转操作是以元素为单位进行的。对于特征向量/矩阵/张量中的每个四元数元素 $f = 0 + f_ 1i + f_ 2j + f_ 3k \in \mathbb{H}$ ，旋转后的四元数由 $R \cdot f \cdot R^\*$ 给出，其中 $R = e^{o \theta / 2}$ 和 $R^\* = e^{-o \theta / 2}$ 是两个四元数。 $R^\*$ 是 $R$ 的共轭。

通过这种方式，f可以表示中间层的特征或相对于特征的梯度。然后，特征或梯度的旋转等变性定义为两种特征/梯度的等变性， $f_ 1 = f_ 2$ ，计算在两种情况下，即，(情况1) 旋转输入并生成四元数特征/梯度 $f_ 1 = g(R \cdot x \cdot R)$ ，和 (情况2) 直接将相同的旋转应用到四元数特征/梯度 $f_ 2 = R \cdot g(x) \cdot R^\*$ 。

修改神经网络的一般规则：为了确保神经网络满足1。特征的旋转等方差，2。特征梯度的旋转等方差和3。网络训练的旋转不变性，我们修正了现有神经网络中的几个分层运算。具体来说，我们修改了卷积、ReLU、批处理规范化、最大池化和三维坐标加权的运算[49]。我们已经证明，对这些分层运算的修正保证了整个神经网络的上述三个性质。此外，我们还介绍了如何将旋转等变特征转化为旋转无关任务的旋转不变特征。

此外，我们发现，当我们观察最远点采样[30]和基于球查询搜索的分组[30]操作时，我们可以将神经网络更改为排列不变。

优点：1）当我们以不同的角度旋转输入点云时，REQNN比传统的神经网络学习到更稳定的特征表示。为此，我们测量了不同旋转下输入属性的稳定性，并证明了由REQNN编码的输入属性比传统神经网络编码的输入归因更稳定。
2）与传统的神经网络相比，REQNN对基于旋转的攻击更具鲁棒性。
3）在没有任何旋转增强的情况下，REQNN在旋转样本上自然表现出很高的泛化能力，这增强了网络训练的稳定性。
4）本研究中提出的规则可用于将大多数现有的神经网络修改为REQNN。相比之下，先前的研究开发了特定的网络架构[59]或设计了特定的操作[41]来实现特征的旋转等变，这损害了广泛的适用性。

本研究的贡献可总结如下：

- 在本研究中，我们学习了具有以下四个性质的用于3D点云处理的神经网络，即特征的旋转等变性、特征梯度的旋转等变性、网络训练的旋转不变性和特征的排列不变性。
- 我们提出了一组通用规则，用于将各种神经网络修改为REQNN。我们证明了这些规则保证了上述四个性质。
- 三维形状分类和重建实验表明，与传统神经网络相比，REQNN在旋转样本上表现出更高的泛化能力，编- -码更稳定的特征表示，对基于旋转的攻击更具鲁棒性，并且具有更稳定的训练过程。

这篇论文的最初版本出现在[35]中。

# III. 方法论

## A. 四元数特征和旋转

四元数基础：四元数数 $q = q_ 0 + q_ 1i + q_ 2j + q_ 3k \in H$ 是一个超复数，包括一个实部 $(q_ 0)$ 和三个虚部 $(q_ 1i, q_ 2j, q_ 3k)$ ，其中 $q_ 0, q_ 1, q_ 2, q_ 3$ 是实数，而 $i, j, k$ 是虚数单位；$H$ 表示四元数代数。虚数单位的乘积定义为：

$$ 
i^2 = j^2 = k^2 = ijk = -1, \quad ij = k, \quad jk = i, \quad ki = j, \quad ji = -k, \quad kj = -i, \quad ik = -j
$$

注意，两个四元数的乘法不是交换的，即 $ij \neq ji$ ， $jk \neq kj$ 和 $ki \neq ik$ 。四元数 $q$ 的共轭定义为 $q^\* = q_ 0 - q_ 1i - q_ 2j - q_ 3k$ 。当四元数的实部为0时，即 $q_ 0 = 0$ ，它是一个纯四元数。当四元数的范数等于1时，即 $\|q\| = \sqrt{q_ 0^2 + q_ 1^2 + q_ 2^2 + q_ 3^2} = 1$ ，它是一个单位四元数。

单位四元数的极分解形式为：

$$
q = e^{o\theta/2} = \cos(\theta/2) + \sin(\theta/2)(o_ 1i + o_ 2j + o_ 3k)
$$

其中 $o = o_ 1i + o_ 2j + o_ 3k$ ； $o_ 1^2 + o_ 2^2 + o_ 3^2 = 1$ 。注意，两个四元数的乘法不是交换的，因此 $e^{o\theta/2}pe^{-o\theta/2} \neq p$ 。

四元数旋转：一个纯四元数 $q = 0 + q_ 1i + q_ 2j + q_ 3k$ ( $q_ 1, q_ 2, q_ 3 \in R$ ) 可以被认为具有一个方向 $[q_ 1, q_ 2, q_ 3]^T$ 。这样，我们可以围绕轴 $[o_ 1, o_ 2, o_ 3]^T$  ( $o_ 1, o_ 2, o_ 3 \in R, o = 0 + o_ 1i + o_ 2j + o_ 3k, \|o\| = 1$ ) 旋转一个角 $\theta \in [0, 2\pi)$ 来旋转一个纯四元数，使用四元数 $R = \cos(\theta/2) + \sin(\theta/2)(o_ 1i + o_ 2j + o_ 3k)$ 及其共轭 $R^\* = \cos(\theta/2) - \sin(\theta/2)(o_ 1i + o_ 2j + o_ 3k)$ 。

$$
q' = R \circ q \circ R^\*
$$

使用四元数表示旋转的优势在于四元数不会遭受奇异性问题，但欧拉角 [46] 和罗德里格斯参数 [37] 会。此外，尽管四元数的冗余比是二 (即 $R \circ q \circ R = (-R) \circ q \circ (-R)$  )，但冗余并不影响特征的旋转等变性。

具有四元数特征的神经网络：为了使神经网络满足前述的四个属性，我们修改了传统神经网络以使用四元数特征，即特征向量/矩阵/张量中的每个元素都是一个四元数。

特别是，输入也以四元数特征表示。然而，这类神经网络的参数仍然是由实数值标量组成的向量/矩阵/张量。相比之下，在传统神经网络中，输入/特征/参数的每个元素都是一个实数值标量。

具体来说，给定一个输入点云，我们使用一个纯四元数来表示每个 u-th 点 $[x_ u, y_ u, z_ u]^T \in R^3$ 为 $x_ u = 0 + x_ ui + y_ uj + z_ uk \in H$ 。类似地，给定一个中间层特征，每个 v-th 元素表示为 $f_ v = 0 + a_ vi + b_ vj + c_ vk \in H$ ， $a_ v, b_ v, c_ v \in R$ 。

对于一个四元数特征向量 $f = [f_ 1, f_ 2, ..., f_ d]^T \in H^d$ 包含 d 个四元数元素，我们可以将 f 视为三个实值特征向量 $a = [a_ 1, a_ 2, ..., a_ d]^T$ ， $b = [b_ 1, b_ 2, ..., b_ d]^T$ 和 $c = [c_ 1, c_ 2, ..., c_ d]^T$ ，其中 f 中的每个四元数元素，即 $f_ v$ ，可以写成 $f_ v = 0 + a_ vi + b_ vj + c_ vk$ ， $1 \leq v \leq d$ 。这样，对四元数特征 f 应用一个实值卷积滤波器 w 等同于对三个实值特征 a、b 和 c 应用 w，即 $w \otimes f = w \otimes (0 + a_ i + b_ j + c_ k) = 0 + (w \otimes a)i + (w \otimes b)j + (w \otimes c)k$ 。

由于输入的四元数元素和特征的四元数元素都有方向，因此我们可以通过将相同的旋转应用于此特征中的每个四元数元素来旋转四元数特征向量/矩阵/张量 f，如下所示：

$$
f' = R \odot f \odot R
$$

其中 $\odot$ 表示逐元素乘法。

## B. 旋转等变性

在本节中，我们定义了以下三个属性：1. 特征的旋转等变性；2. 相对于特征的梯度的旋转等变性；3. 网络训练的旋转不变性。然后，在第三节C中，我们修订了传统神经网络的逐层操作，以确保修订后的逐层操作满足上述三个属性。

特征的旋转等变性：设  $x \in H^n$  和  $y = \Phi(x) \in HC$  分别表示输入点云和神经网络的输出向量。当我们使用旋转四元数  $R = e^{o\theta/2}$  旋转点云  $x$  时， $\Phi(R \cdot x \cdot R) \in HC$  表示给定旋转点云的神经网络的输出向量。然后，网络输出的旋转等变性公式化为如下：

$$
\Phi(x(\theta)) = R \cdot \Phi(x) \cdot R, \quad \text{s.t. } x(\theta) \stackrel{\text{def}}{=} R \cdot x \cdot R
$$

公式(3)意味着，当我们围绕轴  $o$  旋转网络输出角度  $\theta$ （即  $R \cdot \Phi(x) \cdot R$ ），它等同于首先旋转输入点云然后计算网络输出（即  $\Phi(x(\theta))$ ）。

除了网络输出的旋转等变性之外，本研究还专注于一个更雄心勃勃的目标，即使神经网络的每个中间层特征都具有旋转等变性。设  $\Phi(x)$  表示具有  $L$  层的神经网络的级联函数，其中  $\Phi_ l(\cdot)$  表示第  $l$  层的函数。设  $f_ l = \Phi_ l(f_ {l-1}) \in H^d$  表示第  $l$  层的输出。实际上，只要我们确保每层操作（ $\Phi_ 1(\cdot), \ldots, \Phi_ l(\cdot)$ ）的旋转等变性属性（如公式(3)所示），我们就可以递归地确保四元数特征  $f_ l$  的旋转等变性，如下所示（见脚注2）。

$$
\Phi_ l(R \cdot f_ {l-1} \cdot R) = R \cdot \Phi_ l(f_ {l-1}) \cdot R
$$

*使用旋转等变来分离旋转信息，还是使用旋转不变性来忽略旋转信息？实际上，某些应用需要旋转等变网络输出，例如重建任务，而某些应用需要旋转不变网络输出（即  $\Phi(R \cdot x \cdot R) = \Phi(x)$ ），例如分类任务。然而，对于那些需要旋转不变网络输出的应用，我们仍然可以在神经网络的大多数中间层（除了在神经网络的最后几层使用旋转不变特征）中使用旋转等变特征，从而保证中间层特征的旋转鲁棒性。许多先前的研究发现，经典的3D点云处理神经网络对输入点云的旋转不鲁棒[34]，[60]。在中间层特征中分离旋转等变表示（即将点云的方位信息分离到四元数特征的方位中）保证了神经网络的旋转鲁棒性。\*

相对于特征的梯度的旋转等变性意味着在神经网络的反向传播过程中，由每层操作  $\Phi_ l(\cdot)$  计算的特征梯度相对于输入点云的方向是旋转等变的。换句话说，当我们围绕轴  $o$  旋转输入点云角度  $\theta$ （即  $x(\theta) \stackrel{\text{def}}{=} R \cdot x \cdot R$ ），使得  $R = e^{o\theta/2}$ ，特征梯度将以相同的角度旋转，如下所示。

$$
R \cdot \frac{\partial Loss(x)}{\partial f_ l} \cdot R = \frac{\partial Loss(x(\theta))}{\partial f_ l}
$$

这里，损失函数  $Loss(x)$  需要是旋转不变的，即不管我们如何旋转输入点云，损失函数都不会改变。许多任务的损失函数都是旋转不变的。例如，形状分类任务和形状重建任务都具有旋转不变的损失。不管我们如何旋转输入对象，损失不应受到影响，因为重建目标也旋转了。

实际上，上述特征梯度的旋转等变性可以通过每层操作的特征梯度的旋转等变性在数学上保证。具体来说，对于所有层， $\forall l$ ，给定前向函数  $f_ l = \Phi_ l(f_ {l-1})$ ，逐层旋转等变性的特征梯度如下所示。

$$
R \cdot \frac{\partial f_ l(x)^T}{\partial f_ {l-1}} \cdot \nabla_ {f_ l} Loss(x) \cdot R = \frac{\partial f_ l(x(\theta))^T}{\partial f_ {l-1}} \cdot R \cdot \nabla_ {f_ l} Loss(x) \cdot R
$$

其中  $\nabla_ {f_ l} Loss(x) \stackrel{\text{def}}{=} \frac{\partial Loss(x)}{\partial f_ l}$  表示相对于  $f_ l$  的梯度； $\frac{\partial f_ l(x)^T}{\partial f_ {l-1}}$  表示给定输入样本  $x$  时，特征  $f_ l$  相对于特征  $f_ {l-1}$  的梯度； $\frac{\partial f_ l(x(\theta))^T}{\partial f_ {l-1}}$  表示给定旋转样本  $x(\theta)$  时，特征  $f_ l$  相对于特征  $f_ {l-1}$  的梯度。

关于特征梯度旋转等变性的讨论：特征梯度的旋转等变性保证了无论我们如何旋转输入的3D点云，特征梯度的所有维度始终与特征的维度对齐。这个属性确保了在训练神经网络时的稳定性（即网络训练的旋转不变性），当训练样本具有随机方向时。请见在线补充材料中的证明，该证明表明特征梯度的旋转等变性确保了网络训练的旋转不变性。

网络训练的旋转不变性指的是，当我们以任意角度旋转训练样本时，在旋转样本上训练的神经网络将与在未旋转样本上训练的神经网络具有相同的参数。因此，我们可以从以下两个角度定义和数学上保证训练的旋转不变性。首先，我们可以认为关于未旋转训练样本的最优参数与关于旋转训练样本的最优参数相同，如下所示。

$$
\text{arg min}_ w Loss(x(\theta), w) = \text{arg min}_ w Loss(x, w)
$$

其中  $w = \{ w_ l | l = 1, 2, \ldots, L \}$  表示所有层的网络参数。实际上，上述训练的旋转不变性也可以通过关于网络参数的梯度的旋转不变性来定义和数学上保证，如下所示。

$$
\frac{\partial Loss(x)}{\partial w_ l} = \frac{\partial Loss(x(\theta))}{\partial w_ l}
$$

这意味着不管我们如何旋转输入样本，参数梯度保持不变。

## C. 特征旋转等变性的规则

为了确保上述三种逐层属性，即特征的旋转等变性（5）、特征梯度的旋转等变性（7）和网络训练的旋转不变性（9），我们提出了一套规则来修改现有深度神经网络中的逐层操作。这是因为大多数现有的逐层操作不满足上述三种逐层属性，如表I所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1c5c6738e0124b8d8910828df4f12775.png#pic_ center" width="70%" />
</div>

因此，在本研究中，我们提出了一套规则来修改广泛用于3D点云处理的经典神经网络中的逐层操作。在补充材料中，我们已经证明了以下逐层操作的修改可以使每个修改后的层满足特征的旋转等变性。此外，我们还证明了逐层操作的特征旋转等变性在数学上可以确保整个神经网络的特征旋转等变性、特征梯度的旋转等变性和网络训练的旋转不变性。

卷积(Convolution)：将卷积操作  $\text{Conv}(f) = w \otimes f + b$  修改为  $\text{Conv}(f) = w \otimes f$ ，其中去除了偏置项  $b$ 。这里  $w$  是实值卷积滤波器， $f$  是四元数特征。

ReLU：将ReLU操作修改如下：

$$
\text{ReLU}(f_ v) = \|f_ v\| \cdot \max\{\|f_ v\|, c\} \cdot f_ v
$$
 
其中  $f_ v \in H$  表示特征  $f \in H^d$  中的第  $v$  个元素； $c$  是一个正常数。

**批量归一化**：将批量归一化操作修改如下：

$$
\text{norm}(f(i)_ v) = \frac{f(i)_ v}{\sqrt{\mathbb{E}[\|f(j)_ v\|^2]} + \epsilon}
$$
 
其中  $f(i) \in H^d$  表示第  $i$  个样本在批量中的特征； $\epsilon$  是一个很小的正常数，用来避免除以0。

最大池化：将传统的最大池化操作  $\text{maxPool}(f) = \text{maxPool}\{f_ 1, \ldots, f_ d\}$  修改如下：

$$
\text{maxPool}(f) = \hat{f} \text{ s.t. } \hat{v} = \arg\max_ {v=1,\ldots,d}[\|f_ v\|]
$$
 
注意，3D点云处理的神经网络通常使用一种特殊的最大池化操作[29]。给定  $n$  个点的特征  $F = [[f_ {1,1}, \ldots, f_ {1,d}]^\top, \ldots, [f_ {n,1}, \ldots, f_ {n,d}]^\top] \in H^{d \times n}$ ，每个点都有一个  $d$  维的四元数特征向量，对应于  $F$  的每一列。然后，[29]中的特别最大池化操作实际上可以表示为：

$$
\text{maxPool}_ {\text{pc}}(F) = \begin{bmatrix}
\text{maxPool}\{f_ {1,1}, f_ {2,1}, \ldots, f_ {n,1}\} \\
\text{maxPool}\{f_ {1,2}, f_ {2,2}, \ldots, f_ {n,2}\} \\
\vdots \\
\text{maxPool}\{f_ {1,d}, f_ {2,d}, \ldots, f_ {n,d}\}
\end{bmatrix}
$$
 
公式(13)表明，[29]中的特别最大池化操作可以分解为  $d$  个经典的最大池化操作。因此，(13)的修订本质上与(12)相同。

Dropout：对于dropout操作，我们随机丢弃四元数特征中的一定百分比的四元数元素，就像传统神经网络中的dropout操作一样。具体来说，如果第  $v$  个四元数元素  $f_ v = 0 + a_ i + b_ j + c_ k$  被丢弃，那么我们就设置  $f_ v = 0 + 0i + 0j + 0k$ 。

3D坐标加权：3D坐标加权[49]旨在使用3D坐标来计算中间层特征的权重并重新加权特征，即  $F' = F W^\top$ ， $F \in H^{d \times K}$ ， $W \in \mathbb{R}^{M \times K}$ 。具体来说，给定一个3D点  $x_ 0 \in \mathbb{R}^3$  及其  $K$  个邻居  $\{x_ 1, \ldots, x_ K\} \in \mathbb{R}^{3 \times K}$ （见脚注3），这些  $K$  点对应的中间层特征的权重被计算为单层感知网络的结果， $W = \text{perceptron}([x_ 1 - x_ 0, \ldots, x_ K - x_ 0]^\top)$ 。

使3D坐标加权操作满足特征的旋转等变性的基本思想是使相对坐标  $[x_ 1 - x_ 0, \ldots, x_ K - x_ 0]^\top$  变为旋转不变的。为此，我们使用主成分分析（PCA）找出点云的3D点的前三个主成分对应的特征向量，即  $e_ 1, e_ 2, e_ 3 \in \mathbb{R}^3$ 。我们认为点云的三个主成分是旋转等变的。因此，我们只需要将每个点  $x_ k$  投影到这三个主方向上，即  $x'_ k = [x_ k^\top e_ 1, x_ k^\top e_ 2, x_ k^\top e_ 3]^\top$ 。这里， $x'_ k$  是旋转不变的，因此  $[x'_ 1 - x_ 0, \ldots, x'_ K - x_ 0]^\top$  也是旋转不变的。

这样， $W = \text{perceptron}([x_ 1 - x_ 0, \ldots, x_ K - x_ 0]^\top)$  是旋转不变的，从而输出特征  $F' = F W^\top$  关于输入特征  $F$  是旋转等变的。这是因为形式  $F' = F W^\top$  可以被看作是没有偏置项的全连接层。因为全连接层可以被看作是具有 1×1 卷积核的特殊卷积层，根据卷积操作的修订规则，去除偏置项确保操作  $F' = F W^\top$  满足第三节B中提出的三种属性。

请注意，当网络架构包含多个3D坐标加权层时，我们只需要执行一次PCA操作以获得输入样本的PCA对齐的3D坐标。所有的3D坐标加权层使用PCA对齐坐标来计算权重。然而，除了3D坐标加权层之外，所有其他依赖于3D坐标的网络层，在推理时使用原始的3D坐标。

与置换操作正交的操作：没有必要讨论卷积、ReLU、批量归一化、最大池化和Dropout操作的特征置换不变性。这是因为这些操作的输出特征是否置换不变取决于这些操作的输入特征是否置换不变。这些操作并不直接决定神经网络的特征置换不变性。

## D. 置换不变性的规则

除了上述三种属性之外，我们还期望神经网络满足特征的置换不变性。因此，我们修改了逐层操作以使它们成为置换不变的。最远点采样（Farthest Point Sampling, FPS）[30]和基于球查询搜索的分组（Ball-Query-Search-Based Grouping）[30]是两个经典的广泛使用的3D点云处理操作，它们不是置换不变的。因此，我们按照以下方式修改这两个操作以使其成为置换不变的。

最远点采样 (Farthest Point Sampling, FPS)：FPS[30] 是一个从输入点云中选择一组点以提取局部特征的采样操作。采样的目标是从输入点云  $\Omega = [1, 2, ..., n]$  中选择  $i$  个点。如果已经选择了  $i-1$  个点，索引为  $S_ {i-1} = \{s_ 1, s_ 2, ..., s_ {i-1}\}$ ，那么FPS操作选择下一个点  $\hat{j}$  作为距离  $S_ {i-1}$  最远的点，即  $\hat{j} = \arg\max_ {j \in \Omega \setminus S_ {i-1}} \min_ {k \in S_ {i-1}} \| x_ j - x_ k \|$ 。FPS操作选择的第一个点会影响接下来的  $n-1$  个点，因此FPS操作不是置换不变的。为了使FPS成为置换不变的，我们只需要将第一个选择的点固定为输入点云的质心，这是一个虚拟点。4 因此，无论我们如何重新排序输入点，FPS操作总是选择虚拟点作为第一个点，这使得FPS操作成为置换不变的。

基于球查询搜索的分组：为了提取给定中心点的上下文信息，基于球查询搜索的分组操作通常用于为每个给定的中心点找到在一定半径内的  $K$  个邻近点。这个操作不是置换不变的，因为当半径内有超过  $K$  个点时，将根据点的顺序选择前  $K$  个点。我们按照以下方式修改这个操作。当半径内的点数超过所需数量时，我们选择  $K$  个最近邻点。

另外，以下四个操作自然满足特征的置换不变性，因为这些操作仅依赖于点之间的欧几里得距离，包括基于k-NN搜索的分组[45],[49]、密度估计[49]、3D坐标加权[49]和图构建[45]操作。更重要的是，这些操作的输出的置换不变性确保了这些操作之后层的特征的旋转等变性。

与置换操作正交的操作：没有必要讨论卷积、ReLU、批量归一化、最大池化和Dropout操作的特征置换不变性。这是因为这些操作的输出特征是否置换不变取决于这些操作的输入特征是否置换不变或不是。这些操作并不直接决定神经网络的特征置换不变性。在实际实现中，这个虚拟点不参与网络训练。

## E. REQNN的概述

尽管四元数特征有助于实现特征的旋转等变性、特征梯度的旋转等变性以及训练的旋转不变性，但大多数下游任务（例如3D形状分类）需要实数输出，正如我们之前讨论的。因此，在REQNN的大部分层中使用四元数特征后，我们需要在最后几层中将四元数特征转换为由实数组成的普通实值特征。注意，对于那些可以用四元数表示输出的任务，整个神经网络的所有特征都是四元数。例如，在点云重建任务中，重建的输出3D坐标可以用四元数表示。

因此，正如图2所示，REQNN由三个模块组成，即(a)旋转等变四元数模块，(b)四元数到实数模块，以及(c)任务模块。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0bb6dc6cd8ed4dbf9ee4d1e99321767b.png#pic_ center" width="70%" />
</div>

旋转等变四元数模块：除了REQNN顶部的一些层之外，REQNN的其他层构成了旋转等变四元数模块。此模块用于提取旋转等变四元数特征。我们使用在第三节中提出的规则来修改原始神经网络中的逐层操作，使其具有旋转等变性和排列不变性。

四元数到实数模块：四元数到实数模块位于旋转等变四元数模块之后。该模块用于将四元数特征转换为由实数组成的向量/矩阵/张量。具体来说，给定每个四元数特征的第v个元素 $f_ v = 0 + a_ v i + b_ v j + c_ v k$ ，我们计算每个四元数元素的范数的平方作为实值特征元素，即 $\|f_ v\|^2 = a_ v^2 + b_ v^2 + c_ v^2$ 。通过这种方式，我们将四元数特征转换为实值特征，即 $[ \|f_ 1\|^2, \|f_ 2\|^2, ..., \|f_ d\|^2 ]^T \in \mathbb{R}^d$ ，这些特征是旋转不变的。

任务模块：任务模块由REQNN的最后几层组成。任务模块以实值特征为输入，并生成最终输出，以执行与传统神经网络类似的各种任务。

*旋转等变性特征梯度的证明以及训练的旋转不变性。* 在在线提供的补充材料中，我们已经证明了第三节中的修改可以保证逐层特征的旋转等变性。我们进一步证明了特征的旋转等变性可以保证特征梯度的旋转等变性（见定理1和2）。此外，特征的旋转等变性也可以保证训练的旋转不变性（见定理3）。请参阅第三节的实验验证定理2和3。

定理1（旋转等变四元数模块中逐层特征梯度的旋转等变性，在线补充材料中证明）：在REQNN的旋转等变四元数模块中，每一层操作可以写成 $f_ l = \Phi_ l(f_ {l-1})$ 。如果输入点云以旋转四元数 $R$ 旋转，即 $x(\theta) = R \circ x \circ R$ ，则梯度 $\partial f_ {l-1}$ 也将以 $R$ 旋转。因此，逐层特征梯度的旋转等变性可以表述为：

$$ 
R \circ ( \frac{\partial f_ l(x)^T}{\partial f_ {l-1}} \nabla_ {f_ l}Loss(x) ) \circ R = \frac{\partial f_ l(x(\theta))^T}{\partial f_ {l-1}} (R \circ \nabla_ {f_ l}Loss(x) \circ R)
$$

引理1（旋转等变四元数模块输出梯度的旋转等变性，在线补充材料中证明）：在REQNN中，损失函数相对于旋转等变四元数模块的输出特征$f_ {out}$的梯度是旋转等变的，即：

$$ 
R \circ \frac{\partial Loss(x)}{\partial f_ {out}} \circ R = \frac{\partial Loss(x(\theta))}{\partial f_ {out}} 
$$

定理2（特征梯度的旋转等变性和旋转不变性，在线补充材料中证明，并在第三节中实验验证）：根据定理1和引理1，在REQNN中，任何中间层特征$f_ l$的梯度相对于旋转等变四元数模块是旋转等变的，即：

$$ 
R \circ \frac{\partial Loss(x)}{\partial f_ l} \circ R = \frac{\partial Loss(x(\theta))}{\partial f_ l} 
$$

在四元数到实数模块和任务模块的每一层中，特征的梯度是旋转不变的。

定理3（参数梯度的旋转不变性，在线补充材料中证明，并在第三节中实验验证）：在REQNN中（包括旋转等变四元数模块、四元数到实数模块和任务模块中的层），任何中间层参数 $w_ l$ 的梯度相对于损失函数 $Loss(x)$ 是旋转不变的，即：

$$
\frac{\partial Loss(x)}{\partial w_ l} = \frac{\partial Loss(x(\theta))}{\partial w_ l} 
$$

参数梯度的旋转不变性在数学上确保了REQNN训练的旋转不变性（或学习REQNN的稳定性）。这已在在线补充材料中得到证明，并在第三节的实验中得到验证（请参阅表VI的结果）。训练的旋转不变性/稳定性意味着无论我们如何旋转输入样本，REQNN都将被训练以具有相同的参数。这一属性的最直接好处是，我们不需要对输入样本进行旋转数据增强，REQNN可以达到与进行旋转数据增强的REQNN相同的性能。

\*参数数量，特征维度，浮点运算和训练时间。\* 我们进行了实验，比较传统DNN和REQNN之间的参数数量，特征维度，浮点运算和训练时间。我们修改了PointNet++，5 DGCNN，根据第III-G节中的设置，将点转换为相应的RE-QNN。所有DNN都在ModelNet 40上进行了测试[50]数据集。如表II所示，每个REQNN的参数数量与对应的原始DNN的参数数量几乎相同。REQNN的特征维数大约是对应的原始DNN的特征维数的三倍。此外，每个REQNN的浮点运算数量大约是对应的原始DNN的浮点运算数量的三倍。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/f83fc9dab5054054a4ef1ac7e3c0974d.png#pic_ center" width="70%" />
</div>

此外，我们还测量了网络训练的时间，所有模型都在同一环境下进行了评估，包括Python 3.9.12版本，CUDA 11.6版本，numpy 1.22.3版本，torchvision 0.14.1版本，scikit-learn 1. 2. 2版本，GPU类型为NVIDIA TITAN RTX。我们在上述环境中在ModelNet 40数据集上训练了传统DNN和从传统DNN修改的REQNN，表III中的结果显示，REQNN的训练时间约为2- 10分钟。比传统DNN的平均训练时间多4倍

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0fe12a46090e4d34a9589e0c45b1e0e3.png#pic_ center" width="70%" />
</div>

## F. 定理 2 和 3 的验证

我们进行了实验来验证定理 2 中的特征梯度的旋转等变性，定理 3 中的参数梯度的旋转不变性，以及训练的旋转不变性。

验证定理 2 中特征梯度的旋转等变性：我们基于 DGCNN 构建了一个 REQNN，并遵循第三节中的设置，在 ModelNet10 数据集上训练了 REQNN。这个实验的设计是为了证明以下两种方式计算的特征梯度的旋转等价性。第一种方式：给定原始点云 $x$ ，我们计算损失相对于第 $l$ 层特征的梯度，即 $\frac{\partial Loss(x)}{\partial f_ l}$ 。然后，我们将梯度旋转为 $R \circ \frac{\partial Loss(x)}{\partial f_ l} \circ R$ 。第二种方式：给定围绕相同轴旋转的点云 $x(\theta) = R \circ x \circ R$ ，我们计算损失相对于第 $l$ 层特征的梯度，即 $\frac{\partial Loss(x(\theta))}{\partial f_ l}$ 。因此，通过这两种方式计算的两种类型的特征梯度，我们使用度量 $\Delta_ {gradfeature}^l = \left\| \frac{\partial Loss(x(\theta))}{\partial f_ l} - R \circ \frac{\partial Loss(x)}{\partial f_ l} \circ R \right\|_ F \Big / \left\| \frac{\partial Loss(x(\theta))}{\partial f_ l} \right\|_ F$ 来检查我们是否可以通过旋转未旋转样本的特征梯度来合成旋转样本的特征梯度。

根据表 IV，对于所有的 $l$ ， $\Delta_ {gradfeature}^l \approx 0$ 表明旋转样本的特征梯度也可以通过直接旋转未旋转样本的特征梯度来获得。这证明了特征梯度的旋转等变性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/8e84d7b105a348bf9776b0a78407dbe1.png#pic_ center" width="70%" />
</div>

请注意，在 DGCNN 中基于 k-NN 搜索的分组操作中累积的小的系统性计算错误可能破坏特征梯度的旋转等变性。请参见在线补充材料中的详细分析。因此，对于 REQNN 中基于 k-NN 搜索的分组操作的第一层，我们在旋转输入点云时保持了每个给定点的邻近点不变，以避免由小的系统性计算错误造成的问题。我们在第三节中的两个实验验证中应用了这样的设置。此外，我们在第四节中的所有实验中没有固定每个给定 3D 点的邻近点，这更好地反映了 REQNN 的真实性能。

验证定理 3 中参数梯度的旋转不变性以及网络训练的旋转不变性：我们基于 DGCNN 构建了一个 REQNN，并遵循第三节中的设置，然后我们使用以下两种训练集来训练 REQNN。第一个训练集包含 ModelNet10 数据集中的原始点云 $x$ 。第二个训练集包含旋转的点云 $x(\theta) = R \circ x \circ R$ 。因此，这两个 REQNN 分别被称为 REQNNori 和 REQNNrotated。这样，我们检查这两个 REQNN 的参数梯度是否相同，以及这些两个 REQNN 的参数是否相同。

我们使用度量 $\Delta_ {gradparameter}^l = \left\| \frac{\partial Loss(x)}{\partial w_ {ori}^l} - \frac{\partial Loss(x(\theta))}{\partial w_ {rotated}^l} \right\|_ F \Big / \left\| \frac{\partial Loss(x)}{\partial w_ {ori}^l} \right\|_ F$ 来衡量在原始样本上训练的第 $l$ 层参数和在旋转样本上训练的参数之间的差异。根据表 V，对于所有的 $l$ ， $\Delta_ {gradparameter}^l \approx 0$ 表明 REQNNori 和 REQNNrotated 的参数梯度几乎相同。因此，我们证明了参数梯度的旋转不变性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a996691c1b894a23af58d8636f08f591.png#pic_ center" width="70%" />
</div>

此外，我们还使用度量 $\Delta_ {parameter}^l = \frac{\left\| w_ {ori}^l - w_ {rotated}^l \right\|_ F}{\left\| w_ {ori}^l \right\|_ F}$ 来衡量在原始样本上训练的网络参数和在旋转样本上训练的网络参数之间的相对差异。根据表 VI，对于所有的 $l$ ， $\Delta_ {parameter}^l \approx 0$ 表明 REQNNori 和 REQNNrotated 的参数几乎相同，尽管误差会在训练过程中累积。因此，我们证明了网络训练的旋转不变性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/cd7bf28fef174ce1b677b02a350a04e6.png#pic_ center" width="70%" />
</div>

## G. 将传统DNNs修订为REQNNs

在本研究中，我们将以下四种用于3D点云处理的神经网络修订为REQNNs，包括PointNet++ [30]、DGCNN [45]、PointConv [45]和PointNet [29]。

模型1, PointNet++: 为了将PointNet++ [30]修订为用于形状分类的REQNN，我们将最后三层全连接(FC)层作为任务模块，并将其他层作为旋转等变四元数模块。此外，在这两个模块之间有一个四元数到实数模块用于将四元数转换为实数。如表VII所示，我们根据第三节中的规则修订了四种逐层操作以实现旋转等变性，包括卷积、ReLU、批量归一化和最大池化操作。我们还根据第三节中的规则修订了最远点采样和基于球查询搜索的分组操作，使其具有排列不变性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/484e509daf764888a047ad77a337734d.png#pic_ center" width="70%" />
</div>

模型2, DGCNN:为了将DGCNN [45]修订为用于形状分类的REQNN，我们将最后三层FC层作为任务模块，并将其他层作为旋转等变四元数模块。此外，我们在这两个模块之间添加了一个四元数到实数模块。如表VII所示，我们根据第三节中的规则修订了四种逐层操作以实现旋转等变性，包括卷积、ReLU、批量归一化和最大池化操作。请注意，原始DGCNN中的所有逐层操作自然具有排列不变性。

模型3, PointConv:为了将PointConv [49]修订为用于形状分类的REQNN，我们使用最后三层FC层作为任务模块，并将其他层作为旋转等变四元数模块。在这两个模块之间添加了一个四元数到实数模块。如表VII所示，我们根据第三节中的规则修订了四种逐层操作以实现旋转等变性，包括卷积、ReLU、批量归一化和3D坐标加权操作。我们还根据第三节中的规则修订了最远点采样操作，使其具有排列不变性。

模型4, PointNet: 为了构建用于形状重建的REQNN，我们稍微修订了PointNet [29]用于形状分类的架构。我们将PointNet中的所有剩余层作为旋转等变四元数模块，除了最大池化和空间变换网络(STN) [18]。STN丢弃了输入点云的所有空间信息（包括旋转信息）。因此，为了编码旋转信息，我们从原始PointNet中移除了STN。请注意，这个REQNN中没有四元数到实数模块或任务模块，以便在REQNN中的所有特征都是四元数特征。如表VII所示，我们根据第三节中的规则修订了四种逐层操作以实现旋转等变性，包括卷积、ReLU、批量归一化和dropout操作。

# IV. 实验

由于我们已经在REQNN中引入了旋转等变性和排列不变性属性，本节主要进行实验以展示REQNN相比传统神经网络在性能上的优越性。我们使用该方法将不同的经典DNNs修订为不同的REQNNs，并测试了它们的性能。我们还从另外两个角度评估了REQNN的表示能力，即不同旋转下输入归因的稳定性和对旋转攻击的对抗性鲁棒性。此外，在本节的所有实验中，我们设定 $c = 1$ （见公式(10)）和 $\epsilon = 10^{-5}$ （见公式(11)）。我们在反向传播中分离了 $\frac{\|f_ v\|}{\max{\|f_ v\|, c}}$ （见公式(10)）和 $\frac{1}{\sqrt{\mathbb{E}_ j[\|f(j)_ v\|^2] + \epsilon}}$ （见公式(11)）的导数。

## A. REQNNs 在不同任务中的应用

在本节中，我们主要进行了实验来展示REQNN的优越性能。我们使用REQNN对不同的经典神经网络进行了修改，以应用于3D点云处理，并测试了它们的性能。我们还从两个其他角度评估了REQNN的表示能力，即不同旋转下输入归因的稳定性和对抗旋转攻击的鲁棒性。此外，在本节的所有实验中，我们设置了$c = 1$（见公式(10)）和 $\epsilon = 10^{-5}$ （见公式(11)）。我们在反向传播中分离了 $\|fv\|/ \max\{\|fv\|, c\}$ （见公式(10)）和 $1/ \sqrt{\sum_ j\|f(j)_ v\|^2} + \epsilon$ （见公式(11)）的导数。

1)3D形状分类。在本小节中，我们在任意旋转下比较了原始DNN和从原始DNN修改而来的REQNN在3D点云上的分类精度。为此，我们将三种广泛使用的DNN用于形状分类任务修改成了不同的REQNN，包括PointNet++ [30]、DGCNN [45]和PointConv [49]。所有DNN都是基于ModelNet40 [50]数据集提供的[29]、3D MNIST [1]数据集和ShapeNet数据集进行学习的。上述三个数据集分别包含40个类别、10个类别和16个类别。对于每个形状，我们按照[29]、[30]、[45]、[49]中的常见设置选择了原始点集的前1024个点。给定每个基准数据集，我们通过任意旋转原始测试集中的每个样本十次，生成了一个测试集，以测试不同旋转下的分类精度。

我们比较了以下三种模型的分类精度。第一个基线模型是在没有旋转的点云上训练的原始DNN。第二个基线模型是在沿y轴任意旋转的点云上训练的原始DNN。y轴旋转增强在[30]、[45]中已经广泛使用。提出的REQNN是第三种竞争方法，它从传统DNN修改而来，且在没有旋转的点云上进行训练。实际上，我们已经证明了REQNN的训练是旋转不变的，即无论我们是否旋转训练样本，REQNN都会收敛到相同的参数。我们在第III-F节的实验中验证了REQNN的训练的旋转不变性。因此，没有必要比较在随机旋转样本上训练的REQNN和在未旋转样本上训练的REQNN。

比较任意旋转对象的分类精度：表VIII显示了在任意旋转的点云上测试的DNN的分类精度。结果表明，无论传统DNN是否经过旋转增强训练，REQNN的分类精度总是高于所有传统DNN。从DGCNN修改来的REQNN达到了最高的精度84.57%。相比之下，未经旋转增强训练的传统DNN在ModelNet40数据集上的精度非常低，为25.01%-32.08%，在3D MNIST数据集上为44.19%-45.90%，在ShapeNet数据集上为37.03%-44.06%。结果还表明，经过y轴旋转增强训练的传统DNN比未经旋转增强训练的传统DNN具有更高的分类精度。然而，y轴旋转数据增强带来的精度提升是有限的。这是因为y轴旋转不能保证DNN学习任意旋转的信息。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9f116554d61342e4902e7109634525f7.png#pic_ center" width="70%" />
</div>

比较在任意旋转对象上的泛化能力下降：我们发现上述传统DNN在任意旋转的测试对象上通常会出现性能下降，与在原始测试对象上的性能相比。性能下降可以被视为由于缺乏旋转不变性/等变性导致的过拟合问题。因此，对于每个训练的DNN，我们希望比较在原始数据集中的对象和任意旋转对象上的泛化能力。我们在以下两种场景中比较了DNN，即在没有旋转的情况下学习并在没有旋转的情况下测试的DNN（NR/NR），以及在没有旋转的情况下学习并在任意旋转情况下测试的DNN（NR/AR）。我们使用以下两种方法测量了泛化能力：

1. 比较分类精度：我们比较了在原始数据集中的对象和任意旋转对象上的分类精度。
2. 比较训练损失和测试损失之间的差距。

首先，比较分类精度：我们比较了REQNN和八种最先进的3D点云分类DNN。表IX显示，REQNN在NR/NR和NR/AR场景中的分类精度大致相同，因为旋转对REQNN的训练没有影响。分类精度的微小差异是由计算误差引起的。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/141cae94414943268f46fb7eb555810e.png#pic_ center" width="70%" />
</div>

如表IX所示，从DGCNN修改来的REQNN在NR/AR场景中达到了最高的精度84.57%，这表明REQNN对旋转具有显著的鲁棒性。相比之下，传统DNN在NR/AR场景中的分类精度比NR/NR场景中要低得多，包括PointNet [29]、PointNet++ [30]、Point2Sequence [24]、KD-Network [21]、RS-CNN [25]和DGCNN [45]。这是因为传统DNN无法处理具有未见过方向的点云，尽管它们在NR/NR场景中实现了高精度。与在NR/AR场景中比传统DNN分别高出15.72%和10.5%的传统DNN相比，PRIN [53]和QE-CapsuleNet [59]也努力提高了旋转鲁棒性，但仍然不如我们的REQNN。

其次，比较损失差距：我们使用训练损失和测试损失之间的差距来衡量REQNN的分类泛化能力。如果DNN出现过度拟合，那么DNN在旋转样本上的分类泛化将显著下降。为此，我们分析了REQNN和传统DNN在不同测试样本上的分类泛化。具体来说，我们测量了在NR/NR场景中训练损失和测试损失之间的差距，即 $\text{gap}_ {\text{NR/NR}} = |\text{Loss}_ {\text{NR train}} - \text{Loss}_ {\text{NR test}}| \in \mathbb{R}^+$ ，它表示当训练样本和测试样本都未旋转时的泛化误差。类似地， $\text{gap}_ {\text{NR/AR}} = |\text{Loss}_ {\text{NR train}} - \text{Loss}_ {\text{AR test}}| \in \mathbb{R}^+$ 表示当训练样本未旋转而测试样本旋转时的泛化误差。因此，两个差距之间的相对差异 $\text{diff} = \left|\frac{\text{gap}_ {\text{NR/AR}} - \text{gap}_ {\text{NR/NR}}}{\text{gap}_ {\text{NR/AR}}}\right|$ 衡量了由于旋转敏感性导致的DNN的额外泛化误差。

我们还对REQNN和上述两种类型的基线DNN进行了实验。实验结果见表X，所有REQNN的diff值都非常小（不超过0.002），这表明REQNN具有出色的分类泛化能力。相比之下，传统网络（无论是否经过旋转训练）的diff值都非常高（在ModelNet40数据集上为0.821-0.968，在3D MNIST数据集上为0.879-0.964，在ShapeNet数据集上为0.987-0.999），这表明传统网络的分类泛化能力较差。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/6e47f0659ba64394892a02b70527f242.png#pic_ center" width="70%" />
</div>

讨论：当我们在未旋转的样本上测试DNN时，每个REQNN的测试精度都比相应的传统DNN低。例如，在未旋转样本上，DGCNN的测试精度为92.90%，而从DGCNN修改来的REQNN的测试精度为84.64%。这是因为当我们为了旋转等变性将DGCNN修改为REQNN时，代价是增加了模型复杂性。训练复杂的DGCNN比训练相对简单的DGCNN更难优化，这是性能下降的原因。然而，如果我们通过将测试样本旋转到任意方向来消除测试样本中的偏差，那么REQNN就优于传统DNN，正如表IX中的“NR/AR（考虑测试中的旋转）”列所示。这以更有说服力的方式展示了REQNN的真实表征能力。

在未旋转对象上训练与在任意旋转对象上训练的比较。我们进行了一个新的实验，比较了在原始数据集上训练的DNN和在任意旋转对象上训练的DNN。因为在表IX对应的先前实验中，我们发现传统DNN在任意旋转样本上的性能下降，这是由于数据集中样本收集的偏差造成的。因此，新的实验旨在说明数据集偏差的重要性。为此，我们构建了以下新的数据集。第一个新数据集是通过围绕任意轴和任意角度旋转ModelNet40数据集中的每个训练样本构建的，称为随机旋转数据集。第二个新数据集是通过围绕y轴和任意角度旋转ModelNet40数据集中的每个训练样本构建的，称为y轴旋转数据集。然后，对于每种类型的网络架构，我们训练了以下四种模型，包括在原始/未旋转的ModelNet40数据集上训练的传统DNN，在随机旋转数据集上训练的传统DNN，在y轴旋转数据集上训练的传统DNN，以及在原始/未旋转的ModelNet40数据集上训练的REQNN。每个DNN都在随机旋转的对象上进行测试，测试设置与表VIII中的实验相对应。

表XI的结果显示，REQNN在未旋转对象上训练的模型比所有三种传统DNN（包括在未旋转对象、y轴旋转对象和随机旋转对象上训练的DNN）表现出更高的分类精度。这意味着传统DNN对数据集偏差更敏感。此外，即使我们使用无偏数据集（包括随机旋转对象）来训练不同的DNN，REQNN在随机旋转测试对象上的分类精度仍然高于传统DNN。注意，y轴旋转数据集上训练的传统DNN的分类精度略低于原始数据集上训练的传统DNN，因为与旋转增强不同，新构建的y轴旋转数据集没有包含比原始数据集更多的对象，但它具有更多样化的3D点云。对象方向的多样性防止了DNN对某些快捷方向特征的过度拟合，从而增加了训练难度。此外，y轴旋转训练样本和随机旋转测试样本之间仍然存在差距。这解释了在y轴旋转对象上训练的DNN性能的微小下降。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/4197381b66b442b2a7f0f73e09dcc267.png#pic_ center" width="70%" />
</div>

2)3D点云重建。在这个实验中，我们旋转了原始点云的中间层四元数特征，以合成具有目标方向的新点云。为此，我们在ShapeNet [4]数据集上学习了一个从PointNet [29]修改来的REQNN，用于点云重建。在我们的实现中，每个点云由1024个点组成。我们使用了REQNN中顶部第四个线性变换层的输出四元数特征来合成具有不同方向的四元数特征。这些合成的四元数特征被用来重建具有目标方向的点云。

如图3所示，对于每个点云（图3“原始”(a)），我们直接用不同角度旋转它（图3“原始”(b)-(e)）。为了比较，我们旋转了与原始点云对应的四元数特征相同的角度，以合成具有不同方向的四元数特征。这些生成的四元数特征被用来重建点云（图3“重建”(b)-(e)）。我们观察到这些重建的点云与直接旋转原始点云生成的点云具有相同的方向。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1ed2d0c8e0ce49708e0f3bd3ae68f228.png#pic_ center" width="70%" />
</div>

## B. 关于旋转的表示稳定性

在这一部分中，我们提出了一个新的度量标准，用于评估REQNN在不同角度旋转输入点云时编码的输入归因的稳定性。理想情况下，给定两个具有相同3D结构但不同方向的点云，REQNN应该编码相似的特征表示。因此，当点云旋转时，不同点云区域的归因应该保持不变。

具体来说，我们按照[34]将整个点云均匀地划分为n个云区域，记为 $N = \{1, 2, ..., n\}$ 。然后，我们计算了每个点云区域的Shapley值。Shapley值已广泛用于测量每个输入变量（这里，每个点云区域）对神经网络输出分数的贡献[15]、[26]、[33]、[34]、[39]。

为了计算点云中每个区域的Shapley值，我们定义了神经网络的输出分数 $v(S)$ ，给定点云区域的一个子集 $S \subseteq N$ ，定义如下。设$x_ S$表示只包含S中区域的点云，而 $N \setminus S$ 中的区域已被从点云中移除。考虑到现有的3D点云处理神经网络通常可以处理固定数量的点，我们按照[61]将 $N \setminus S$ 中的点的坐标重置为整个点云的中心，而不是物理移除这些点。给定输入 $x_ S$ ，预训练的神经网络对形状分类的输出分数计算为 $v(S) = \log \frac{p}{1-p}$ ，其中 $p = p(y = y_ {\text{truth}} | x_ S)$ 表示真实类别的概率。通过这种方式，区域i对整体网络输出的数值归因被估计为Shapley值 $\phi_ i = \sum_ {S \subseteq N \setminus \{i\}} \frac{|S|! (n-|S|-1)!}{n!} (v(S \cup \{i\}) - v(S))$ 。 $\phi_ i$ 是通过[3]中的基于采样的近似方法计算的。

给定一个输入点云 $x$ ，设 $x(1) = \theta_ 1(x)$ 和 $x(2) = \theta_ 2(x)$ 分别表示通过两个不同的旋转操作 $\theta_ 1$ 和 $\theta_ 2$ 获得的两个点云。我们测量了不同点云区域归因的分布，即Shapley值 $\Phi = [\phi_ 1, ..., \phi_ n]^T \in \mathbb{R}^n$ 。神经网络编码的区域归因在不同旋转下的稳定性被量化如下：

$$
\text{stability} = \mathbb{E}_ x \mathbb{E}_ {\theta_ 1, \theta_ 2}[\cos(\Phi_ {x(1) = \theta_ 1(x)}, \Phi_ {x(2) = \theta_ 2(x)})].
$$

其中 $\cos(\Phi_ {x(1) = \theta_ 1(x)}, \Phi_ {x(2) = \theta_ 2(x)}) = \frac{\Phi_ {x(1)}^T \Phi_ {x(2)}}{\|\Phi_ {x(1)}\| \|\Phi_ {x(2)}\|} \in \mathbb{R}$ 测量了两个点云的区域归因之间的相似性。

我们比较了第IV-A1节中提到的三种基线神经网络的稳定性，即在没有旋转的样本上训练的传统神经网络，在具有y轴旋转增强的样本上训练的传统神经网络，以及在没有旋转的样本上训练的相应REQNN。我们在任意旋转的样本上计算了测试集的稳定性，即给定每个测试点云$x$ ，我们在第IV-A1节中介绍的十种不同的旋转操作 $\{\theta(x)\}$ 下测量稳定性。表XII显示，每个REQNN的输入归因几乎保持不变，无论点云如何旋转。微小的变化是由计算的系统误差引起的。相比之下，传统神经网络的输入归因对旋转敏感。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a1b13d190781479b8f3c0cae1c222129.png#pic_ center" width="70%" />
</div>

## C. 对旋转攻击的对抗性鲁棒性

先前的研究[60]证明了经典的3D点云处理神经网络容易受到旋转攻击。也就是说，即使不扰乱输入点云，人们也可以通过简单地旋转输入来成功攻击神经网络。然而，REQNN应该对旋转攻击具有鲁棒性。因此，我们使用了[60]中的黑盒旋转攻击方法来评估REQNN对旋转攻击的对抗性鲁棒性。旋转攻击的目标如下所示：

$$
\min_ {\theta} p(y = y_ {\text{truth}} | \theta(x)),
$$

其中 $p(y = y_ {\text{truth}} | \theta(x))$表示给定旋转样本 $\theta(x)$ 时，真实类别的概率。如果在达到停止条件时， $p(y = y_ {\text{truth}} | \theta(x))$ 仍然是所有类别中最大的概率，则认为对 $x$ 的攻击失败。然后，将旋转攻击的对抗性鲁棒性定义为攻击失败的比率。

我们比较了REQNN和两种类型的基线神经网络在旋转攻击下的失败率，即在没有旋转的样本上训练的基线和在具有y轴旋转增强的样本上训练的基线。表XIII显示，使用旋转攻击很难攻击REQNN，而攻击传统神经网络则相对容易。

表XIII 展示了不同神经网络对旋转攻击的对抗性鲁棒性，即旋转攻击的失败率。结果表明，REQNN在对抗旋转攻击方面具有显著的优势，这进一步证明了其在处理3D点云数据时的鲁棒性和可靠性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ca741a8415f64f6095a5d024e5ed6cea.png#pic_ center" width="70%" />
</div>

# V. 结论

在本文中，我们提出了一套通用规则，用于修改各种3D点云处理神经网络的逐层操作，以构建旋转等变四元数神经网络（REQNN）。修改后的逐层操作已被证明可以使REQNN满足四个属性，包括特征的旋转等变性、特征梯度的旋转等变性、训练的旋转不变性以及特征的排列不变性。在各种任务上的实验表明，与传统神经网络相比，REQNN表现出更优越的旋转鲁棒性。
声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
