# [ConvMatch: Rethinking Network Design for Two-View Correspondence Learning](https://ieeexplore.ieee.org/document/10323178/)
## 题目：ConvMatch：重新思考两视图对应学习网络设计
**作者：Shihua Zhang; Jiayi Ma**  
**源码：https://github.com/SuhZhang/ConvMatch**  
****
# 摘要
多层感知器（MLP）已成为双视图对应学习的事实骨干，因为它可以单独提取无序对应点的有效深度特征。然而，由于天生缺乏上下文信息，其性能受到限制，尽管在后续研究中附加了许多捕获上下文的模块。在本文中，我们从一个新的角度出发，设计了一个名为ConvMatch的对应学习网络，首次可以利用卷积神经网络（CNN）作为骨干，具有固有的上下文聚合能力。具体来说，我们观察到稀疏运动向量和密集运动场可以通过插值和采样相互转换，我们通过隐式估计密集运动场来规范候选运动向量，然后使用CNN纠正局部区域异常值引起的错误，并最终从修正后的运动场中获得正确的运动向量。此外，我们提出了全局信息注入和双边卷积，以更好地适应整体空间变换，并容纳大场景视差情况下运动场的不连续性。广泛的实验表明，ConvMatch在相对姿态估计、单应性估计和视觉定位方面一致性地超越了现有的最先进技术。

# 关键词
- 卷积神经网络
- 运动场
- 异常值排斥
- 点对应
- 双视图几何

# I. 引言
在计算机视觉中识别双视图对应关系是一个基本问题。它的目的是建立两个视图图像之间的稀疏特征对应/匹配，并估计几何关系，为许多复杂的视觉问题提供前提，如从运动中恢复结构[1]、同时定位和映射[2]、视觉定位[3]和图像融合[4]。最经典的几何匹配流水线从特征提取和匹配开始，在这里手工制作或基于学习的检测器和描述符上花费了巨大的努力，例如SIFT[5]和SuperPoint[6]。然后应用异常值排斥以保留正确的匹配（即内点）并拒绝错误的匹配（即异常值），从而鲁棒地估计相对姿态。本文专注于去除异常值。

为了准确获得双视图对应关系和几何关系，传统的异常值排斥方法如RANSAC[7]、LPM[8]和VFC[9]在真实场景中常常失败，因为内点比例极低。因此，研究人员寻求更强大的基于学习的技术的帮助。在PointCN[10]首次将异常值排斥视为一个二元分类问题，并在多层感知器（MLP）框架下进行处理之后，提出了许多其他具有类似MLP基础网络的算法，例如PointACN[11]、OANet[12]、LMCNet[13]和CLNet[14]。这种统一的网络设计的基本原因是点对应关系的稀疏和无序特性，似乎只有MLP才能在这种情况下稳定地提取深度特征。然而，MLP的独立特征提取特性导致了一个巨大的缺陷，即缺乏上下文信息，这对于双视图对应和几何学习是不可或缺的。因此，基于MLP的方法必须设计额外的模块附加到骨干上来捕获上下文作为一种补救措施。尽管使用此框架取得了有希望的性能，但仍然存在缺点。例如，在PointCN和PointACN中，归一化操作仅捕获全局上下文信息。在OANet和LMCNet中，排列不变池化操作增加了训练的难度。而在CLNet中，排序操作容易加剧不稳定性。显然，所有这些额外模块中的问题都对上下文信息的整合产生了负面影响，其中MLP的固有缺陷不断限制了其性能。在这种情况下，人们可能会想如何进一步解决这个问题：是继续改进用于补偿天生缺乏上下文的MLP的上下文捕获模块，还是直接用一个固有具有上下文感知能力的网络替换它？

众所周知，卷积神经网络（CNN）可以直接整合局部信息，并逐渐整合全局信息，如在图像识别或分割中[15]、[16]。如果我们可以使用CNN而不是MLP来提取对应点的深度特征，那么由于CNN的固有局部感知能力，特别是局部上下文的缺失可以自然地得到解决。然而，CNN仅在有序数据上操作，如图像和特征图，而点对应关系完全是无序的。

幸运的是，通过观察到稀疏运动向量和密集运动场可以通过插值和采样相互转换，运动场可以充当无序对应点和有序运动向量之间的桥梁，如图1(a)所示。特别是，在计算了每个对应点的运动向量之后，我们插值出一个密集的运动场 $F$，然后以相等的间隔进行采样，以获得有序数据。显然，逆过程也是可行的。通过这种方式，CNN就可以应用于双视图对应学习。

基于以上分析，我们提出了一个新的双视图对应学习框架，称为ConvMatch，如图1(b)所示。我们首先通过隐式插值操作估计运动场 $F$，将无序运动向量规范化为类似图像结构的数据。然后我们使用CNN通过上下文信息纠正每个有序运动向量的错误。从另一个角度来看，由于运动场的局部一致性先验[9]，由异常值引起的错误可以被视为噪声，CNN可以自然地由于其低通特性而将它们过滤出来，因此修正类似于去噪操作。最后，通过类似的插值操作估计修正后的运动场 $\tilde{F}$，我们从修正后的有序向量中获得所有对应点的正确运动向量，并且相应地区分内点和异常值。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d8c5502b26e14bef9219ba2cae944063.png" width="70%" /> </div>


然而，在构建对应学习CNN骨干时仍需考虑一些问题。一方面，全局上下文是利用整体空间变换的可选补充[10]、[13]、[14]，而CNN在浅层直接聚合全局信息存在困难。另一方面，一个简单的CNN在图1(b)中过滤异常值是基于运动场是连贯和平滑的假设，但这并不总是适当的，特别是在大场景视差的情况下。具体来说，由例如不同场景深度中的多个对象引起的运动场中的不连续性，容易被一个简单的CNN过滤器错误地平滑，导致错误的匹配和真实对应点的丢失[9]、[17]。为了解决这些问题，我们引入了全局信息注入，直接在每一层检索和聚合全局上下文，并提出了双边卷积以保留真实运动场的巨大不连续性。这些特定问题的结构将构成本文CNN骨干的主要部分，以提高其在对应学习中的性能。

总结来说，我们的主要贡献如下：
- 与基于MLP的网络以及额外的上下文捕获模块相比，我们设计了一个新的框架，使用CNN作为骨干，该CNN天生就整合了上下文信息，从而打破了MLP的瓶颈并避免了额外模块的缺点。据我们所知，这是第一次利用CNN作为骨干来解决异常值排斥问题。
- 通过观察到稀疏运动向量和密集运动场可以相互转换，我们通过隐式估计运动场来规范稀疏无序运动向量，使它们可以被CNN而不是仅限于基于MLP的等价排列网络处理。
- 我们提出了两种特定的结构来构成CNN骨干进行对应学习。具体来说，全局信息注入聚合全局上下文以直接检索整体空间变换，双边卷积保留运动场的不连续性以应对复杂场景中大深度差异的情况。
- 我们实现了一个新框架的网络，命名为ConvMatch。我们证明了其在相对姿态估计、单应性估计和视觉定位方面的有效性，一致性地超越了当前的最先进技术，包括基于MLP的方法。我们进一步分析了CNN骨干和特定问题结构、规范化和修正过程的效果。

这份手稿的初步版本出现在[18]中。本文是对会议版本的全面扩展。我们特别为双视图对应学习重构了CNN骨干，而不是之前由简单的Resblocks[15]堆叠的，这在某些复杂场景中效果不佳。主要改进如下。首先，全局信息注入被引入以在最开始而不是深层捕获全局上下文，以便更好地拟合全局变换，尽管存在大视点变化或仅有稀疏纹理可用。其次，提出了双边卷积以适应巨大的不连续性，同时大致平滑运动场，从而在大场景视差的情况下保留真实运动。然后，我们将新方法应用于更具挑战性的视觉定位任务，这是双视图几何估计在实践中的一个重要应用，以验证其优越性能。最后，对ConvMatch进行了进一步分析，以证明我们方法的稳定性，并揭示特殊结构的有效性。

# III. 方法论
我们网络的关键创新在于使用卷积神经网络（CNN）来内在地捕获上下文信息，同时提取深度特征，而不是使用缺乏上下文感知的多层感知器（MLP）。为此，我们将无序的运动向量转换为有序的，以便CNN可以处理，其中转换是通过隐式估计的运动场实现的。然后，我们通过CNN整合上下文信息来纠正有序运动向量，类似于去噪问题，并特别设计了CNN模块以更好地适应整体空间变换并适应运动场中的真实不连续性。如图2所示，给定N个假设对应点 {(xi, yi)|i = 1, ..., N, xi ∈ R^2, yi ∈ R^2}，网络的输入是假设的运动向量 {mi = (xi, di)|i = 1, ..., N, mi ∈ R^4}，其中xi和yi是两个对应关键点的坐标，di = yi − xi是位移。输出是用于内联/外联分类的logits {ˆzi|i = 1, ..., N}。具体来说，我们首先在高维空间中初始化运动向量，以及有序运动向量的坐标。然后，通过CNN对无序运动向量进行规范化，然后对有序运动向量进行校正，其中大部分由全局信息注入和双边卷积构成。最后，我们从校正后的有序运动向量中依次恢复无序运动向量。然后通过比较最终的无序运动向量与原始运动向量来预测内联/外联分类结果。基于Regularize、Rectify和Recover操作，ConvMatch总共堆叠了L次。在以下内容中，我们将详细介绍用于异常值剔除的新框架，并详细描述ConvMatch的特别设计结构。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/3b10919c7f9d4d51b77dd84eece31cf6.png" width="70%" /> </div>

## A. 运动向量初始化
我们使用稀疏运动向量 {mi} 作为输入，因为如图1(a)中作为桥梁的运动场是通过它们插值的。然而，运动向量的维度太低，无法提取深度特征，因此，正如LMCNet [13]所做的，我们将其转换为高维运动向量 $f_i ∈ R^C$ 作为输入层（第0层）：

$$
(0)f_i = E(mi), i = 1, ..., N,
$$


其中 E(·) 表示将运动向量的维度提升到 C，这里我们选择 C = 128。参考 [47]，在提升过程中进行位置嵌入，而不是直接将 mi 映射到 $f_i$ ：

$$
E(mi) = E_1(di) + E_2(xi), i = 1, ..., N.
$$

 $E_1$ 和 $E_2$ 将位移和位置分别映射到高维空间，使用不同的简单MLP层。位置信息通过求和操作嵌入。

然后，由于CNN所需的图像结构化数据，我们希望在相同的间隔处采样运动场。为此，我们将图像的有界2D空间划分为 K × K 网格，在网格中心 $xgrid_j$ ,k 采样以获得有序运动向量。同样，为了更好地进行网络采样，我们将 $xgrid_j$ ,k 嵌入到高维空间：

$$
Xgrid_{j,k} = Up(xgrid_{j,k}), j, k = 1, ..., K,
$$

其中 Up(·) 是一个简单的MLP，将变量从低维空间映射到高维空间， $Xgrid_{j,k} ∈ R^C$ ，C = 128。

## B. 有序运动向量生成

如上所述，为了满足CNN对输入数据的要求，我们应该插值一个密集的运动场，然后在等间隔位置采样以获得有序运动向量。在传统方法中，插值运动场通常通过正则化实现。该过程可以简要表示为：

$$
F = \phi({mi}), i = 1, ..., N,
$$

其中 φ(·) 是插值函数，F是估计的运动场。然后，有序运动向量 $mgrid_{j,k}$ 可以很容易地从 F 在网格中心 $xgrid_{j,k}$ 处采样：  

$$
mgrid_{j,k} =
\begin{bmatrix}
xgrid_{j,k}, dgrid_{j,k}
\end{bmatrix}
$$

$$
=\begin{bmatrix}
xgrid_{j,k}, F(xgrid_{j,k})
\end{bmatrix}
$$

$$
=\begin{bmatrix}
xgrid_{j,k}, \phi({mi})|xgrid_{j,k}
\end{bmatrix}.
$$  

然而，函数 φ(·) 依赖于手工制作的核和正则化参数 [9]，并且难以在我们的网络中处理高维表达式。因此，我们使用 GAT 网络 [49] G(·, ·)（稍后在 (8) 到 (12) 中明确定义）来隐式插值运动场 F 并直接生成有序运动向量：

$$
mgrid_{j,k} = G
\begin{Bmatrix}
{mi}, xgrid_{j,k}
\end{Bmatrix}.
$$

重写为高维空间，根据 (1) 和 (3) 的矩阵形式：

$$
(ℓ)F_{grid} = G
\begin{Bmatrix}
{(ℓ)f_i}, {Xgrid_{j,k}}
\end{Bmatrix}
= G
\begin{Bmatrix}
(ℓ)F, Xgrid
\end{Bmatrix},
$$

其中上标 (ℓ) 表示第 ℓ 层， $(ℓ)F_{grid} = {(ℓ)fgrid_{j,k}}$ ， $(ℓ)fgrid_{j,k}$ 是 $mgrid_{j,k}$ 的高维嵌入。通过简化 (ℓ)F 和 Xgrid 的变量符号，我们进一步定义 G(F, X) 为：

$$
G(F, X) = Comb(X, Aggr(X, F )),
$$

其中 Aggr(·, ·) 尝试通过考虑所有已知运动向量 F 来估计 X 位置处的运动场，Comb(·, ·) 尝试将运动场信息和网格中心点的位置信息结合起来，以获得新的运动向量 $F_{grid}$ 。具体来说：

$$
Aggr(X, F ) = Softmax(QK^T)V,
$$

$$
Q = W_1X + b_1,
$$

$$
[K; V] =
\begin{bmatrix}
W_2 W_3
\end{bmatrix}
F +
\begin{bmatrix}
b_2 b_3
\end{bmatrix},
$$

其中 $W_1$ , $W_2$ , $W_3$ 是可学习的权重， $b_1$ , $b_2$ , $b_3$ 是可学习的偏置，Comb(X, A) = X + Comp(X∥A)，其中 A 表示函数 Aggr(·, ·) 的结果，∥ 表示按通道连接，Comp(·) 将连接的表示压缩到与 X 相同的通道。

随着有序运动向量的生成，我们将假设的运动向量转换为有序的。转换是通过隐式插值密集运动场，然后采样实现的，即 (7)。接下来，我们的目标是将有序运动向量重构为类似图像结构的数据，并用 CNN 处理以纠正异常值引起的错误。

## C. 使用CNN校正运动向量

根据 (7) 获得的有序运动向量 $(ℓ)F_{grid} = {(ℓ)fgrid_{j,k}}$  可以表示由于运动场的局部一致性而密集的运动场 F。因此，通过顺序选择运动向量，我们将 ${(ℓ)fgrid_{j,k} ∈ R^C, j, k = 1, ..., K}$ 重塑为类似图像结构的数据 (ℓ)I ∈ R^{K×K×C}，可以被视为密集模拟运动场的数字形式。由于由内联点构建的运动场比包含异常值的运动场更平滑和局部一致 [9], [13]，一旦获得一个包含大量异常值的运动场，我们可以平滑它以获得更一致的运动场 (ℓ) ̃F。通过这样做，被污染的运动场被修复以滤除外部值。因此，我们进一步将异常值剔除视为一个去噪问题，其中异常值是噪声信号，使用CNN块校正运动场中由异常值引起错误：

$$
(ℓ) \tilde{I} = CNN((ℓ)I),
$$

其中 (ℓ) ̃I ∈ R^{K×K×C} 是按图像格式排列的校正后的有序运动向量。

请注意，除了作为平滑滤波器以减少异常值的不利影响外，CNN块还充当局部和全局上下文的提取器，并将信息从邻近到所有运动向量整合，因为CNN层变得更深。上下文有助于校正与邻居大不相同的向量，使其类似于它们，这无法用MLP实现。

然而，尽管像ResNet块[15]这样的简单CNN结构可以实现平滑和上下文捕获功能，并且在某些情况下表现出色，但它在两视图对应学习任务中仍存在几个缺点。首先，全局信息仅在CNN的非常深的层次中出现，这是由于CNN的邻域感知机制，从而导致在浅层次上遗漏整体空间变换。其次，运动场是局部一致且大致平滑的假设在大场景视差的情况下并不总是合适的，可学习卷积核的低通特性使得网络倾向于忽略非主导流或运动场中存在的不连续性。因此，为了消除简单结构的潜在问题，我们为两视图对应学习任务特别设计了一个CNN骨干网络，主要包括全局信息注入和双边卷积，以解决上述提到的问题。新CNN骨干网络的结构如图3所示。我们通过连接、维度缩减和快捷连接将全局信息注入的输出 $（ℓ）I_G$ 和双边卷积的输出 $（ℓ）I_B$ 融合起来，获得CNN块（ℓ）I在第ℓ层的输出，即图像结构化的校正运动向量：

$$
(ℓ) \tilde{I} = CNN((ℓ)I) = \text{Down}\left((ℓ)I_G \Vert (ℓ)I_B\right) + (ℓ)I,
$$

其中 [·∥·] 表示通道上的连接，Down(·) 表示将通道减少到原始大小的一半。降维操作由两个1×1的卷积层组成，后面是批量归一化[50]和ReLU激活函数[51]，第一个卷积层减少了通道。注意实际上我们在层中简单地将相同的CNN块序列化三次，这足以获得令人满意的结果，证明了CNN骨干网络的有效性。在以下部分，我们将详细介绍CNN骨干网的主要组成部分，包括全局信息注入和双边卷积。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/024c5db93082468581f6e81f286f3547.png" width="70%" /> </div>


*1) 全局信息注入：*全局信息注入试图在每个CNN块的开始就考虑全局上下文，并将上下文注入有序运动向量中，以便检索整体空间变换。具体来说，考虑到CNN块输入的(ℓ)I ∈ R^{K×K×C}（如式(13)），我们对每个通道执行全局平均池化：

$$
(ℓ)I_G = \text{AvgPool}((ℓ)I),
$$

其中AvgPool(·)计算通道正交的平均值， $(ℓ)I_G ∈ R^{1×1×C}$ 呈现整个运动场的全局信息，以便在浅层次上获得全局上下文。然后我们将全局信息重新注入原始运动场中，注入是通过可学习权重和有序运动向量之间的逐元素乘法来执行的，其中权重学习由提取的全局信息(ℓ)I引导。全局信息注入的整体表达式如下：

$$
(ℓ)I_G = (ℓ)I \odot \sigma(FC((ℓ)I)),
$$

其中FC(·)是一个简单的全连接层，σ(·)表示sigmoid激活函数，⊙表示逐元素乘法， $(ℓ)I_G ∈ R^{K×K×C}$ 是第ℓ层全局信息注入的输出。全局信息注入的结构如图4所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e2824605ddaa4be3bc52bbe6b0d728b5.png" width="70%" /> </div>


*2) 双边卷积：*为了适应大场景视差的场景，双边卷积试图保留真实运动场中的巨大不连续性，这受到双边滤波器的启发。与标准高斯滤波器不同，双边滤波器是一种保留边缘和减少噪声的平滑滤波器[52]。核的权重不仅与空间位置有关，还与相对强度有关。对于输入信号M，双边滤波可以表示为：

$$
BF[M]_ p = \frac{1}{W_p} \sum_{q \in Rp} GS(\|p - q\|) \frac{GI(M_p - M_q)M_q}{GI(M_p - M_q)},
$$

其中BF[·]表示双边滤波器的输出，下标p或q表示特定元素，q是p的局部区域中的（即，q ∈ Rp），Wp是归一化因子，G(·)表示高斯核的权重，其中GS(·)是空间位置依赖的，GI(·)是强度依赖的。因此，Mp首先通过基于强度的滤波器进行调制，然后通过基于空间的滤波器，如(17)所示。由于GI(·)和GS(·)两个高斯核分别相对于强度和位置调整权重，当Mp周围的强度变化不大时，GI(·)变化稳定，(17)就像一个正常的高斯滤波器一样去除噪声（即， $BF[M]_ p ≈ \frac{1}{W_p} \sum_{q \in Rp} GS(\|p - q\|)Mq$ ），当强度急剧变化时，GI(·)抑制邻居的影响，(17)倾向于保留原始值（即， $BF[M]_p ≈ Mp$ ）。拥有这种意识，双边滤波器能够在滤除噪声的同时保留有意义的突变。因此，合理的认为双边滤波操作可以保留由大场景视差引起的真实运动场中显著的边缘（即巨大的不连续性），同时捕获良好的上下文。因此，为了利用数据驱动方法和双边滤波器的优势，我们引入了双边卷积。将(17)重写为：

$$
BF[M]_ p = \frac{1}{W_p} \sum_{q \in Rp} WS(p, q) \frac{WI(M_p, M_q)M_q}{WI(M_p, M_q)},
$$

其中WS(·)表示与位置相关的权重，WI(·)表示与强度相关的权重，归一化因子Wp可以相应地消除。对于基于强度的滤波器，我们进一步放宽它，使得p的调制权重WI(·)不仅基于q的强度，而且考虑了q周围局部区域的强度Mr，其中r ∈ Rq，新的公式可以写为：

$$
BF[M]_ p = \sum_{q \in Rp} WS(p, q) \sum_{r \in Rq} WI(q, r)Mr \frac{Mq}{WI(M_p, M_q)},
$$

其中与强度相关的权重定义为所有附近元素的加权和，如括号中所示，而不是(18)中的单个元素，并且加权求和本质上与卷积操作相同。因此，为了进一步将(19)应用到我们的对应学习网络中，我们将(ℓ)I替换为M作为输入，将其用矩阵形式重写：

$$
(ℓ)I_B = \text{ConvS}
\begin{bmatrix}
\text{ConvI}((ℓ)I \odot (ℓ)I) \\
\text{Intensity-based filter} \\
\end{bmatrix}
\odot
\text{ConvI}((ℓ)I),
$$

其中 $(ℓ)I_B ∈ R^{K×K×C}$ 表示第ℓ层的输出，结构如图5所示。与(17)中的双边滤波器相比，首先卷积块ConvI(·)感应邻居之间的不连续性的存在，以调整原始运动场的保留强度，通过逐元素乘法，充当基于强度的滤波器，然后ConvS(·)滤除运动场中的错误，充当基于空间的滤波器。Conv(·)函数包含两个3×3的卷积层，后面是批量归一化[50]和ReLU激活函数[51]。我们将在第IV-D5节中进一步验证双边卷积保留不连续性的能力，以证明其对具有大视差的两视图对应学习场景的有效性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/805bc36d2d704fb198452ecc1d3aec1c.png" width="70%" /> </div>


通过全局信息注入和双边卷积，有序的数字运动场通过特别设计的CNN骨干网络进行平滑和校正，同时注入全局上下文并保留巨大的不连续性。这意味着原始运动场(ℓ)F被转换为(ℓ) ̃F，然后(ℓ) ̃I可以重新扩展为序列 $(ℓ) ̃F_{grid} = {(ℓ) ̃fgrid_{j,k}}$ ，其中 $(ℓ) ̃fgrid_{j,k}$ 描述了新运动场(ℓ) ̃F中相应网格中心的运动向量。

## D. 无序运动向量恢复

理想情况下，由异常值引起的运动向量误差在校正后获得 $(ℓ)F_{grid}$ 。然而，单个Rectifying Conv Layer可能效果不佳，通常需要通过多层逐步滤除外部值[8]。为此，我们将校正后的有序运动向量转换回无序的。与(6)相同，我们隐式估计校正后的运动场 ˜F 并获取每个对应点的新运动向量：

$$
\hat{m}_ i =
\begin{bmatrix}
x_i, \tilde{F}(x_i)
\end{bmatrix}
$$  

$$
= G\begin{Bmatrix}
\{(ℓ) \tilde{m}_ {grid_{j,k}}\}, x_i
\end{Bmatrix},
$$

在高维空间中重写为矩阵形式：

$$
(ℓ+1)F = (ℓ) \tilde{F} = G
\begin{Bmatrix}
(ℓ) \tilde{F}_{grid}, (ℓ)X
\end{Bmatrix},
$$

其中 $(ℓ)X = {(ℓ)X_i}$ 应该是第 ℓ 层中 ${x_i}$ 的高维嵌入，以提供原始运动向量的位置信息。特别是，这样的信息在原始高维运动向量 (ℓ)F 中是可用的，我们可以将 (ℓ)X 替换为它，这也有助于我们的网络在规范化过程中检索可能丢失的信息：

$$
(ℓ+1)F = (ℓ) \tilde{F} = G
\begin{Bmatrix}
(ℓ) \tilde{F}_{grid}, (ℓ)F
\end{Bmatrix}.
$$

注意 (0)F 是从 (1) 获得的，G 定义与 (8) 相同。新的高维运动向量 $(ℓ) ̃F = {(ℓ) ̃f_i}$ 是下一层的输入，即 $(ℓ+1)F = {(ℓ+1)f_i}$ 。

## E. 内联预测器

通过 (23)，新运动向量 $(ℓ) ̃f_i$ 替换了 $(ℓ)f_i$ 在一个完整的 Rectifying Conv Layer 之后。判断对应点 $(x_i, y_i)$ 是否为内联点的常见方法是使用欧几里得距离比较 $(ℓ)f_i$ 和 $(ℓ) ̃f_i$ 的相似性，其作用是，内联点的运动向量在校正后不应有太大变化，而外联点的变化显著，从而使用阈值对内联/外联进行分类。然而，计算相似性会使训练过程容易不稳定。参考其他基于学习的方法 [11], [47]，在训练期间我们在每一层添加了一个额外的内联预测器，但在推理中只保留最后一个。内联预测器的输入是 $(ℓ) ̃f_i − (ℓ)f_i$ ，预测器将其映射到一维，然后输出用于分类的 $logit (ℓ) ˆz_i$ 。注意，只有在推理中才使用最后一层的输出 $ˆz_i = (L−1) ˆz_i$ 来分类内联点。内联预测器的结构如图 6 所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/7fcc1208f7414f02b120db55aadbe0bb.png" width="70%" /> </div>

## F. 损失函数
我们的 ConvMatch 输出类似于 PointCN[10]、OANet[12] 等的 $logits (ℓ) ˆZ = {(ℓ) ˆz_i}$ ，因此我们使用相同的分类损失函数和回归损失函数：

$$
L = \sum_{ℓ=0}^{L-1} L_{cls}(z, (ℓ) ˆz) + λL_{reg}(E, (ℓ) \tilde{E}),
$$

其中我们对所有层的分类和回归损失进行求和。分类损失函数 $L_{cls}$ 是一个简单的二元交叉熵损失，z 是通过计算 Sampson 距离 [53] 后，使用 10^{-4} 的阈值判断的弱监督标签。 $L_{reg}$ 也是通过 Sampson 距离获得的：

$$
L_{reg}(E, \tilde{E}) = \sum_{i=1}^{N} \left[ \frac{y_{T_i} \cdot \tilde{x}_i}{\|\tilde{x}_i\|_2} \right]^2 \left[ 1 + \|\tilde{x}_i\|_2^2 \right] + \left[ \frac{\tilde{E} \cdot y_i}{\|\tilde{E}\|_2} \right]^2 \left[ 1 + \|\tilde{E}\|_2^2 \right],
$$

其中 ̃E 是通过加权八点算法 [10] 计算的，∥v∥[m] 表示向量 v 的第 m 个元素。λ 是平衡两个损失函数的超参数。
## G. 实现细节
在我们的实现中，对应的坐标被归一化到 [-1, 1] 的范围，使用图像大小和相机内参（如果可用）。我们的 ConvMatch 包含相同堆叠 6 次的 Rectifying Conv Layer，这意味着 L = 6, ℓ ∈ {0, ..., 5}，我们在规范化过程中设置 K = 16 以平衡性能和时间，更多的细节在参数分析中讨论。假设对应点是通过 SIFT 特征和最近邻方法建立的，我们为每对图像提取多达 2k 个对应点。我们使用 Adam [54] 优化器进行训练，在前 80k 次迭代中学习率为 10^{-4}，之后的迭代中为 5 × 10^{-5}，批量大小为 32。权重 λ 在前 20k 次迭代中为 0，之后为 0.5，与 OANet [12] 相同。所有训练和测试都在 Ubuntu 18.04 上使用单个 NVIDIA RTX3090 GPU 进行。

# IV. 实验结果

我们进行实验以评估ConvMatch在相对姿态估计、单应性估计和视觉定位任务上的性能。此外，我们讨论了参数设置，并全面分析了我们的方法，以证明CNN骨干及其特定问题结构、规范化和修正过程的有效性。请注意，我们使用ConvMatch*来表示初步版本[18]中的实验结果。

## A. 相对姿态估计

相对姿态估计旨在估计捕获图像对的相机之间的位置关系（旋转和平移），通过预测内点来揭示异常值排斥方法的性能。在这方面，我们在户外和室内场景中评估了我们的ConvMatch，并将其与其他异常值排斥方法以及一些标准的稀疏匹配器和端到端密集匹配器进行了比较。

*1) 与异常值排斥方法的比较：*我们将所提出的方法与其他离群排斥方法进行了比较，它们都在双视图对应和几何估计管道上的同一位置。

*数据集：* 我们根据OANet[12]中的相同设置，利用户外的YFCC100M[55]和室内的SUN3D[56]数据集。YFCC100M由来自互联网的1亿张户外图像组成，分成72个序列[57]。我们选择68个序列作为训练和验证数据，其余4个序列作为测试。SUN3D由室内图像组成，这些图像从原始RGBD视频中每10帧采样一次。我们选择239个序列进行训练和验证，15个序列进行测试。在本文中，对于异常值排斥方法，使用SIFT特征和最近邻（NN）方法检测的输入假设对应点多达2k个。

*评估协议：* 我们计算不同阈值（5°、10°、20°）下旋转和平移的最大姿态误差的累积误差曲线下的面积（即AUC），以评估姿态估计的准确性[10]、[12]。此外，我们报告了F分数，该分数综合考虑了精确度和召回率，以评估内点/异常值分类性能[39]。我们认为如果对应点的极线距离[53]小于某个特定阈值（例如，10^-4），则是正确的。

*比较方法：* 我们将ConvMatch与经典的异常值排斥方法如RANSAC[7]、NGRANSAC[25]、CRC[28]、GMS[30]、LPM[8]、MCDM[33]、VFC[9]以及基于学习的异常值排斥方法如PointCN[10]、OANet[12]、CLNet[14]、LMCNet[13]、MS2DGNet[39]进行了比较。我们还添加了先前版本作为ConvMatch*进行比较，以验证新设计的网络结构的增强。

*结果：* 对于户外数据，使用RANSAC[22]作为鲁棒本质矩阵估计器的估计结果如表I所示。我们还报告了在没有它的情况下学习基础方法的结果，在表II中，相对姿态是直接从网络预测的本质矩阵计算得出的，使用了加权八点算法[10]。用于分类对应点的阈值设置为0，这意味着如果 $\hat{z}_i > 0$ ，则 $(x_i, y_i)$ 是内点。然而，我们发现阈值的大小确实影响了估计结果，尤其是在基于学习的方法中。因此，我们在不同阈值下测试了它们，并在图7中绘制了AUC@5°和F分数的结果。结合表I和图7，我们可以得出结论，无论使用固定阈值还是可变阈值，ConvMatch都比现有的最先进技术表现得更好，因为它在AUC指标上具有更高的AUC度量，或者在AUC和F分数指标上具有更高的峰值。即使我们不使用RANSAC作为鲁棒估计器，我们的方法也比OANet[12]的性能更好。对于室内数据，我们也分别在表III和表IV中报告了使用/不使用RANSAC的结果。ConvMatch仍然优于其他方法，提供了最好的AUC。我们进一步说明了异常值排斥和相对姿态估计的定性结果，如图8所示，ConvMatch能够捕获更强的局部和全局上下文，从而恢复更多的内点和更少的异常值，并且在一些其他方法失败的情况下（例如，视点和尺度变化剧烈（第4行）或只有稀疏纹理可用（第8行））仍然能够估计出良好的相对姿态。此外，新版本与以前的方法（ConvMatch*）相比有显著的改进（即，在不使用RANSAC的户外和室内场景的AUC@5°上分别提高了9.69%和3.65%）。这是由于我们为两视图对应学习任务特别设计的CNN骨干。我们将在第IV-D5节中进一步证明这一观点。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d330c35068c2405383029c7d075c4517.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/acd80b76eb4b4becb01570610e71082d.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/6e1073d694bc47309cb7e207e3a3af3d.png" width="70%" /> </div>


有些结果不寻常，值得解释，即在比较表III和表IV时，一些方法（例如，LMCNet、ConvMatch*和ConvMatch）在使用RANSAC作为鲁棒估计器而不是加权八点算法[10]时，姿态估计的准确性有所下降。作为一种假设和验证策略，RANSAC需要一个预定义的内点阈值，对噪声敏感，导致过滤掉一些可能有助于姿态估计的模糊内点，特别是在纹理较少的室内场景中。然而，加权八点算法考虑了每个对应点的内点置信度，因此高度依赖于预测的内点logits的准确性。与早期的工作（如OANet）相比，像ConvMatch这样更强大的算法可以预测更准确的内点logits，这一点从图9中可以看出，ConvMatch在图9中的真实正样本的logit值始终大于OANet，而对于假正样本，ConvMatch的值小于OANet。最好的预测允许加权八点算法在保证鲁棒性的同时使用所有已识别的匹配进行姿态估计，从而比RANSAC表现得更好。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e61680ee4ac6461e9545d02f653cdb66.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e0a36a8f51d9412aa0c1126f491c8475.png" width="70%" /> </div>


*2) 与匹配器的比较：* 我们进一步将ConvMatch与不同的匹配器进行了比较，而前者主要基于检测器-描述符方法，后者包括标准的稀疏匹配器和端到端的密集匹配器。我们还尝试在最先进的匹配器SuperGlue[40]之后应用ConvMatch，以查看它是否带来了显著的改进。

*数据集：* 我们使用与第IV-A1节相同的数据集。对于标准的稀疏匹配器，图像被调整大小，使得它们最长的维度等于户外场景的1600像素，或在室内场景中调整为640×480。我们为所有特征检测多达4k的关键点。所有设置几乎与SuperGlue[40]相同。对于端到端的密集匹配器，为了避免沉重的计算负担，我们将所有图像调整大小，使得它们最短的维度等于480像素，遵循PDC-Net[58]中的设置。

*评估协议：* 这里仍然报告AUC来表示性能。并参考SuperGlue的代码，使用RANSAC进行内点阈值为1像素除以焦距，而不是第IV-A1节中使用的恒定阈值，这提高了性能，如实验结果所示。

*比较方法：* 我们将ConvMatch与标准的稀疏匹配器和端到端的密集匹配器进行了比较。对于前者，我们选择SIFT[5]、SuperPoint[6]和D2Net[59]作为特征，使用最近邻（NN）方法和SuperGlue（仅在SuperPoint上，因为其他模型未公开）进行匹配。对于后者，我们选择了包括WarpC[60]、GLU-Net[43]、COTR[45]、PDC-Net[58]、PDC-Net+[44]和LoFTR[46]在内的最先进的密集匹配方法。此外，我们在SuperPoint+SuperGlue之后尝试应用ConvMatch以进一步过滤异常值。设置与LMCNet[13]类似，LMCNet不采用SuperGlue的过滤策略，而是保留所有对应点作为输入。注意，SuperGlue和密集匹配器的输入是与它们的位置相关联的视觉描述或整个原始图像，但我们的方法只需要假设匹配的坐标。因此，我们只将一些视觉信息（即，视觉描述、关键点的尺度和角度）嵌入到我们的方法中，以对齐输入并增强结果。此外，如图7所示，阈值的大小确实影响了估计结果，最佳性能出现在阈值约为2.0时。因此，我们将ConvMatch的阈值设置为2.0。我们将新模型称为ConvMatch†。在初步版本[18]中，我们发现新方法具有提高性能的巨大潜力，因此我们还尝试将ConvMatch†的主干替换为具有相同组件的U-Net类网络，将其称为U-ConvMatch†。所有这些努力都是为了表明，尽管本文的主要贡献是使用基于CNN的框架解决异常值排斥问题，但ConvMatch完全可以通过一些简单的设计来提高其性能。

*结果：* 户外和室内场景的结果分别显示在表V和表VI中。两个表都分为三个部分。顶部主要展示了ConvMatch通过简单的逐步修改逐步提高的性能。中间部分显示了广泛使用的标准稀疏匹配器，包括最先进的SuperPoint+SuperGlue。底部部分报告了端到端密集匹配器的性能。对于稀疏方法，ConvMatch在户外和室内数据上通过一些简单的额外设计超过了SuperPoint+SuperGlue。对于密集方法，在户外数据上，ConvMatch的性能甚至超过了LoFTR。在室内数据上，所提出的方法显示出有竞争力的结果，尽管密集匹配的视觉编码器比稀疏方法强大得多，可以在纹理较少的场景中捕获更丰富的信息，但计算使用量也更大。这些结果揭示了所提出方法的巨大潜力，我们只是探索了部分可能的改进，然后就赶上甚至超越了最先进技术。此外，我们在SuperPoint+SuperGlue之后应用ConvMatch以进一步过滤异常值，并获得了户外场景中的39.75/60.17/76.28和室内场景中的7.94/20.14/37.40的结果，这些结果远优于仅有SuperPoint+SuperGlue的结果（即户外场景中的38.46/58.77/75.05和室内场景中的7.49/18.73/34.86）。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/3bbc4ad7de564f4ea115e6b3bc8dc464.png" width="70%" /> </div>


## B. 单应性估计

单应性估计是计算机视觉中的一个基础但关键任务，旨在找到一个线性的图像到图像的映射。我们在HPatches基准[61]上进行单应性估计实验，使用鲁棒（RANSAC）和非鲁棒（DLT）估计器。

*数据集：* HPatches由116个场景和696张图像组成，其中57个场景在不同照明下拍摄，其他则经历了视点变化。每个场景由6张图像组成，一张作为参考，其他作为目标图像，具有地面真实单应性。我们使用SIFT检测多达4k的关键点，并使用最近邻（NN）方法进行匹配。

*评估协议：* 对于评价指标，我们遵循SuperPoint[6]的建议，采用单应性误差来分类估计是否准确，这里的阈值为3个像素。我们计算所有图像对的平均准确度作为Acc.，包括DLT/RANSAC估计器，以及用于内点/异常值分类的F分数（F.）。我们还报告了每对图像的平均时间成本。

*比较方法：* 我们比较了ConvMatch与几乎所有与相对姿态估计相同的基于学习的异常值排斥方法。我们使用在YFCC100M数据集上用SIFT特征训练的模型。

*结果：* 总结的结果在表VII中。ConvMatch凭借特别设计的CNN骨干在单应性估计中优于其他方法，无论是使用鲁棒（RANSAC）还是非鲁棒（DLT）估计器，都取得了最好的Acc.，尽管在F分数上只取得了竞争性表现。这是因为ConvMatch可以更全面和均匀地在图像中使用CNN捕获上下文信息，这对于单应性估计更加友好，即使在其他方法失败的困难场景中也是如此。请注意，新版本的ConvMatch相较于旧版本（ConvMatch*）的提升有限，因为单应性变换的运动场几乎没有不连续性，性能提升主要来自于全局信息注入，它更好地捕获了全局上下文。此外，我们的方法与最近的方法（CLNet、LMCNet和MS2DGNet）具有可比的推理时间。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/8495e1436f924bb19978c0a1add571c6.png" width="70%" /> </div>


## C. 视觉定位

在对重要的基础问题进行评估之后，对应学习的进步可以惠及诸如视觉定位[62]、[63]等实际问题，其目的是估计查询图像相对于3D模型的6-DOF位置。

*数据集：* 按照现有工作[64]，我们将我们的方法集成到官方HLoc[62]流程中进行视觉定位，并在Aachen Day-Night基准[63]、[65]上进行评估，该基准评估了在日间和夜间条件下的性能，因此需要准确的异常值排斥。基准提供了4328张Aachen城市的图像和922张查询图像，包括824张日间图像和98张夜间图像，由手机相机拍摄。

*评估协议：* 按照官方HLoc[62]流程，我们报告查询在几个距离和方向阈值内的姿姿估计准确性。具体来说，我们使用不同特征提取每张图像的多达4096个关键点，使用稀疏匹配器（例如，互近邻（MNN）方法与异常值排斥算法结合，或端到端匹配器SuperGlue[40]）进行匹配，从已知姿态的日间图像中三角化一个SfM模型，并使用预测的内点和COLMAP[66]注册日间和夜间查询图像。我们还尝试用端到端密集匹配器（如COTR[45]或LoFTR[46]）替换匹配过程，以进行更广泛的比较。

*比较方法：* 我们选择了与相对姿态估计任务几乎相同的比较方法。所有异常值排斥算法都基于MNN方法。

*结果：* 表VIII报告了视觉定位任务的结果。ConvMatch在日间和夜间场景中表现良好，并与最先进方法相比取得了有竞争力甚至更好的性能，特别是当与SuperPoint[6]或D2Net[59]结合时。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f9c72b5e3bed4408a7356817feab2c60.png" width="70%" /> </div>


## D. 分析

我们在这一部分进一步分析ConvMatch，包括在不同数据集上使用多个描述符的泛化能力，使用相同参数模型的评估，参数分析以确定网络结构和训练超参数，方法对异常值比例的鲁棒性，网络对网格偏移的稳定性，以及消融研究以揭示我们的CNN基础框架和特定问题结构的有效性。

*1) 泛化能力：* 现有的双视图对应学习方法通常在不同的描述符和场景中泛化能力较差，例如，在SIFT[5]描述符上学习到的模型几乎无法在SuperPoint[6]上工作，或者在户外场景中表现良好的模型很难在室内场景中排除异常值。然而，ConvMatch通过隐式构建运动场并自然地使用CNN纠正错误，可以减轻泛化能力的损失，而其他方法则直接学习内点和异常值的特征。为了证明ConvMatch的泛化能力，我们在户外数据集YFCC100M上使用RootSIFT[67]和SuperPoint，以及在室内数据集SUN3D上使用SIFT、RootSIFT和SuperPoint，使用仅在YFCC100M上使用SIFT训练的模型重复相对姿态估计。注意，我们生成的RootSIFT关键点多达2k，与SIFT相同，但SuperPoint关键点多达1k，并使用NN方法为所有条件获得假设对应点。如表IX所示，ConvMatch在所有情况下都取得了优越的性能，而考虑运动场的LMCNet[13]紧随其后，这证明了我们方法中CNN的鲁棒上下文整合能力，并暗示了通过纠正运动场而不是直接提取特征来解决异常值排斥问题更为自然。此外，与ConvMatch*相比的改进揭示了全局信息注入和双边卷积的效率。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b4037fd8225348d9b77dbd24c8be1527.png" width="70%" /> </div>


*2) 参数分析：* 层数L和网格数K的参数值对性能有很大影响，因为更大的L导致更多的修正过程，更大的K使有序运动向量更精细，理论上有助于提高性能，但消耗更多。经过多次尝试，我们选择L=6，K=16以实现性能和消耗的平衡。表X中不同L和K的户外相对姿态估计结果可以支持我们的选择，其中更大的L或K获得了相似的性能，但消耗更多，而更小的一个会导致姿态估计的明显下降。此外，超参数λ和它开始有效迭代的次数（记为Iter.）也可能影响训练过程，从而影响最终性能。我们也尝试了不同的值，并最终选择了λ=0.5和Iter.=20k，与OANet[12]相同。表XI中不同λ和Iter.的户外相对姿态估计结果可以支持我们的选择。过大或过小的值都会导致轻微的性能下降。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0979ce91ca7c4501853437502b6bbcc6.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c1478978d4214db8ad6e81ed18c5cc6e.png" width="70%" /> </div>


*3) 对异常值比例的鲁棒性：* 为了验证所提出方法对异常值的鲁棒性，我们尝试在不同异常值比例的数据集上重复户外姿态估计实验。具体来说，我们保留了所有由地面真实姿态标记的内点，并随机添加异常值，使异常值比例达到特定值。我们将异常值比例从70%变化到95%。我们使用在YFCC100M上使用SIFT训练的模型进行评估，并绘制了OANet[12]、MS2DGNet[39]、ConvMatch*[18]和ConvMatch随比例变化的AUC指标。结果如图10所示。尽管异常值比例的增加确实降低了所有方法的准确性，特别是当异常值比例高于90%时，所提出的ConvMatch在比例低于85%时波动较小，并且一致性地优于其他方法，显示出更好的鲁棒性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/6ee9551babf04faca6fa3280fd0efb6f.png" width="70%" /> </div>


*4) 对网格偏移的稳定性：* 为了验证ConvMatch在运动场生成期间的稳定性，我们通过沿X轴和Y轴方向扰动网格位置来测试模型（尤其是(7)中的插值操作）对网格偏移的稳定性。具体来说，我们沿X轴和Y轴方向平移网格点，并使用未进行平移的模型来估计相机相对姿态。我们在图11(a)中使用三次立方插值绘制了结果，稳定性通过AUC在网格沿图像长度的25%（即4个网格）偏移时没有显著变化来证明。而当偏移大于25%时，运动场将难以恢复，因此AUC急剧下降。此外，在图11(b)中，AUC图大约关于原点对称，证明了网络对偏移的各向同性。中心附近存在一个较大的平坦区域，代表了模型对网格偏移的强大稳定性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/755a2702547e4d8bbb3e7ac06b615496.png" width="70%" /> </div>


*5) 消融研究：* 我们在本节进行消融研究。我们方法的主要贡献可以概括为ConvMatch的四个主要组成部分，包括图2中显示的规范化和修正过程，基于CNN的骨干，全局信息注入和双边卷积，我们分别将它们记为R-R、CNN、Global和Bilateral，如表XII所示。首先，在第1行中，我们报告了完整的ConvMatch在YFCC100M数据集上使用SIFT描述符且不使用RANSAC的相对姿态估计性能。然后，我们逐步移除上述提到的不同组件，直到只剩下基线模型，该模型直接使用GAT网络与SuperGlue[40]相同对无序对应点进行分类。在第2行和第3行中，为了验证(16)中全局信息注入和(20)中双边卷积的可用性，我们分别移除了这两个组件中的每一个。我们还在第4行中移除了它们全部，其中CNN骨干仅由与完整ConvMatch相同深度的Resblocks[15]堆叠而成。与完整模型相比，全局信息注入和双边卷积可以单独使用来提高性能，并且将它们一起使用将进一步增强我们的模型，这表明专门设计的CNN骨干能够更好地捕获全局上下文并保留正确的运动场，尽管存在不连续性，从而实现更好的性能。然后，为了进一步证明CNN骨干的积极影响，该骨干比MLP更好地整合了上下文信息，并且更自然地修正了运动场，我们将CNN换成了MLP基础网络，后接Context Normalization[10]以捕获上下文。如第5行所示，性能下降，这表明CNN确实比MLP更好地捕获了上下文信息。在最后一行中，我们最终移除了规范化和修正过程，得到了基线模型。结果明显下降，这揭示了即使使用传统的基于MLP的网络后接全局或局部上下文捕获模块，对有序数据上的运动场进行修正也可以排除更多的异常值。表XII中的结果展示了每个组件的巨大贡献。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/736407185036466a9779c276577baefd.png" width="70%" /> </div>


此外，为了验证双边卷积能够在大场景视差情况下保留真实运动场中的不连续性（即，在双边滤波器的保留边缘的同时进行平滑），从而减少错误的匹配并恢复更多的正确匹配，我们在图12中显示了表XII中第4行（记为ConvMatch-Resblock）和第2行（记为ConvMatch-Bilateral）的异常值排斥的可视化结果。与前者（左列）相比，后者（右列）在保留远离建筑物的小灯的异常值（第1行和第2行）、保留更远椅子（第3行）和更近打印机（第4行）上的正确匹配项，在场景深度变化大的情况下，以及在受桌子上真实匹配影响的地面运动场上进行修正（第5行）。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b4135c49e456448b8f7bf4f31a3fcb8d.png" width="70%" /> </div>


# V. 结论

在本文中，我们设计了一个名为ConvMatch的新网络，采用卷积神经网络（CNN）作为骨干，而不是多层感知器（MLP），以捕获更好的上下文信息。本着密集运动场和稀疏运动向量可以相互转换的理念，我们规范化了候选运动向量，以便我们可以使用CNN纠正异常值的错误。我们并没有简单地堆叠通用的CNN网络，而是为双视图对应学习设计了特殊结构，直接捕获全局信息，并保留运动场中真实的、广泛的不连续性，这些结构构成了CNN骨干的主体。广泛的实验表明，我们的方法优于现有的最先进技术，并且设计的CNN骨干可以实现进一步的改进。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
