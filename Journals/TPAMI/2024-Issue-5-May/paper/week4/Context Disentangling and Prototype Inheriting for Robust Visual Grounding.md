# [Context Disentangling and Prototype Inheriting for Robust Visual Grounding](https://ieeexplore.ieee.org/document/10342826/)
## 题目：上下文解耦与原型继承：用于鲁棒视觉定位
**作者：Wei Tang; Liang Li; Xuejing Liu; Lu Jin; Jinhui Tang; Zechao Li**  
**源码：https://github.com/WayneTomas/TransCP**  
****
# 摘要
视觉定位（VG）旨在根据给定的语言查询在图像中定位一个特定目标。来自上下文的区分信息对于区分目标和其他对象非常重要，特别是对于那些与其他人具有相同类别的目标。然而，大多数先前的方法低估了这类信息。此外，它们通常设计用于标准场景（没有任何新对象），这限制了它们在开放词汇场景中的泛化能力。在本文中，我们提出了一个新颖的框架，通过上下文解耦和原型继承来实现鲁棒的视觉定位，以处理这两种场景。具体来说，上下文解耦分离了指代和上下文特征，实现了它们之间更好的区分。原型继承通过原型库从解耦的视觉特征中发现原型，并充分利用所见数据，特别是对于开放词汇场景。通过利用原型的解耦语言和视觉特征的Hadamard积融合特征，然后附加一个特殊标记并输入到视觉Transformer编码器进行边界框回归。在标准和开放词汇场景上进行了广泛的实验。性能比较表明，我们的方法在两种场景中都优于最先进的方法。

# 关键词
- 上下文解耦
- 开放词汇场景
- 原型发现
- 鲁棒定位
- 视觉定位（VG）

# 引言
视觉定位（VG）[1]，[2]，[3]，[4]旨在根据自然语言表达式在图像中定位一个特定对象或区域，如图1所示。这个被引用的对象或区域称为指代对象，它通常作为语言表达式的主题。这是一个基本任务，用于实现人机之间的认知交互[5]，例如视觉问答[6]、社交图像理解[7]和机器人导航[8]。视觉定位的目标是使计算机通过将人类语言与视觉数据联系起来，更好地理解人类语言。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e5a95f9b968e4dd481f67e6fe77d87b5.png" width="70%" /> </div>


先前的视觉定位方法大致可分为三类，即两阶段方法、基于锚点的单阶段方法和无锚点的单阶段方法（即基于Transformer的方法）。先锋两阶段方法[9]，[10]，[11]，[12]，[13]通过一组区域提议（例如，Fast-RCNN）和语言特征之间的跨模态匹配来学习指代对象的边界框。相比之下，基于锚点的单阶段方法[14]，[15]，[16]，[17]，[18]，[19]根据密集锚点的最大置信度和融合的双模态特征来生成边界框。最近的无锚点基于Transformer的方法直接通过将定位问题转换为坐标回归任务来生成边界框，从而摆脱了预定义的区域提议和锚点[2]，[4]，[20]，[21]。然而，先前的方法在从上下文中学习区分信息以更好地分离指代对象和其他上下文对象方面仍有改进空间。按照[2]，我们将上下文分为两种类型，即视觉上下文和语言上下文。我们认为除了图像中的指代对象之外的所有内容为视觉上下文，而除了表达式中的指代对象之外的所有内容，包括类别、属性、关系和关系对象（上下文对象）为语言上下文。有效利用上下文中的区别信息可以提高视觉定位的性能，特别是当指代对象与上下文对象具有相同类别时[1]，[2]，[5]，[22]。

此外，大多数先前的方法都是为标准场景（没有任何新对象）设计的，因此它们很难泛化到开放词汇场景（基础和新对象混合在一起），如图1所示。最近的工作尝试引入外部知识，如辅助数据集[23]或多模态知识图谱[24]，以学习有关新对象的知识。然而，添加的新对象越多，所需的外部知识就越多。一些其他工作如MDETR[25]、UniTab[26]和OFA[27]探索了大规模视觉-语言预训练模型，以更好地泛化开放词汇场景中的视觉定位。尽管如此，这些模型需要大量的计算资源，并且可能很难训练和微调。由于参数众多和架构复杂，整个模型在特定任务数据集上的微调甚至可能会破坏学习良好的特征。因此，需要替代的视觉定位方法，可以更好地利用数据中的潜在结构和模式。基于认知心理学和神经科学研究的发现[28]，[29]，无论新颖与否，基于粗粒度的聚类级别信息，即原型[30]，[31]，是人类固有的先验。然而，先前的方法通常基于细粒度的视觉和语言特征来定位指代对象，因此在开放词汇场景中受到限制。

受到这些观察结果的启发，我们提出了一个鲁棒的视觉定位框架TransCP，通过解耦上下文和继承原型来处理标准和开放词汇场景。我们首先增强了图像和文本中指代对象的区分能力。然后，原型通过原型库从解耦的视觉特征中发现并继承，充分利用所见数据来定位两种场景中的指代对象。

正如图 2 所示，TransCP 由四个模块组成，包括双分支编码器、上下文解耦模块、原型发现和继承模块以及边界框回归模块。具体来说，上下文解耦模块被用来从双分支编码器中解耦视觉和语言特征，以便更好地区分指代对象和上下文。在视觉分支中，显著对象（通常是出现在指代表达中的名词，包含指代对象）可以通过交叉注意力机制和歧视增强从其他对象或区域中显著区分出来。在文本分支中，上下文信息通过短语注意力模块得到强调，通过自适应地为上下文分配比指代对象更高的软权重。其次，原型发现和继承模块将分散的细粒度视觉语义信息凝聚到聚类级别的原型中，这些原型捕捉了对象的基本特征并存储在原型库中。在推理阶段，原型被继承以帮助定位指代对象，特别是开放词汇场景中的新对象。然后，这些视觉和语言特征通过无参数的哈达玛德积融合，这有助于鲁棒性。与依赖参数的多模态融合策略不同，哈达玛德融合更适合我们的方法。采用这种策略，解耦的语言信息作为注意力掩码进一步增强和选择与语义相关的视觉特征，有助于鲁棒性。此外，它避免了对已经很好地解耦的视觉和语言特征进行剧烈调整。最后，在边界框预测方面，我们将其建模为一个边界框回归问题，通过将融合的特征附加一个特殊标记输入到视觉Transformer编码器中。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/bb21ba9983a74fcd8b4d57851c59e726.png" width="70%" /> </div>


总结来说，我们的贡献如下：
- 我们提出了一个新的框架，用于鲁棒的视觉定位，能够处理标准和开放词汇场景，使其在两种场景下都更加通用和灵活。
- 我们设计了一个上下文解耦模块来分离指代对象和上下文，这更好地增强了两种类型特征的区分能力。
- 我们引入了一个原型继承模块，将分散的语义凝聚到聚类级别的原型中。在测试阶段，这些原型被继承，用于两种场景，特别是开放词汇场景。

# III. 提出的方法

## A. 概述
给定图像 $I$ 和语言表达式查询 $Q$，视觉定位的目标是预测语言表达式中所指对象的边界框。在TransCP中，我们尝试直接回归一系列四维离散边界框参数的序列 $B = \{b_i\}$。大多数先前的方法专注于标准场景。当它们遇到开放词汇场景时，它们往往会给出较差的性能。我们致力于在同一个框架中处理标准和开放词汇的视觉定位。如图2所示，我们提出的TransCP包含四个主要组件，即双分支编码器、上下文解耦模块、原型发现和继承模块以及边界框回归模块。

## B. 双分支编码器
与依赖于预定义区域特征的两阶段或基于锚点的一阶段方法不同，具有有限预定义区域的模型可能具有有限的表征能力。因此，对于我们的视觉分支，考虑了一个合作可训练的编码器，它构成了我们端到端流水线的基础组件。CNN主干关注局部特征，而后来结合的Transformer编码器则借助多头注意力和位置编码模块更多地关注全局特征。具体来说，整个图像 $I$ 首先被送入ResNet主干以提取2D视觉特征图 $F_C \times H \times W^v$。然后，堆叠的Transformer编码器被用来让模型更多地关注全局特征。最终，具有局部和全局属性的视觉特征表示为 $F'^v$。

$$
F'^v = E^v(I, \theta^v) \in R^{C \times H \times W}
$$

其中 $E^v(\cdot, \theta^v)$ 是上述的视觉编码器。 $\theta^v$ 表示编码器参数。类似地，对于语言分支，我们遵循其他工作，使用12层BERT作为我们的指代表达编码器。通过编码器，语言序列 $Q$ 被编码为一系列语言特征 $F^l$。

$$
F^l = E^l(Q, \theta^l) \in R^{C \times L}
$$

其中 $L$ 是文本序列的长度， $\theta^l$ 表示语言编码器中的参数。


## C. 上下文解耦

视觉定位中的一个关键问题是如何增强指代对象与上下文之间的区分度。为了实现这一目的，我们提出了一个上下文解耦模块。它由两个子模块组成，分别用于视觉和语言分支。视觉上下文解耦通过增强显著对象的特征（包含指代对象）并使用跨模态注意力和歧视系数来间接增强这种区分度。我们称这一过程为视觉上下文解耦。同时，语言上下文解耦模块达到了类似的目标，它通过自适应的语言特征后的短语注意力来关注上下文信息（例如关系、属性）。我们称这一过程为语言上下文解耦。

对于视觉上下文解耦模块，我们提议利用跨模态注意力来初步筛选出现在指代表达中的所有显著对象。语义特征 $F_s$ 首先通过语言特征的引导被聚合。具体来说，我们使用视觉特征 $F'^v$ 作为查询，语言特征 $F^l$ 作为键和值，以获得语义特征 $F_s$：

$$
F_s = \text{softmax}\left( \frac{W_Q F'^v (W_K F^l)^T}{\sqrt{\text{dim}K}} \right) \cdot (W_V F^l)
$$

其中 $W_Q$、 $W_K$ 和 $W_V$ 分别是查询、键和值在注意力机制中的投影权重。dimK 表示键的通道维度。 $F_s$ 可以被视为视觉和语言特征之间的共享语义（在某种程度上，它们代表语言特征中的显著对象）。一旦我们获得了语义特征 $F_s$，我们的目标是识别视觉特征中的显著对象。具体来说，语义特征 $F_s$ 和视觉特征 $F^v$ 被投影到相同的度量空间中，以测量它们的歧视系数，以进一步增强它们的区分度。

$$
E = \alpha \cdot \exp\left(-\frac{(1 - S(F'^v, F_s))^2}{2\delta^2}\right)
$$

其中 $S(\cdot)$ 是我们使用的相似性度量，我们使用简单的投影和点积在我们的模型中。 $\alpha$ 和 $\delta$ 是两个可学习的参数。歧视系数按像素对视觉特征 $F'^v$ 进行加权，以强调显著对象（见图2中的原型库之前的补丁，透明度为60%，或从原型库输出的红色和绿色补丁，透明度也为60%）。

$$
F^v = F'^v \cdot E
$$

更清楚地说，由于方程（4）的高斯形式，如果视觉特征 $F'^v$ 与语义特征 $F_s$ 相关，它们的系数值会高于与不相似的特征，并且有明显的差距。因此，视觉特征中指代对象（包含在显著对象中）与上下文的区分度得到了增强。这使得模型对具有较高系数的显著对象更加敏感，同时抑制了具有较低系数的上下文。

对于语言上下文解耦模块，我们以类似的方式处理语言特征。然而，在这一分支中，语言特征中包含的上下文信息通过更高的软权重得到强调，而指代对象信息则通过分配较低的注意力权重来保留。因此，语言特征中指代对象和上下文之间的区分度得到了增强。实际上，在预训练的BERT之后，利用Bi-GRU网络进一步适应由预训练BERT提取的语言特征 $F^l$ 以适应所见数据。

$$
F'^l = [\rightarrow F'^l, \leftarrow F'^l] = E^l(F^l, \theta'^l) \in R^{C \times L}
$$

其中 $F'^l$ 表示适应后的语言特征，它连接了Bi-GRU网络的双向输出。 $E^l$ 和 $\theta'^l$ 分别是Bi-GRU编码器及其参数。然后，适应后的语言特征 $F'^l$ 被用作短语注意力的输入，后者为每个单词分配软注意力权重，如下所示。

$$
\phi = \text{softmax}(F^C(F'^l))
$$

其中 $F^C(\cdot)$ 是一个全连接层。短语注意力是随机初始化的，并由成对训练数据监督。当我们获得注意力权重时，它们将被加权到语言特征上。

$$
\hat{F}^l = F'^l \cdot \phi
$$

总之，短语注意力重新考虑了语言特征中每个单词的不同重要性。上下文通过更高的注意力权重得到突出，而指代对象则通过分配较低的权重来保留，从而增强了指代对象和上下文之间的区分度。

## D. 原型发现和继承
来自视觉分支的特征是分散的，包含大量的低级特征。有时这些低级特征可能对视觉定位有害，因为语言表达通常表现为高级和抽象信息。相反，聚类级别的信息，即原型，对于两种场景都有帮助，因为原型反映了对象的基本属性，并且可以作为粗粒度的指导。因此，我们提出了一个原型发现和继承模块，该模块利用原型库。有了原型库，我们可以在训练阶段从分散的视觉特征中发现聚类级别的原型。这些原型随后被存储在库中，并在推理阶段继承，以协助标准和开放词汇的视觉定位。图2中用原型标识符标记的彩色补丁表示通过利用原型获得的量化视觉特征。具体来说，我们使用的原型库定义如下：

$$
P = \{p_i\}, i = 1...k
$$

其中 $k$ 是原型的数量，每个原型是一个具有 $c$ 维的嵌入向量。接下来，分散的细粒度视觉特征被分配到这些聚类级别的原型：

$$
F'^v = p_j
$$

其中 $j$ 是被分配的索引。它可以通过以下公式计算：

$$
j = \text{min} \, \text{dist}(\hat{F}^v, P)
$$

其中 $\text{dist}(\cdot)$ 是距离度量，我们使用欧几里得距离，因为它通常用于度量两个视觉特征之间的距离。实际上，我们使用的策略是最近邻搜索。然而，原型是离散的。从原型库中选择最小距离的方式不是可微的，并且不能通过梯度反向传播进行优化。为了解决这个问题，我们采用了[57]中的优化方法，使用停止梯度策略。具体来说，可以通过重新制定的公式(11)进行优化：

$$
\hat{F}^v = \text{sg}[p_j - \hat{F}^v] + \hat{F}^v
$$

其中 $\text{sg}[\cdot]$ 是在参数上停止梯度的操作。先前参数的梯度通过辅助项 $\hat{F}^v$ 传递，这使得原型库可训练。

与大多数基于深度学习的聚类方法类似，我们的原型库也需要在一个小批量内更新原型。传统的聚类方法，例如k-means，通过计算分配给中心点的数据的平均值来更新聚类中心。按照[56]，我们采用动量平均优化来代替直接计算视觉特征 $F^v$ 分配给原型的平均值。如果属于原型的视觉特征数量不为零，则将特征的平均值更新为新的原型。

通常，原型库对视觉特征执行小批量深度学习基础的聚类。通过它，具有相同或相似语义的视觉特征被聚合在一起。分散的视觉特征随后通过利用聚类级别的原型进行转换。这样的转换将有利于跨模态匹配，因为原型反映了对象的基本属性，并且从语义尺度的角度来看，与语言特征更接近。此外，存储在原型库中的原型在推理阶段被继承，以指导标准和开放词汇场景，其工作方式与查找表相同。

## E. 边界框回归
当我们获得量化的视觉特征 $\hat{F}^v$ 和解耦的语言特征 $\hat{F}^l$ 后，下一个问题是如何融合这些特征以获得用于边界框回归的多模态特征。传统方法采用复杂的融合方法，有时会忽略视觉和语言特征之间的平衡。与这些方法不同，我们提出使用简单但有效的汉德霍尔特积作为融合策略。解耦的语言信息作为注意力掩码，进一步增强和选择与语义相关的视觉特征，这有助于鲁棒性。此外，无参数的汉德霍尔特融合策略保持了解耦的语言特征和解耦的视觉原型之间的重要性平衡，避免了对学习良好的特征进行剧烈调整。

$$
F^m = \tanh(\hat{F}^v) \odot \tanh(\hat{F}^l)
$$

其中 $F^m$ 是融合的多模态特征。受到TransVG[21]的启发，我们的架构还遵循了可学习的特定嵌入（即，[REG]标记）用于边界框回归的设计。与TransVG中使用的Vision-Language Transformer编码器不同，该编码器通过视觉和语言标记的连接执行模态内和模态间关系推理，我们只利用视觉Transformer编码器作为替代方案。这一决定是基于这样一个事实，即多模态特征已经通过汉德霍尔特融合得到。此外，由于回归将受到dropout层的方差漂移问题的影响[58]，我们通过将dropout概率设置为零来减少这些层的影响，从而在回归Transformer编码器中解决了这个问题。

$$
[REG] \text{output} = E^r([\text{REG} \text{initial}, F^m], \theta^r)[0] \in R^{C \times 1}
$$

其中 $E^r$ 是视觉Transformer编码器， $\theta^r$ 是相应的权重。此外，[REG]标记是随机初始化的，并连接到 $F^m$ 的开头。然后，[REG]标记的输出状态被用作边界框回归头的输入。回归头由三层MLP层和两个相应的ReLU激活函数组成。它的输出是按(x, y, w, h)组织的序列B，表示回归边界框的左上角坐标、宽度和高度。

$$
B = \text{MLP}([\text{REG} \text{output}])
$$

## F. 训练和推理
与依赖于区域提议或预定义锚点的传统视觉定位方法不同，我们的框架直接回归包含确定图像中边界框所需参数的序列，给定指代表达式。在模型的训练阶段，我们采用广泛使用的L1和IoU损失作为损失函数。

$$
L = \lambda_{l1} L_{l1}(B, \hat{B}) + \lambda_{GIoU} L_{GIoU}(B, \hat{B})
$$

其中 $b$ 和 $\hat{b}$ 分别表示真实边界框和回归边界框。 $\lambda_{l1}$ 和 $\lambda_{GIoU}$ 是两个权衡因子，用于平衡这两个损失，经验设置为5和2。此外，在推理阶段，原型库被冻结，像查找表一样工作。通过原型库，聚类级别的原型被继承以帮助在两种场景中定位指代表达对象，特别是开放词汇场景。



# IV. 实验

## A. 数据集
ReferIt: ReferIt [59]，也被称为ReferCLEF，包含20000张标记图像。该数据集包括一些不确定的查询，例如“任何”，“整个”以及一些错误标记的数据。按照之前的作品[2], [21]，为训练、验证和测试分割了三个子集，分别包含54,127, 5,842和60,103个查询。

Flickr30k Entities: Flickr30k Entities [60]中的每张图像都附有五个参考表达式。该数据集包含31783张图像和427k个短参考短语，而不是长句子。此外，它不专注于具有相同类别的多个对象的图像。

RefCOCO/RefCOCO+/RefCOCOg: 这些数据集的图像都来自MSCOCO [61]数据集。RefCOCO包含19,994张图像中的50,000个对象和142,209个参考表达式。它被正式分割为训练、验证、testA和testB，分别有120,624、10,834、5,657和5,095个表达式。对于testA，表达式主要指代人物，而在testB中，它们主要与除人物之外的各种对象相关。RefCOCO+包含19,992张图像中的49,856个对象和141,564个参考表达式。与RefCOCO不同，RefCOCO+中禁止使用绝对位置表达式，例如“右边”，“顶部”和“旁边”。因此，RefCOCO+更加关注指代对象的外观属性。训练、验证、testA和testB被正式分割，分别有120,191、10,758、5,726和4,889个表达式。RefCOCOg有25,799张图像、49,822个对象和95,010个相应的参考表达式。该数据集有两个分割版本，即RefCOCOg-google [33]和RefCOCOg-umd [32]。为了简化第IV-E节中的开放词汇评估，我们只报告了RefCOCOg-google（val-g）上的结果，遵循[1], [18], [19], [20]和[62]。

Ref-Reasoning: Ref-Reasoning [40]是建立在GQA [63]之上的大规模真实世界数据集，用于结构化指代表达式推理，包含83,989张图像中的791,956个指代表达式。指代表达式是基于Visual Genome数据集[64]的场景图，通过一组表达式模板自动生成的。它包含1,664个对象类别、308个关系类别和610个属性类别。因此，生成的表达式极其复杂，每个表达式最多描述5个对象及其关系。数据集被划分为训练、验证和测试集，分别包含721,164、36,183和34,609个图像-表达式对。重要的是要注意，为了验证所提出方法在面对极其复杂的表达式时的泛化能力，我们在Ref-Reasoning上进行了额外的评估，如第IV-F节所示。

## B. 实施细节
数据预处理：我们遵循[21]和[2]，将每个输入图像调整为640 × 640，同时保持原始的纵横比。较长的边会被调整，较短的边会被填充。对于指代表达，我们将最大查询长度设置为20，包括特殊标记[CLS]和[SEP]。同样，较长和较短的查询将分别被截断和填充。

指标：遵循最近的作品[2], [21], [47], [48]，我们采用top-1准确率(%)作为我们的评估指标。如果回归的边界框与真实边界框的IoU大于0.5，则被认为是正确的。

训练细节：我们使用AdamW优化器来训练模型。初始学习率为我们的模型设置为1e-4，对于双分支特征编码器，我们将学习率设置为1e-5。CNN主干和Transformer编码器的权重由DETR模型[65]初始化。对于公式(4)中的可学习参数α和σ，我们经验性地将它们的初始值分别设置为1和0.5。我们使用Xavier初始化作为我们模型中其他参数的初始化策略。我们训练RefCOCO/RefCOCO+/RefCOCOg 100个周期。对于其他数据集，经验性地设置为90个周期。此外，在60个周期后，我们将学习率降低了10倍。为了避免原型发现和继承模块中的冷启动问题，即由于随机初始化原型导致的错误的优化方向，我们在前10个训练周期内冻结了CNN主干。由于我们将视觉定位问题视为坐标回归问题，应该注意到dropout层中存在的方差漂移问题[58]。作为解决方案，我们将边界框回归模块中的dropout概率设置为零。按照[2]，我们经验性地将损失函数中的权衡系数设置为λl1 = 5和λGIoU = 2。

推理：我们在标准场景上训练我们的模型，并根据最佳验证准确率进行测试。例如，我们在ReferIt数据集上训练和验证。在测试阶段，我们将其分为两个不同的部分。首先，我们在标准数据集上测试模型。其次，我们测试它以验证开放词汇定位的鲁棒性。为了简化开放词汇测试，我们训练模型1)在ReferIt上训练并在RefCOCO/RefCOCO+/RefCOCOg和Flickr30k实体上测试；2)在RefCOCO上训练并在ReferIt和Flickr30k上测试；3)在Flickr30k上训练并在RefCOCO/RefCOCO+/RefCOCOg和ReferIt上测试；由于这种跨数据集测试完全符合包括基础和新颖对象的开放词汇场景的定义。

## C. 比较方法
为了评估我们的TransCP的性能，我们将其与不同类型的视觉定位方法进行了比较。

两阶段方法：CMN [9], VC [10], ParalAttn [11], MAttnNet [12], Similarity Net [13], CITE [66], DDPN [67], LGRANs [68], DGA [69], RvG-Tree [70], A-ATT [43], NMTree [71] 和 Ref-NMS [72]。

一阶段基于锚点的方法：SSG [14], ZSGNet [15], FAOA [16], RCCF [17], ReSC-Large [18] 和 LBYL-Net [19]。

基于Transformer的无锚点方法：VGTR [4], Pseudo-Q [48], RefTR [47], SeqTR [20], VLTVG [2] 和 TransVG [21]。

此外，LBYL-Net [19], RefTR [47], VGTR [4], SeqTR [20], VLTVG [2], 和 TransVG [21] 作为开放词汇场景的比较方法，在我们的环境中使用相同的随机种子、批量大小和PyTorch版本重新训练，标记有†符号。无监督方法Pseudo-Q [48]用∗符号标记。特别是，VGTR和SeqTR适应了适当的语言嵌入层大小进行开放词汇测试。

## D. 标准场景中的比较

在表I中，我们报告了我们的TransCP和其他最先进的方法在ReferIt和Flickr30k实体测试集上的top-1准确率(%)。为了简化第IV-E节中的实验，我们仅报告了使用ResNet-50（除了SeqTR使用DarkNet-53）作为主干的基于Transformer的模型的性能。比较的模型被分为三种类型，即两阶段方法、一阶段方法（一阶段基于锚点的方法的缩写）和基于Transformer的方法（即一阶段无锚点方法）。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/6faec6d7651b415fa77f201d96cf0f5e.png" width="70%" /> </div>


我们可以观察到，我们的模型在两个数据集上都取得了最佳性能。具体来说，TransCP在ReferIt测试集上获得了72.05%的top-1准确率。从两阶段模型的角度来看，尽管我们的模型可能拥有更强的视觉后端，如MAttnNet、CITE和DDPN，但我们的模型显著优于这些模型。这一现象表明，建立在预定义区域提议之上的两阶段方法可能没有被优化，因为它们的表征能力受到预训练的对象检测器的限制。从一阶段组来看，我们可以观察到，我们在ReferIt数据集上的性能比最强的一阶段模型LBYL-Net高出绝对4.58%。对于基于Transformer的组，我们的方法在这些方法上取得了一致的改进。具体来说，我们的方法在Flickr30k实体测试集上使用相同规模的主干超过了Psueduo-Q、RefTR、VGTR、VLTVG和SeqTR，绝对准确率分别为19.63%、5.29%、4.60%、1.42%和3.78%。与我们的基线方法TransVG相比，TransCP在ReferIt和Flickr30k实体上分别实现了高达2.19%和1.97%的绝对改进。

为了进一步验证我们提出的框架，我们分别在RefCOCO、RefCOCO+和RefCOCOg上进行了实验。我们的方法与其他最先进方法的比较结果如表II所示。从结果中可以看出，我们的方法一致地超过了两阶段方法（A-ATT、NMTree、Ref-NMS等）有显著的优势。具体来说，TransCP比最强的两阶段方法Ref-NMS高出绝对3.55%、3.38%和3.74%，在RefCOCO的所有三个分割集上（即，val、testA和testB）。他们次优结果的原因是相同的，因为他们依赖于预训练的对象检测器，这并不涉及视觉定位的训练步骤。当语言表达变得长而复杂时，即在RefCOCOg中，我们的方法显示出有希望的性能72.06%，超过了所有以前的最先进方法。对于一阶段方法，我们可以观察到最好的LBYL-Net在RefCOCO上低于最强的两阶段方法Ref-NMS，但在RefCOCO+上略高于Ref-NMS。令人惊讶的是，我们的TransCP在RefCOCO+的val、testA和testB上分别超过了最好的LBYL-Net绝对4.11%、4.53%和3.91%。与基于Transformer的方法相比，我们可以观察到它们都超过了以前的一阶段方法（除了无监督方法Pseudo-Q）。这可能是因为Transformer提供了比传统的基于YOLO的一阶段方法更有希望的跨模态交互。从基于Transformer的方法的角度来看，我们的方法在RefCOCO和RefCOCOg数据集上一致地实现了最佳性能。当RefCOCO+禁止绝对位置表达式时，我们的方法在val和testA上分别实现了73.07%和78.05%。结果超过了最具竞争力的方法VLTVG。然而，我们的TransCP在RefCOCO+ testB上略低于它。这个结果表明，在充分利用指代对象的区分信息方面，我们的方法仍有改进空间。与我们的基线TransVG相比，我们的方法在RefCOCO、RefCOCO+和RefCOCOg的所有分割上都取得了全面改进，因为我们充分利用了解耦特征。此外，TransCP从原型发现中学习了更具代表性的特征。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/a84a2090cea448ac91b9ffd7fb21667d.png" width="70%" /> </div>


总之，实验结果证明了我们的方法在标准场景中的优越性。

## E. 开放词汇场景中的比较
为了更全面地了解提出的TransCP，我们在开放词汇设置下进行了实验。具体来说，我们在标准场景中训练模型，并在第IV-B节中详细说明的三种不同的开放词汇场景中测试它们。

在表III中，我们报告了开放词汇性能，模型在ReferIt上训练，并在RefCOCO/RefCOCO+/RefCOCOg和Flickr30k实体上测试。我们可以观察到TransCP在Flickr30k和RefCOCOg数据集上取得了最佳性能。然而，与VLTVG相比，我们的方法在RefCOCO上取得了可比的性能，在RefCOCO+上略低。这可能是因为ReferIt中的表达非常复杂，包含错误标记的区域和模糊的查询，如“任何”和“没有”。低质量的上下文信息可能对原型的发现和继承产生负面影响。此外，RefCOCO+中禁止使用关系如“右边”和“旁边”。然而，我们的模型依赖于解耦的语言特征。如果关系的区分信息不可用，它也可能对我们的模型在开放词汇场景中产生负面影响。这一结论也可以通过图6中的可视化总结。相反，VLTVG只关注获得显著的特征，这在面对RefCOCOg中的长而复杂的指代表达式时，受低质量表达式和不工作的关系的影响较小。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/cc4791b6790b4cf09ddf3047846cb19e.png" width="70%" /> </div>


在表IV中，我们报告了开放词汇性能，模型在RefCOCO上训练，并在ReferIt和Flickr30k实体数据集上测试。我们的方法在测试分割上取得了最佳性能。与在ReferIt上训练并在RefCOCO上测试的模型相反，当我们在RefCOCO上训练并在ReferIt上测试时，TransCP以显著的优势超过了VLTVG。这种现象证明了我们的假设，即训练数据量少会对原型学习产生有害影响，从而在开放词汇场景中产生负面影响。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/42b285f540f644949b5fdca59d44d1c5.png" width="70%" /> </div>


我们可以从表V中观察到，我们的方法在ReferIt、RefCOCO+和RefCOCOg上一致地超过了当前的最先进方法。此外，它与VLTVG在RefCOCO数据集上取得了可比的性能。与基线TransVG方法相比，我们的方法在所有分割上实现了超过2%的绝对改进。此外，由于Flickr30k实体中的表达式是短语，并且不专注于具有相同类别的多个对象的图像，因此在面对RefCOCOg中的长而复杂的指代表达式时，仅基于显著分数增强指代对象在VLTVG中将不会有效。

此外，从表III、表IV、表V中可以发现，VGTR和SeqTR（语言分支的LSTM特性）对处理开放词汇场景几乎没有泛化能力，而LBYL-Net、RefTR、VLTVG、TransVG和TransCP（语言分支的BERT特性）具有一定的泛化能力。这一现象表明，良好的语言特征在开放词汇场景（当然在标准场景）中)具有重要的意义，因为模型需要全面理解在图像中识别哪个参照点。我们在IV-G节的消融研究也证实了同样的结论。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/88eeffc5d1fc4334a88673f8d0276b55.png" width="70%" /> </div>


## F. 在Ref-Reasoning上的比较
为了评估TransCP在面对开放词汇场景中的极其复杂的指代表达式时的泛化能力，我们在开放词汇设置下对Ref-Reasoning数据集进行了实验。具体来说，我们在与ReferIt相同的设置下训练Ref-Reasoning训练集。然后，我们使用训练好的模型在Flickr30k实体、ReferIt和RefCOCO数据集上进行开放词汇验证。

如表VI所示，通常，TransCP在大多数开放词汇场景中的性能超过了比较的方法。例如，在Flickr30k测试分割上，TransCP的准确率分别超过了VLTVG和TransVG 0.69%和1.01%。特别是对于像RefCOCO testB分割这样包含更多样化对象集合的数据集，我们的方法达到了绝对46.59%，显著优于VLTVG（+2.06%）。结果表明，从训练数据中学习到的解耦特征和原型的模式比其他方法学习到的模式更适合开放词汇场景。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/581008a0eba449a4a9fabd2e1fbcb737.png" width="70%" /> </div>


此外，我们通过结合在ReferIt上训练并在RefCOCO上测试（表III）的实验，提供了全面的分析。需要强调的关键区别在于，ReferIt也是一个包含大量模糊表达式的复杂数据集，这使得转移到开放词汇场景变得具有挑战性。ReferIt和Ref-Reasoning之间的关键区别在于，ReferIt包含许多模糊的表达式，而Ref-Reasoning包含更长和更复杂的表达式。从表VI和III中，我们发现在Ref-Reasoning上训练并在Flickr30k实体上测试的模型（VLTVG、TransVG和TransCP）实现了与在ReferIt上训练并在Flickr30k实体上测试的模型相似的性能。因此，我们考虑Flickr30k实体上的性能作为一个锚点，以便我们可以同时比较表VI和III中的结果。可以得出以下一些观察结果：

1) 当处理训练集中极其复杂的表达式时，从所见数据中学习适合开放词汇场景的信息更具挑战性。在Ref-Reasoning上训练并在RefCOCO上测试的模型（VLTVG、TransVG和TransCP）的开放词汇性能比在ReferIt上训练并在RefCOCO上测试的模型下降了超过15%。

2) 当指代表达式的复杂性增加时，TransCP显示出比比较方法更强的鲁棒性。从表III中，我们可以观察到VLTVG在RefCOCO上略微优于我们的方法。然而，从表VI中，模型经过再推理训练，包含更长、更复杂的关系时，我们的方法获得了比VLTVG获得更好的开放词汇表性能，如val分割（+0.83%）和testB分割（+2.06%）。

## G. 消融研究

首先，我们研究了不同的融合策略的影响，包括无参数的汉德霍尔特融合和参数依赖的视觉-语言融合。然后，为了探究我们模型中不同组件的影响，我们对在RefCOCO数据集上训练的模型进行了消融研究。消融研究在汉德霍尔特融合、视觉上下文解耦、语言上下文解耦和原型发现及继承模块上实施。这些组件分别简称为“H.D.”、“L.D.”、“V.D.”和“P.T.”。我们还在这一节中对原型库的不同大小和用于边界框回归阶段的视觉Transformer编码器的不同层数进行了实验。为了进一步分析所提出的TransCP，提供了在训练和推理阶段所需时间的比较。

1) 融合策略：在表VII中，我们展示了结合不同融合策略的TransCP的结果。基于结果，我们验证了TransCP中使用的H.D.有助于两种场景下的鲁棒性。我们可以观察到，在标准场景下的所有RefCOCO分割中，使用H.D.融合策略的TransCP比配备V.L.的模型高出约1%。在开放词汇场景下，使用H.D.的TransCP的优势更加明显。具体来说，使用H.D.的TransCP分别达到了29.01%和27.71%的绝对提升，这比使用V.L.的模型高出超过3%。H.D.在两种场景中有助于鲁棒性的原因可能如下：首先，解耦的语言信息作为注意力掩码进一步增强和选择与语义相关的视觉特征，这有助于鲁棒性。另一个原因是汉德霍尔特融合是一种无参数策略。与V.L.不同，后者从训练数据中学习视觉和语言特征之间的动态融合重要性，H.D.保持两种类型特征的同等重要性。它避免了对学习良好的解耦视觉和语言特征进行剧烈调整。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/1ffe33354def4e6481093751018b838d.png" width="70%" /> </div>


2) TransCP的组件：在表VIII中，我们展示了在RefCOCO数据集上使用不同模块的TransCP的结果。我们可以得出以下几个观察结果。首先，如果没有我们在本文中使用的所有模块，我们的模型实际上退化为基线模型TransVG [21]。接下来，我们研究了分别只结合L.D.、V.D.和P.T.的模型。我们可以观察到，仅使用L.D.、V.D.和P.T.，模型在标准和开放词汇场景中的表现是合理的。例如，仅使用L.D.时，模型仅比基线提高了约2%。这一现象表明，仅使用L.D.或V.D.时，模型仍无法充分利用从解耦特征中挖掘出的区分信息。这两个模块是相互依赖的。V.D.使模型能够专注于最有可能的几个区域，而L.D.可以利用区分性的上下文信息（关系、属性等，见第V-A节的更多细节），以进一步确定真正的指代对象。当我们关注仅使用P.T.的模型时，我们观察到它达到了与TransVG相当的性能。这表明P.T.的有效性取决于先前解耦的视觉特征。这些解耦的视觉特征确保了原型发现模块可以学习到好的原型，从而增强了模型在两种场景下的鲁棒性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b84f4912a12d4236b3547f378cd585e7.png" width="70%" /> </div>


在引入汉德霍尔特积融合模块后，我们可以观察到在标准场景（RefCOCO）和开放词汇场景（ReferIt）中性能分别提高了两个百分点以上。这些改进可能是因为适应后的视觉和语言特征已经是高质量的。两种不同融合策略的详细比较在第IV-G1节中讨论。基于结果和分析，最好在应用边界框回归模块之前，使用无参数策略融合两种解耦特征，以保持它们的同等重要性。其次，通过利用语言解耦，TransCP的性能得到了进一步提高。例如，在RefCOCO testB数据集和ReferIt test数据集上，TransCP分别实现了1.86%和2.40%的绝对改进。这一现象表明，通过解耦语言特征并进一步增强上下文，指代对象和上下文之间的区分度得到了改善。当我们用V.D.替换L.D.时，性能与使用H.D.和L.D.的模型相当。对于仅使用V.D.的情况，我们可以发现H.D.对开放词汇场景而不是标准场景有所贡献。这进一步证明，不仅原型发现模块有助于泛化特性，而且其他设计，包括上下文解耦模块和汉德霍尔特积融合策略也有贡献。此外，H.D.与P.T.结合的情况也支持了我们的发现。第三，令人惊讶的是，在标准场景中加入视觉上下文解耦并没有带来很多改进。然而，我们很高兴观察到视觉上下文解耦可以通过大约1.26%的绝对提升进一步增强开放词汇场景下ReferIt数据集验证集的性能，以及在测试集上0.52%的提升。我们从经验中推断，语言解耦已经为我们的模型提供了足够的能力，从标准场景数据集中学习丰富的区分信息。因此，当TransCP在标准场景中达到瓶颈时，增加视觉上下文解耦可能无法有效克服它。相反，TransCP在开放词汇场景中的表现并没有表现出这样的瓶颈。解耦的视觉特征突出了显著的对象。然后，解耦的语言特征提供了突出的上下文信息，这有助于模型更好地区分同一类别中的新颖指代对象。最后，结合原型的完整模型克服了前述的局限性。从所见数据中挖掘出的聚类级原型不仅有利于标准场景，导致性能提高了约0.3%，而且在开放词汇场景的ReferIt测试数据集上还实现了额外的0.6%绝对增益。通过将这两种类型的特征的尺度拉近，原型可以协助Transformer在边界框回归中进行视觉和语言特征的语义匹配。此外，从开放词汇场景中完整模型的结果来看，这些原型的继承可以指导两种场景中的下游定位任务。

3) 原型库的大小：图3显示了原型库大小不同对模型性能的影响。我们可以从结果中观察到几件事情。首先，原型库的不同大小对我们模型在标准视觉定位中的性能影响很小。原型库的大小实际上控制了从所见数据中提取的原型的粗糙度。由于标准视觉定位场景拥有有限的类别，通常所有原型都可以在训练阶段的许多周期内从不同的粗糙度中学习。其次，当处理开放词汇场景中的新对象时，我们模型的性能在很大程度上受到原型库大小的影响。过大或过小的原型库都不会带来令人满意的性能。原型越粗糙，概念信息的歧义性和普遍性就越大，反之亦然。这表明在将这些原型转移到处理开放词汇场景时，我们应该很好地控制粗糙度。为了在标准和开放词汇场景中实现平衡，根据图3，我们选择在我们的提议模型中使用2048的大小。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e4e616c5a6c64a7d9f83c86a53c381be.png" width="70%" /> </div>


4) Transformer编码器层数：图4显示了在边界框回归阶段使用不同数量的视觉Transformer编码器层对性能的影响。我们可以观察到，当我们添加更多层时，RefCOCO testA和testB的标准场景的准确性持续增加。当层数增长到6层时，我们的模型在testA和testB上实现了最佳性能。然而，当Transformer层数变大时，准确性略有下降。总的来说，对于我们的模型在标准场景中，性能在6层内保持稳定。我们可以在图4的右侧观察到另一种情况，即在开放词汇场景中，层数对ReferIt val和test集的准确性有很大的影响。此外，我们发现6层Transformer编码器是我们模型的最佳选择。考虑到标准和开放词汇场景的两个方面，我们在模型中使用了一个6层视觉Transformer编码器进行边界框回归。

5) 时间成本：为了更好地分析所提出的TransCP，我们进行了实验来测量训练和推理阶段所需的时间。在训练阶段，时间成本被记录为10个随机选择样本的前向和后向过程的平均时间（毫秒）。在推理阶段，只记录前向时间成本。比较模型VLTVG、TrasVG、没有P.T.的TransCP和TransCP在单个GTX 3090 24 GB GPU上执行，批量大小为1。我们选择RefCOCO testB分割作为实验数据集。结果如表IX所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/05f273d93591431885be2de4fc8725c5.png" width="70%" /> </div>


可以观察到，所提出的TransCP在训练成本方面总体上是最经济的，与TransVG和VLTVG相比。所提出的TransCP在训练期间每个样本大约需要273毫秒。没有P.T.，TransCP在训练阶段每个样本实现了大约9%的相对加速，同时减少了大约1M的可训练参数。然而，从表VIII中我们可以看到，TransCP的完整模型在开放词汇场景的ReferIt测试分割上超过了没有P.T.的TransCP大约2%的相对准确率。此外，在推理阶段的时间成本和可训练参数的数量在所有考虑的模型中都是可比的。所有这些观察结果表明，维护原型库并不会大幅增加训练成本。此外，通过这个模块，性能可以在两种场景中显著提高。

# V. 定性结果

## A. 上下文解耦的可视化
上下文解耦模块中视觉和语言分支的定性结果分别展示在图5和图6中。图5展示了视觉分支中歧视系数的可视化，我们可以观察到，在指代表达式中出现的所有显著对象都以更高的系数被提取出来，而上下文则通过较低的系数被抑制。与视觉解耦不同，图6中展示的短语注意力权重揭示，语言分支中的上下文解耦更加关注上下文信息。例如，查询“焦点之外的食物”和“右边的狗”集中于“之外”和“右边”，它们是指代对象的上下文关系。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/8d19ad20fb5341ae8db4eae06eea4718.png" width="70%" /> </div>


## B. 定性比较
此外，在图7中，我们展示了我们与最新方法VLTVG的定性比较。从图7中，我们可以观察到，即使给定的语言内容对机器来说非常难以理解，我们的模型仍然能够成功地定位表达式中表达的对象。例如，在图7(b)中，“焦点之外”是一个非常具有挑战性的陈述，它要求模型具有强大的理解能力。通过上下文解耦和原型继承，我们的模型准确地学习了来自其他训练数据的焦点之外对象的基本特征，然后继承它们以区分(b)中的焦点之外的食物与其他食物。图5和图6中的现象也可以证明这一点。此外，值得注意的是，VLTVG未能在图7(a)，(b)，(d)，(g)和(j)中定位指代对象。基于VLTVG的这些失败，我们假设当视觉空间中存在多个同一类别的对象时，它更关注较大的对象。因此，VLTVG未能定位(a)，(b)，(d)和(g)，但成功地定位了(c)，(e)（其中没有同一类别的对象），以及(f)，(h)，(i)（其中指代对象是同一类别对象中较大的对象）。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c5d6f52253c9484caad86f11e92950c4.png" width="70%" /> </div>


我们展示了开放词汇场景（在RefCOCO训练集上训练并在ReferIt验证集上测试）的定性结果如图8所示。基于结果，可以观察到我们的模型从聚类级原型的使用中受益。例如，即使面对错误或新颖的指代词，如图8(a)中的‘litehouse’和(i)中的‘night stand DRAWER’，我们的模型仍然能够正确识别并定位指代对象。其他案例，如(e)，(g)和(h)，展示了TransCP正确定位了‘mountains’，‘foliage’和‘bldg’（新对象）的位置，而VLTVG则在水、人和雕像周围绘制了错误的边界框。示例(b)，(d)和(f)表明，我们的模型通过上下文解耦，在同类中更好地区分了指代对象和上下文。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/409f9b84d67f4364a1b0a6634888f3ed.png" width="70%" /> </div>


从两次定性比较中，我们可以看到，我们的模型比现有方法更加稳健，能够更好地处理视觉定位的标准和开放词汇场景。

## C. 失败案例研究
为了更好地分析TransCP，一些来自标准场景和开放词汇场景的失败案例如图9所示。从图9中，我们可以得出几个观察结果。首先，当指代表达式缺少主语或清晰的指代对象时，两个模型都可能失败，因为它们都依赖于增强显著对象，如图(a)，(b)，(f)和(g)所示。第二，如果指代对象高度模糊，表达式缺乏区分真实指代对象的区分信息，TransCP和VLTVG都无法与真实情况对齐。这是因为视觉定位数据集通常只提供一个区域作为真实情况，而给定的指代表达式在图像中对应于多个区域，如图(c)中的“键盘”或(h)中的“红衬衫”所示。第三，虽然TransCP在开放词汇场景中学习了解耦的视觉特征上的原型，以帮助定位（如图8(a)，(h)和(i)所示的成功案例），但它可能无法将罕见的（或拼写错误的）关系或属性转移到开放词汇场景（如图(d)中的“2点钟”和(i)中的“rt”）。 

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/67a8e2c9df4b46df9406a7b63006c69e.png" width="70%" /> </div>


存在其他失败案例。例如，两个模型的OCR（光学字符识别）能力有限，如图9(e)所示。此外，如果语义在训练数据的视觉特征中不常见，原型发现模块可能无法学习适合继承的原型，用于开放词汇场景（TransCP未能将“天空”的广泛原型从训练数据转移到图9(j)中）。

# VI. 结论和局限性

在本文中，我们提出了一个新颖的框架TransCP，基于Transformer，具有上下文解耦和原型继承功能。它可以处理标准和开放词汇场景，实现鲁棒的视觉定位。我们的上下文解耦模块增强了视觉和语言特征中指代对象和上下文之间的区分。原型库在训练期间发现原型，并在推理期间存储它们以指导视觉定位，特别是对于开放词汇场景。此外，无参数的哈达玛德融合策略保持了视觉和语言特征的平等重要性，这有利于鲁棒性。在两种场景中的广泛实验展示了我们方法的优越性。

我们工作的主要局限性是，我们模型在开放词汇场景中的绝对性能仍然不能令人满意，对于实际应用来说还不够。此外，处理最近视觉定位方法中极其复杂的指代表达式也是具有挑战性的。将来，我们将把它与视觉-语言预训练（VLP）模型集成以提高性能，同时适应它以纳入推理方法来处理更复杂的指代表达式。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
