# [CenterNet++ for Object Detection](https://ieeexplore.ieee.org/document/10356840/)
## 题目：用于目标检测的CenterNet++
**作者：Kaiwen Duan; Song Bai; Lingxi Xie; Honggang Qi; Qingming Huang; Qi Tian**  
**源码：https://github.com/Duankaiwen/PyCenterNet**  
****

# 摘要
本文介绍了一种新的自底向上的对象检测方法，名为CenterNet++。与主流的自顶向下方法相比，我们证明了自底向上的方法在性能上具有竞争力，并且具有更高的召回率。CenterNet++通过关键点三元组（左上角、右下角和中心点）来检测每个对象。我们首先根据设计的线索对角点进行分组，并基于中心关键点确认对象位置。角关键点允许方法检测各种规模和形状的对象，而中心关键点减少了大量误报提议引入的混淆。我们的方法是一种无锚点检测器，因为它不需要定义显式的锚框。我们将我们的方法适配到具有不同结构的骨干网络，包括“沙漏”型网络和“金字塔”型网络，分别在单分辨率和多分辨率特征图上检测对象。在MS-COCO数据集上，使用Res2Net-101和Swin-Transformer的CenterNet分别达到了53.7%和57.1%的平均精度（APs），超越了所有现有的自底向上检测器，并达到了最先进的性能。我们还设计了一个实时CenterNet模型，实现了准确性和速度之间的良好折中，以30.5帧每秒（FPS）的速度达到了43.6%的AP。

# 关键词
- 无锚点（Anchor-free）
- 自底向上（Bottom-up）
- 深度学习（Deep learning）
- 对象检测（Object detection）

# I. 引言
目前，有两种主要类型的对象检测方法：自底向上方法[12]、[30]、[31]、[69]和自顶向下方法[9]、[10]、[38]、[46]。许多研究人员认为，自底向上方法耗时且引入了更多的误报，而自顶向下方法由于在实践中的有效性，已逐渐成为主流方法。自顶向下方法通过将每个对象建模为一个先验点或预定义的锚框，并预测到边界框的相应偏移量。自顶向下方法能够感知整体对象，这简化了生成边界框的后处理步骤。然而，它们通常在感知具有特殊形状的对象（例如，具有大宽高比的对象）时存在困难。图1(a)显示了自顶向下方法未能覆盖“火车”对象的一个案例。我们在第三节中对此问题进行了更详细的分析。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/deb01a6c6a5d4b96bf6917926e7836eb.png" width="70%" /> </div>


另一方面，我们发现自底向上方法在定位任意几何形状的对象方面可能更好，因此具有更高的召回率。然而，传统的自底向上方法通常会产生许多误报，这导致无法准确表示对象。例如，CornerNet[30]，作为一种代表性的自底向上方法，使用一对角关键点来建模每个对象，并实现了最先进的对象检测精度。尽管如此，CornerNet的性能仍然受到其相对薄弱的全局信息确定能力的制约；也就是说，由于每个对象是根据一对角关键点构建的，算法在没有意识到哪些关键点对应该组合成对象的情况下敏感地检测对象的边界。因此，如图1(b)所示，CornerNet经常生成错误的边界框，其中大部分可以很容易地通过一些补充信息（例如，宽高比）过滤掉。

受自底向上方法分析的驱动，我们的假设是如果提高它们感知对象全局信息的能力，自底向上方法与自顶向下方法具有竞争力。在本文中，我们提出了一个低成本但有效的解决方案，名为CenterNet，这是一种强大的自底向上对象检测方法，它根据三元组关键点（左上角和右下角以及中心）检测每个对象。CenterNet探索了一个提议的中心部分，即接近一个框的几何中心的区域，与常规方法相比，它多出了一个关键点。我们假设如果预测的边界框与真实框具有较高的交并比（IoU），那么预测边界框中心区域的中心关键点为同一类别的概率很高，反之亦然。因此，在推理过程中，当根据角关键点对生成提议后，我们通过验证提议的中心区域内是否有属于同一类别的中心关键点来确定该提议确实是一个对象。这一概念在图1(c)中展示。

我们为适应不同结构的网络设计了两个框架。第一个框架是为“沙漏”型网络开发的，这类网络在单一分辨率的特征图上检测对象。沙漏网络在执行关键点估计任务时非常流行，我们应用这种类型的网络来更好地预测角和中心关键点。我们还设计了我们的框架以适应“金字塔”型网络，这类网络在多分辨率特征图上检测对象。这种方法有两个主要优点：更强的泛化能力，因为大多数网络都有“金字塔”型结构，例如ResNet及其变体；以及更高的检测精度，因为不同尺度的对象在不同的感受野中被检测。尽管金字塔结构已在自顶向下方法中得到广泛应用，但据我们所知，这是第一次在自底向上方法中使用。

我们在MS-COCO数据集[37]上评估了所提出的CenterNet，这是最流行的大规模对象检测基准数据集之一。CenterNet，使用Res2Net-101[18]和Swin-Transformer[39]，分别达到了53.7和57.1的平均精度（APs），大幅度超越了所有现有的自底向上检测器。我们还设计了一个实时CenterNet，在30.5帧每秒（FPS）的速度下达到了43.6的AP，实现了准确性和速度之间的良好折中。CenterNet高效且性能接近现有的自顶向下方法的最先进水平。

本文的初步版本发表在[13]中。在这个扩展版本中，我们在以下方面改进了工作。(i)原始的CenterNet仅使用沙漏网络[43]作为骨干网络，在其中所有对象仅在单一分辨率的特征图上被检测。我们将CenterNet的思想扩展到具有金字塔结构的网络，从而允许CenterNet在多分辨率特征图上检测对象。为此，我们提出了检测关键点（包括角和中心关键点）和分组关键点的新方法。(ii)在这个版本中，由于CenterNet的新设计，我们研究了更多具有金字塔结构的骨干网络，包括ResNet[25]、ResNext[61]和Res2Net[18]。此外，我们甚至报告了使用Transformer[39]作为骨干网络的检测结果。实验结果表明，引入金字塔结构显著提高了检测精度，这使得网络可以使用更丰富的感受野来检测对象。(iii)我们展示了一个实时CenterNet，与流行的检测器相比，它实现了更好的准确性/速度折中。

这项工作的主要贡献可以总结如下：
- 我们提出了一种名为CenterNet的强大自底向上对象检测方法。CenterNet将每个对象检测为三元组关键点，因此可以定位任意几何形状的对象，并感知对象的全局信息。
- 我们设计了两个框架以适应不同结构的网络，这提高了我们方法的泛化能力。因此，我们的方法基本上适用于所有网络。
- CenterNet在自底向上方法中实现了最先进的检测精度，并与现有的自顶向下方法的最先进性能紧密匹配。
- 通过适当降低结构复杂性，CenterNet实现了准确性和速度之间的令人满意的折中。因此，我们证明了自底向上方法与自顶向下方法是必要的，并且具有竞争力。

本文的其余部分组织如下。第二节简要回顾了相关工作，第三节介绍了所提出的CenterNet的详细信息。实验结果在第四节提供，结论在第五节讨论。

# III. 我们的方法
## A. 顶下方法的一个缺点

我们注意到，对象检测方法主要分为两大类：顶下方法和底上方法。基于上述部分的讨论，我们认为底上方法在定位任意几何形状的对象方面具有更好的潜力，因此可能实现更高的召回率。大多数顶下方法都是基于锚点的，这些方法非常依赖经验（例如，为了提高效率，只考虑具有常见尺寸和纵横比的锚点）；此外，它们的形状和位置相对固定，尽管随后的边界框回归过程可能会稍微改变它们的状态。因此，检测器倾向于错过形状奇特的对象。图1(a)显示了一个典型的例子，即错过了检测“火车”对象。我们还提供了定量研究，如表I所示。三种代表性方法和我们的工作都在MS-COCO验证数据集上进行了评估。表I显示，顶下方法比底上方法获得显著更低的召回率，特别是对于形状奇特的对象，例如，尺寸大于300^2像素或纵横比大于5:1的对象。这个结果并不令人惊讶，因为，一方面，对于Faster R-CNN[48]，一个典型的基于锚点的顶下方法，没有预定义的锚点可以匹配这些对象。另一方面，FCOS[56]，一个典型的无锚点顶下方法，难以准确回归边界和提议之间的长距离。由于底上方法通常检测对象的各个部分并将它们组合成对象，这个问题有所减少。此外，我们报告了所提出的CenterNet的结果，证明CenterNet具有底上方法的优势，灵活地定位对象，特别是那些形状奇特的对象。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/27a6d9388a334fc3a0a57745889e8974.png" width="70%" /> </div>


尽管底上方法有较高的召回率，但它们经常产生许多误报。以CornerNet[30]为例，它产生了两个热图用于检测角点：一个用于检测左上角点的热图和一个用于检测右下角点的热图。这些热图表示不同类别的关键点位置，并用于为每个关键点分配置信度分数。此外，CornerNet预测每个角点的嵌入和一组偏移量（如图2所示）。这些嵌入用于确定两个角点是否来自同一对象。偏移量用于将角点从热图重新映射到输入图像。为了生成对象边界框，根据它们的分数选择热图中的前k个左上角和右下角，然后计算一对角点的嵌入向量之间的距离，以确定配对的角点是否属于同一对象。如果距离小于指定的阈值，则生成对象边界框。边界框被赋予一个置信度分数，等于角点对的平均分数。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/ce25619bc07a4b3587a95ab1eadc47bc.png" width="70%" /> </div>


在表II中，我们提供了CornerNet性能的更深入分析。我们基于MS-COCO验证数据集计算了CornerNet的平均误报发现率（AF1），定义为错误边界框的比例。定量结果表明，即使在低IoU阈值下，错误边界框也占边界框总数的很大一部分，例如，CornerNet在IoU为0.05时获得32.7%的AF分数。这一结果表明，平均每100个对象边界框中有32.7个与真实值的IoU低于0.05。还有更多的小错误边界框，AF分数为60.3%。这种结果的一个可能原因是CornerNet无法搜索边界框内的区域。为了允许CornerNet感知边界框内的视觉模式，一个潜在的解决方案是将CornerNet适应为一个两阶段检测器，使用RoI池化[21]来感知边界框内的视觉模式。然而，这些范式在计算上很昂贵。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/4ff6982d324c479d9e26480188cb1f86.png" width="70%" /> </div>


在本文中，我们提出了一个高效的替代方案，称为CenterNet，用于探索每个边界框内的视觉模式。为了检测一个对象，我们的方法使用三元组而不是成对的关键点。因此，我们的方法专注于中心信息，同时保持最小的计算成本，并继承了RoI池化的功能。此外，我们为在单分辨率特征图和多分辨率特征图中检测对象设计了两个框架。前者应用于关键点估计网络，以提高检测角点和中心关键点的性能。后者在目标检测任务中更受欢迎，因为它具有更好的通用性并且获得更丰富的检测感知场。两个框架的设计略有不同，我们将在下一子节中提供详细说明。

## B. 将对象检测视为关键点三元组

1) 单分辨率检测框架：受到姿态估计的启发，我们应用通常用于姿态估计的网络来更好地检测角点和中心关键点，其中大多数在单分辨率特征图中检测关键点，例如沙漏网络[43]。整体网络架构如图2所示。我们通过一个中心关键点和一对角点来表示每个对象。具体来说，我们在CornerNet的基础上嵌入一个中心关键点的热图，并预测中心关键点的偏移量。然后，我们使用CornerNet[30]中应用的方法来生成前k个边界框。为了有效地识别错误的边界框，我们使用检测到的中心关键点和以下程序：(1) 根据它们的分数选择前k个中心关键点。(2) 使用相应的偏移量将这些中心关键点重新映射到输入图像。(3) 为每个边界框定义一个中心区域，并验证中心区域是否包含中心关键点。注意，检查的中心关键点的类别标签应与边界框的类别标签相同。(4) 如果在中心区域检测到中心关键点，我们保留边界框。边界框的分数被替换为三点的平均分数，即左上角、右下角和中心关键点的分数。如果在中心区域没有检测到中心关键点，则删除边界框。

2) 多分辨率检测框架：整体网络架构如图3所示。网络从提取基于输入图像的特征的主干（例如，ResNet[25]，ResNeXt[61]）开始。我们选择主干的C3-C5特征图作为输入到特征金字塔网络（FPN）。然后，FPN输出P3-P7特征图作为最终预测层。在每个预测层中，我们使用热图和回归来预测关键点。在基于热图的预测方法中，我们使用三个轻量级二进制热图来预测角点和中心关键点。热图的分辨率与预测层的分辨率相同，因此，我们预测每个关键点的额外偏移量，以学习将关键点从热图重新映射到输入图像。在基于回归的预测方法中，为了解耦左上角和右下角，我们沿着几何中心将真实框分成四个子真实框，并选择左上角和右下角子真实框来监督回归过程。以左上角框的回归为例，我们选择一些在左上角子真实框内的特征点，每个选定的特征点用于预测两个向量，这些向量指向左上角和中心关键点。此外，我们为每个选定的特征点分配一个类别标签来监督分类过程。我们应用常见的无锚点检测方法来训练网络预测子边界框（例如FCOS[56]和RepPoints[8]）。此外，我们强调子边界框的回归精度高于完整边界框的精度，因为表I显示像FCOS[56]这样的无锚点方法在长距离回归上精度较低，而我们的子边界框有效地将回归距离减半。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0deb72fec19b451daf3715b358aa10f5.png" width="70%" /> </div>


在推理过程中，回归的向量作为线索，用于识别相应热图中最近的键点，以细化键点的位置。这种方法成功减少了由热图引入的误报角点。接下来，每个有效的键点对用于定义一个边界框。这里，有效表示两个键点属于同一类别（即，相应类别的左上角和右下角子边界框），并且左上角点的x和y坐标小于右下角点的坐标。最后，我们为每个边界框定义一个中心区域，并验证中心区域是否包含预测的中心关键点。如果且仅当在中心区域检测到两个中心关键点时，才保留边界框，否则删除边界框。边界框的分数被替换为点的平均分数，即左上角、右下角和中心关键点的分数。

3) 中心区域定义：边界框中的中心区域大小会影响检测结果。例如，中心区域较小会导致对小边界框的召回率降低，而中心区域较大则会导致对大边界框的精确度降低。因此，我们提出了一个基于边界框大小自适应调整的尺度感知中心区域。令 $tlx$ 和 $tly$ 分别表示边界框左上角的坐标， $brx$ 和 $bry$ 表示右下角的坐标。定义中心区域 $j$ ，令 $ctlx$ 和 $ctly$ 分别表示中心区域左上角的坐标， $cbrx$ 和 $cbry$ 表示右下角的坐标。然后， $tlx$ 、 $tly$ 、 $brx$ 、 $bry$ 、 $ctlx$ 、 $ctly$ 、 $cbrx$ 和 $cbry$ 应满足以下关系：

$$
\begin{cases} 
ctlx = \frac{(n+1)tlx + (n-1)brx}{2n} \\
ctly = \frac{(n+1)tly + (n-1)bry}{2n} \\
cbrx = \frac{(n-1)tlx + (n+1)brx}{2n} \\
cbry = \frac{(n-1)tly + (n+1)bry}{2n}
\end{cases}
$$

其中 $n$ 为奇数，决定中心区域 $j$ 的尺度。在本文中，对于小于等于150尺度的边界框， $n$ 被设为3；对于大于150尺度的边界框， $n$ 被设为5。图4展示了 $n=3$ 和 $n=5$ 时的中心区域。我们可以使用上述公式确定尺度感知的中心区域，并验证中心区域是否包含中心关键点。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/9ac761eaf2544750b61bd893d7b980e5.png" width="70%" /> </div>


## C. 丰富中心和角点信息

中心关键点和角点都与对象有严格的几何关系，但包含的对象视觉模式有限。我们以全监督的方式训练网络，以学习这些几何关系和有限的视觉特征来定位关键点。如果为中心关键点和角点引入更多的视觉模式，它们可以被更好地检测。

**中心池化（Center Pooling）**：对象的几何中心并不总是传达可识别的视觉模式（例如，人头包含强烈的视觉模式，但中心关键点通常位于人体中部）。为了解决这个问题，我们提出了中心池化来捕获更丰富和更可识别的视觉模式。图5(a)展示了中心池化的原理。中心池化的详细过程如下：主干网络输出一个特征图，为了确定特征图中的一个像素是否是中心关键点，我们需要找到水平和垂直方向上的最大值并将这两个值相加。中心池化有助于提高中心关键点的检测。

**级联角池化（Cascade Corner Pooling）**：角点经常位于对象外部，缺乏局部外观特征。CornerNet[30]使用角池化来解决这个问题。角池化的原理如图5(b)所示。角池化的目的是在边界上找到最大值以确定角点。然而，这种方法使角点对边缘敏感。为了解决这个问题，角点从对象的中心区域提取特征。级联角池化的原理在图5(c)中提出。级联角池化沿着边界寻找边界上的最大值，然后在最大边界值的位置内部的框中搜索内部最大值；然后，两个最大值相加。级联角池化允许角点获得对象的边界信息和视觉模式。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/2aadf23431314d34b709ce1fb561142a.png" width="70%" /> </div>


中心池化和级联角池化可以通过在不同方向应用角池化[30]来实现。图6(a)展示了中心池化模块的结构。为了确定特定方向上的最大值，例如水平方向，我们只需要顺序连接左右池化。图6(b)展示了级联顶部角池化模块的结构，其中白色矩形表示3×3卷积后跟批量归一化。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/37327daa1b8140ae8f800e5644af12f2.png" width="70%" /> </div>


## D. 训练和推理

**训练**：我们在8个Tesla V100 (32 GB) GPU上训练CenterNet。对于单分辨率检测框架，我们的基线是CornerNet[30]。接下来，我们使用堆叠的沙漏网络（Hourglass）[43]，层数分别为52和104，后者有两个沙漏模块，而前者只有一个。在[30]中对沙漏架构所做的所有修改都保留了。当我们使用Hourglass作为主干时，网络从头开始训练。此外，为了展示该框架可以推广到其他网络架构，我们研究了另一种主干，名为HRNet[54],[55]，在特征提取过程中保持高分辨率表示。输入图像的分辨率是511×511，从而产生128×128的热图。我们使用[30]中呈现的数据增强策略来训练一个健壮的模型。Adam[26]用于优化训练损失：

$$
L_{s} = L_{co}^{kp} + L_{ce}^{kp} + \alpha L_{co}^{pull} + \beta L_{co}^{push} + \gamma (L_{co}^{off} + L_{ce}^{off}),
$$

其中 $L_{co}^{kp}$ 和 $L_{ce}^{kp}$ 分别表示用于训练网络检测角点和中心关键点的焦点损失。 $L_{co}^{pull}$ 是角点的“拉”损失，用于最小化属于同一对象的嵌入向量之间的距离。 $L_{co}^{push}$ 是角点的“推”损失，用于最大化属于不同对象的嵌入向量之间的距离。 $L_{co}^{off}$ 和 $L_{ce}^{off}$ 是 $\ell_1$ 损失[21]，用于训练网络预测角点和中心关键点的偏移量。 $\alpha$ 、 $\beta$ 和 $\gamma$ 分别表示相应损失的权重，分别设为0.1、0.1和1。 $L_{kp}$ 、 $L_{pull}$ 、 $L_{push}$ 和 $L_{off}$ 都在CornerNet中定义，详细信息提供在[30]。我们使用批量大小为48。最大训练周期数为100。前88个周期的学习率为2.5×10^-4，之后额外训练12个周期，学习率为2.5×10^-5。

对于多分辨率检测框架，我们使用基于ImageNet[11]预训练权重的ResNet[25]、Res2Net[18]、ResNeXt[61]和Swin Transformer[39]作为我们的主干。FPN[35]用于输出不同尺度的检测层。我们应用单尺度和多尺度训练策略。对于单尺度训练，每个输入图像的较短边为800像素，而对于多尺度训练，每个输入图像的较短边在[480, 960]范围内随机选择。我们使用[63]中呈现的数据增强策略来训练一个健壮的模型。随机梯度下降（SGD）用于优化训练损失，如下所示：

$$
L_{m} = \frac{1}{2} \left( L_{tl}^{cls} + L_{br}^{cls} \right) + \hat{\alpha} \left( L_{tl}^{reg} + L_{br}^{reg} \right) + \hat{\beta} \left( L_{co}^{kp} + L_{ce}^{kp} \right) + \hat{\gamma} (L_{co}^{off} + L_{ce}^{off}),
$$

其中 $L_{tl}^{cls}$ 和 $L_{br}^{cls}$ 分别表示用于训练网络对左上角和右下角子边界框进行分类的焦点损失。 $L_{tl}^{reg}$ 和 $L_{br}^{reg}$ 表示GIoU损失[49]，用于训练网络回归左上角和右下角子边界框。 $\hat{\alpha}$ 、 $\hat{\beta}$ 和 $\hat{\gamma}$ 分别表示相应损失的权重，分别设为2、0.25和1.0。我们使用批量大小为16。最大训练周期数为24。前16个周期的学习率为0.01，然后在第16个和第22个周期后，学习率分别衰减10倍。

**推理**：对于单分辨率检测框架，我们遵循[30]中描述的过程。对于单尺度测试，我们将原始分辨率的原始图像和水平翻转图像输入网络。对于多尺度测试，我们将分辨率分别为0.6、1、1.2、1.5和1.8的原始图像和水平翻转图像输入网络。我们从热图中选择前70个中心关键点、前70个左上角和前70个右下角来检测边界框。我们在水平翻转图像中检测到的边界框进行翻转，并与原始边界框混合。使用Soft-NMS[2]来去除多余的边界框。我们最终根据它们的分数选择前100个边界框作为最终检测结果。

对于多分辨率检测框架，我们遵循[63]中描述的过程。对于单尺度测试，我们将每个图像根据较短边800像素调整大小作为网络的输入，而对于多尺度测试，我们将每个图像根据较短边[400, 600, 800, 1000, 1200, 1400]调整大小，并且合并所有尺度的检测结果。应用阈值为0.6的NMS来去除多余的结果。在多尺度评估中仅使用翻转论证。

## E. 与先前工作的关联

我们的方法结合了底上和顶下方法的优点。顶下方法可以感知提议中的全局视觉内容；然而，它们通常在定位精度上存在缺陷，尤其是对于形状奇特的对象。底上方法可以定位任意几何形状的对象，但通常会产生许多错误的边界框（误报）。我们的方法使用关键点三元组来表示对象；因此，我们的模型仍然是一个底上方法，可以以最小的成本感知提议中的视觉内容。

# IV. 实验

## A. 数据集、指标和基线

我们基于MS-COCO数据集[37]评估我们的方法。该数据集包含80个类别和超过150万个对象实例。大量的小对象使其成为一个极具挑战性的数据集。我们使用'train2017'集（即11万张训练图像）进行训练，并基于test-dev集测试模型。我们使用'val2017'集作为验证集，以执行消融研究和可视化实验。

MS-COCO数据集[37]使用AP（平均精度）和AR（平均召回率）指标来描述检测器的性能。AP基于十个不同的IoU阈值（即，0.5：0.05：0.95）为所有类别计算，而AR基于每张图像的固定数量检测（即，1、10和100）并结合不同的IoU阈值计算。此外，AP和AR也可以用来评估不同对象尺度的性能，包括小对象（面积 < 32^2）、中等对象（32^2 < 面积 < 96^2）和大对象（面积 > 96^2）。AP被认为是MS-COCO数据集上最重要的指标。

## B. 与最先进检测器的比较

表III显示了基于MS-COCO test-dev集的所提方法与最先进检测器之间的比较。与基线CornerNet[30]相比，所提出的CenterNet显示出显著提高的性能。例如，SR-CenterNet（Hourglass-52）在单尺度测试中获得了41.6%的AP，比CornerNet获得的37.8% AP提高了3.8%，在多尺度测试中获得了43.5%的AP，比CornerNet在同一设置下获得的39.4% AP提高了4.1%。当使用更深的主干（即，Hourglass-104）时，CenterNet在CornerNet[30]的基础上实现了改进的AP，分别提高了4.4%（从40.5%提高到44.9%）和4.9%（从42.1%提高到47.0%）。我们还报告了MR-CenterNet的检测结果，它在SR-CenterNet的基础上取得了显著的改进。MR-CenterNet使用Res2Net-101作为主干，在单尺度测试和多尺度测试中分别获得了51.5%和53.7%的AP。目前基于Transformer的顶下检测器[39]实现了最先进的准确性，我们探索了基于Transformer的主干在底上方法中的应用。带有Transformer主干的CenterNet在单尺度测试和多尺度测试中分别实现了53.2%和57.1%的AP，据我们所知，超过了所有其他底上方法。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b8bf5dbcd0b948e39086c4d8ec923d91.png" width="70%" /> </div>


最大的改进是在小对象上观察到的。例如，SR-CenterNet（Hourglass-52）将小对象的AP提高了5.5%（单尺度）和6.4%（多尺度）。对于使用Hourglass-104主干的CenterNet，小对象的AP分别提高了6.2%（单尺度）和8.1%（多尺度）。这种性能提升是由于中心关键点建模的中心信息：错误的边界框尺度越小，检测到中心区域内的中心关键点的概率就越低。图7(a)和(b)展示了定性比较，证明了CenterNet在减少错误小边界框数量方面的有效性。CenterNet还在减少错误中等和大边界框数量方面取得了显著改进。表III显示，SR-CenterNet（Hourglass-104）将中等和大边界框的单尺度测试AP分别提高了4.7%（从42.7%提高到47.4%）和3.5%（从53.9%提高到57.4%）。图7(c)和(d)展示了减少错误中等和大边界框数量的定性比较。值得注意的是，与基线的AR分数相比，AR也显著提高，最佳性能是在多尺度测试条件下实现的。这是因为我们的方法去除了更多的错误边界框，这相当于提高了具有准确位置但相对分数较低的边界框的置信度。

与顶下方法相比，CenterNet的性能也具有竞争力，例如，SR-CenterNet（Hourglass-52）的单尺度测试AP与顶下方法ReﬁneDet[65]（41.6%对41.8%）相当，MR-CenterNet（Res2Net-101）的单尺度测试AP与GFLV2[32]（53.7%对53.3%）相当。MR-CenterNet（Swin-L）实现的57.1%多尺度测试AP与顶下方法Swin Transformer[39]实现的最先进的58.7% AP相当。我们在图8中展示了一些定性检测结果。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/fce9571ca69841bf897bf2fc647fb897.png" width="70%" /> </div>


## C. 多分辨率检测提高精度

如表III所示，所提出的MR-CenterNet与现有方法相比，在对象检测精度上有所提高。例如，在相同的网络深度（Hourglass-52与ResNet-50）下，MR-CenterNet将对象的AP提高了4.8%。此外，由于MR-CenterNet框架的强泛化能力，我们可以为中心网应用更强大的主干。

我们设计了两个比较实验来证实多分辨率检测过程对MR-CenterNet性能的贡献。第一个实验是对照实验，所以我们使用默认网络。对于第二个实验，我们通过网络增加两个上采样卷积网络和从低层到输出的跳跃连接，以获得更高分辨率的输出。输出层的分辨率是输入图像的1/8。两个网络结构的图表分别如图9所示。表IV报告了基于MS-COCO验证数据集的两个实验的检测结果。具有多分辨率检测层的MR-CenterNet比其他模型实现了更高的精度。多分辨率检测结构为不同尺度的对象提供了更丰富的感知场，有助于提高检测精度。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/091fc0d390fd4dbea193260fa74c5417.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/884d3e18e94c468f872ac2cceb1bafd4.png" width="70%" /> </div>


## D. 实时CenterNet

我们还设计了一个实时版本的CenterNet，称为CenterNet-RT。CenterNet-RT基于CenterNet的多分辨率检测框架（如图3所示），我们应用以下技巧来加速CenterNet：（i）将输入图像的分辨率从800降低到512（短边）和从1333降低到736（长边）。 （ii）受FCOS-RT[56]和BlendMask-RT[6]的启发，我们移除了P6和P7层，以减少计算成本，因为低分辨率输入图像降低了更高特征层P6和P7的重要性。 （iii）移除角点和中心关键点检测热图。对于CenterNet-RT，我们仅使用角点和中心关键点的回归结果来确定关键点的最终预测。这在寻找热图中最近的键点时节省了大量时间。 （iv）在训练期间，将SGD优化器替换为AdamW[40]优化器，并将训练周期数增加到32。我们在NVIDIA Tesla-V100 GPU上测试了我们方法的推理速度。我们还按照FCOS[56]的方式，以端到端的方式评估FCOS-RT、YOLOv3和我们的实时CenterNet，包括预处理步骤到最终输出框的生成。

结果如表V所示。CenterNet-RT在准确性和速度之间实现了良好的折衷，并在其他典型方法中展现出竞争力。在我们的会议版本中，CenterNet（SR-CenterNet）运行缓慢，推理速度不到7 FPS。在本文中，我们为中心网配备了金字塔结构，并在多分辨率特征层中检测对象，这显著提高了速度和准确性。使用ResNet-50，MR-CenterNet实现了45.7% AP，速度为14.5 FPS。此外，我们在MR-CenterNet的基础上提出了CenterNet-RT，实现了43.2% AP，速度为30.5 FPS。这种准确性与SR-CenterNet（HG-104）相当，但推理速度快了约6倍。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c0d6015c5f834496b75224777ac1e6b2.png" width="70%" /> </div>


## E. 错误边界框减少

AP [37] 指标反映了网络能够预测的高质量对象边界框（通常 IoU ≥ 0.5）的数量；然而，这个指标并没有直接反映网络生成的错误对象边界框（通常 IoU ≪ 0.5）的数量。AF率是一个合适的指标，它反映了错误边界框的比例。表VI显示了CornerNet和CenterNet的AF率。CornerNet即使在IoU = 0.05的阈值下，也会产生许多错误边界框，即CornerNet511-52和CornerNet511-104分别获得了35.2%和32.7%的AF分数。另一方面，CornerNet比中大型错误边界框产生更多的小型错误边界框，AF分数分别为CornerNet511-52的62.5%和CornerNet511-104的60.3%。我们的CenterNet通过探索中心区域，减少了基于所有标准的AF率。例如，CenterNet511-52和CenterNet511-104都将AF5分数降低了4.5%。此外，小型边界框的AF率下降最大，AF分数分别为CenterNet511-52的9.5%和CenterNet511-104的9.6%。这也是为什么AP改进对于小型对象比中型和大型对象更加突出的原因。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d32a380b411b49609443f73a449bc615.png" width="70%" /> </div>


## F. 消融研究

我们的工作有三个主要贡献，包括中心区域探索、中心池化和级联角池化。为了分析每个单独组件的贡献，我们进行了消融研究。基线是CornerNet511-52 [30]。我们一个接一个地向基线添加这三个组件，并按照第IV节A中详细说明的默认参数设置进行操作。结果如表VII所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c3fdfd7bee464991a5e4e911ca08753c.png" width="70%" /> </div>


中心区域探索：为了理解中心区域探索的重要性（见表中的CRE），我们向基线添加了一个中心热图分支，并使用关键点三元组来检测边界框。对于中心关键点检测，我们只使用传统的卷积。如表VII第三行所示，当添加中心区域探索时，AP提高了2.3%（从37.6%提高到39.9%）。此外，我们发现对于小型对象的改进（即4.6%）比其他对象尺度更为显著。对于大型对象的改进几乎可以忽略不计（从52.2%提高到52.3%）。这并不奇怪，因为小型错误边界框的数量更多，这些边界框通常不包含对象的中心关键点，因此更有可能从中心关键点的过滤中受益。

中心池化：为了证明所提出的中心池化的有效性，我们接下来向网络添加了中心池化模块（见表中的CTP）。表VII第四行显示，添加中心池化将AP提高了0.9%（从39.9%提高到40.8%）。值得注意的是，添加中心池化后，大型对象的AP提高了1.4%（从52.2%提高到53.6%），这比使用传统卷积获得的改进（即1.4%对0.1%）要大得多。这一结果表明，我们的中心池化对于检测对象的中心关键点，特别是大型对象非常有效。我们的解释是，中心池化可以用来提取更丰富的内部视觉模式，而大型对象比小型对象包含更多可访问的内部视觉模式。图7(e)显示了使用和不使用中心池化检测中心关键点的结果。使用传统卷积的模型未能定位到牛的中心关键点，而使用中心池化的模型成功地定位到了中心关键点。

级联角池化：我们用级联角池化替换了角池化[30]来检测角点（见表中的CCP）。表VII第二行显示了基于CornerNet511-52的结果。我们发现，添加级联角池化将AP提高了0.7%（从37.6%提高到38.3%）。最后一行显示了基于CenterNet511-52的结果，表明AP提高了0.5%（从40.8%提高到41.3%）。第二行的结果表明，对于大型对象的AP几乎没有变化（即52.2%对52.2%），但AR提高了1.8%（从74.0%提高到75.8%）。这一结果表明，级联角池化有助于在大型对象中获得更多的内部视觉模式，但过于丰富的视觉模式可能会干扰其对边界信息的感知，导致大量不准确的边界框。在向CenterNet添加级联角池化后，不准确边界框的数量得到了有效减少，大型对象的AP提高了2.2%（从53.6%提高到55.8%）。图7(f)显示了使用角池化和级联角池化检测角点的结果。使用级联角池化的模型成功地为左侧的猫定位了一对角点，而使用角池化的模型则没有。

## G. 错误分析

每个边界框内的视觉模式探索取决于中心关键点。换句话说，一旦错过了中心关键点，所提出的CenterNet就会错过边界框内的视觉模式。为了理解中心关键点的重要性，我们用真实值替换预测的中心关键点，并基于MS-COCO验证数据集评估模型性能。表VIII显示，使用真实中心关键点将CenterNet511-52的AP从41.3%提高到56.5%，将CenterNet511-104的AP从44.8%提高到58.1%。CenterNet511-52的小、中、大型对象的AP分别提高了15.5%、16.5%和14.5%，CenterNet511-104的小、中、大型对象的AP分别提高了14.5%、14.1%和13.3%。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f228ce6f38fa43f8bade683cf70fe672.png" width="70%" /> </div>


# V. 结论

在本文中，我们提出了CenterNet，这是一种新的底上对象检测方法，它使用包括一个中心关键点和两个角点在内的关键点三元组来检测对象。我们的方法解决了传统底上方法缺乏对裁剪区域的额外调查的问题，通过以最小的成本探索每个提议区域内的视觉模式。此外，我们基于金字塔结构框架扩展了CenterNet，以提高多尺度对象检测性能。实验结果表明，CenterNet在所有现有的底上方法中表现突出，并且在与最先进的顶下方法相比时也具有竞争力，尤其是在召回率方面。我们还设计了一些实时CenterNet模型，它们在准确性和速度之间实现了良好的折衷。

重要的是，我们证明了底上方法在定位任意几何形状的对象方面比顶下方法更灵活，并且对每个提议区域进行额外的调查对于提高模型精度是必要的。我们希望CenterNet能够吸引更多的关注，并促进底上方法的进一步探索。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
