# 题目：[From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10361604)  
## 从 NeRFLiX 到 NeRFLiX++: 一种通用的 NeRF-无关恢复范式
**作者：Kun Zhou; Wenbo Li; Nianjuan Jiang; Xiaoguang Han; Jiangbo Lu** 

****
# 摘要
神经辐射场（NeRF）在新视图合成方面表现出了巨大的成功。然而，由于潜在的校准信息不完善和场景表示不准确，现有的基于 NeRF 的方法在从现实世界场景中恢复高质量细节方面仍然面临挑战。即使使用高质量的训练帧，NeRF 模型生成的合成新视图仍然存在明显的渲染伪影，如噪声和模糊。为了解决这个问题，我们提出了 NeRFLiX，一种学习降解驱动的视点间混合器的通用 NeRF-无关恢复范式。特别地，我们设计了一种 NeRF 样式的降解建模方法并构建了大规模的训练数据，使得深度神经网络能够有效地去除 NeRF 原生的渲染伪影。此外，除了降解去除外，我们还提出了一种视点间聚合框架，该框架融合了高度相关的高质量训练图像，将最先进的 NeRF 模型的性能推向了新的水平，并生成高度逼真的合成视图。在此基础上，我们进一步提出了 NeRFLiX++，它具有更强的两阶段 NeRF 降解模拟器和更快的视点间混合器，实现了卓越的性能和显著提高的计算效率。值得注意的是，NeRFLiX++ 能够从噪声较大的低分辨率 NeRF 渲染视图中恢复逼真的超高分辨率输出。大量实验表明 NeRFLiX++ 在各种新视图合成基准上具有出色的恢复能力。


# 关键词
- 神经网络场
- 退化模拟
- 对应估计
- 深度学习



## I. 引言


逼真的新视图合成是计算机视觉和图形学领域的一个长期问题。近年来，出现了基于学习的方法，如 NeRF（神经辐射场）及其后续方法，这些方法利用神经网络来表示 3D 场景，并采用各种渲染技术来合成新视图。为了实现高质量的渲染，设计物理感知系统以优化几何、环境照明、物体材料和相机姿态等多个因素至关重要。然而，尽管取得了进展，最先进的 NeRF 模型在仅依赖有限数量的输入视图时，仍可能出现不理想的渲染伪影，如文献[20], [24], [34], [64], [82], [83]所述。

为了实现高质量的新视图合成，我们提出了 NeRFLiX [86]，它开创性地研究了模拟大规模 NeRF 样式配对数据以训练 NeRF-无关恢复器的可行性。该系统包括两个主要组件：（1）NeRF 样式降解模拟器（NDS）和（2）视点间混合器（IVM）。受实际图像恢复方法[58], [77]的启发，NeRFLiX 系统地分析了典型的 NeRF 渲染伪影，并提出了三种手动设计的降解来模拟 NeRF 渲染的噪声。我们利用 NDS 生成大量模拟的训练数据，并进一步开发了一个深度恢复器，即 IVM，以去除 NeRF 样式的伪影。因此，NeRFLiX 在合成高保真新视图方面表现出了卓越的性能，从而将 NeRF 模型的能力扩展到了新的领域。然而，还有两个方面需要进一步研究：（1）手动降解设计在解释实际 NeRF 渲染伪影分散性方面的不足；（2）使用大型视点间混合器处理高分辨率帧的困难。

在此基础上，我们通过引入两阶段降解模拟方法，并结合更高效的引导视点间混合器，将 NeRFLiX 扩展为 NeRFLiX++。这一改进框架不仅实现了更优或可比的性能，还显著提高了推理效率。





![](https://img-blog.csdnimg.cn/direct/9eaed13db8fa40ea86e93aeeaddd9851.png)


![](https://img-blog.csdnimg.cn/direct/c7e1d0c33d194a4bbca688ecae7d3fbe.png)



## III. 预备知识
在本节中，我们回顾了基于 NeRF 的新视图合成的一般流程，并讨论了潜在的渲染伪影。如图 2 所示，渲染过程涉及三个主要步骤。

1. **射线投射:** 为了渲染特定视图中目标像素的颜色，NeRF 利用相机校准参数 $\pi$ 生成穿过该像素的射线 $r(o,d)$，其中 $o$ 和 $d$ 分别是相机中心和射线方向。
2. **射线行进:** 沿所选射线采样一组 3D 点，这些点在神经辐射场表示的 3D 场景中移动。NeRF 对 3D 场景进行编码，并预测这些点的颜色和密度。
3. **辐射累积:** 通过积分采样的 3D 点的预测辐射特征来提取像素颜色。


![](https://img-blog.csdnimg.cn/direct/0a91050c22f242f0b35b279e172d3483.png)



讨论：我们看到，建立 2D 照片与相应 3D 场景之间的关系需要相机校准。不幸的是，精确校准相机姿态非常具有挑战性，导致 3D 采样噪声。同时，一些先前的工作[24], [62], [70], [75] 也提出了其他问题，包括非线性的小孔相机模型[24] 和形状-辐射歧义[79]。由于这些固有的限制，如第 I 节所述，NeRF 模型可能会合成不满意的新测试视图。


## IV. NeRFLiX
在这项工作中，我们提出了 NeRFLiX，一种通用的 NeRF-无关恢复器，它采用降解驱动的视点间混合器来增强 NeRF 模型渲染的新视图图像。它由两个基本组件组成：NeRF 样式降解模拟器（NDS）和视点间混合器（IVM）。如图 4(a)所示，在训练阶段，我们使用所提出的 NDS 创建大规模配对训练数据，然后用它训练 IVM，以利用两个参考图像（参考视图）改进 NeRF 渲染的视图。在推理阶段，如图 4(b)所示，IVM 被用于通过融合选定的最相关参考视图中的有用信息来增强渲染的视图。

### A. NeRF 样式降解模拟器 (NDS)
由于在各种环境下收集定位良好的场景并为每个场景训练 NeRF 模型的难度，直接收集大量配对 NeRF 数据以去除伪影是不可行的。为了应对这一挑战，受到 BSRGAN [77] 的启发，我们设计了一种通用的 NeRF 降解模拟器，以生成在视觉和统计上与 NeRF 渲染的图像（视图）相当的大规模训练数据集。

首先，我们从 LLFF-T1 和 Vimeo90K [68] 收集原始数据，其中相邻帧被视为原始序列。每个原始序列由三张图像 $\{I_{gt}, I_{r1}, I_{r2}\}$ 组成：一个目标视图 $I_{gt}$ 及其两个参考视图 $\{I_{r1}, I_{r2}\}$。为了从原始序列构建配对数据，我们使用所提出的 NDS 降解 $I_{gt}$ 并获得模拟视图 $I$，如图 4(a)所示。

降解流程如图 3 所示。我们设计了三种类型的降解来处理目标视图 $I_{gt}$：溅射高斯噪声（SGN）、重新定位（Re-Pos.）和各向异性模糊（A-Blur）。需要注意的是，可能还有其他模型用于这样的模拟，我们仅利用这一途径来评估和验证我们想法的可行性。

**溅射高斯噪声:** 尽管加性高斯噪声经常用于图像和视频去噪，但 NeRF 渲染噪声显然不同。由于相机参数噪声，射到 3D 点的射线会在附近的 2D 区域内重新投射。因此，NeRF 样式的噪声会分散在 2D 空间中。这个观察结果促使我们提出了溅射高斯噪声，其定义为

$$
I_{D1} = (I_{gt} + n) * g
$$

其中 $n$ 是一个与 $I_{gt}$ 分辨率相同的 2D 高斯噪声图， $g$ 是一个各向同性高斯模糊核。

**重新定位:** 我们设计了一种重新定位降解来模拟射线抖动。我们为位置 $(i, j)$ 的像素添加了一个随机 2D 偏移 $\delta_i, \delta_j \in [-2, 2]$，概率为 0.1，如下所示

$$
I_{D2}(i, j) = 
\begin{cases}
I_{D1}(i, j) & \text{if } p > 0.1 \\
I_{D1}(i + \delta_i, j + \delta_j) & \text{else } p \leq 0.1
\end{cases}
$$

其中 $p$ 在 $[0, 1]$ 内均匀分布。

**各向异性模糊:** 从我们的观察来看，NeRF 合成帧也包含模糊内容。为了模拟模糊模式，我们使用各向异性高斯核来模糊目标帧。

神经辐射场通常用不平衡的训练视图进行监督。因此，给定一个新视图，投影的 2D 区域具有不同的降解级别。因此，我们在空间上以不同方式执行每个采用的降解。更具体地说，我们定义了一个掩码 $M$，作为二维定向各向异性高斯[19]，如下所示

$$
M(i, j) = G(i - c_i, j - c_j ; \sigma_i, \sigma_j, A)
$$

其中 $(c_i, c_j)$ 和 $(\sigma_i, \sigma_j)$ 是均值和标准差， $A$ 是一个方向角。之后，我们使用掩码 $M$ 线性混合每个降解的输入和输出，最终实现区域自适应降解。

最后，通过我们的 NDS，我们可以获得大量的训练对，每个配对数据由两个高质量参考视图 $\{I_{r1}, I_{r2}\}$、一个模拟降解视图 $I$ 和相应的目标视图 $I_{gt}$ 组成。接下来，我们展示了如何使用构建的配对数据 $\{I, I_{r1}, I_{r2} \mid I_{gt}\}$ 训练我们的 IVM。

### B. 视点间混合器 (IVM)
**问题公式:** 给定由我们的 NDS 或 NeRF 模型生成的降解视图 $I$，我们旨在从其两个高质量参考视图 $\{I_{r1}, I_{r2}\}$ 中提取有用信息，并恢复增强版本 $\hat{I}$。

**IVM 架构:** 对于多帧处理，现有技术要么使用光流[6], [55], [74]，要么使用可变形卷积[13], [32], [57]来实现一致位移的对应估计和聚合。相比之下，NeRF 渲染的视图和输入视图可能来自非常不同的角度和位置，这使得精确聚合具有挑战性。

为了解决这个问题，我们提出了 IVM，一种混合递归视点间“混合器”，逐步融合来自两个高质量参考视图的像素级和补丁级内容，实现更有效的视点间聚合。它由三个模块组成，即特征提取、混合视点间聚合和重建，如图 5 所示。在特征提取阶段，使用两个卷积编码器分别处理降解视图 $I$ 和两个高质量参考视图 $\{I_{r1}, I_{r2}\}$。然后，我们使用基于视点间窗口的注意模块和可变形卷积来实现递归补丁级和像素级聚合。最后，使用重建模块在监督下生成增强视图 $\hat{I}$

$$
\text{Loss} = \lvert \hat{I} - I_{gt} \rvert ，\text{其中 } \hat{I} = f(I, I_{r1}, I_{r2}; \theta)
$$

其中 $\theta$ 是 IVM 的可学习参数。架构细节在我们的补充材料中提供。



![](https://img-blog.csdnimg.cn/direct/16e7950d681344eab9f9b68d7acabc08.png)

![](https://img-blog.csdnimg.cn/direct/08af18cf10074e18a05552217b5ef7e1.png)


![](https://img-blog.csdnimg.cn/direct/433bf365622540eabb00b5a7492fb779.png)




### C. 视图选择
在推理阶段，对于 NeRF 渲染的视图 $I$，我们的 IVM 通过聚合来自两个相邻高质量视图的内容来生成增强版本。尽管提供了多个高质量视图（用于训练），但只有一部分与 $I$ 大部分重叠。我们只采用最相关的视图，这些视图对视点间聚合有用。

为此，我们开发了一种视图选择策略，从与渲染视图 $I$ 最重叠的输入视图中选择两个参考视图 $\{I_{r1}, I_{r2}\}$。具体来说，我们基于小孔相机模型公式化视图选择问题。任意 3D 场景可以大致近似为图 6 中的一个边界球体，相机围绕它放置以拍照。当相机发射的射线击中球体时，会产生一组交点。我们将 3D 点集称为 $\Phi_i = \{p_{i0}, p_{i1}, \ldots, p_{iMi}\}$ 和 $\Phi_j = \{p_{j0}, p_{j1}, \ldots, p_{jMj}\}$，分别对应于第 $i$ 和第 $j$ 个相机。对于视图 $i$ 的第 $m$ 个交点 $p_{imi} \in \Phi_i$，我们搜索视图 $j$ 中与之最近的点，其 $L_2$ 距离为

$$
p_{i \rightarrow jmi} = \arg \min_{p \in \Phi_j} \lVert p - p_{imi} \rVert^2_2
$$

然后从视图 $i$ 到视图 $j$ 的匹配成本计算为

$$
C_{i \rightarrow j} = \sum_{mi = 0}^{Mi} \lVert p_{imi} - p_{i \rightarrow jmi} \rVert^2_2
$$

我们最终得到视图 $i$ 和视图 $j$ 之间的相互匹配成本

$$
C_{i \leftrightarrow j} = C_{i \rightarrow j} + C_{j \rightarrow i}
$$

在这种情况下，两个参考视图 $\{I_{r1}, I_{r2}\}$ 在最小的相互匹配成本下被选择用于增强 NeRF 渲染的视图 $I$。注意，我们也采用这种策略来决定训练阶段 LLFF-T [39] 数据的两个参考视图。







![](https://img-blog.csdnimg.cn/direct/a6843b3c13304ecabafc70bfb98f8be2.png)



## V. NeRFLiX++
基于 NeRFLiX，我们提出了 NeRFLiX++，它具有两阶段降解建模策略和引导视点间混合器，以进一步提高恢复性能和效率。

### A. 两阶段降解建模
提出的两阶段降解建模方法包括手工设计的降解模拟器和深度生成降解模拟器，如图 7 所示。在第一阶段，我们使用多种手工设计的降解从选定的干净视图生成初始化的降解帧，受到 NeRFLiX 的启发。在第二阶段，使用深度生成降解模拟器来优化第一阶段的结果并生成最终的模拟视图。


![](https://img-blog.csdnimg.cn/direct/993063bdfa8c4d66abe29044a4522f0e.png)



1. **手工降解模拟器:** 除了 NeRFLiX 使用的三种基本降解（溅射高斯噪声、重新定位和各向异性高斯模糊），我们引入了两种补充降解模式以增强模拟的现实感。对于这两种额外的降解，我们采用与 NeRFLiX 相同的区域自适应降解策略。

**照明射流:** 为了解释视角依赖阴影引起的照明变化，我们提出了一个伽马调整，应用于目标视图和参考视图。调整定义为

$$
y = \text{power}(x, \gamma)
$$

其中“power”表示指数函数， $\gamma$ 是从 [0.95,1.05] 随机采样的线性调整常数。

**亮度压缩:** 为了模拟 NeRF 渲染中可能出现的结构缺陷，我们提出了一个图像压缩过程，以降解目标帧的灰度密度。具体来说，我们首先将 RGB 帧转换为 LAB 颜色空间，并使用 JPEG 算法在随机选定的压缩级别（介于 20% 和 90% 之间）压缩 L 分量。然后，我们将降解的 L 通道与原始的 AB 通道合并，并将其转换回 RGB 颜色空间。

2. **深度生成降解模拟器:** 如第 I 节所述，手动设计的降解可能无法捕捉到实际 NeRF 样式伪影的全范围。为了解决这一限制，我们提出了一种深度生成降解模拟器，以优化手工降解阶段的结果，并缩小模拟域和目标域之间的差距。

生成对抗网络（GANs）在图像到图像的翻译任务中显示了显著的结果，当有大量训练样本时。然而，神经辐射场（NeRF）提供的数据稀缺性对使用 GANs 直接拟合潜在降解分布提出了重大挑战。为了解决这个问题，受到 Beby-GAN [30] 的启发，我们提出了一种加权 top-K 相似性损失（WKS），作为辅助损失函数，以帮助对抗训练。如图 7 所示，我们使用 UNet 处理第一阶段降解视图 $I_{S1}$，以获得优化结果 $I_{S2}$。除了传统的对抗性和重建损失外，我们还利用 WKS 来生成更多样化的 $I_{S2}$ 结果。

**WKS:** 图 8 展示了加权 top-K 监督。给定 $I_{S2}$ 的第 $i$ 个补丁，表示为 $g_{S2i}$，我们使用三重距离函数从相应的真实渲染视图 $I$ 中搜索 top-K 相似补丁 $g^*_{i,{1,2,\ldots,K}}$。三重距离函数定义为

$$
g^\star_ {i,{1,2,\ldots,K}} = \text{topK}_ {g \in G} \left( \alpha \lVert g - g_{S2i} \rVert^2_2 + \beta \lVert g - g_{gti} \rVert^2_2 \right)
$$

其中 $g_{gti}$ 是相应的真实渲染补丁， $G$ 是通过展开真实渲染视图 $I$ 生成的候选补丁集， $\alpha, \beta$ 是两个缩放因子，用于平衡两个距离项。根据 Beby-GAN [30] 的实验证据，我们将它们设置为 1，以获得更好的评估结果。在获得 top-K 相似补丁后，提出的 WKS 被公式化为

$$
L_{\text{TopK}} = \sum_{k=1}^{K} \lVert (g^*_ {ik} - g_{S2i}) * w_{ik} \rVert_1
$$

其中 $w_{ik}$ 是第 $k$ 个归一化权重，计算公式为

$$
d_{ik} = - \frac{1}{2} \lVert g^\star_ {ik} - g_{S2i} \rVert^2_2
$$

$$
w_{ik} = \exp(d_{ik}) / \sum_{m=1}^{K} \exp(d_{im})
$$

$d_{ik}$ 是预测补丁 $g_{S2i}$ 和其第 $k$ 个最相似补丁之间的缩放负 $L2$ 距离。


![](https://img-blog.csdnimg.cn/direct/da09586f074e435b886ab5740f11542d.png)



**讨论:** 我们提出的加权 top-K 相似性损失采用了一种动态策略，从真实渲染帧中搜索多个相关补丁，丰富了监督信号的多样性。这种方法鼓励模型找到比预定义标签更接近降解程度的高相似目标补丁，从而实现更准确和有效的训练。在我们的实验中，我们展示了这种设计的有效性，并证明当 NeRF 渲染帧的数据有限时，它显著提高了 GANs 的性能。

### B. 引导视点间混合器
在典型的 NeRF 设置下，高质量的输入视图是免费提供的，它们可以作为渲染视图恢复的潜在参考基础。为了实现视点间混合，NeRFLiX 提出了一个递归聚合模型来处理不同视角的变化。然而，如第 I 节所述，由于计算开销高，处理高分辨率帧仍然不切实际。

为了解决这一限制，我们提出了一种引导视点间混合器（称为“G-IVM”），具有高效的多视图融合模块。如图 9 所示，G-IVM 的框架架构。我们的方法首先利用现成的光流模型在低分辨率下预测渲染视图 $I$ 和其参考视图 $\{I_1, I_2\}$ 之间的粗对应关系。在粗预测的指导下，我们提出了一个金字塔神经网络来进行粗到精的聚合。

我们的引导视点间混合器是 NeRFLiX [86] 中引入的 IVM 方法的扩展，包括三个基本模块：特征提取、引导视点间聚合和金字塔重建。

**粗对应估计:** 为了在给定输入视图 $I$ 和其参考视图 $\{I_1, I_2\}$ 之间建立粗对应关系，我们利用预训练的 SPyNet [44] 模型在 $\frac{1}{4}$ 缩小尺度下预测光流 $C^\frac{1}{4}_{\{1,2\}}$。

**特征提取:** 我们引入了两个卷积编码器，称为“Encoder-1/2”，分别从渲染视图 $I$ 及其两个参考视图 $I_{\{1,2\}}$ 中提取深度金字塔图像特征 $F^{\frac{1}{8}, \frac{1}{4}, \frac{1}{2}}$ 和 $F^{\frac{1}{8}, \frac{1}{4}, \frac{1}{2}}_{\{1,2\}}$。具体来说，通过应用三个步幅长度为 2 的卷积层，金字塔特征分别位于 $\frac{1}{8}, \frac{1}{4}, \frac{1}{2}$ 尺度。

**引导视点间聚合:** 鉴于在渲染视图 $I$ 和其参考视图 $\{I_1, I_2\}$ 之间准确估计大位移的困难，我们提出了一种粗到精的引导视点间聚合方法。我们的方法采用光流引导的可变形卷积（FDCN）技术，利用 SPyNet 计算的光流来促进 $F^\frac{1}{8}$ 及其相应参考视图 $F^\frac{1}{8}_1, F^\frac{1}{8}_2$ 的聚合。该过程公式化为

$$
C^\frac{1}{8}_i = \frac{1}{2} \text{Bilinear} \left( C^\frac{1}{4}_i, \frac{1}{2} \right)
$$

$$
M^\frac{1}{8}_i = \left[ F^\frac{1}{8}, F^\frac{1}{8}_i, C^\frac{1}{8}_i \right]
$$

$$
F^\frac{1}{8}_i = \text{FDCN} \left( F^\frac{1}{8}_i, M^\frac{1}{8}_i, C^\frac{1}{8}_i \right)
$$

其中 $i \in \{1, 2\}$ 是参考索引， $\text{Bilinear}(\cdot, s)$ 是一个双线性插值函数（ $s$ 是缩放因子）， $M^\frac{1}{8}_ i$ 是一个偏移特征， $F^\frac{1}{8}_ i$ 表示从第 $i$ 个参考视图对目标图像对齐的特征。在获得两个聚合特征 $F^\frac{1}{8}_{\{1,2\}}$ 后，我们采用 3D 卷积层将它们与目标视图特征 $F^\frac{1}{8}$ 融合

$$
F^\frac{1}{8} = \text{Conv3D} \left( F^\frac{1}{8}_{\{1,2\}}, F^\frac{1}{8} \right)
$$

接下来，我们进行 $\frac{1}{4}$ 尺度聚合阶段。我们选择 2× 上采样的对等 $F^\frac{1}{8}$ 作为目标视图特征，而不是直接使用特征 $F^\frac{1}{4}$。这种选择是由于渲染视图 $I$ 中可能存在伪影。鉴于 $F^\frac{1}{8}$ 已经从参考视图聚合了高质量细节，它更适合进行 $F^\frac{1}{4}_i$ 和目标视图的对应估计

$$
F^\frac{1}{8} \uparrow 2_i = \text{Bilinear} \left( F^\frac{1}{8}, 2 \right)
$$

$$
M^\frac{1}{4}_i = \left[ F^\frac{1}{8} \uparrow 2_i, F^\frac{1}{4}_i, C^\frac{1}{4}_i \right]
$$

$$
F^\frac{1}{4}_i = \text{FDCN} \left( F^\frac{1}{4}_i, M^\frac{1}{4}_i, C^\frac{1}{4}_i \right)
$$

之后，我们采用另一个 3D 卷积来混合两个聚合特征 $F^\frac{1}{4}_{\{1,2\}}$

$$
F^\frac{1}{4} = \text{Conv3D} \left( F^\frac{1}{4}_{\{1,2\}}, F^\frac{1}{4} \right)
$$

最后，我们进行第三层聚合，使用与第二阶段相似的处理步骤获得融合特征 $F^\frac{1}{2}$。

**金字塔重建和多尺度监督:** 我们采用金字塔聚合特征 $F^{\frac{1}{8}, \frac{1}{4}, \frac{1}{2}}$ 生成多尺度输出。首先，从 $F^\frac{1}{8}$ 开始，我们使用卷积块获得最低尺度输出 $\hat{I}^\frac{1}{8}$。随后，我们上采样该输出并结合 $F^\frac{1}{4}$ 学习更高尺度的图像残差，生成 $\hat{I}^\frac{1}{4}$。通过遵循这一策略，我们最终预测出增强视图 $\hat{I}$。为了提高重建质量，我们在训练中引入了多尺度监督

$$
L^{\frac{1}{8}, \frac{1}{4}} = \lVert \hat{I}^{\frac{1}{8}, \frac{1}{4}} - I^{\frac{1}{8}, \frac{1}{4}}_{gt} \rVert_1
$$

$$
L_f = \lVert \hat{I} - I_{gt} \rVert_1
$$

$$
L = 0.1 * L^{\frac{1}{8}, \frac{1}{4}} + L_f
$$

其中 $I_{gt}, I^{\frac{1}{8}, \frac{1}{4}}_{gt}$ 是全分辨率和下采样的真实视图。


## VI. 实验
### A. 实现细节
首先，我们训练深度生成降解模拟器 150 K 次迭代。在此之后，我们冻结深度生成降解模拟器和 G-IVM 中使用的光流模型的权重，再训练 300 K 次迭代。然后，我们联合训练深度生成降解模拟器和 G-IVM 额外 300 K 次迭代，使用批量大小为 16 和裁剪的输入大小为 128× 128。我们使用与 NeRFLiX [86] 相同的数据增强技术，并采用 Adam 优化器和余弦退火学习率方案。

### B. 数据集和指标
按照 NeRFLiX，我们在三个流行数据集上进行实验：LLFF [39]、Tanks and Temples [26] 和 Noisy LLFF Synthetic [40]。前两个基准测试分别包含八个和五个真实场景。Noisy LLFF Synthetic 包含八个虚拟场景，我们手动应用相机射流到精确的相机姿态，以模拟不完美的野外校准。

我们使用 PSNR (↑)、SSIM [60] (↑) 和 LPIPS [80](↓) 指标来评估我们的方法，这与 NeRF 模型的评估标准一致。

### C. 相对于 SOTA NeRFs 的改进
我们验证了 NeRFLiX++ 的有效性，通过在各种数据集上持续提高最先进 NeRF 模型的性能。此外，我们进行了 NeRFLiX++ 和 NeRFLiX 之间的全面定量和定性比较，同时评估了它们各自的推理效率。

**LLFF:** 为了检验我们的 NeRFLiX++ 的增强潜力，我们调查了六个代表性模型，包括 NeRF [40]、TensoRF [9]、Plenoxels [17]、NeRF-mm [62]、NLF [1] 和 RegNeRF [42]。使用 NeRF 方法的渲染视图（以及它们的参考视图）作为我们的模型输入，我们旨在进一步提高合成质量。定量结果如表 II 所示。在两种协议下，NeRFLiX++ 表现出与 NeRFLiX 可比的改进，将 NeRF 模型的性能提升到前所未有的水平。例如，NeRFLiX++ 在 Plenoxels [17] 数据集上在 PSNR/SSIM/LPIPS 方面分别取得了 0.61 dB/0.025/0.054 的显著改进。值得注意的是，NeRFLiX++ 相比 NeRFLiX 展示了 2.4 倍更小的模型容量和 9.2 倍更快的 1024× 1024 图像处理速度，如表 II(c) 所示。此外，NeRFLiX++ 在处理 2048× 2048 的超高分辨率帧时，仅需 1.5 秒，表现出显著的效率。

**Tanks and Temples:** 与 LLFF 相比，Tanks and Temples 的相机视点变化较大。因此，即使是最近的先进 NeRF 模型，如 TensoRF [9] 和 DIVeR [63]，也未能合成出高质量的结果。如表 III(a) 所示，NeRFLiX 和 NeRFLiX++ 在这些模型上均表现出显著的性能提升。特别是，NeRFLiX++ 表现出增强的泛化能力，导致更显著的性能提升。例如，NeRFLiX++ 在 TensoRF [9] 模型上在 PSNR/SSIM/LPIPS 方面取得了 0.81 dB/0.017/0.035 的显著改进。

**Noisy LLFF Synthetic:** 除了上述野外基准测试，我们还展示了我们模型在 Noisy LLFF Synthetic 上的增强能力。从表 III(b) 显示的结果中可以看出，我们的 NeRFLiX++ 在两个 SOTA NeRF 模型上取得了显著的改进。


![](https://img-blog.csdnimg.cn/direct/48fabeb87fc242b2a6e3a20696b0ddad.png)




**性能与计算成本的权衡:** 我们研究了使用 NeRFLiX 和 NeRFLiX++ 时性能提升与计算开销之间的权衡。图 10 说明了 PSNR、推理时间和内存使用之间的关系。NeRFLiX++ 在保持高分辨率输入的可接受处理速度的同时，显著提高了最先进的 NeRF 模型。例如，NeRFLiX++ 仅增加了 0.77% 的推理时间（处理 1024× 1024 帧所需的 0.43 秒中的 56.04 秒），就提升了 NeRF 模型 0.75 dB。

**定性结果:** 图 11 提供了用于视觉评估的定性示例。结果表明，NeRFLiX++ 有效地恢复了更清晰的图像细节，同时显著减少了 NeRF 样式的伪影，突显了我们方法的有效性。

### D. NeRF 模型的训练加速
在本节中，我们展示了 NeRFLiX(++) 如何使 NeRF 模型即使在训练时间减半的情况下也能产生更好的结果。更具体地说，我们利用 NeRFLiX 和 NeRFLiX++ 在将两种 SOTA NeRF 模型训练期缩短至公开规定时间的一半后，改善其渲染图像。增强的结果优于全程训练的对比组，如表 III(c) 所示。值得注意的是，NeRFLiX 和 NeRFLiX++ 都将 Plenoxels [17] 的训练期从 24 分钟减少到 10 分钟，同时持续提高了渲染图像的质量。


![](https://img-blog.csdnimg.cn/direct/1a3fc5f1152e475ea0d89960e37130ab.png)


![](https://img-blog.csdnimg.cn/direct/75bcc8b7dba847a8bab4f7c50ce712ca.png)



### E. 消融研究
在本节中，我们在 LLFF [39] 上使用 LLFF-P1 协议进行全面实验，以分析我们的每个设计。我们使用 TensoRF [9] 作为基准。数据模拟质量。为了评估模拟质量，我们测量了由各种降解模型生成的真实渲染帧和模拟帧之间的分布相似性：BSR [77]、NeRFLiX、NeRFLiX++ 中的深度生成降解和 NeRFLiX++。我们使用 t-SNE [53] 可视化使用 Inception-v3 [49] 提取的深度图像特征，如图 12 所示。值得注意的是，我们的两阶段模拟器 NeRFLiX++ 生成的模拟数据在统计上最接近真实渲染图像，超过了其他模型。定量分析进一步支持了这一发现，如表 IV 和表 V 所示。

此外，我们进行了彻底的分析，以评估降解的各个组件，即人类设计的模拟器（H-C.S）和深度生成模拟器（D-G.S）。我们训练了八个模型，逐步整合了更多的降解策略。表 V 中的结果突显了每个降解组件在实现理想结果中的重要性。值得注意的是，使用生成降解模拟器的模型优于仅依赖手工降解模拟器的模型。

**加权 Top-K 相似性损失 (WKS):** 我们评估了提出的 WKS 的性能。为了比较，我们训练了一个使用传统 $L_1$ 损失作为监督的额外 G-IVM 模型。表 VI 中的结果表明，与使用提出的 WKS 训练的模型相比，该模型的性能显著较差。这一结果强调了 WKS 对深度降解训练的有效性。此外，我们研究了 WKS 监督中不同数量（K）相似补丁的影响。我们训练了四个额外的 G-IVM 模型。如表 VI 所示，随着相似补丁数量从 $K = 1$ 增加到 $K = 5$，PSNR 值逐步提高，之后改进趋于饱和。这种行为是预期的，因为相对较小的相似性图像补丁对整体性能的贡献较少。

**G-IVM 中的金字塔融合:** 为了利用视点间帧的多尺度上下文信息，我们引入了金字塔引导聚合结构。表 VII(a) 显示，额外聚合和重建级别的整合持续提高了最终性能。值得注意的是，我们的完整模型（Model-A）取得了最高的 PSNR/SSIM 分数。

**G-IVM 中的流引导:** 为了解决高分辨率帧中的不同视点变化，我们引入了利用粗光流引导聚合过程。为了评估这种策略的重要性，我们训练了一个称为“NG-IVM”的额外模型，在相同的实验设置下进行训练，但没有使用光流引导。表 VII(b) 中的结果清楚地表明，我们的引导视点间混合器比 NG-IVM 模型表现出显著的优势，突显了我们设计的有效性。

**G-IVM 中的目标对齐:** 在第 $l$ 层融合中，我们偏离现有方法[7], [28], [57], [85]，这些方法将 $F^l$ 视为目标特征。相反，除了第一层对齐 $(l = 0)$ 外，我们建议使用先前聚合的特征 $F^{l-1}(l > 0)$。为了验证这种设计选择的有效性，我们比较了这两种策略，结果如表 VII(c) 所示。我们的融合策略在 PSNR、SSIM 和 LPIPS 方面优于现有解决方案，表明我们的设计更适合 NeRF-无关恢复任务。

**对应估计规模:** 在第 I 节中，我们讨论了在降低分辨率（缩小因子为 $\times 4$）下使用粗对应估计的潜在优势。在这里，我们展示了其他尺度（ $\times 1, \times 2, \times 8, \times 16$）的比较。表 VIII 提供了四个 NeRFLiX++ 模型（NeRFLiX++ $\{\times 1, \times 2, \times 8, \times 16\}$）的结果，此外还有默认设置（称为“NeRFLiX++ $\times 4$”）。值得注意的是，NeRFLiX++ $\times 4$ 在 PSNR 和 SSIM 方面取得了优异的结果。值得注意的是，更大的降尺度（例如， $\times 16$）可能会影响高分辨率聚合的准确引导性能。

此外，我们评估了不同对齐尺度的计算成本。观察到进行高分辨率光流估计额外增加了大约 350 ms 的推理时间。我们的 $\times 4$ 模型在性能和效率之间实现了良好的平衡，相比于其他模型。


![](https://img-blog.csdnimg.cn/direct/2df9909b0774471eb34d4bef86b8e7d3.png)


![](https://img-blog.csdnimg.cn/direct/83555d120e794ef4a1bff109e7144c1c.png)


![](https://img-blog.csdnimg.cn/direct/401167012751413d98d5fa4d234634b8.png)



## VII. NeRFLiX++ 用于 4K 图像
除了低分辨率新视图渲染中常见的挑战（如伪影和模糊）之外，使用现有 NeRF 模型渲染高分辨率图像（即 4K 分辨率）需要大量计算资源。即使是 TensoRF [9] 使用高度优化的数据结构，如张量分解，在 NVIDIA RTX 3090 上训练 TensoRF 模型以获得 2K 和 4K 图像仍然是不切实际的，因为 GPU 内存有限。

在本节中，我们研究了利用 NeRFLiX++ 对不同 NeRF 模型生成的低分辨率图像进行超分辨率和增强的潜力，从而生成高质量的 4K 结果。我们首先定义问题，然后讨论对 G-IVM 模型所做的修改。随后，我们进行定量和定性分析，以评估 NeRFLiX++ 在 4K 图像上的有效性。

### A. 问题公式化
给定由 NeRF 模型生成的低分辨率（1K）目标帧 $I$ 及其两个 4K 参考视图 $\{I_1, I_2\}$，NeRFLiX++4K 旨在恢复具有照片般真实细节的 4K 输出。

### B. 框架
我们提出的 G-IVM 框架如图 9 所示。为了实现 4K 图像的恢复，我们对 G-IVM 框架进行了最小的修改。

**编码器:** 为了适应输入帧 $I \in \mathbb{R}^{H \times W}$ 和其参考视图 $\{I_1, I_2\} \in \mathbb{R}^{4H \times 4W}$ 之间的分辨率差异，我们引入了以下调整。对于编码器-1，我们加入了两个步幅为 2 的卷积层，从而获得了降采样的参考特征 $F^{\frac{1}{4}, \frac{1}{2}, 1}_ {\{1,2\}} \in \mathbb{R}^{\{H \times W, 2H \times 2W, 4H \times 4W\}}$。编码器-2 不涉及任何下采样。因此，两个最低分辨率参考特征 $F^\frac{1}{4}_{\{1,2\}} \in \mathbb{R}^{H \times W}$ 匹配输入特征 $F \in \mathbb{R}^{H \times W}$ 的空间分辨率。

我们训练了两个 NeRFLiX++4K 模型，一个使用 $L_1$ 损失（NeRFLiX++4K $-L_1$），另一个使用 $L_1$ 和 GAN 损失的组合（NeRFLiX++4K $-GAN$）。

**实现细节:** 与原始 NeRFLiX++ 相比，当使用 1K 渲染帧作为输入时，我们用其 4K 对应帧（从 LLFF-T4 获得）替换了两个 1K 参考帧，同时保持其他训练细节不变。

对于从 Vimeo 数据集中获得的样本，我们最初将输入帧下采样至 $\times 4$，从而建立与 LLFF-T 相同的设置。换句话说，该配置涉及低分辨率输入视图和两个高分辨率参考视图。

### C. NeRFs 用于 4K 图像的改进
为了评估 NeRFLiX++4K 的有效性，我们使用不同的恢复方法（M1-M5）从低分辨率输入生成 4K 图像，输入由三种最先进的 NeRF 模型生成：TensoRF [9]、Plenoxels [17] 和 DVGO [48]。表 IX 的结果表明，所有涉及 NeRFLiX 和 NeRFLiX++ 的模型（M2-M5）均优于简单的双三次上采样（M1），表明了 NeRFLiX 和 NeRFLiX++ 的恢复能力。特别是，NeRFLiX++4K $-L_1$（M4）和 NeRFLiX++4K $-GAN$（M5）在 PSNR、SSIM 和 LPIPS 方面取得了最佳性能。此外，图 13 直观地展示了 NeRFLiX++4K $-L_1$（M4）生成了高质量的 4K 帧，具有更清晰的纹理和减少的渲染伪影。同时，NeRFLiX++4K $-GAN$（M5）生成了更多的高频细节和更锐利的边缘，产生了视觉上令人愉悦的结果。

**与 4K-NeRF 的比较:** 我们还将 NeRFLiX++4K 与 4K-NeRF [61] 进行了比较。表 X(a) 的结果表明，NeRFLiX++4K $-L_1$ 相比 4K-NeRF $-L_1$ 在 PSNR 上取得了显著的 0.77 dB 改进。此外，NeRFLiX++4K $-GAN$ 在感知质量方面超过了 4K-NeRF。图 14 直观地展示了 4K-NeRF $-GAN$ 未能重建细微的图像结构，而 NeRFLiX++4K $-GAN$ 有效地从噪声 1K 照片中恢复了自然图像内容，产生了优越的视觉增强效果。

**与 NeRF-SR 的比较:** 我们还将 NeRFLiX++4K $-L_1$ 与 NeRF-SR [54] 进行了比较。NeRF-SR 是一种两阶段的新视图合成方法。在第一阶段，他们提出了一种超采样 NeRF 模型，从低分辨率训练照片生成超分辨率新视图。然后，他们利用一个精细化模块来增强第一阶段的结果。为了确保公平比较，我们利用我们的 NeRFLiX++4K $-L_1$ 模型来增强其第一阶段的结果，并将其与精细化结果进行定量比较。表 X(b) 表明，NeRFLiX++4K $-L_1$ 显著优于 NeRF-SR，突显了我们方法的有效性。

除了卓越的性能外，与需要重新训练新场景的 4K-NeRF 和 NeRF-SR 模型不同，NeRFLiX++4K 提供了 NeRF 无关和场景无关的优势。这一特性允许快速高效地部署 NeRFLiX++4K 于各种场景中。

**与现有图像和视频恢复器的比较:** 此外，我们还将我们的 NeRFLiX++4K 模型与最先进的图像和视频超分辨率方法进行了比较，如 SwinIR [33]、RealESRGAN [58] 和 RealBasicVSR [8]。使用 TensoRF [9] 作为基准，我们利用这些模型生成增强的高分辨率图像，并在表 XI 中详细展示了结果。尽管这些模型为一般的现实世界图像生成了有希望的恢复结果，但它们在表现上均不如我们的 NeRFLiX++4K 模型，后者展示了 NeRFLiX++ 在 NeRF 渲染照片上的出色恢复能力。


![](https://img-blog.csdnimg.cn/direct/7e7ef5c6ef6c4cd5a61a394483f75dc1.png)

![](https://img-blog.csdnimg.cn/direct/eb1b15fe91b142cab3dc3fabd411e1fb.png)

![](https://img-blog.csdnimg.cn/direct/f4a0e538585d4f99999c1d7559454668.png)

![](https://img-blog.csdnimg.cn/direct/95c2dce44e4c490096ea340a22dd5143.png)

![](https://img-blog.csdnimg.cn/direct/174cf29c50834569b2525beebbc5a5f5.png)



**4K 视频演示:** 我们准备了一个视频演示，展示了我们提出的 NeRFLiX++ 方法的增强能力，可以在 [YouTube](https://www.youtube.com/watch?v=YiXvgQXiWII) 上观看。该视频分为三个部分。视频的前两部分强调了 TensoRF 和 Plenoxels 在生成令人满意的 1K 新视图时的挣扎，而 NeRFLiX++ 能够从这些低分辨率的噪声视图中恢复超高分辨率的输出。值得注意的是，NeRFLiX++ 甚至在 4K 分辨率下恢复了可识别的字符和更锐利的纹理。最后一部分展示了 NeRFLiX++ 可以显著提高各种 NeRF 模型（如 TensoRF [9]、Plenoxels [17]、RegNeRF [42]、NLF [1]、DIVeR [63]、NeRF-mm [62] 等）的视觉质量。

## VIII. 结论
我们介绍了 NeRFLiX，这是一种通用的 NeRF-无关范式，用于高质量的神经视图合成恢复。我们系统地分析了 NeRF 渲染流程，并引入了 NeRF 样式的降解概念。为了消除 NeRF 样式的伪影，我们提出了一种新颖的 NeRF 样式降解模拟器，并构建了大规模模拟数据集。通过在模拟数据集上训练最先进的深度神经网络，我们成功去除了 NeRF 伪影。此外，我们提出了一种视点间混合器，通过聚合多视图帧来恢复 NeRF 渲染帧中缺失的细节。大量实验验证了 NeRFLiX 的有效性。

为了进一步增强 NeRFLiX 的恢复能力和推理效率，我们提出了 NeRFLiX++。它通过引入更好的降解建模和更快的视点间聚合技术，改进了 NeRFLiX。NeRFLiX++ 实现了逼真的 4K 视图合成能力，并在我们的广泛实验中展示了卓越的定量和定性性能。
# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。










