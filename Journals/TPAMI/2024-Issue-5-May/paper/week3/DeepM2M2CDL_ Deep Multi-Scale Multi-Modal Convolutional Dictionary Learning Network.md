# 题目：[DeepM2M2CDL: Deep Multi-Scale Multi-Modal Convolutional Dictionary Learning Network](https://ieeexplore.ieee.org/document/10323520/)  
## DeepM2M2CDL：深度多尺度多模态卷积字典学习网络
**作者：Xin Deng; Jingyi Xu; Fangyuan Gao; Xiancheng Sun; Mai Xu** 

**源码链接：** https://github.com/JingyiXu404/TPAMI-DeepM2CDL
****
# 摘要

对于多模态图像处理，由于模态间的复杂依赖性，网络的可解释性至关重要。最近，一个有前景的研究方向是通过展开策略将字典学习整合到深度学习中，以实现可解释的网络。然而，现有的多模态字典学习模型都是单层和单尺度的，这限制了其表示能力。在本文中，我们首先介绍了一个多尺度多模态卷积字典学习（M2CDL）模型，该模型采用多层策略执行，以粗到细的方式关联不同的图像模态。然后，我们提出了一个名为DeepM2CDL的统一框架，该框架源自M2CDL模型，用于多模态图像恢复（MIR）和多模态图像融合（MIF）任务。DeepM2CDL的网络架构完全匹配M2CDL模型的优化步骤，使每个网络模块具有良好的可解释性。与手工制作的先验知识不同，字典和稀疏特征先验都是通过网络学习的。我们在多种MIR和MIF任务上评估了所提出的DeepM2CDL的性能，其定量和定性结果都显示出比许多最先进的方法更优越。此外，我们还可视化了从网络中学到的多模态稀疏特征和字典滤波器，这展示了DeepM2CDL网络的良好可解释性。

# 关键词

- 卷积字典学习
- 可解释网络
- 多模态图像处理

# I. 引言

多模态图像指的是在同一场景下由不同传感器捕获的图像，例如RGB图像、深度图像、近红外（NIR）图像、多光谱图像等。不同的图像模态可以提供互补的信息，以更全面地表示图像场景，因此在自动驾驶[1]、视频监控[2]和虚拟现实[3]等应用中扮演着越来越重要的角色。然而，由于获取传感器的限制，不同的图像模态在图像质量上可能会有显著差异。多模态图像恢复（MIR）和多模态图像融合（MIF）是多模态图像处理中两个不可或缺的任务。MIR任务的目标是在高质量模态的引导下恢复低质量模态，而MIF任务的目标是融合两种不同的图像模态以获得全面的信息。这两个任务在输入模态、特征提取和信息交互方面有许多相似之处。因此，在本文中，我们尝试通过一个通用框架来同时解决这两个任务。

现有的MIR和MIF方法大致可以分为两类，基于模型的方法和最近的基于深度学习的方法。对于基于模型的方法，它们通常基于联合图像滤波[4][5][6][7]或字典学习[8][9][10][11]。联合图像滤波方法利用不同图像模态之间的结构相似性进行多模态图像恢复或融合。代表性的工作包括联合双边滤波器（JBF）[4]、引导图像滤波器（GIF）[5]和相互引导图像滤波器（muGIF）[7]。对于基于字典学习的方法，基本思想是不同的图像模态应该共享一些共同的稀疏表示，这些表示由特别设计的字典表示，以便跨模态的显著特征可以紧密相关。相关工作包括用于多模态图像超分辨率的耦合稀疏表示（CSR）[9]，用于图像融合的多尺度稀疏表示（MSR）[11]等。然而，图像滤波和基于字典学习的方法都面临着同一个问题，即算法非常耗时。为了克服这个缺点，已经提出了许多基于深度学习的神经网络，如RGB引导的深度图像超分辨率[12][13][14]、多焦点图像融合[15][16]等。尽管性能良好，但基于深度学习的方法纯粹是数据驱动的，通常忽略了模态间的多模态先验。此外，深度网络通常不易解释，即难以弄清楚网络内部发生了什么。

对于多模态图像恢复和融合，网络的可解释性非常重要，因为不同图像模态之间的关系非常复杂。考虑到基于模型方法的良好可解释性，许多工作[17][18][19][20]专注于将基于模型方法的先验知识整合到深度神经网络中，以赋予网络良好的可解释性。例如，Deng等人[17]首先提出了通过联合字典学习建模多模态图像，然后通过展开迭代收缩和阈值算法（ISTA）将稀疏先验整合到网络设计中。然而，CoISTA字典学习模型[17]是通过将图像划分为小块来执行的，这忽略了全局依赖性。为了克服这个缺点，Marivani等人[18]设计了一个可解释的多模态网络，通过展开学习到的多模态卷积稀疏编码（LMCSC），该编码在图像级别上执行。然而，这些工作有几个缺点，如表1所示。首先，字典学习模型都是单层和单尺度的，这限制了其表示能力。其次，网络中只学习了稀疏表示，而更重要的字典学习被忽略了。这意味着学习到的字典不适应不同的多模态图像。第三，CoISTA和LMCSC都使用展开策略设计网络，这要求稀疏先验为 $\ell_ 1$ 范数。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/2df2fd309bed49c291e433d33f6b0421.png#pic_ center" width="70%" />
</div>

在本文中，为了克服上述缺点，我们提出了一个深度多尺度多模态卷积字典学习（DeepM2CDL）网络，用于MIR和MIF任务。与现有方法CoISTA[17]和LMCSC[18]相比，我们的创新和改进如下：1）与CoISTA和LMCSC中的单层和单尺度模型相比，我们提出了一个多层和多尺度的卷积字典模型，能够提取更详细的特征。2）与CoISTA和LMCSC只更新稀疏表示不同，我们的DeepM2CDL交替更新不同模态的字典和稀疏表示。这样，学习到的字典能够适应不同的多模态图像，从而提高了MIR和MIF任务的性能。3）我们设计了一个网络来学习多模态图像先验，而不是手工制作的 $\ell_ 1$ 范数稀疏先验。此外，我们的网络架构是严格按照字典学习过程设计的，而不是展开策略。本文的主要贡献总结如下：

- 我们介绍了一个多尺度和多模态卷积字典学习（M2CDL）模型，该模型采用多层策略执行，以粗到细的方式关联不同的图像模态。
- 我们从M2CDL模型派生出一个统一的网络，用于MIR和MIF任务。网络架构具有良好的可解释性，完全匹配M2CDL模型的多尺度和多层优化步骤。
- 我们可视化了从网络中学到的多模态稀疏特征和字典滤波器，展示了所提出的DeepM2CDL网络的良好可解释性。

本文的其余部分组织如下。第二节回顾了多模态字典学习和模型驱动的可解释网络的相关研究。第三节分别介绍了MIR和MIF任务的多尺度多模态卷积字典学习（M2CDL）模型。第四节详细介绍了所提出的DeepM2CDL网络的架构。第五节讨论了不同MIP任务上的实验结果，第六节对本文进行了总结。

# II. 相关工作

## A. 多模态字典学习

多模态字典学习旨在为不同的图像模态学习一组多模态字典滤波器，这在许多多模态图像处理任务中已被广泛使用，例如引导图像超分辨率[9]、[21]、医学图像融合[22]和图像分离[23]。传统的多模态字典学习方法大多在块级别上执行，即整个图像被分割成小块进行字典学习。例如，Kwon等人[10]提出了为RGB和深度图像学习一组多尺度字典，以实现RGB引导的深度图像超分辨率。这些字典需要共享相同的稀疏表示，以密切关联不同的模态。对于图像分离，Deligiannis等人[23]提出了为每种模态学习两个字典，其中一个字典用于共同特征表示，另一个字典用于不同特征表示。这种耦合字典学习方法也已用于多模态图像超分辨率和去噪[9]、[24]。除了上述低级任务，多模态字典学习还被用于一些高级视觉任务，如图像分类[25]和个人重新识别[26]。

上述方法基于块级别的字典学习，通常忽略了整个图像的全局一致性，导致学习效率低下。为了解决这个问题，提出了多模态卷积字典学习，它可以在图像级别上捕获不同模态之间的全局关系。例如，Liu等人[27]将卷积字典学习引入多模态图像融合，显著提高了融合性能并减少了对图像错位的敏感性。对于多模态图像去噪，Gao等人[28]提出了一个在离散傅里叶变换（DFT）域中的耦合卷积字典学习，它可以从不同的图像模态中提取共同信息以提高去噪性能。对于RGB+NIR成像，Hu等人[29]应用多通道卷积稀疏编码从混合测量中重建RGB和NIR图像。对于神经活动检测，Tour等人[30]在大脑信号上应用了多变量卷积稀疏编码，能够检测出典型的时间波形和非正弦μ形模式。多模态字典学习方法具有良好的可解释性，然而，它们的计算复杂性非常高，这使得它们在实际应用中难以应用。

## B. 模型驱动的可解释深度网络

网络的可解释性一直是深度学习的一个重大问题。最近，许多研究工作专注于通过展开策略将图像的先验知识整合到网络设计中，以提高网络的可解释性。众所周知，许多基于模型的参数估计算法通过迭代更新参数，例如用于稀疏估计的迭代收缩和阈值算法（ISTA）、用于压缩感知的近似消息传递（AMP）算法，以及用于一般逆问题的交替方向乘子法（ADMM）。直观地，我们可以通过展开迭代步骤，将这些迭代算法转化为深度网络，即每个迭代可以被视为一个网络层或块。例如，Sun等人[31]提出了将ADMM算法的迭代步骤展开成深度网络，以快速重建磁共振成像（MRI）。对于盲图像去模糊，Li等人[32]从传统的梯度域总变分正则化方法中推广了一个迭代算法，并将其映射到神经网络中。最近，为了图像去噪，Xu等人[33]将其表述为一个多尺度卷积稀疏编码问题，并提出了一个迭代解决方案来解决这个问题。这个迭代解决方案随后被转化为一个可解释的网络，该网络在参数较少的情况下实现了良好的去噪性能。

与上述只涉及一种图像模态的任务相比，对于具有多个图像模态的MIP任务，网络的可解释性更加迫切和必要。在这个领域，已经进行了许多工作，例如多模态图像超分辨率[17]、[18]、[19]，图像配准[34]和图像融合[19]、[35]等。具体来说，对于多模态图像超分辨率，Deng等人[17]提出了一个联合字典学习模型，然后通过两个学习到的迭代收缩和阈值算法（LISTA）[36]分支展开模型以生成深度网络。Marivani等人[18]建立了一个具有边信息的卷积稀疏编码模型，用于引导图像超分辨率。然后他们提出了一个可以展开成可解释网络的近端解决方案。对于图像配准，Blendowski等人[34]将基于CNN的多模态特征整合到经典图像配准流程中，并提出了一个用于对齐MRI和CT图像的监督迭代下降算法（SUITS）。对于图像融合，Xie等人[35]构建了一个多光谱和高光谱图像融合模型，然后通过近端梯度算法优化了这个模型。优化过程被映射到一个深度网络中，其中每个模块都有清晰物理意义。对于医学图像分割，Zhang等人[37]设计了一个模态感知模块，以可学习的方式适应性地聚合模型特定特征，这使得跨模态的信息交换可解释。

# III. 多尺度多模态卷积字典学习 (M2CDL) 模型

对于多模态图像恢复和融合，最重要的问题是如何开发和建模不同模态之间的依赖性。在本节中，我们从卷积字典学习的角度对多模态依赖性进行建模，分别介绍了用于MIR和MIF任务的多尺度多模态卷积字典学习模型。本节使用的主要符号列表在表II中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c9dca81633874d6b9cb15c472b29353f.png#pic_ center" width="70%" />
</div>

## A. MIR任务的M2CDL模型

多模态图像恢复（MIR）任务的问题可以被表述如下：给定一个来自某一模态的低质量（LQ）图像  $x \in \mathbb{R}^{m \times n}$ ，以及另一个模态的高质量（HQ）图像  $y \in \mathbb{R}^{m \times n}$ ，我们的目标是在  $y$  的引导下恢复  $x$  的 HQ 版本。这里，我们使用符号  $z \in \mathbb{R}^{m \times n}$  来表示模态  $x$  的恢复 HQ 图像。与其他方法不同，我们以粗到细的方式对  $x$ 、 $y$  和  $z$  之间的关系进行建模，以提高建模精度。首先，我们分别通过多尺度卷积字典学习对  $x$  和  $y$  进行建模，其中  $x$  和  $y$  可以分解为几个尺度，每个尺度都有自己的字典滤波器，如下所示：

$$
x = \sum_ {s=1}^{S} x_ s = \sum_ {s=1}^{S} \sum_ {c=1}^{C} d_ {s,c} \ast u_ {s,c}
$$ 

$$
y = \sum_ {s=1}^{S} y_ s = \sum_ {s=1}^{S} \sum_ {t=1}^{T} h_ {s,t} \ast v_ {s,t}
$$ 

这里， $S$  表示尺度的总数，符号  $\ast$  表示卷积操作。 $x_ s$  和  $y_ s$  分别表示  $x$  和  $y$  在尺度  $s$  的图像分解。对于尺度  $s$ ， $\{ d_ {s,c} \}_ {c=1}^{C} \in \mathbb{R}^{k_ {s,c} \times k_ {s,c} \times C}$  和  $\{ h_ {s,t} \}_ {t=1}^{T} \in \mathbb{R}^{k_ {s,t} \times k_ {s,t} \times T}$  是  $x_ s$  和  $y_ s$  的卷积字典滤波器，滤波器大小分别为  $k_ {s,c} \times k_ {s,c}$  和  $k_ {s,t} \times k_ {s,t}$ 。字典滤波器的总数对于  $x_ s$  和  $y_ s$  分别是  $C$  和  $T$ 。相应地， $\{ u_ {s,c} \}_ {c=1}^{C} \in \mathbb{R}^{m \times n \times C}$  是通过字典滤波器  $\{ d_ {s,c} \}_ {c=1}^{C}$  从  $x_ s$  提取的一组稀疏特征，而  $\{ v_ {s,t} \}_ {t=1}^{T} \in \mathbb{R}^{m \times n \times T}$  是通过  $\{ h_ {s,t} \}_ {t=1}^{T}$  从  $y_ s$  提取的一组稀疏特征。

由于图像  $x$  的质量较低，如果我们只使用从  $x$  提取的稀疏特征， $z$  的恢复可能不令人满意。考虑到  $x$  和  $y$  是在同一场景下获取的，从高质量图像  $y$  提取的特征可以帮助我们更好地从  $x$  恢复  $z$ 。这也是大多数MIR方法的动机。基于这一观察，我们定义了一组非线性映射函数  $\{ F_ s^1(\cdot) \}_ S$ 。对于尺度  $s$ ， $F_ s^1(\cdot)$  利用两种模态的特征，生成联合特征  $\{ f_ {s,c,1} \}_ {c=1}^{C}$  从  $\{ u_ {s,c} \}_ {c=1}^{C}$  和  $\{ v_ {s,t} \}_ {t=1}^{T}$ ，如下所示：

$$
\{ f_ {s,c,1} \}_ {c=1}^{C} = F_ s^1 ( \{ u_ {s,c} \}_ {c=1}^{C}, \{ v_ {s,t} \}_ {t=1}^{T} )
$$ 

这里，联合稀疏特征  $\{ f_ {s,c,1} \}_ {c=1}^{C} \in \mathbb{R}^{m \times n \times C}$  通过整合  $x_ s$  和  $y_ s$  的特征获得。因此，它比  $\{ u_ {s,c} \}_ {c=1}^{C}$  对  $z$  的恢复更有用。为了使字典更好地匹配联合稀疏特征，我们需要通过以下转换函数  $B_ s^1(\cdot)$  进一步更新字典  $\{ d_ {s,c} \}_ {c=1}^{C}$  到  $\{ b_ {s,c,1} \}_ {c=1}^{C} \in \mathbb{R}^{k_ c \times k_ c \times C}$ ：

$$
\{ b_ {s,c,1} \}_ {c=1}^{C} = B_ s^1 ( \{ d_ {s,c} \}_ {c=1}^{C} )
$$ 

对于MIR任务，恢复图像  $z$  的大部分内容来自图像  $x$ ，因此新的字典在(3)中只基于  $x_ s$  的字典  $\{ d_ {s,c} \}_ {c=1}^{C}$  更新。在我们获得了每个尺度的  $\{ f_ {s,c,1} \}_ {c=1}^{C}$  和  $\{ b_ {s,c,1} \}_ {c=1}^{C}$  之后，可以生成一个中间恢复图像  $\hat{x}_ 1 \in \mathbb{R}^{m \times n}$ ，如下所示：

$$
\hat{x}_ 1 = \sum_ {s=1}^{S} \hat{x}_ s^1 = \sum_ {s=1}^{S} \sum_ {c=1}^{C} b_ {s,c,1} \ast f_ {s,c,1}
$$ 

有了联合特征和更新的字典，图像  $\hat{x}_ 1$  预计比  $x$  有更高的图像质量，可以被视为目标图像  $z$  的一个粗略版本。接下来，考虑到  $\hat{x}_ 1$  作为新的输入，我们可以进一步对  $\hat{x}_ 1$  和  $y$  进行建模，如下所示：

$$
\hat{x}_ 1 = \sum_ {s=1}^{S} \sum_ {c=1}^{C} d_ {s,c,1} \ast u_ {s,c,1}
$$ 

$$
y = \sum_ {s=1}^{S} \sum_ {t=1}^{T} h_ {s,t,1} \ast v_ {s,t,1}
$$ 

然后，以相同的方式，我们可以更新不同尺度上的联合稀疏特征  $\{ f_ {s,c,2} \}_ {c=1}^{C}$  和字典  $\{ b_ {s,c,2} \}_ {c=1}^{C}$ ，如下所示：

$$
\{ f_ {s,c,2} \}_ {c=1}^{C} = F_ s^2 ( \{ u_ {s,c,1} \}_ {c=1}^{C}, \{ v_ {s,t,1} \}_ {t=1}^{T} )
$$ 

$$
\{ b_ {s,c,2} \}_ {c=1}^{C} = B_ s^2 ( \{ d_ {s,c,1} \}_ {c=1}^{C} )
$$ 

通过将  $\{ f_ {s,c,2} \}_ {c=1}^{C}$  与  $\{ b_ {s,c,2} \}_ {c=1}^{C}$  进行卷积，我们可以生成第二个中间恢复图像  $\hat{x}_ 2$ ：

$$
\hat{x}_ 2 = \sum_ {s=1}^{S} \hat{x}_ s^2 = \sum_ {s=1}^{S} \sum_ {c=1}^{C} b_ {s,c,2} \ast f_ {s,c,2}
$$ 

按照相同的方式，可以生成一系列中间恢复图像  $\{ \hat{x}_ q \}_ N^{q=1}$ ，逐渐接近目标图像  $z$ 。最终，我们可以将恢复图像  $z$  作为最后一个中间图像  $\hat{x}_ N$  获得：

$$
z = \hat{x}_ N = \sum_ {s=1}^{S} \hat{x}_ s^N = \sum_ {s=1}^{S} \sum_ {c=1}^{C} b_ {s,c,N} \ast f_ {s,c,N}
$$ 

以上完成了MIR任务的M2CDL模型的描述。

## B. MIF任务的M2CDL模型

对于多模态图像融合（MIF）任务，我们的目标是生成一个新的图像  $z$ ，它包含了源图像  $x$  和  $y$  的有利信息。与MIR任务不同，在MIF任务中， $x$  和  $y$  模态被认为具有相同的图像质量。因此，我们对MIF任务的M2CDL模型进行了微调。首先，与MIR任务一样，我们按照(1)分别对  $x$  和  $y$  进行多尺度建模。然后，考虑到每种模态都包含另一种模态所缺失的信息，我们为尺度  $s$  的  $x$  和  $y$  模态分别生成联合特征  $\{ f_ {s,c,1} \}_ {c=1}^{C}$  和  $\{ g_ {s,t,1} \}_ {t=1}^{T}$ ，如下所示：

$$
\{ f_ {s,c,1} \}_ {c=1}^{C} = F_ s^1 ( \{ u_ {s,c} \}_ {c=1}^{C}, \{ v_ {s,t} \}_ {t=1}^{T} )
$$ 

$$
\{ g_ {s,t,1} \}_ {t=1}^{T} = G_ s^1 ( \{ u_ {s,c} \}_ {c=1}^{C}, \{ v_ {s,t} \}_ {t=1}^{T} )
$$ 

在上述方程中，我们使用非线性映射函数  $F_ s^1(\cdot)$  和  $G_ s^1(\cdot)$  分别为  $x$  和  $y$  模态在尺度  $s$  生成联合特征。相应地，它们的字典通过转换函数  $B_ s^1(\cdot)$  和  $E_ s^1(\cdot)$  更新，如下所示：

$$
\{ b_ {s,c,1} \}_ {c=1}^{C} = B_ s^1 ( \{ d_ {s,c} \}_ {c=1}^{C}, \{ h_ {s,t} \}_ {t=1}^{T} )
$$ 

$$
\{ e_ {s,t,1} \}_ {t=1}^{T} = E_ s^1 ( \{ d_ {s,c} \}_ {c=1}^{C}, \{ h_ {s,t} \}_ {t=1}^{T} )
$$ 

这里， $\{ b_ {s,c,1} \}_ {c=1}^{C} \in \mathbb{R}^{k_ c \times k_ c \times C}$  和  $\{ e_ {s,t,1} \}_ {t=1}^{T} \in \mathbb{R}^{k_ t \times k_ t \times T}$  分别表示每个模态在尺度  $s$  更新后的字典，与联合特征相匹配。然后，我们可以生成中间融合图像  $\hat{x}_ 1$  和  $\hat{y}_ 1$ ，如下所示：

$$
\hat{x}_ 1 = \sum_ {s=1}^{S} \hat{x}_ s^1 = \sum_ {s=1}^{S} \sum_ {c=1}^{C} b_ {s,c,1} \ast f_ {s,c,1}
$$

$$
\hat{y}_ 1 = \sum_ {s=1}^{S} \hat{y}_ s^1 = \sum_ {s=1}^{S} \sum_ {t=1}^{T} e_ {s,t,1} \ast g_ {s,t,1}
$$ 

有了联合特征和字典，图像  $\hat{x}_ 1$  包含了  $x$  和  $y$  的信息，图像  $\hat{y}_ 1$  也是如此。这意味着我们更接近我们的融合目标  $z$ 。在下一步中，我们考虑  $\hat{x}_ 1$  和  $\hat{y}_ 1$  作为新输入，以相同的方式生成第二组中间融合图像  $\hat{x}_ 2$  和  $\hat{y}_ 2$ 。最后，我们可以生成一系列中间融合图像  $\{ \hat{x}_ q, \hat{y}_ q \}_ N^{q=1}$ 。理想情况下，随着迭代次数  $q$  的增加，生成的中间图像  $\hat{x}_ q$  和  $\hat{y}_ q$  将更接近目标融合图像  $z$ 。经过  $N$  次迭代后，融合图像  $z$  可以通过  $\hat{x}_ N$  和  $\hat{y}_ N$  的加权平均获得。通过引入权重变量  $W_ x$  和  $W_ y$ ，MIF任务中的最终融合结果  $z$  可以按照以下方式获得：

$$
z = W_ x \cdot \hat{x}_ N + W_ y \cdot \hat{y}_ N
$$ 

注意，同一位置的  $W_ x$  和  $W_ y$  中的元素之和应等于1。这完成了MIF任务的M2CDL模型的描述。

# IV. 网络架构

基于第III节中的两个M2CDL模型，我们开发了一个统一的DeepM2CDL网络来解决MIR和MIF任务，如图1所示。在本节中，我们首先在第IV-A节介绍用于MIR任务的DeepM2CDL网络，然后在第IV-B节介绍用于MIF任务的网络。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ad1e8dad48d34bb4a648f0675e50ace2.png#pic_ center" width="70%" />
</div>

## A. DeepM2CDL 网络在多模态图像恢复 (MIR) 任务中的应用

如图 1(a) 所示，所提出的 DeepM2CDL 网络架构由多个多模态图像恢复 (MIR) 模块 (MIRMs) 组成。每个 MIRM 模块由三个块组成：针对 x 模态的多尺度字典学习块 (MDLBx)，针对 y 模态的多尺度字典学习块 (MDLBy)，以及恢复块 (RB)。由于每个 MIRM 模块的结构相同，我们以第一个 MIRM 模块为例来详细说明每个部分，如下所述。

**多尺度字典学习块**：在我们的网络中，针对 x 模态的多尺度字典学习块 (MDLBx) 的架构与针对 y 模态的块 (MDLBy) 相同。因此，为了简化说明，本节中我们仅详细介绍 MDLBx 块。

在 (2) 和 (3) 中，为了生成联合特征和字典，我们首先需要从每个单一模态中学习每个尺度的稀疏特征和卷积字典。MDLBx 块旨在为 x 模态实现此目标。MDLBx 块的输入是图像 x，输出是一组稀疏特征 $\{u_ {s,c}\}^{S,C}_ {s=1,c=1}$ 和一组卷积字典 $\{d_ {s,c}\}^{S,C}_   {s=1,c=1}$ 。该过程可以表述为以下优化问题：

   $$
   argmin_ {u_ {s,c},d_ {s,c}} \frac{1}{2} \left\| x - \sum_ {s=1}^{S} \sum_ {c=1}^{C} d_ {s,c} \* u_ {s,c} \right\|^2_ 2 + \lambda_ u \sum_ {s=1}^{S} \sum_ {c=1}^{C} \psi(u_ {s,c}) + \lambda_ d \sum_ {s=1}^{S} \sum_ {c=1}^{C} \phi(d_ {s,c})
   $$
   
   其中， $\psi(\cdot)$ 衡量特征的稀疏性， $\phi(\cdot)$ 表示字典的正则化， $\lambda_ u$ 和 $\lambda_ d$ 是正则化参数。为了计算所有 S 尺度的 $u_ {s,c}$ 和 $d_ {s,c}$ ，我们假设较低的尺度对应于粗糙的图像细节，而较高的尺度对应于更精细的图像细节。根据我们的假设，第一尺度包含了大部分图像内容，并且应该能够很好地重建原始图像 x。因此，我们首先令 $x_ 1 = x$ 并更新第一尺度的 $u_ {1,c}$ 和 $d_ {1,c}$ ，同时固定其他尺度的 $u_ {s,c}$ 和 $d_ {s,c}$ ，可以表述为： 
   
   $$
   argmin_ {u_ {1,c},d_ {1,c}} \frac{1}{2} \left\| x_ 1 - \sum_ {c=1}^{C} d_ {1,c} \* u_ {1,c} \right\|^2_ 2 + \lambda_ u \sum_ {c=1}^{C} \psi(u_ {1,c}) + \lambda_ d \sum_ {c=1}^{C} \phi(d_ {1,c})
   $$
   
   按照 [38] 中的逐步迭代更新方法，上述优化问题可以通过引入辅助变量 $\{u'_ {1,c}\}^C_ {c=1}$ 和 $\{d'_ {1,c}\}^C_ {c=1}$ 被分解为以下两个子问题：
   
   $$
   argmin_ {u'_ {1,c},u_ {1,c}} \frac{1}{2} \left\| x_ 1 - \sum_ {c=1}^{C} d_ {1,c} \* u_ {1,c} \right\|^2_ 2 + \lambda_ u \sum_ {c=1}^{C} \psi(u_ {1,c}) + \alpha_ u \sum_ {c=1}^{C} \left\| u_ {1,c} - u'_ {1,c} \right\|^2_ 2
   $$ 
   
   $$
   argmin_ {d'_ {1,c},d_ {1,c}} \frac{1}{2} \left\| x_ 1 - \sum_ {c=1}^{C} d'_ {1,c} \* u_ {1,c} \right\|^2_ 2 + \lambda_ d \sum_ {c=1}^{C} \phi(d_ {1,c}) + \alpha_ d \sum_ {c=1}^{C} \left\| d'_ {1,c} - d_ {1,c} \right\|^2_ 2
   $$
   
   其中 $\alpha_ u$ 是衡量辅助变量 $\{u'_ {1,c}\}^C_ {c=1}$ 与目标稀疏特征 $\{u_ {1,c}\}^C_ {c=1}$ 之间误差的正则化参数。 $\alpha_ d$ 是衡量辅助变量 $\{d'_ {1,c}\}^C_ {c=1}$ 与目标卷积字典 $\{d_ {1,c}\}^C_ {c=1}$ 之间误差的正则化参数。公式 (15a) 可以进一步分解为以下两个优化问题，以交替更新 $\{u_ {1,c}\}^C_ {c=1}$ 和 $\{u'_ {1,c}\}^C_ {c=1}$ ：
   
   $$
   u'_ {1,c}(i) = argmin_ {u^\*_ {1,c}} \frac{1}{2} \left\| x_ 1 - \sum_ {c=1}^{C} d_ {1,c}^{(i-1)} \* u^\*_ {1,c} \right\|^2_ 2 + \alpha_ u \sum_ {c=1}^{C} \left\| u^\*_ {1,c} - u_ {1,c}^{(i-1)} \right\|^2_ 2
   $$
   
   $$
   u_ {1,c}(i) = argmin_ {u^\*_ {1,c}} \sum_ {c=1}^{C} \psi(u^\*_ {1,c}) + \alpha_ u \frac{\lambda_ u}{C} \sum_ {c=1}^{C} \left\| u'_ {1,c}(i) - u^\*_ {1,c} \right\|^2_ 2
   $$
   
   这里，i 表示迭代次数。 $u_ {1,c}(i)$ 和 $u_ {1,c}(i)$ 分别是在第 i 次迭代中更新的 $u_ {1,c}$ 和 $u_ {1,c}$ ，其中 $u_ {1,c}(0)$ 被初始化为一个随机矩阵。公式 (16a) 有一个封闭形式的解，我们将其称为辅助稀疏表示 (ASR) 单元。对于公式 (16b)，由于 $\psi(\cdot)$ 的非线性，不可能得到封闭形式的解。因此，我们设计了一个名为稀疏表示更新 (SRU) 的网络来解决这个优化问题。在公式 (16b) 中，我们尝试将 $u_ {1,c}(i)$ 映射到一个更好的 $u_ {1,c}(i)$ ，这类似于图像到图像的映射问题。因此，采用了 U-Net 风格的架构来解决这个问题 [39]。U-Net 中的下采样分支可以作为 $\psi(\cdot)$ 的角色，逐渐提取最具代表性的特征。图 3(a) 展示了 SRU 单元的网络架构，它接受 $u'_ {1,c}(i)$ 和 $\alpha_ u \lambda_ u$ 作为输入，并输出更新后的 $u_ {1,c}(i)$ 。ASR 单元的解决方案细节和 SRU 的网络结构细节在补充材料中介绍。
   
   类似地，公式 (15b) 可以通过交替更新 $\{d_ {1,c}\}^C_ {c=1}$ 和 $\{d'_ {1,c}\}^C_ {c=1}$ 通过以下两个公式求解：
   
   $$
   d_ {1,c}(i) = argmin_ {d^\*_ {1,c}} \frac{1}{2} \left\| x_ 1 - \sum_ {c=1}^{C} d^\*_ {1,c} \* u_ {1,c}(i) \right\|^2_ 2 + \alpha_ d \sum_ {c=1}^{C} \left| d^\*{1,c} - d{1,c}^{(i-1)} \right|^2_ 2
   $$
   
   $$
   d_ {1,c}(i) = argmin_ {d^{1,c}} \sum{c=1}^{C} \phi(d^{1,c}) + \alpha_ d \frac{\lambda_ d}{C} \sum{c=1}^{C} \left| d'{1,c}(i) - d^\*{1,c} \right|^2_ 2
   $$
   
   公式 (17a) 的封闭形式解被称为辅助字典计算 (ADC) 单元。 $d_ {1,c}(0)$ 被初始化为一个零矩阵。正如 [40] 中所展示的，图像的字典应该具有清晰的结构和模式，这些可以通过使用 DCT 或小波进一步稀疏表示。受到这一启发，公式 (17b) 中的字典约束 $\phi(d_ {1,c}^\*)$ 被指定为：
   
   $$
   phi(d_ {1,c}^\*) : d_ {1,c}^\* = W u_ c, \quad \|u_ c\|_ 0 \leq l
   $$
   
   在上述等式中， $W$ 表示先验矩阵，它使 $d_ {1,c}^\*$ 在其上稀疏。通常，先验矩阵可以是固定的 DCT 框架或小波，而在本文中，我们设计了一个名为字典过滤器更新 (DFU) 的网络来隐式学习它。DFU 网络是一个简单的残差神经网络，包含几个卷积层和 Relu 激活函数。网络的输入是辅助字典 $d_ {1,c}(i)$ 和 $\alpha_ d \lambda_ d$ ，输出是更新后的字典 $d_ {1,c}(i)$ 。图 3(b) 展示了 DFU 单元的网络架构。由于空间限制，DFU 单元的详细结构也在补充材料中介绍。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ccc9814465b5449abd9773aefe56711f.png#pic_ center" width="70%" />
</div>

   上述解决 (14) 的过程应该重复 $J_ x$ 次迭代。然后，我们将迭代解映射到一个循环网络块中，即针对 x 模态的字典学习块 (DLBx) 块。如图 2(b) 所示，DLBx 块由多个 ASR 和 SRU 单元组成，用于更新稀疏特征，以及多个 ADC 和 DFU 单元用于更新字典过滤器。经过 $J_ x$ 次迭代后，DLBx 块的最终输出是学习的稀疏特征 $u_ {1,c}(J_ x)$ 和字典过滤器 $d_ {1,c}(J_ x)$ 。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/4d9fa4877dd4453184379f94f4d1997c.png#pic_ center" width="70%" />
</div>

   然而，仅使用一个尺度无法从 x 模态中提取信息而不丢失任何细节。根据我们的假设，剩余的更精细的细节可以通过第二尺度提取，如下所示：

$$
argmin_ {u_ {2,c},d_ {2,c}} \frac{1}{2} \left\| x_ 2 - \sum_ {c=1}^{C} d_ {2,c} \* u_ {2,c} \right\|^2_ 2 + \lambda_ u \sum_ {c=1}^{C} \psi(u_ {2,c}) + \lambda_ d \sum_ {c=1}^{C} \phi(d_ {2,c})
$$

这里 $x_ 2$ 表示在第一尺度的字典学习之后剩余的细节，即 $x_ 2 = x - \sum_ {c=1}^{C} d_ {1,c}(J_ x) \* u_ {1,c}(J_ x)$ 。为了方便表达，我们定义 $\tilde{x}_ 1 = \sum_ {c=1}^{C} d_ {1,c}(J_ x) \* u_ {1,c}(J_ x)$ ，它表示第一尺度的重建图像，如图 2 所示。公式 (19) 可以通过与 (14) 相同的方法通过一个 DLBx 块求解。最后，对于第 S 尺度， $u_ {S,c}$ 和 $d_ {S,c}$ 可以按照以下方式计算：

$$
argmin_ {u_ {S,c},d_ {S,c}} \frac{1}{2} \left\| x_ S - \sum_ {c=1}^{C} d_ {S,c} \* u_ {S,c} \right\|^2_ 2 + \lambda_ u \sum_ {c=1}^{C} \psi(u_ {S,c}) + \lambda_ d \sum_ {c=1}^{C} \phi(d_ {S,c})
$$

其中 $x_ S$ 表示在所有先前尺度的字典学习之后剩余的图像，即 $x_ S = x - \sum_ {s=1}^{S-1} \sum_ {c=1}^{C} d_ {s,c}(J_ x) \* u_ {s,c}(J_ x)$ 。为了表达方便，我们在这里省略了上标 Jx。

如图 2(a) 所示，每个 MDLBx 块包含 S 个 DLBx 块，以实现多尺度信息提取。MDLBx 块的输出是多尺度稀疏特征集合 $\{u_ {s,c}\}^{S,C}_ {s=1,c=1}$ 和卷积字典集合 $\{d_ {s,c}\}^{S,C}_ {s=1,c=1}$ 。针对 y 模态的 MDLBy 块的结构与 MDLBx 块相同。MDLBy 块的输出是多尺度稀疏特征集合 $\{v_ {s,t}\}^{S,T}_ {s=1,t=1}$ 和卷积字典集合 $\{h_ {s,t}\}^{S,T}_ {s=1,t=1}$ 。

关于稀疏性约束 $\psi(\cdot)$ 的讨论：在本文中，我们不要求稀疏性约束 $\psi(\cdot)$ 是特定格式，如 $\ell_ 1$ 范数。相反，我们使用网络来学习约束，以找到最适合特定任务和不同输入的先验。通过网络训练，学习到的稀疏性约束可以自适应于不同的任务和不同的输入，这使得结果比固定和特定的约束更准确。

**恢复块 (RB)**：在 DLBx 和 DLBy 块之后，我们可以分别从 x 和 y 模态提取稀疏特征 $\{u_ {s,c}\}^{S,C}_ {s=1,c=1}$ 和 $\{v_ {s,t}\}^{S,T}_ {s=1,t=1}$ 。接下来，我们需要根据 (2) 生成联合稀疏特征，以利用 y 模态的特征来指导 x 模态在所有尺度上的恢复。相应地，我们需要通过 (3) 更新卷积字典，以使它们与联合稀疏特征相匹配。之后，可以通过 (4) 获得恢复图像。恢复块 (RB) 被设计来完成上述过程，对应于 (2) 中的非线性映射函数 $F^{(s)}_ 1(\cdot)$ ，(3) 中的字典过滤器更新函数 $B^{(s)}_ 1(\cdot)$ 以及 (4) 中的图像恢复。RB 块的结构如图 4(a) 所示，其中每个棕色虚线块表示一个尺度的联合稀疏特征生成和字典更新过程。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0888ef33e6ee4b6fa466463e87a1a7ab.png#pic_ center" width="70%" />
</div>

如该图所示，一方面，我们使用一系列具有 ReLU 激活的卷积层来生成每个尺度的联合稀疏特征 $f^{(s)}_ {c,1}$ ，输入为 $u^{(s)}_ {c}$ 和 $v^{(s)}_ {t}$ 。另一方面，我们通过几个具有 Relu 激活的卷积层来更新字典过滤器 $b^{(s)}_ {c,1}$ 从 $d^{(s)}_ {c}$ 。这些卷积层的详细结构在补充材料中介绍。最后，不同尺度的 $b^{(s)}_ {c,1}$ 和 $f^{(s)}_ {c,1}$ 分别卷积，根据 (4) 生成一组多尺度恢复图像 $\{\hat{x}^{(s)}_ 1\}^{S}_ {s=1}$ ，它们相加生成恢复图像 $\hat{x}_ 1$ 。上述完成了第一个 MIRM 模块的设计，整个第一个 MIRM 模块的设计过程在算法 1 中总结。

**MIRMs 之间的连接**：上述第 IV-A1 至 IV-A3 节完成了第一个 MIRM 模块的网络结构设计。如图 1 所示，我们有一系列 MIRM 模块来实现 MIR 任务。每个 MIRM 模块采用前一个 MIRM 的恢复中间图像和原始图像 y 作为输入，产生一个新的中间图像。具体来说，第一个 MIRM 模块的输入是 x 和 y 图像，输出是恢复的中间图像 $\hat{x}_ 1$ 。在第一个 MIRM 之后，y 模态的有用信息已经被纳入 x 中以生成中间图像 $\hat{x}_ 1$ 。然而，中间图像 $\hat{x}_ 1$ 只是目标图像 z 的粗略恢复版本。因此，在第二个 MIRM 中，我们将 $\hat{x}_ 1$ 和 y 作为新输入，以生成第二个中间图像 $\hat{x}_ 2$ 。图像 $\hat{x}_ 2$ 的恢复质量应该比 $\hat{x}_ 1$ 更好，更接近我们的目标图像 z。按照这种方式，经过第 N 个 MIRM 后，我们可以得到 $\hat{x}_ N$ ，这是我们的最终恢复图像 z。

为了训练 MIR 任务的 DeepM2CDL 网络，我们选择最小化预测恢复图像 z 和真实图像 zgt 之间的平均绝对误差 (MAE) 损失，如下所示：

$$
L_ {MIR} = L_ {MAE}(z, z_ {gt})
$$

上述完成了 MIR 任务的 DeepM2CDL 网络的介绍。

## B. DeepM2CDL 网络在多模态图像融合 (MIF) 任务中的应用

图 1(b) 展示了 DeepM2CDL 网络用于 MIF 任务的框架，该框架由多个多模态图像融合 (MIF) 模块 (MIFMs) 组成。每个 MIFM 模块由一个 MDLBx 块、一个 MDLBy 块和一个融合块 (FB) 组成。MDLBx 和 MDLBy 块与 MIR 任务中的块相同，因此本节我们只介绍 FB 块。

FB 块是根据 (9)、(10) 和 (11) 设计的，其架构如图 4(b) 所示。具体来说，通过 MDLBx 和 MDLBy 块，我们可以获取多尺度稀疏特征 $\{u_ {s,c}\}^{S,C}_ {s=1,c=1}$ 和 $\{v_ {s,t}\}^{S,T}_ {s=1,t=1}$ ，以及相应的卷积字典 $\{d_ {s,c}\}^{S,C}_ {s=1,c=1}$ 和 $\{h_ {s,t}\}^{S,T}_ {s=1,t=1}$ 。之后，以这两个稀疏特征作为输入，对于第 s 个尺度，我们通过一系列卷积层为模态 x 和 y 生成联合特征 $f^{(s)}_ {c,1}$ 和 $g^{(s)}_ {t,1}$ 。这个过程对应于公式 (9a) 和 (9b)。

类似地，以两个字典作为输入，我们可以分别更新模态 x 和 y 的字典过滤器 $b^{(s)}_ {c,1}$ 和 $e^{(s)}_ {t,1}$ 。这个过程对应于公式 (10a) 和 (10b)。最后，每个尺度的 $f^{(s)}_ {c,1}$ 与相同尺度的 $b^{(s)}_ {c,1}$ 卷积，根据公式 (11a) 生成基于 x 的中间融合图像 $\hat{x}_ 1$ ； $g^{(s)}_ {t,1}$ 与 $e^{(s)}_ {t,1}$ 卷积，根据公式 (11b) 生成基于 y 的中间融合图像 $\hat{y}_ 1$ 。这完成了第一个 MIFM 模块的网络架构。输入到第一个 MIFM 模块的是图像 x 和 y，输出是两个中间融合图像 $\hat{x}_ 1$ 和 $\hat{y}_ 1$ 。这完成了第一个 MIFM 模块的网络架构设计。该过程在算法 1 中进行了总结。

第一个 MIFM 模块的输出 $\hat{x}_ 1$ 和 $\hat{y}_ 1$ 然后被送入第二个 MIFM，以生成新的融合图像 $\hat{x}_ 2$ 和 $\hat{y}_ 2$ 。最后，在第 N 个 MIFM 之后，我们可以获取 $\hat{x}_ N$ 和 $\hat{y}_ N$ 。根据公式 (12)，最终的融合结果 z 是 $\hat{x}_ N$ 和 $\hat{y}_ N$ 的加权和。MIF 任务的损失函数与 MIR 任务相同，即公式 (21)，它最小化了预测和真实融合图像之间的 MAE 损失。

# V. 实验

在本节中，我们评估了所提出的 DeepM2CDL 网络在四种典型的多模态图像处理 (MIP) 任务中的有效性，包括闪光灯引导的非闪光灯图像去噪、RGB引导的深度图像超分辨率、多聚焦图像融合和多曝光图像融合。第 V-A 节介绍了实验设置，包括数据集、评估指标和实现细节。从第 V-B 节到第 V-E 节展示了不同 MIP 任务上的定量和定性结果。第 V-F 节可视化了网络中学习到的中间特征和字典，以展示网络的良好可解释性。最后，第 V-G 节对一些重要组件进行了消融研究并进行了讨论。

## A. 实验设置

### 数据集
在本文中，我们在四种不同的 MIP 任务上评估了我们的 DeepM2CDL 的性能。具体来说，对于闪光灯引导的非闪光灯图像去噪任务，我们从 Aksoy et al. [52] 中随机选择了 400 对图像进行训练，并使用 12 对图像进行测试。测试图像对涵盖了不同类别的图像内容。对于 RGB 引导的深度图像超分辨率任务，训练数据来自 DPDN 数据集 [53]，提供了 1,000 对合成的 RGB 和深度图像对。测试数据集来自 Middlebury 数据集 [54] 和 Sintel 数据集 [55]。对于多聚焦图像融合任务，我们从 DIV2K 数据集 [56] 生成训练数据。具体来说，我们通过对随机选定区域应用高斯模糊来生成互补的近聚焦和远聚焦图像对。多聚焦图像融合的测试图像对来自 Lytro 多聚焦图像数据集 [57]。对于多曝光图像融合，训练数据来自 SICE 数据集 [58]，其中包含具有七个不同曝光水平的多曝光图像序列。我们随机选择了 340 对极度过曝和欠曝的图像进行训练，涵盖了自然风光、建筑、人类活动等场景。测试图像来自 SICE 数据集 [58]、MEFB 数据集 [59] 和 PQA-MEF 数据集 [60]。本文中使用的所有训练和测试图像可以从以下链接下载：https://drive.google.com/drive/folders/1Dpjl7KPrDtrbstNjgxzwYjhvlu4OWlrb?usp=sharing。

### 评估指标
对于 MIR 任务，我们采用三个指标来评估恢复图像的质量，包括均方根误差 (RMSE)、峰值信噪比 (PSNR) 和结构相似性指数 (SSIM)。给定恢复图像 $I_ r \in \mathbb{R}^{m \times n}$ 和真实图像 $I_ {gt} \in \mathbb{R}^{m \times n}$ ，这三个指标定义如下：
- RMSE: 均方根误差 (RMSE) 已广泛用于衡量 RGB 引导深度图像超分辨率中的深度图像质量。RMSE 的计算定义如下：
  
$$
RMSE(I_ r, I_ {gt}) = \sqrt{\frac{1}{mn} \sum_ {i=1}^{m} \sum_ {j=1}^{n} (I_ r - I_ {gt})^2}
$$
  
  
- PSNR: 峰值信噪比 (PSNR) 是另一个广泛使用的图像质量评估指标。基于 RMSE 的定义，PSNR 定义如下：
  
$$
PSNR(I_ r, I_ {gt}) = 20 \log_ {10} \left( \frac{255}{RMSE} \right)
$$

  
  
- SSIM: 结构相似性指数度量 (SSIM) [61] 基于结构信息的退化来评估图像质量。它主要考虑图像的三个关键特征，即亮度、对比度和结构。与 RMSE 和 PSNR 相比，SSIM 与人类视觉感知更为一致，计算如下：
  
$$
SSIM(I_ r, I_ {gt}) = \frac{(2\mu_ r \mu_ {gt} + C_ 1)(2\sigma_ r,gt + C_ 2)}{(\mu_ r^2 + \mu_ {gt}^2 + C_ 1)(\sigma_ r^2 + \sigma_ {gt}^2 + C_ 2)}
$$
  
  
  其中 $\mu_ r$, $\mu_ {gt}$ 分别是 $I_ r$ 和 $I_ {gt}$ 的均值值， $\sigma_ r$, $\sigma_ {gt}$ 表示 $I_ r$ 和 $I_ {gt}$ 的标准偏差， $\sigma_ r,gt$ 表示协方差值， $C_ 1$, $C_ 2$ 是常数正值。

对于多曝光图像融合，我们采用四个常用指标来衡量融合图像的质量，包括特征互信息 (FMI) [62]、多曝光图像融合结构相似性 (MEF-SSIM) [60]、非线性相关信息熵 (QNCIE) [63] 和峰值信噪比 (PSNR) [64]。FMI 指标表示从源图像传输到融合图像的特征信息量。MEF-SSIM 和 QNCIE 分别评估融合图像与源图像之间的结构相似性和非线性相关性。这些四个指标的值越高，表明多曝光图像融合的性能越好。对于多聚焦图像融合，也使用了四个指标来衡量融合图像质量，包括标准差 (SD) [65]、平均梯度 (AG) [66]、边缘强度 (EI) [67] 和视觉信息保真度 (VIF) [68]。具体来说，SD 表示融合图像的分布和对比度，AG 衡量融合图像的梯度信息，EI 计算融合图像中的边缘强度信息，VIF 反映融合图像的信息保真度。这些四个指标的值越高，表明融合性能越好。

### 实现细节

对于所有任务，我们使用 Adam 优化器来训练网络，使用标准参数。初始学习率设置为 $1 \times 10^{-4}$ ，并在每 25,000 次迭代后减半。在本文中，MIRMs 和 MIFMs 的数量 N 都设置为 2。DLBx 块中的迭代次数 Jx 和 DLBy 块中的迭代次数 Jy 都设置为 2。网络中使用的卷积过滤器大小为 5 × 5。权重矩阵 $W_ x$ 和 $W_ y$ 都设置为所有元素为 0.5。网络通过正交分布初始化，并训练了大约 1e6 个周期。对于两个 MIR 相关任务，训练补丁大小为 128 × 128，总共约有 500,000 个训练补丁。对于多聚焦图像融合和多曝光图像融合，补丁大小分别为 200 × 200 和 64 × 64，总共约有 300,000 个训练补丁。

## B. 闪光灯引导的非闪光灯图像去噪

在这项任务中，我们使用所提出的 DeepM2CDL 网络来去噪非闪光灯图像，并以相应的闪光灯图像为指导。为了证明我们方法的有效性，我们将其与八种最先进的单图像或引导图像去噪方法进行了比较，包括 CBM3D [42]、DnCNN [43] 和 MAXIM [44] 用于单图像去噪，DJFR [45]、MuGIF [7]、CU-Net [19]、DKN [46]、MM-CNN [48]、UMGF [47]、MN [49]、FGDNet [50] 和 RIDFnF [51] 用于引导图像去噪。这些比较方法的结果是通过运行作者提供的官方代码获得的。

表 III 展示了我们方法和比较方法在不同高斯噪声标准差 $\sigma = 25, 50$ 和 $75$ 下的去噪结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fd4db92533704fb69d33f115a2dea506.png#pic_ center" width="70%" />
</div>

从表中可以看出，我们的方法在几乎所有测试图像上都优于其他比较方法。具体来说，我们的方法在 $\sigma = 25, \sigma = 50$ 和 $\sigma = 75$ 噪声水平下，平均 PSNR 分别比第二好的方法高出 0.65 dB、1.09 dB 和 1.12 dB。可以看出，随着噪声水平的提高，我们方法的 PSNR 提升更加明显。这表明我们的方法在处理严重噪声污染的图像时更加有利。

除了数值结果，我们还在图 5 中可视化了不同方法在 $\sigma = 75$ 下的去噪图像。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9a7307a742c44ef38ab2a7991ecb9cdf.png#pic_ center" width="70%" />
</div>

如图所示，我们的方法能够有效地去除噪声以恢复足够的图像细节，而其他方法要么导致边缘模糊 [19]、[43]、[49]，要么导致图像细节不足 [7]。定量和定性结果都证明了我们的 DeepM2CDL 在利用跨模态信息进行引导图像去噪方面的卓越性能。

## C. RGB 引导的深度图像超分辨率

在这项任务中，我们评估了 DeepM2CDL 网络在 RGB 引导的深度图像超分辨率任务中的性能。具体来说，我们首先将深度图像下采样 4 倍，然后通过双三次插值进行上采样。插值的深度图像质量较低，然后以高分辨率 RGB 图像为指导进行增强。为了使通道数与深度图像一致，我们将 RGB 图像转换为 YCbCr 格式，并仅使用 Y 通道作为指导。

为了证明所提出方法的有效性，我们将其与多种最先进的方法进行了比较，这些方法可以分为三类：1) 提出用于单图像超分辨率的方法，包括 SCN [71]、EDSR [72] 和 SRFBN [74]。2) 特别设计用于 RGB 引导的深度图像超分辨率的方法，包括 Lu et al. [69]、Gu et al. [70]、RCGD [73]、DepthSR-Net [13]、RADAR [75]、PMBANet [76]、AHMF [77]、DCTNet [78]、CODON [79] 和 GeoDSR [80]。3) 旨在解决一般多模态图像恢复问题的方法，包括 DJFR[45]、CoISTA[17]、CU-Net[19]、LMCSC[18]、DKN[46]、UMGF [47]。在上述方法中，SCN [71]、CoISTA [17]、CU-Net [19] 和 LMCSC [18] 是由字典学习驱动的可解释网络，与我们的方法更为接近。所有比较结果都是通过官方代码获得的。

表 IV 展示了我们的 DeepM2CDL 和比较方法在 RGB 引导的 4 倍深度图像超分辨率中的 RMSE 和 SSIM 结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0dd9c33ef3a841d4a908108658640cc4.png#pic_ center" width="70%" />
</div>

较低的 RMSE 值和较高的 SSIM 值表示更好的超分辨率性能。从表中可以看出，我们的方法在大多数情况下在 RMSE 和 SSIM 方面都优于其他比较方法。具体来说，我们方法的平均 SSIM 值为 0.9948，在所有方法中最高。此外，我们方法的平均 RMSE 值为 1.59，远低于第二好的方法，后者仅达到 1.88。为了直观展示我们方法与比较方法的优势，我们在图 6 中可视化了不同方法的超分辨率深度图像。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/6900c58ae669446bb8de663fcc517517.png#pic_ center" width="70%" />
</div>

如图所示，我们的方法能够以更清晰的边缘恢复深度图像，无论是在前景还是背景区域。对于比较方法，一些方法如 Gu et al. [70] 和 DJFR[45] 导致边缘模糊，而其他方法如 Lu et al. [69]、LMCSC [18] 和 DCTNet [78] 则产生扩散伪影。

## D. 多曝光图像融合

在多曝光图像融合中，我们的目标是将一张极度欠曝的图像与另一张极度过曝的图像融合，生成具有高动态范围 (HDR) 的图像。训练数据集来自 SICE 数据集 [58]。SICE 数据集中的每对多曝光图像包含几个曝光水平，我们选择第一级作为欠曝输入，第六级作为过曝输入。测试数据集包含来自 SICE 数据集 [58]、MEFB 数据集 [59] 和 PQA-MEF 数据集 [60] 的 80 对图像。

我们将 DeepM2CDL 与四种最先进的多曝光图像融合方法进行了比较，包括 Paul et al. [81]、MEF-OPT [82]、DEM [83]、IFCNN [16]、DPE-MEF [84]、SwinFusion [85] 和 DeFusion [86]。有四个指标被采用来评估融合性能，包括 FMI [62]、MEF-SSIM [60]、QNCIE [63] 和 PSNR [64]。这些四个指标的值越高，表明多曝光图像融合的性能越好。如表 V 所示，我们的方法在 FMI、MEF-SSIM 和 PSNR 方面取得了所有比较方法中最好的结果，在 QNCIE 方面排名第二。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/60d58468bb08469797b849244b852241.png#pic_ center" width="70%" />
</div>

为了定性比较，我们在图 7 中可视化了四组多曝光融合结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/865a716ebaef47848a8bde835966a3ed.png#pic_ center" width="70%" />
</div>

如图所示，Paul et al. IFCNN 和 SwinFusion 的融合结果整体较暗，色彩饱和度低。对于 MEF-OPT 方法，可以获得更好的色彩饱和度，但图像的亮度分布不均匀，如从图 7 中最后一行的汉堡包可以看出。相比之下，我们的 DeepM2CDL 可以生成色彩饱和度高、图像细节充足、视觉效果愉悦且整体亮度一致的图像。

## E. 多聚焦图像融合

我们还在多聚焦图像融合任务中评估了 DeepM2CDL 的有效性。在这项任务中，将一张近聚焦图像和一张远聚焦图像融合，生成一张全聚焦图像。表 VI 展示了我们方法的融合结果，以及包括 CSR [27]、CNN [15]、ECNN [87]、IFCNN [16]、SESF [88]、SwinFusion [85]、ZMFF [89] 和 DeFusion [86] 在内的比较方法。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b563b71524984ad5a9110105200c1b28.png#pic_ center" width="70%" />
</div>

有四个常用指标用于评估多聚焦融合性能，包括 SD [65]、AG [66]、EI [67] 和 VIF [68]。如表所示，我们的方法在所有四个指标上都优于其他比较方法。具体来说，我们方法的平均 EI 值为 74.40，比排名第二的 IFCNN 方法的 71.98 高出 2.42。此外，我们方法的平均 SD 值为 63.11，比排名第二的结果 61.59 高出 1.52。

为了定性比较，我们在图 8 中可视化了三组多聚焦融合结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/726f4740ef3c4362aa3750941675d688.png#pic_ center" width="70%" />
</div>

差异图表示融合图像与远聚焦源图像之间的差异。理想情况下，差异图中的背景区域应该是全零，因为它们在融合图像和远聚焦图像中应该是相同的。如图所示，我们的背景在所有方法中最干净。比较方法的背景较乱，特别是 IFCNN 和 ZMFF。对于前景区域，我们的方法提供了非常干净和清晰的边缘以及图像细节，而比较方法则无法实现。这表明我们的方法能够在融合过程中清晰地区分近聚焦和远聚焦区域，并避免这两个部分的过度重叠。

## F. 中间特征的可视化

在 DeepM2CDL 网络中，每个模块对应于一个特定的字典学习过程，这为其提供了良好的可解释性。因此，探索网络内部发生的情况是非常有趣的，尤其是引导模态如何有助于目标模态的恢复。为了这个目的，我们在图 9 中可视化了网络为两个多模态图像恢复 (MIR) 任务学习到的稀疏特征，包括闪光灯引导的非闪光灯图像去噪和 RGB 引导的深度图像超分辨率。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/af089aac491644cab08049dec0c7a07c.png#pic_ center" width="70%" />
</div>

具体来说，图 9(b) 和 (c) 显示了 MDLBx 和 MDLBy 块分别学习到的稀疏特征。图 9(d) 可视化了恢复块 (RB) 中的联合稀疏特征。如图所示，对于去噪任务，MDLBx 块提取的特征受到严重噪声污染，而 MDLBy 块提取的特征则干净。联合特征无噪声，边缘清晰，细节充足。这证明了 MDLBx 和 MDLBy 块在提取稀疏特征方面的有效性，以及 RB 块在恢复特征方面的有效性。

我们同样为 RGB 引导的深度图像超分辨率任务可视化了稀疏特征，可以观察到类似的现象。具体来说，从低分辨率深度图像提取的特征边缘模糊，丢失了许多结构细节，例如背景中的怪物。相比之下，从 RGB 图像提取的特征边缘清晰，纹理细节充足。通过我们方法中的 RB 块将这些特征结合起来，我们可以得到具有锐利边缘和完整结构的联合特征。换句话说，RGB 图像的稀疏特征有助于锐化和完善低分辨率深度图像的稀疏特征，这有助于高分辨率深度图像的恢复。所有这些可视化结果证明了我们的 DeepM2CDL 网络具有良好的可解释性。

可视化学习到的字典：由于我们的 DeepM2CDL 是从多模态字典学习模型衍生而来，因此它具有良好的可解释性。因此，可视化网络学习到的字典滤波器是很有趣的事情。在这里，我们还设计了一个基于 CNN 的网络，其架构与我们的 DeepM2CDL 相同，只是我们将字典学习部分替换为 CNN。图 10 可视化了基于 CNN 的网络学习到的滤波器和我们的 DeepM2CDL 学习到的字典滤波器，在 RGB 引导的深度图像超分辨率和多聚焦图像融合任务中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/586d219f8c0546b0be01758d81077021.png#pic_ center" width="70%" />
</div>

如图所示，我们学习到的字典滤波器比 CNN 滤波器有两个优势。1) 我们的 DeepM2CDL 学习到的字典滤波器更加稀疏，并且具有规则形状，而 CNN 学习到的滤波器则密集且混乱。这是因为我们的网络是按照字典学习模型设计的，该模型对学习到的字典滤波器有约束。2) 我们的字典滤波器比 CNN 滤波器具有更有意义的结构。例如，从用红色突出显示的块中可以看出，对于 RGB 引导的深度图像超分辨率任务，我们学习到的字典滤波器具有垂直或对角线结构，这有助于提取深度图像中的边缘。对于多聚焦图像融合任务，学习到的字典滤波器更像是门结构。这与多聚焦图像融合任务的需求一致，该任务需要检测聚焦和失焦区域。所有这些结果都验证了我们所提出方法的良好可解释性。

## G. 消融研究

在本小节中，我们分析了所提出的 DeepM2CDL 网络中一些重要参数的影响。前四个消融研究在 RGB 引导的深度图像超分辨率任务上进行，而最后一个消融研究是关于融合规则的，它在多聚焦图像融合任务上进行。

尺度数量的影响:我们的网络源于多尺度字典学习模型。因此，探索网络在不同数量的尺度下的性能是必要的。为此，我们逐渐增加尺度数量 S 从 1 到 3，然后重新训练网络。图 11(a) 显示了不同数量尺度下的 PSNR、RMSE 和 SSIM 结果。如图所示，随着尺度数量的增加，PSNR 和 SSIM 值增加，RMSE 值减少。这表明我们提出的多尺度结构有助于性能提升。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/7055bbb581d64244b73ca6ee6d97038e.png#pic_ center" width="70%" />
</div>

DLBx 和 DLBy 中迭代次数的影响:DLBx 和 DLBy 块分别负责 x 和 y 模态的字典学习。如图 2 所示，DLBx 和 DLBy 块内部存在多次迭代。通常，迭代次数越多，我们可以学习到更准确的字典过滤器和稀疏特征，这有助于恢复任务。因此，在这项消融研究中，我们探索了 DLBx 和 DLBy 中迭代次数对恢复性能的影响。为了简化，我们用符号 J 表示迭代次数。图 11(b) 显示了当 J 从 1 增加到 3 时的 PSNR 和 SSIM 结果。如图所示，随着迭代次数的增加，可以产生更高的 PSNR 和 SSIM 值，表明我们可以获得更好的恢复结果。这与我们前述的分析一致。这些结果验证了 DLBx 和 DLBy 块在我们网络中的有效性。

损失函数的影响:损失函数在网络训练过程中起着重要作用，因此有必要评估网络在不同损失函数下的性能。在本文中，我们采用简单的 MAE 损失来训练网络，图 11(c) 也展示了使用其他两种广泛使用的损失函数 MSE 损失和 SSIM 损失时的性能。如图所示，MAE 损失在所有三种损失函数中实现了最高的 PSNR 和 SSIM 值。具体来说，使用 MAE 损失的平均 PSNR 值为 45.61 dB，比使用 MSE 损失高 0.19 dB，比 SSIM 损失高 0.14 dB。在 SSIM 方面，MAE 损失达到 0.9948，高于 MSE 损失的 0.9942 和 SSIM 损失的 0.9945。这些结果验证了 MAE 损失在我们网络训练中的有效性。

单个模块的准确性:我们的网络由多个 MIRM 模块组成，用于 MIR 任务。为了验证各个模块的准确性，我们测试了 RGB 引导的深度图像超分辨率任务在 MIRM 数量 N 为 1、2 和 3 时的性能。如表 VII 所示，PSNR 和 SSIM 值随着 MIRM 数量的增加而逐渐增加，RMSE 值随着 MIRM 数量的增加而减少。具体来说，当 N 分别为 1、2 和 3 时，PSNR 值分别为 45.34 dB、45.61 dB 和 45.64 dB。这验证了每个单独模块对我们网络性能提升的贡献。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e0b3e0728da84990a7ef40b66e189354.png#pic_ center" width="70%" />
</div>

融合规则的影响:为了验证融合规则对融合性能的影响，我们为权重矩阵 $W_ x$ 和 $W_ y$ 设置了不同的值，然后重新训练网络。注意， $W_ x$ 和 $W_ y$ 中相同位置的元素之和应等于 1。表 VIII 展示了在不同权重矩阵下的融合性能。如图所示，当 $W_ x$ 和 $W_ y$ 都等于 0.5 时，融合性能最好，所有评估指标都达到了最佳。可能的原因是两个源图像对融合同等重要。当这两个矩阵不相等时，网络将更多地关注一个源图像，同时忽略了另一个图像中的信息，导致融合性能下降。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/38770511903d4324ad99cc0356207b6a.png#pic_ center" width="70%" />
</div>

## H. 运行速度

由于我们的 DeepM2CDL 网络源于多模态字典学习模型，因此在辅助稀疏表示 (ASR) 单元和辅助字典计算 (ADC) 单元中涉及一些矩阵计算。因此，测试这些两个单元以及整个网络的运行速度是必要的。表 Ⅸ 展示了 ASR、ADC 和整个网络在 RGB 引导的深度图像超分辨率任务上的平均运行速度。时间是在装有 NVIDIA Corporation GV100GL GPU 的 PC 上记录的。如图所示，由于封闭形式的解决方案，涉及矩阵计算的 ASR 和 ADC 单元具有非常快的运行速度。对于整个网络，运行速度随着输入图像大小的增加而增加，对于尺寸为 256×512、512×512 和 1024×512 的图像，运行时间分别为 0.182 秒、0.322 秒和 0.609 秒。这对于常见的实际应用也是可接受的。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e8826ae78c6c41bd8d1bbcb806c77289.png#pic_ center" width="70%" />
</div>

# VI. 结论

在本文中，我们提出了一种新颖的可解释网络，即 DeepM2CDL，旨在解决一般的多模态图像恢复和融合任务。该网络通过将多尺度多模态字典学习整合到深度学习中设计，使其既保留了深度学习的效率，又具备了字典学习的可解释性。与其它网络中使用的手工制作先验不同，我们通过网络训练学习到字典和稀疏先验，这些先验更适合多模态图像内容。通过在各种任务上的广泛实验，包括闪光灯引导的非闪光灯图像去噪、RGB 引导的深度图像超分辨率、多聚焦和多曝光图像融合，验证了所提出网络的有效性。实验结果表明，所提出的 DeepM2CDL 在所有这些任务上均实现了一致的最先进性能。此外，我们还可视化了从网络中学习到的中间稀疏特征和字典，展示了我们网络的良好可解释性。

在未来，探索卷积字典学习在弱监督、自监督或无监督多模态图像处理中的潜力是有趣的。同时，我们的 DeepM2CDL 可以从几个方面进行增强。例如，可以采用注意力机制来探索不同模态之间的全局信息。此外，当前的工作仅关注多模态图像处理。设计可解释网络用于多模态视频处理，通过跨帧和跨模态字典学习，是一个有前景的未来工作方向。




