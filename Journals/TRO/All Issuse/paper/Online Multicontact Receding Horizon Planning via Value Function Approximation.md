# 题目：[Online Multicontact Receding Horizon Planning via Value Function Approximation ](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506550)  
## 在线多接触递减地平线规划通过价值函数近似
**作者：Jiayi Wang； Sanghyun Kim；Teguh Santoso Lembono；Wenqian Du；Jaehyun Shim；Jaehyun Shim；Ke Wang；Vladimir Ivan；Sylvain Calinon；Sethu Vijayakumar；Steve Tonneau** 

****
# 摘要
在递减地平线方式下规划多接触运动需要一个价值函数来指导规划以应对未来的情况，例如，为了穿越大型障碍物而建立动量。传统上，价值函数是通过计算一个预测地平线（永不执行）来近似的，这个预测地平线预见了执行地平线之外的未来。然而，鉴于多接触运动的非凸动力学，这种方法计算成本很高。为了使在线递减地平线规划（RHP）的多接触运动成为可能，我们找到了价值函数的高效近似方法。具体来说，我们提出了基于轨迹的方法和基于学习的方法。在前者中，即具有多种模型保真度级别的RHP，我们通过使用凸松弛模型计算预测地平线来近似价值函数。在后者中，即局部引导的RHP，我们学习一个预言机来预测运动任务的局部目标，我们使用这些局部目标来构建局部价值函数，以引导短期地平线的RHP。我们通过模拟规划人形机器人在中等坡度上行走的质心轨迹，以及在机器人无法保持静态平衡的大坡度上行走的轨迹来评估这两种方法。我们的结果表明，局部引导的RHP实现了最佳的计算效率（95%-98.6%的周期在线收敛）。这种计算优势使我们能够在动态环境中实时演示我们的真实人形机器人Talos行走的RHP
# 关键词
- 人形机器人
- 腿部运动
- 多接触运动
- 优化和最优控制

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/0d8c9d956cae453e9f02908831070fdd.png)

## I. 引言


本文考虑了为腿部机器人计算运动计划以穿越不平坦地形（非水平表面）的问题，其中规划器需要找到一系列接触点以及可行的状态轨迹。这个问题被称为多接触运动规划，它是高维的、非线性的，并且由于打破和建立接触时出现的离散动态变化而变得复杂[1]，[2]，[3]，[4]，[5]，[6]，[7]，[8]，[9]。鉴于这些复杂性，传统的机器人控制方法通常是离线规划多接触运动，然后使用控制器进行跟踪[2]，[6]，[10]，[11]。然而，当腿部机器人部署在现实世界中时，它们可能会遇到环境变化和状态漂移。这些扰动可能导致预先计划的运动变得无效，需要在线（重新）规划[10]，[12]，[13]，[14]。为了促进在现实世界中的可靠运行，我们的长期目标是使腿部机器人具备在线重新规划运动的能力。
为此，递减地平线规划（RHP）[12]，[13]可以是一个有希望的解决方案。RHP的概念[12]，[13]与模型预测控制（MPC）[15]，[16]，[17]，[18]类似，它们都旨在基于机器人状态和环境不断更新立即执行的最优动作。在MPC中，最优动作对应于跟踪参考轨迹的最优控制命令，而在RHP中，最优动作是指计划执行的运动。在MPC和RHP中，最优动作是通过解决有限地平线轨迹优化（TO）问题[19]来计算的。
为了确保在不平坦地形上成功进行多接触RHP，关键的是计划执行的运动能够促进未来的操作。例如，为了克服大型障碍物（斜坡和间隙），通常需要提前建立动量。为此，Bellman建议利用价值函数来指导最优动作的规划[20]。这个价值函数旨在告知在给定任务的完成方面某个状态的效用。然而，对于复杂的动力系统来说，找到一个确切的价值函数模型是具有挑战性的，因此，我们需要近似。近似价值函数的一个常见方法是考虑一个预测地平线（PH）（不执行），它预见了执行地平线（EH）（要执行的最优动作）之外的未来。这个PH可以被视为价值函数的轨迹基近似——通过评估从给定状态完成任务的可行性和预期的努力来指导EH（见图2中的一个例子）。
传统上，RHP框架通常使用准确的动力学模型计算整个地平线。这确保了EH始终在动力学上是一致的，同时允许PH尽可能准确地近似价值函数。然而，使用准确模型规划PH可能会导致昂贵的计算，特别是当需要考虑长的规划地平和复杂的动力学时，例如规划多接触运动以穿越大斜坡。在这项工作中，我们考虑传统的RHP方法作为我们的基线。
为了加速计算速度，一个选择是减少近似价值函数所需的计算负担。基于这个想法，我们提出了一种基于轨迹的方法和一种基于学习的方法，可以提高计算效率，实现价值函数近似。我们在模拟中比较了这两种方法在规划人形机器人Talos在不平坦地形上行走的质心轨迹的上下文中。更具体地说，我们的基于轨迹的方法——我们称之为具有多种模型保真度级别的RHP——遵循传统的形式主义，通过在PH中计划的轨迹来模拟价值函数。然而，我们并不是考虑准确的动力学模型，而是放宽了PH的模型精度。这使我们能够减少RHP问题的整体复杂性。在本文中，我们探索并比较了三种候选的多保真度RHP（MF-RHP），其中PH考虑了质心动力学模型的不同凸松弛（见图5中的示例）。
或者，我们可以通过使用学习模型进一步改进计算效率，近似价值函数[22]。然而，学习多接触问题的价值函数可能很具有挑战性。主要困难在于价值函数定义在耦合的州环境空间中，需要灵活的表示来捕捉价值函数相对于不同环境变化的景观变化[23]。在本文中，我们通过学习一个预言机来预测基于当前机器人状态、目标位置和环境模型的局部目标（完成给定任务的中间目标状态）。然后，我们基于这些局部目标构建局部价值函数，并使用它们引导短期RHP规划EH朝向预测的局部目标。我们称这种方法为局部引导的递减地平线规划（LG-RHP）。为了获得预言机，我们采取了监督学习方法，其中我们训练了从使用准确模型计算整个地平线的传统RHP离线计算的数据集中学习到的预言机。

为了评估多保真度递减地平线规划（MF-RHP）和局部引导的递减地平线规划（LG-RHP）的性能，我们考虑了一个在线RHP设置，要求每个周期在时间预算内收敛——即当前周期要执行的运动（EH）的持续时间。根据我们的实验结果，我们获得了以下见解。首先，MF-RHP的结果表明，通过在PH中权衡模型精度，可以实现在线计算。然而，这可能会影响由PH建模的价值函数的准确性。因此，我们的MF-RHP存在到达病态状态的风险，从这些状态出发，轨迹优化（TO）可能无法收敛。此外，我们还注意到，在PH中纳入角动量动力学对多接触RHP的收敛至关重要。另一方面，由于LG-RHP具有缩短的规划地平线，它实现了最高的在线收敛率（95.0% - 98.6%的周期在线计算），与传统的RHP（基线）和MF-RHP相比。然而，由于预言机的预测误差，我们的LG-RHP也可能到达病态状态并无法收敛。我们展示了通过数据增强技术可以缓解这个问题，其中我们添加了数据点来演示如何从导致收敛失败的状态中恢复。
为了验证我们的方法，我们通过在模拟中使用全身逆动力学控制器[24]来跟踪规划的轨迹，验证了动态可行性。此外，我们通过真实世界实验验证了LG-RHP，其中我们展示了在动态变化的环境中，我们的仿人机器人Talos实现在线多接触RHP的能力（见图1和11中的示例）。实验视频可以在 https://youtu.be/STBYJl7jvsg 找到。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/66c2e2922b3245278e4f7cea1e7b7eb4.png)


#### A. 贡献

我们提出了两种新方法，可以实现多接触运动的在线RHP质心轨迹。我们方法的关键思想是通过寻找计算上高效的价值函数近似来降低计算复杂性。我们的贡献如下：

具有多种模型保真度级别的RHP，我们通过在PH中计算轨迹并考虑凸松弛模型来近似价值函数。这使我们能够降低TO的整体计算复杂性，并促进在线计算。
LG-RHP，其中价值函数是用学习到的预言机近似的。这个预言机被设计来预测局部目标，作为完成给定任务的中间目标状态，同时考虑机器人周围的环境模型。我们使用这些局部目标构建局部价值函数，以指导短期TO规划EH。
对MF-RHP和LG-RHP的计算性能进行了广泛的评估和分析，以及使用全身逆动力学控制器在模拟中验证了规划轨迹的动态可行性。
在真实世界的仿人机器人Talos上进行了实验，证明了我们的LG-RHP方法在不平坦地形和动态变化环境中实现在线多接触RHP的有效性。 

#### B. 与我们先前工作的比较和文章概述 

本文是我们早期会议论文[25]和[26]的扩展，我们在其中最初提出了MF-RHP和LG-RHP的想法。与我们之前的工作相比，本文的新内容包括以下部分。首先，我们在Bellman最优性原理[20]的框架下统一了RHP问题的描述和MFRHP和LG-RHP的概念。其次，我们对MF-RHP和LG-RHP在一系列多接触场景下的计算性能进行了严格的模拟评估。第三，我们在仿人机器人Talos上进行了多次真实世界实验，展示了LG-RHP实现在线RHP的有效性。我们考虑了在运行时可以动态变化的环境和具有挑战性的不平坦地形。最后，我们对MF-RHP和LG-RHP的优点和缺点进行了定性分析。 本文的其余部分组织如下。第II节回顾了基于优化的多接触运动规划的文献，以及加速其计算速度的基于学习的方法。第III节描述了RHP问题，并介绍了MF-RHP和LG-RHP的原理。第IV节列出了我们工作中所做的假设，第V节介绍了基线方法——使用准确动力学模型计算整个地平线的传统RHP。第VI节和第VII节介绍了MF-RHP和LG-RHP的技术方法。第VIII节展示了我们的模拟研究，第IX节展示了我们在仿人机器人Talos上的真实世界实验结果。在第X节中，我们讨论了MF-RHP和LG-RHP的优点和缺点。最后，第XI节总结了本文。


## III. 问题描述

我们用 $x \in \mathbb{R}^n$ 表示机器人的状态，用 $u \in \mathbb{R}^m$ 表示控制输入。在循环预测控制（RHP）中，每个周期都需要计算一个用于下一周期立即执行的运动计划。我们将这样的运动计划定义为状态轨迹 ${x_0, ..., x_T}$ 从一个给定的初始状态 $x_0$ 和一个控制轨迹 ${u_0, ..., u_{T-1}}$ 开始。

为了计算这个运动计划，传统框架通常需要解决一个轨迹优化（TO）问题，这个问题符合一般形式的贝尔曼方程[20]：

$$
\min_{u_0, ..., u_{T-1}} \left[ \sum_{t=0}^{T-1} l(x_t, u_t) + V(x_T) \right]
$$

$$
\text{其中} \quad x_{t+1} = F(x_t, u_t),
$$

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/2ed6ab9892b44763a97609761cb6be5c.png)


这里 $l(\cdot)$ 是运行成本， $V(\cdot)$ 是价值函数， $F(\cdot)$ 表示系统动态约束。贝尔曼建议由运动计划生成的离散时间动力学应该不仅要最小化运行成本 $l(x_t, u_t)$，还要优化完成任务的价值函数 $V(x_T)$。价值函数被建模为从 $x_T$ 到完成任务的最优成本。同时，系统动力学约束得到满足。

价值函数 $V(x_T)$ 反映了从任一状态 $x_T$ 开始，完成给定任务所需的未来成本，并为指导向一个有利于未来的状态 $x_T$ 的政策提供梯度（即将执行的动作）。然而，评估价值函数并不容易，我们需要近似。

传统上，RHP框架通过考虑从时间 $T$ 到 $T_P$ 的有限视野轨迹来近似价值函数，其中 $T_P \rightarrow \infty$：

$$
V(x_T) = \min_{u_{T_P}, ..., u_{\infty}} \left[ \sum_{t=T}^{T_P-1} l(x_t, u_t) + \phi(x_{T_P}) \right]
$$

$$
\text{其中} \quad x_{t+1} = F(x_t, u_t),
$$

这里的最优成本从时间 $T_P$ 到无穷大被归入末端成本项 $\phi(x_{T_P})$ 中。通过将 (3) 结合到 (1) 中，我们可以得到一个具有扩展规划视野的轨迹优化问题。

$$
\min_{u_0, ..., u_{T-1}, u_{T}, ..., u_{T_P-1}} \left[ \sum_{t=0}^{T-1} l(x_t, u_t) + \sum_{t=T}^{T_P-1} l(x_t, u_t) + \phi(x_{T_P}) \right]
$$

$$
\text{其中} \quad x_{t+1} = F(x_t, u_t),
$$

通过将规划视野延伸到 $T_P$，我们可以制定一个运动计划，它从时间 $T$ 到 $T_P$ 的运动规划近似于从时间 $T$ 到 $T_P$ 的最佳政策。尽管（4）具有有限的规划地平线，但对于复杂的动力系统，如腿部机器人，在线计算仍然具有挑战性。在预测视界（PH）的规划中，特别是在非线性动力学约束（4b）的考虑下，计算复杂性主要来自于对腿部机器人等复杂动力系统的处理。这增加了问题的维度和非凸性，使之成为一个具有挑战性的问题。

为了提高计算效率，一个有前途的方向是减轻计算价值函数所需的计算负担。在这项工作中，我们提出了两种新方法，可以在减少计算复杂度的同时近似价值函数。

我们的第一种方法遵循轨迹基础的形式主义，通过计算一个展望未来的预测视界来近似价值函数。然而，与其在PH中考虑一个精确的系统动力学约束（通常是非凸的），我们提出使用一个放松的系统动力模型来规划PH。这引入了一个新的轨迹优化（TO）公式，该公式具有多层次模型精度的规划视界。

$$
\min_{x_0, ..., x_{T_P}, u_0, ..., u_{T_P}} \left[ \sum_{t=0}^{T-1} l(x_t, u_t) + \sum_{t=T}^{T_P-1} l(x_t, u_t) + \phi(x_{T_P}) \right]
$$

$$
\text{满足} \quad \forall t \in [0, T], \quad x_{t+1} = F(x_t, u_t),
$$

$$
\forall t \in [T, T_P], \quad x_{t+1} = \tilde{F}(x_t, u_t),
$$

其中执行视界（EH）保持与精确的动力学模型（5b）相一致，而预测视界用放松的动力学模型（5c）来近似价值函数。我们将这种方法称为具有多级模型精度的RHP（MF-RHP）。与传统的TO公式（4）相比，我们的MF-RHP确保EH始终动力学一致，同时减少了TO的总体计算复杂性。在这项工作中，我们展示并测试了三种候选的MF-RHPs，其中PH考虑了离心动力模型的不同凸松弛。

或者，另一个近似价值函数的选项是从过去的经验中学习一个参数化模型 $\tilde{V}(x|\theta)$。根据这个学习到的价值函数模型，我们可以将规划视界缩短，仅覆盖EH：

$$
\min_{x_0, ..., x_T, u_0, ..., u_T} \left[ \sum_{t=0}^{T-1} l(x_t, u_t) + \tilde{V}(x_T | \theta) \right]
$$

$$
\text{满足} \quad x_{t+1} = F(x_t, u_t),
$$

然而，为多接触问题学习价值函数可能具有挑战性。主要困难来源于环境模型的考虑，这引入了在耦合的状态-环境空间中找到一个灵活参数化来表示价值函数的挑战。为了解决这个问题，我们提出学习一个能够预测完成给定任务所需的中间目标状态 $x^*$ 的预测模型 $\Omega$，基于当前机器人状态 $x_0$，最终目标状态 $x_g$ 和环境模型 $\Omega$。

$$
x^* = \Omega(x_0, x_g, \Omega),
$$

我们将这些中间目标状态称为局部目标，并用它们来构建局部二次价值函数：

$$
\tilde{V} (x_ T|x^* ) = (x_ T - x^* )^T (x_ T - x^*).
$$

然后我们使用这些局部价值函数来指导短期视界TO，以计划EH朝向预测的局部目标 $x^*$。我们将这种方法称为LG-RHP，并在图 3(d) 中说明了这一点。虽然从过去的经验中直接学习预测EH的最优政策是可能的，但学习误差可能导致违反系统动力学并引起跟踪失败。因此，在这项工作中，我们决定使用TO计算EH，这保证了动作的动力学可行性。接下来，从第IV节到第VII节，我们将介绍我们方法在多接触动作规划上下文中的技术细节。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/2cb49f85463146efb05634ece7c0d7a0.png)

## IV. 假设
在我们的工作中，我们做了以下假设：

1) 我们专注于规划在不平坦地形上行走的人形机器人的质心轨迹。我们定义每个步态包含以下三个阶段：预摆动（双支撑）、摆动（单支撑）和着陆后（双支撑）。这引出了一个多阶段的轨迹优化（TO）公式化，其中每个阶段的动态和运动学约束由该阶段的接触配置特征化。

2) 我们定义执行地平线（EH）总是覆盖制作一个单一步骤的运动计划（前三个阶段），而预测地平线（PH）可以提前规划多个步骤。预言机被设计用来预测制作一个步骤的局部目标。

3) 我们将机器人的脚建模为矩形补丁。正如通常所做的，我们将矩形的每个顶点建模为一个接触点。

4) 我们将质心（CoM）的运动学约束和接触点的相对位置近似为凸多面体。为了生成这些多面体，我们首先离线采样大量的机器人配置，从中我们可以提取给定末端执行器框架中的CoM位置和脚的位置。然后，我们计算这些多面体作为这些CoM位置和脚位置的凸包[57]。

5) 我们将环境建模为一组矩形接触面。我们预定义了摆动脚将着陆的这些接触面的序列，同时我们优化接触位置（在表面内）和接触时机。

6) 在我们计算出质心运动计划之后，摆动脚的轨迹被插值。这是通过用样条连接计划中的接触位置来实现的。

## V. 多接触运动规划的传统RHP公式

在本节中，我们介绍传统的循环预测控制（RHP）方法，用于规划人形机器人在不平坦地面上行走的质心轨迹。这种传统的RHP被视为我们工作的基准。


我们将RHP问题描述如下。在每个规划阶段，给定一个步数为 $n$ 的规划视界，一个初始状态 $x_{\text{init}}$，一个最终目标状态 $x_g$，以及机器人将停留的一系列接触面 $S_i$，RHP框架旨在计算一个多阶段的运动计划，包括一个质心轨迹 $X^q$，一个接触位置序列 $P$，和一系列相位切换时刻 $T$。质心轨迹 $X^q$ 的每个阶段由广义坐标的轨迹组成，每个阶段我们将轨迹离散化为 $N_q$ 个节点。

我们定义状态向量 $x$ 如下：

$$
x = [c^T, \dot{c}^T, L^T]^T,
$$

其中 $c \in \mathbb{R}^3$ 是质心位置， $\dot{c} \in \mathbb{R}^3$ 是质心速度， $L \in \mathbb{R}^3$ 是表达在质心处的总角动量。控制输入轨迹 $U^u$ 同样被离散化为每个相位 $N_u$ 个节点，控制向量输入定义为接触力 $f_i \in \mathbb{R}^3$ 的集合。

为确保运动的动态一致性，我们引入了以下约束条件：

1. 方程 (9b) 确保状态轨迹从给定的初始状态 $x_{\text{init}}$ 开始。
2. 方程 (9c) 保证相位切换时刻 $t^q$ 在规定的最大运动持续时间 $T_{\text{max}}$ 内。
3. 方程 (9d) 限制每个接触位置 $p^i$ 保持在指定的接触面 $S_i$ 上。
4. 方程 (9e) 保证相对可达性，即每个接触位置 $p^i$ 都位于前一脚印 $p^{i-1}$ 定义的可达工作空间 $Z_i$ 内。
5. 方程 (9f) 是质心可达性约束，限制每个阶段的质心位置 $c^k$ 必须保持在每个活跃接触点定义的可达空间 $K^q$ 内。
6. 方程（9g）施加了系统动力学约束。我们通过前向欧拉积分方法近似积分，并考虑质心动力学模型：

$$
c_{k+1} = c_k + \tau \dot{c}_k,
$$

$$
\dot{c}_ {k+1} = \dot{c}_ k + \tau \left( \frac{1}{m} \sum_ {c^q \in C^q} f_ k^q - g \right),
$$

$$
L_{k+1} = L_k + \tau \sum_{c^q \in C^q} (p_c - c_k) \times f_k^q,
$$

其中 $m$ 是机器人的总质量， $g$ 是重力加速度， $C^q$ 是包含所有接触点索引的集合， $p_c$ 是接触点的位置， $f_k^q$ 是接触点处的接触力。这些动力学模型（例如，非线性质心动力学模型 (10)）会显著增加TO的维度和非凸性，这阻碍了在线计算。为了实现在线多接触RHP，在接下来的章节中，我们将展示我们的方法简化价值函数近似计算的技术细节。


如第三节所讨论的，除了执行地平线（EH），传统的递减地平线规划（RHP）通常还需要考虑一个预测地平线（PH），它作为价值函数的轨迹基近似。然而，传统的RHP常常使用一个精确的动力学模型来规划整个地平线，例如，非线性质心动力学模型（10）。这可能会显著增加轨迹优化（TO）问题的维度和非凸性，从而阻碍在线计算。为了实现多接触RHP的在线计算，在接下来的章节中，我们将介绍我们的方法来简化价值函数近似的计算。

## VI. 具有多层模型精度的RHP

在本节中，我们引入了具有不同模型精度层次的循环预测控制（RHP）。与传统的RHP（见图4(a)）不同，它在整个视界内使用精确的动力学模型，我们的多层精度RHP（MF-RHP，见图4(b)）采用了质心动力学模型在预测视界（PH）中的凸松弛，用于价值函数近似。这简化了轨迹优化（TO）的整体计算复杂性。接下来，我们将介绍三个候选的MF-RHP，它们具有不同的凸松弛。

#### A. 候选1：线性质心动力学

在我们的第一个候选方案中，预测视界（PH）仅考虑由以下方程定义的线性质心动力学：

$$
c_{k+1} = c_k + \tau \dot{c}_k,
$$

$$
\dot{c}_ {k+1} = \dot{c}_ k + \tau \left( \frac{1}{m} \sum_{c^q \in C^q} f_k^q - g \right),
$$

其中 $m$ 是机器人的总质量， $g$ 是重力加速度， $C^q$ 是包含所有接触点索引的集合， $p_c$ 是接触点的位置， $f_k^q$ 是接触点处的接触力。

#### B. 候选2：具有矩形接触的凸角动力学

在我们的第二个候选方案中，除了线性质心动力学外，还在预测视界（PH）中对角动力学（10c）进行了凸松弛，该凸松弛基于文献[47]中描述的方法。角动量动力学中的非凸性主要来自叉乘项产生的双线性项。我们通过以下方程来近似这些双线性项：

$$
\alpha\beta = \frac{1}{4}(\psi^+ - \psi^-),
$$

$$
\psi^+ = (\alpha + \beta)^2,
$$

$$
\psi^- = (\alpha - \beta)^2.
$$

为了保持状态向量 $x = [c^T, \dot{c}^T]$ 的低维模型，我们避免了对角动量 $L$ 的显式建模。相反，我们将 $\psi^+$ 和 $\psi^-$ 在运行成本中作为代理，以最小化角动量变化率及质心加速度。最后，与第一候选方案一样，我们在预测视界（PH）中固定了相位切换时机。与质心动力学模型相比，我们的第二候选模型由于引入了用于逼近角动力学的辅助变量 $\psi^+$ 和 $\psi^-$，其维度增加了。尽管如此，这也使得我们的第二候选模型能够完全凸化。
#### C. 候选3：具有点接触的凸角动力学

为了降低角动力学的凸松弛维数，我们提出了第三种候选模型，在该模型中我们将矩形足转换为点足，并将接触面切换到点接触。这样，控制输入简化为 $u = [F_L^T, F_R^T]$，其中 $F_L$ 和 $F_R$ 分别代表左右足的接触力。这减少了辅助变量（ψ+ 和 ψ−）的数量以及由（11）引入的与信任域约束相关联的数量。

为了提供这些候选模型计算复杂性的直观理解，我们在图5中展示了它们的示意图，并在表I中比较了它们在维度、非凸和凸约束数量方面的模型复杂性。

## VII. 局部引导的滑动视界规划

在本节中，我们介绍LG-RHP，它使用学习的模型来近似价值函数。我们方法的核心思想是学习一个能够预测完成给定任务的局部目标的预言者，这些局部目标基于初始机器人状态、最终目标和环境模型。然后这些局部目标用于构建引导执行视界（EH）规划的局部价值函数。接下来，在第VII-A节中，我们将介绍预言者在多接触规划中的建模。之后，在第VII-B节中，我们将描述与短视界TO的接口。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/0705c8f70ed54aeb946e4d15a59ee9c9.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/961f43a76f9548ee82d3901ef316ce31.png)


#### A. 多接触规划中的预言者建模

在本节中，我们首先描述在多接触规划上下文中的预言者建模，并介绍相关变量定义（见图6(a)）。遵循第III节中的思想，我们定义预言者 $\Omega$ 如下：

$$
x^* , p^* , T^*  = \Omega(\delta t_r, x_0, p_0, \Sigma, x_g),
$$

预言者旨在预测进行步态的目标配置，包括以下内容：
1) $x^*$：目标质心状态。
2) $p^*$：摆动脚达到的目标接触位置。
3) $p_0$：摆动脚的初始位置。
4) $\Sigma = \{S_0, S_1, ..., S_n\}$：一组局部预览的环境片段。我们定义 $S_0$ 为机器人初始站立的表面， $S_1, ..., S_n$ 为机器人可能在未来步行的表面。每个表面由其四个顶点表示： $S = \{v_1, ..., v_4\}, v_i \in \mathbb{R}^3$ 是顶点的三维位置。
5) $x_g$：机器人前方的最终目标位置（世界坐标系中固定）。

我们定义了空间量，如摆动脚、环境模型，以及所谓的接触-足架框架 $W_z$。该框架位于非摆动脚的位置，并具有与接触表面相同的方向。此外，我们将目标接触位置 $p_1$变换到接触表面参数化空间上，得到两个向量 $p^* = q_1 + r_1 q_2^T$，其中 $q_1, q_2 \in \mathbb{R}^3$，并缩放沿着表面边界 $r_1, r_2 \in [0, 1]$。

预言者通过神经网络模型建模，该模型被设计为在线RHP中快速计算。为此，该神经网络包含4个隐藏层，每层具有256个带有修正线性单元（ReLU）的神经元。之前的研究已经证明，这种紧凑的神经网络模型能够对本任务提供足够的动力学可行性。此外，我们的模型灵活足以包含足够的隐藏层和神经元，不仅不会影响预测准确性，而且不会带来性能改进。我们使用Tensorflow框架，并且训练和预测都在CPU上完成。

为了训练预言者，我们采用增量训练方法。每次训练都通过添加数据点来提高方向精度，这些数据点证明了在试验中遇到的失败。如图7所示，在每个训练持续时间 $\Delta t_i$内，我们首先使用LG-RHP和当前训练的预言者 $\Omega^t_i$计算增强数据集，然后在先前采样的环境中使用传统RHP计算恢复动作，直到循环收敛到地面真实轨迹（同一地面上）。我们观察到，在回滚前的1到3个周期内，LG-RHP常常显示出较大的偏离地面真实轨迹。我们重复该过程，直到没有进一步改善收敛率为止。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/08f6e3eb0d8544e0a5353c5142f5c120.png)


#### B. 短视界TO的接口

为了引导LG-RHP，我们采用短视界版本的公式(9)，只计划执行视界，并进行以下更改：

首先，我们用 $(x_ T - x^* )^T(x_ T - x^* ) + (p_ 1 - p^* )^T(p_ 1 - p^* )$ 替换终端成本，强调达到预测的目标状态 $x^\star$ 和接触位置 $p^\star$。其次，我们引入约束以将相位切换时刻 $t^\star$ 拉近到其预测值附近，其中 $\epsilon$ 是用户定义的阈值：

$$
(1 - \epsilon) t^* \leq t \leq (1 + \epsilon) t^*,
$$

这是因为我们经验性地发现，缩小相位切换时刻的搜索空间可以带来更高效的计算，而不是使用成本项来偏置它们的决策。
## VIII. 仿真研究

在本节中，我们在一系列多接触场景中通过仿真评估了MF-RHP和LG-RHP的计算性能。据作者所知，目前还没有专门针对通过简化价值函数近似来加速多接触RHP计算速度的方法。因此，为了突出我们方法的计算优势，我们将它们与传统的RHP（基线）进行了比较，后者通过使用精确的动力学模型规划PH来近似价值函数。我们的仿真视频可以在 https://youtu.be/STBYJl7jvsg 找到。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/3562c5a559464cd086ce5ce9d3b470ad.png)

#### A. 评估设置

我们考虑以下两种类型的地形：1) 中等坡度地形（见图9）和2) 大坡度地形（见图10）。在这些地形上规划多接触运动可能具有挑战性。关键问题是允许的接触力受到接触面朝向的限制。因此，为了找到质心（CoM）的可行动量轨迹，规划算法必须仔细选择接触位置和时机[47]。

在这些地形上，我们使用每个RHP框架离线计算人形机器人Talos[21]的质心轨迹，以递减地平线的方式。为了更详细地说明，我们考虑一个RHP循环，其中每个规划周期旨在计算下一个周期要执行的运动计划。在假设控制器可以跟踪计划的运动而不会有大的偏差的情况下，我们强制下一个周期的运动计划始终从当前周期计划的执行地平线（EH）的终端状态开始。

为了突出我们提出的RHP框架的计算优势，我们考虑了一个在线设置，在每个周期中施加计算时间限制。为了更详细地说明，我们表示如果在一个周期的时间预算内——当前周期要执行的运动（EH）的持续时间——TO收敛，则该周期实现了在线计算。在TO未能在时间预算内收敛的情况下，我们仍然让TO计算直到收敛，除非没有找到解决方案（无法收敛）。

我们在LG-RHP训练期间未见过的地形上测试了所有RHP框架，我们将在每个地形上的试验称为一个情节。为了验证计划轨迹的动态可行性，我们使用全身逆动力学控制器[24]在仿真中跟踪它们。更具体地说，在我们的仿真中，我们使用逆动力学控制器来验证执行计划运动所需的可行接触力和关节扭矩的存在。这是在基于Pinocchio[60]实现的前向积分循环内进行的。

#### B. 实现细节

我们使用软件包CasADi[61]在Python中建模TO问题，并使用KNITRO 10.30[62]的内点法求解它们。此外，我们还利用CasADi通过自动微分提供梯度和黑塞矩阵。尽管像Pinocchio[60]这样的分析方法可以进一步提高这些导数的计算速度，但我们使用CasADi，因为使用自动微分计算导数并不是主要的计算瓶颈（用于导数计算的总计算时间的10%）。

在我们的工作中，所有的计算都在一台配备有Intel i9-CPU（3.6 GHz）和64 GB内存的台式机上进行。对于LG-RHP，我们为两种类型的地形分别训练了不同的预言机。这是因为我们发现这些地形的数据分布具有不同的模态，即在穿越大坡度时，机器人倾向于展示比在中等坡度上行走时更大的动量变化。将这些数据点混合在一起可能会导致不连续和不平衡的数据集，单个NN模型可能难以插值。在第X节中，我们讨论了可以跨这两种模态泛化的潜在选项。

为了训练预言机，我们采用了第VII-A节中描述的增量训练方案，我们展示了这种训练方案可以在第VIII-E节中提高预言机的预测精度。我们使用ADAM算法[63]训练预言机，并将批量大小设置为1280个数据点，学习率设置为1×10^-5。我们使用均方误差函数作为损失函数。在测试LG-RHP时，我们使用了经过5次训练迭代后达到最佳预测精度的预言机。

我们通过随机抽样过程生成了训练环境和测试环境。具体来说，对于给定的一组接触面，我们首先通过均匀抽样确定它们的方向，即围绕横滚轴或俯仰轴旋转。之后，我们使用均匀抽样来决定每个接触面的坡度角度。为了增加覆盖大量训练示例的机会，我们采样了3852个中等坡度地形和6245个大坡度地形进行训练。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/28ed20513ffe46ff825cf9495bdc45cb.png)

#### C. 案例研究1（CS1）：中等坡度

在本节中，我们展示了我们的第一个案例研究的实验结果：在中等坡度上行走（见图9）。尽管我们可以快速找到这种类型地形的准静态运动[58]，我们有兴趣使用TO规划动态行走运动。这为我们提供了一种统一的方法来处理非准静态情况，例如大坡度地形。

3 这是因为与每个接触面相关的摩擦锥力矢量可以抵消重力。

授权限制使用仅限于香港理工大学。下载时间为2024年5月11日上午9:02:41 UTC，来自IEEE Xplore。适用限制。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/6123747170d54895a59072f98a64443d.png)

表II列出了中等坡度地形（CS1）的计算性能。

此外，动态行走还可以允许更有效的任务完成。在这个案例研究中，我们为LG-RHP设置了相位切换时间约束的松弛度ϵ=0.6。我们发现这可以增加找到解决方案的机会（扩大搜索空间），而不会牺牲太多计算时间。

我们根据情节成功率和周期成功率评估每个RHP框架的性能，无论是离线还是在线设置。我们宣布一个情节是成功的，如果所选的RHP框架能够计算出情节内所有周期的运动计划。在这个案例研究中，我们定义每个情节最多包含28个周期。

如表II所示，在离线模式（无限时间预算）下，基线在中等坡度地形上仅使用1步PH就能实现100%的情节成功率。这意味着基线可以成功地为所有周期找到解决方案（100%的离线周期成功率）。然而，由于质心动力学约束的非凸性质，基线有将近一半的周期（48.69%）在线计算时失败（超时）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/18b3673d633542acbe26f1699c64024a.png)


相比之下，MF-RHP的实验结果表明，我们可以通过在PH中权衡模型精度来提高计算效率。然而，这种权衡不能是任意的。例如，尽管我们的第一个MF-RHP候选者在PH中具有最简单的模型（线性CoM动力学），但无论我们为PH分配多少步前瞻，它总是在几个周期后永远无法完成一个情节。这表明在PH中考虑角动力学是至关重要的。事实上，尽管我们的第二和第三个MF-RHP候选者考虑了角动力学的凸松弛约束，但它们都能以仅有1步PH实现72.5%至79.49%的离线情节成功率。此外，由于在PH中采用的放松动力学模型，我们的第二个MF-RHP候选者可以实现69.22%的周期在线收敛，超过了基线（51.31%的周期在线计算）。这表明减少TO问题中的非凸性可以提高计算效率。而且，由于我们的第三个MF-RHP候选者降低了凸松弛的维度（切换到点脚），它进一步提高了计算效率，并将在线周期成功率提高到75.11%。在表III中，我们列出了基线和我们的第二和第三个MF-RHP候选者在中等坡度地形上使用1步PH的平均计算时间。

另一方面，我们也注意到我们的第二和第三个MF-RHP候选者仍然存在无法收敛的风险，即在考虑1步PH时，第二和第三个MF-RHP候选者在20.52%至27.50%的情节中由于收敛问题而失败。尽管我们可以通过延长PH的长度来提高收敛率，但这会增加TO问题的维度，并阻碍在线计算。例如，当我们考虑3步PH时，我们的第二和第三个MF-RHP都可以实现接近基线的高情节成功率（97%）。然而，这导致了50.14%和94.25%的周期无法实现在线计算。在图8中，我们展示了我们第三个MF-RHP候选者的一系列仿真快照，它在所有MF-RHP候选者中实现了最佳的在线收敛率，使用1步PH。

与基线和MF-RHP相比，我们展示了LG-RHP实现了最快的计算速度，其中98.63%的周期在线收敛。此外，由于快速计算，我们的LG-RHP可以维持67.57%情节的在线计算，而基线和MF-RHP则难以连续完成一个情节的在线计算。此外，如表IV所示，LG-RHP平均只消耗了时间预算的19%。这表明了在真实机器人控制中使用LG-RHP的潜力，因为剩余的时间预算可以分配给开销，例如数据传输。然而，由于预言机的预测误差，LG-RHP也有机会无法收敛，即LG-RHP在24.43%的情节中失败，因为机器人被引导到可能导致收敛失败的病态状态。在图9中，我们展示了LG-RHP的一系列仿真快照。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/f3c462eed6f843cab2ab560e2125739d.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/5d3638e13e3e4e4fa165346433cfc49a.png)


#### D. 案例研究2（CS2）：大坡度

在本节中，我们展示了大坡度地形的实验结果，在这种地形上，机器人无法保持静态稳定性，必须动态地穿越地形。我们定义每个情节从大坡度被捕获在前瞻地平线内的那个周期开始，以机器人离开大坡度的那个周期结束。对于LG-RHP，我们将相位切换时间约束的松弛度设置为ϵ=0.15，这是经验性确定的，可以在成功率和计算速度之间提供良好的平衡。

我们在表V中列出了每个RHP框架的计算性能。正如我们所观察到的，在离线设置中，基线仍然可以在我们考虑的大坡度地形上实现100%的情节成功率。然而，这需要基线考虑2步PH，这可以显著增加计算复杂性。结果，基线只有7.99%的周期在线收敛。

另一方面，我们发现MF-RHP候选者在大坡度地形上难以收敛。更具体地说，与CS1类似，由于我们的第一个MF-RHP忽略了PH中的角动力学，它在大坡度地形上永远无法完成一个单一的情节。然而，尽管我们的第二和第三个MF-RHP候选者考虑了角动力学的凸松弛，它们仍然无法完成41.81%至63.49%的情节。这个结果表明，我们在PH中使用的凸松弛模型可能无法准确表示穿越大坡度的高动态运动的动量变化，需要进一步研究模型精度和计算复杂性之间的平衡。

相比之下，尽管地形复杂性增加，LG-RHP仍然在所有RHP框架中实现了最高的计算效率。更具体地说，我们的实验结果表明，LG-RHP有95.99%的周期成功收敛，95.0%的周期实现了在线计算。由于快速计算，我们的LG-RHP还可以维持76.1%情节的在线计算。对于那些无法连续实现在线计算的情节，3.8%是由于超时，其余的（20.1%）是由于收敛失败造成的。此外，如表IV所示，我们的LG-RHP平均只消耗了时间预算的18%。在图10中，我们展示了LG-RHP在大坡度地形上的一系列仿真快照。

#### E. 通过增量训练方案提高预测精度

本节展示了我们在第VII-A节中描述的增量训练方案的有效性。在表VI中，我们列出了在不同迭代次数的数据增强过程中训练的预言机在训练环境中实现的LG-RHP的情节成功率。我们的结果显示，添加感兴趣的校正数据点可以提高预测精度，从而提高LG-RHP的情节成功率。我们发现成功率在5次训练迭代后饱和。





## X. 讨论

在本节中，我们根据实验结果比较每个RHP框架的优势和劣势。

从基线结果来看，我们首先验证了在PH中考虑准确的系统动态模型可以保证高收敛率（对于我们考虑的地形为100%）。这是意料之中的，因为准确的系统动力学模型允许PH尽可能准确地近似价值函数。此外，我们还发现，尽管PH不需要无限长，但拥有足够长度的PH对基线的收敛至关重要。例如，我们的实验结果表明，基线只需要1步PH就能在中等坡度地形上成功实现RHP。然而，要穿越静态稳定性无法维持的大型坡度地形，基线可能需要2步PH。尽管收敛率高，基线的缺点是由于考虑了非凸质心动力学模型，导致计算时间长，这阻碍了其在线使用。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/afdbd89560604084b973be03c7124f32.png)


为了便于在线多接触RHP，我们探索了PH中计算效率和模型精度之间的权衡。这引出了MF-RHP，我们通过在PH中采用凸松弛模型来减少TO的复杂性。从我们的实验结果中，我们可以得出以下结论。首先，我们发现如果我们只在PH中考虑线性CoM动力学（候选人1），MF-RHP总是无法完成一个情节。这表明PH中采用的凸松弛不能是任意的，考虑角动力学很重要。这一发现引出了我们的第二和第三个MF-RHP候选人，其中我们用凸松弛模型模拟了角动力学。结果表明，我们的第二和第三个MF-RHP候选人可以提高计算效率，例如，在中等坡度上，它们比基线在线计算的周期多20%到25%（在时间预算内收敛）。另一方面，使用放松模型规划PH不可避免地会影响PH建模的价值函数的准确性，这可能导致收敛失败。尽管如此，由于PH考虑了精心设计的凸松弛，我们的第二和第三个MF-RHP候选人的收敛失败的发生率是边缘的，例如，只有1%到2%的周期无法收敛（见表II）。尽管我们可以通过延长PH的长度进一步提高MF-RHP的收敛率，但这会增加TO问题的维度，阻碍在线计算。此外，我们意识到在大型坡度地形上，我们的MF-RHP未能完成大约一半的情节，延长PH的长度并没有显著提高收敛率。这表明使用我们提出的凸松弛计算PH可能会导致大型坡度地形的价值函数近似不准确。我们猜测不准确来自以下两个因素。首先，所提出的凸松弛可能不够紧密，无法捕捉高度动态运动的动量变化[6]。其次，PH中手动固定的相位切换时间可能不适用于模拟这种动态运动。总之，MF-RHP的结果成功地证明了我们可以通过沿规划地平线放宽模型精度来实现在线多接触RHP。对于未来的研究，我们认为值得改进MF-RHP在具有挑战性的场景（如大斜坡案例）中的表现，例如，寻找动态的紧密凸松弛和接触时间优化的凸公式。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/633a30ca56aa483d9aea646b3f75866d.png)


为了进一步提高多接触RHP的计算效率，我们提出了LG-RHP，我们通过学习模型近似价值函数。更具体地说，我们引入了一个预言机来预测实现给定任务的局部目标，然后我们构造局部价值函数来吸引EH朝向这些预测的局部目标。这种方法具有缩短的规划地平线（仅规划EH），我们证明了LG-RHP在模拟中实现了最佳的在线收敛率（95%至98.63%的周期在线收敛），与基线和MF-RHP相比。这种计算优势还使我们能够在动态变化的环境中在真实的仿人机器人平台Talos上演示在线RHP（见第IX节）。

然而，LG-RHP在以下两种情况下仍然存在困难。首先，由于拟合不完美和数据覆盖不足，预言机可能存在预测误差。这可能导致不准确的价值函数，使机器人朝向病态状态，导致收敛失败。尽管我们可以通过增量式训练方案来缓解这个问题，该方案演示了从未见的状态的恢复动作，但我们发现很难涵盖所有可能的机器人状态和环境模型组合。为了进一步提高预测精度，我们认为值得改进采样方法，更有效地覆盖预言机的输入空间。同时，探索在短期TO中施加安全约束的方法也可能有助于提高收敛率。其次，尽管LG-RHP只计算EH，但它仍然是一个NLP问题，没有计算时间的保证，可能无法在线收敛。为了缓解这个问题，一个可行的选择是通过用参数化曲率表示轨迹（例如，贝塞尔曲线[41]）来减少决策变量的数量。此外，如第VIII-B节所述，我们发现两种地形类型的数据点表现出不同的模态。当在组合数据集上训练单个NN时，这可能会带来挑战。尽管我们通过使用单独的NN捕获了这两种模态，但值得探索一种更统一的方法来处理多模态数据，例如使用混合密度网络[65]。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/d5929be8377e4de7966b879ea4592e2f.png)

在这项工作中，我们假设接触面序列是预定的[5]，[58]，步态模式的选择是给定的，即脚与环境接触和断开接触的序列[66]。理想情况下，这些离散决策应该由优化自动解决。然而，这引发了组合问题，这些问题难以解决。在未来，我们建议扩展MF-RHP和LG-RHP，以考虑多接触规划问题的组合方面。此外，也值得将我们的方法扩展到不同任务的泛化，例如考虑不同的目标位置和行为模式（转弯和侧向行走）。

## XI. 结论

在本文中，我们提出了MF-RHP和LG-RHP两种新方法，它们能够在不平坦地形上实现在线多接触递减地平线规划（RHP）。我们方法的核心思想是找到价值函数的计算高效近似。为此，
MF-RHP通过使用凸松弛模型计算预测地平线（PH）来近似价值函数。另外，LG-RHP专注于学习价值函数模型，其中我们训练一个预言机来预测完成给定任务的局部目标，然后基于这些局部目标构建局部价值函数。

MF-RHP的实验结果表明，通过在PH中放宽模型精度，可以实现在线计算。这种方法直接易于实现。然而，考虑在PH中放宽模型不可避免地会影响价值函数近似的准确性，这可能导致收敛失败。为了提高MF-RHP的性能，我们认为未来的研究在计算效率和模型精度之间找到平衡点是重要的。

由于缩短了规划地平线，我们的LG-RHP在所有RHP框架中实现了最佳的在线收敛率。这种计算优势使我们能够在动态变化的环境中，在真实的仿人机器人平台Talos上演示在线RHP。然而，我们发现由于数据覆盖不足，预言机可能会有预测误差，从而导致收敛失败。为了缓解这个问题，我们采用了增量式训练方案，从导致收敛失败的状态中添加数据点。我们发现即使使用这种方法，也很难实现100%的预测精度，这表明进一步研究提高学习精度是必要的。
