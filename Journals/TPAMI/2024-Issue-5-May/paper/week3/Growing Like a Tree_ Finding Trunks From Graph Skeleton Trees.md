# [Growing Like a Tree: Finding Trunks From Graph Skeleton Trees](https://ieeexplore.ieee.org/document/10330013/)
## 题目：Growing Like a Tree: 从图骨架树中识别主干

**作者：Zhongyu Huang; Yingheng Wang; Chaozhuo Li; Huiguang He**  
**源码：https://github.com/zhongyu1998/GTR**

****
# 摘要
图神经网络（GNNs）多年来一直以消息传递范式为基础，在广泛的应用领域取得了巨大成功。尽管这一范式具有优雅性，但它为图级任务带来了几个意想不到的挑战，例如长距离问题、信息瓶颈、过度压缩现象和有限的表达能力。在本研究中，我们旨在克服这些主要挑战，并打破图级任务中传统的“节点和边中心”思维模式。为此，我们从信息影响的角度对信息瓶颈的原因进行了深入的理论分析。基于理论结果，我们提供了独特的见解来打破这一瓶颈，并建议从原始图中提取骨架树，然后以一种独特的方式在该树上传播信息。受到自然树的启发，我们进一步提出了从图骨架树中找到树干的方法，以创建强大的图表示，并为图级任务开发相应的框架。在多个真实世界数据集上的广泛实验表明了我们模型的优越性。进一步的综合实验分析突出了其捕获长距离依赖性和缓解过度压缩问题的能力，从而为图级任务提供了新的见解。

# 关键词
- 图神经网络
- 图级任务
- 长距离模式
- 过度压缩
- 骨架树
- 树干

# I. 引言
图神经网络（GNNs）最近在解决图结构上的现实世界学习问题方面取得了显著的成功。这些成功大多由消息传递框架[1]支撑，其中连接的节点彼此交换消息（即节点和边特征）并更新其节点表示[2]。尽管消息传递范式主要关注节点级和边级计算，但它在图级任务中也显示出巨大潜力，例如图嵌入[3]、图分类[4]和图回归[5]。

一般来说，图级任务遵循节点级任务中使用的消息传递范式。在这个范式中，每个节点通过聚合其邻居的表示来迭代更新其表示。在k次聚合迭代之后，节点的表示捕获了其k跳网络邻域内的结构信息[4]。为了生成图级任务的整个图的表示，进一步应用图级读出函数来总结最终迭代中所有节点的表示。尽管这个范式多年来一直很流行，但它为图级任务带来了几个意想不到的挑战。

在消息传递框架中，每个节点作为中心对象，在每次迭代中与邻近节点交换消息，以捕获其局部邻域的信息。通常，在堆叠k个消息传递层之后，框架执行k次聚合，允许它为每个中心节点收集k跳局部邻域的信息。由于局部邻域结构在节点级任务中起着关键作用，这个范式可以为大多数节点级任务生成令人满意的节点表示。相比之下，图级任务更强调全局图结构，它包含丰富的长距离信息，如图1(a)所示。为了捕获节点的远距离k跳邻居的长距离信息，消息传递框架需要一个较大的k值和大量的迭代。否则，相距较远的节点无法相互感知。然而，增加k可能导致一系列具有挑战性的问题，例如高计算成本、过度平滑[6]，特别是过度压缩[7]。

随着迭代次数（k）的增加，每个节点的接受域内的节点数量呈指数增长[8]。此外，较大的k值增加了遇到具有高学位节点的可能性，这加速了接受域的扩展。由于需要将来自呈指数增长的接受域的大量信息“压缩”到固定大小的节点表示向量中，这个过程在模型中创建了一个信息瓶颈，如图1(b)所示。因此，在压缩过程中可能会丢失远距离信息，随着 $k$ 的增加，消息传递框架更容易受到过度压缩现象的影响。总之，消息传递框架受限于短程信息和局部邻域结构[9]，[10]，未能捕获来自远端节点的长程信息和全局图结构。

另一个常被忽视的关键模块是图级读出函数。即使消息传递过程能够学习到良好的节点表示，一个不合适的图级读出函数可能会阻碍将学习到的节点表示汇聚成一个富有表现力的图表示。例如，许多常用的读出函数（例如，SUM(·)、MEAN(·)和MAX(·)）与图结构无关。这些读出函数在生成图表示时不能利用甚至完全忽视图结构，导致表现力减弱[11]、[12]（见图1(c)）。实际上，先前关于表现力的研究[4]已经证明，读出函数和聚合函数同等重要。在我们看来，一个图神经网络（GNN）的总体表现力遵循著名的李比希最小法则1，这意味着它取决于GNN最薄弱的部分，即聚合或读出函数。即使有一个强大的聚合函数，由于读出函数的限制，GNN也可能无法发挥其全部潜力。不幸的是，虽然一些最新的进展[14]、[15]、[16]已经认识到了这个问题，并努力设计出强大的读出函数，但大多数以前的工作只研究了聚合函数，而忽略了读出函数。

根据上述分析，我们得出结论，当前的消息传递范式并不适合图级任务。因此，本研究旨在开发一种新的图级任务框架，该框架打破了传统的“节点和边中心”思维模式，并克服了传统消息传递范式中的上述挑战。高层思想是，图级任务通常不需要像节点级任务那样学习每个节点的极其精确的表示。具体来说，我们首先使用树分解算法从原始图中提取一个骨架树，如图1(d)所示。骨架树中的每个节点代表了原始图中的一个基本子结构模式，包括一个边（例如，引文网络中的引文链接）、一个环（例如，分子图中的苯环）、一个团（例如，社交网络中的特定社区）或一个重要节点（例如，交通网络中的交通枢纽）。接下来，我们将骨架树分解成不同层级的树干，然后应用长短期记忆网络（LSTM）[17]来学习每个树干沿其对应路径的表示，如图1(f)所示。最后，我们组合所有树干表示来创建原始图的表示。由于LSTM能够捕获长距离依赖性[18]、[19]，这种方法有效地解决了长距离问题。此外，如图1(e)所示，我们方法中的接受域内的节点数量和信息量仅随着传播步数线性增加，这显著缓解了信息瓶颈和过度压缩现象。我们不是单独设计复杂的聚合和读出函数，而是将这两种类型的函数集成到一个单一的传播函数中（即LSTM），并直接用它来生成树干表示。以这种方式，所提出的框架通过强调包含全局图结构的树干表示，有效确保了表现力，与传统的仅捕获局部邻域结构的节点表示相反。广泛的实验评估进一步证明了所提出模型在捕获长距离依赖性和缓解过度压缩问题方面的优越性，从而为图级任务提供了新的见解。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c8ea03acd8894448ae0f121a517201c8.png" width="70%" /> </div>


# III. 预备知识
设 $G = (V, E) \in G$ 是一个图，其中顶点集 $V = \{v_1, v_2, ..., v_{NG}\}$ 和边集 $E = \{e_1, e_2, ..., e_{MG}\}$， $NG = |V|$ 和 $MG = |E|$ 分别代表图中顶点和边的数量。设 $A \in \mathbb{R}^{NG \times NG}$ 是图 $G$ 的邻接矩阵， $X \in \mathbb{R}^{NG \times c_0}$ 是其节点特征矩阵。对于每个节点 $v \in V$， $x_v \in \mathbb{R}^{c_0}$ 表示其节点特征向量， $d_v$ 表示其度数（即 1 跳邻居的数量）。节点 $v$ 的 1 跳邻居集合表示为 $N(v)$。给定一组图 $\{G_1, G_2, ..., G_L\} \subseteq G$，我们的目标是使用图神经网络 (GNN) 为每个图 $G$ 学习一个表示向量 $h_G$。当涉及到图分类任务时，每个图 $G$ 都与一个标签 $y_G$ 相关联，我们的目标是将图 $G$ 正确分类到 $C$ 类中的一个。

在图 $G$ 中的一条路径是有向顶点和边的有限序列，例如 $v_0, e_1, v_1, e_2, ..., e_m, v_m$，其中每条边 $e_i = (v_{i-1}, v_i)$。一条路径可能包含重复的边。一条迹是一条所有边都不同的路径。一条路径是一条所有顶点（因此所有边）都不同的迹（除了可能 $v_0 = v_m$）。我们用长度为 $m$ 的顶点序列 $(v_0, v_1, ..., v_m)$ 表示一条路径 $P$。如果 $v_0 = v_m$，则称一条迹或路径为闭合的，并且至少包含一条边的闭合路径称为环 [52]。

在无权图 $G$ 中，两个顶点之间的最短路径是它们之间的长度最小的路径，其长度被称为这两个顶点之间的（最短路径）距离。我们用 $dist(u, v)$ 表示一对顶点 $u$ 和 $v$ 之间的最短路径距离。图 $G$ 中的最长最短路径是任意一对顶点之间的长度最大的最短路径。图 $G$ 的直径是任意一对顶点之间的最大距离（即最长最短路径的长度），定义为 $d = \max \{dist(u, v) | \forall u, v \in V\}$。

在本文中，我们使用普通字母 $d$ 表示节点度数，用哥特字母 $d$ 表示图的直径。

*一个完全图*是一个简单的无向图，其中每一对不同的顶点都是相邻的。图 $G$ 中的一个团是一个完整的诱导子图，大小为 $k$ 的团称为 $k$-团。

# IV. 图树干网络：理论分析与方法论
在本节中，我们详细阐述了我们提出的 Graph Trunk Network (GTR) 方法。我们首先介绍树分解算法，该算法从原始图中提取出一个骨架树，以建立其结构框架和高层次描述。此外，我们从信息影响的角度深入理论分析了信息瓶颈的原因，并基于此分析结果，提供了打破这一瓶颈的独特见解，并建议沿着骨架树的最短路径传播信息。最后，我们通过借鉴自然界中的树并开发相应的网络架构，提出了我们的技术实现。我们提出方法的流程图在图 2 中进行了说明。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/5957d617f66e4e5ca1884791fc79484a.png" width="70%" /> </div>


## A. 树分解
在传统的消息传递神经网络 (MPNNs) 中，计算图通常与输入图对齐。通过在原始输入图上传播信息并执行计算，这些 MPNNs 捕获了其拓扑结构和节点特征。然而，如第 I 节中讨论的，许多图由于其复杂的结构使得 GNNs 容易受到信息瓶颈和表达性瓶颈的影响。此外，先前的研究 [5] 已经证明 MPNNs 无法检测图中的循环。为了克服这些挑战，一些先进的 GNN 工作 [27]、[28]、[29] 提出了一种有希望的方法，该方法将计算图与输入图解耦，并将输入图转换为树。这种转换使计算图独立于输入图，并消除了由循环子结构引起的计算障碍，从而促进了计算图上的信息传播和计算。
一种流行的转换技术是树分解，它将图 G = (V, E) 映射到树 T = (B, R)。这里 B = {b1, b2, ..., bNT} 是树 T 的顶点集，每个元素 bi ⊆ V（通常称为一个包）是图 G 的顶点集 V 的子集，而 R 是树 T 的边集。此外，树 T 满足以下属性：
- 节点覆盖：图 G 中的每个顶点 v ∈ V 至少属于树 T 中的包 B 的一个 b ∈ B，即 v ∈ b。
- 边覆盖：对于图 G 中的每条边 e = (u, v)，树 T 中存在一个包 b ∈ B 同时包含两个端点 u 和 v，即 {u, v} ⊆ b。
- 一致性：如果树 T 中的两个包 bi 和 bj 都包含图 G 中的顶点 v ∈ V，则树 T 中 bi 和 bj 之间的路径上的所有包 bk 都包含 v，即 bi ∩ bj ⊆ bk。

受到树分解思想的启发，我们提出从原始图中提取一个骨架树。类似于动物的骨骼，骨架树是一个树状的结构框架，支撑着原始图的主体。骨架树中的每个节点（即包）代表了原始图中的一个基本子结构模式，包括一个边（例如引文网络中的引文链接）、一个环（例如分子图中的苯环）、一个团（例如社交网络中特定社区）或一个重要节点（例如运输网络中的运输枢纽）。

具体来说，给定一个图 G，我们首先找到所有的团、环和不属于任何团或环的边。这些团、环和边中的每一个都被当作一个单独的包。此外，我们找到至少属于三个包的重要节点，并将这些节点作为单节点包添加。接下来，我们通过在所有共享公共节点的包之间添加加权边来构建一个骨架图。对于任何两个共享公共节点的包 b 和 c，让 Sb 和 Sc 分别表示图 G 中包 b 和包 c 对应的子结构。如果包 b 是一个单节点包，则将边 r = (b, c) 的权重设置为另一个包 c 的子结构 Sc 中的边数；否则，将边 r 的权重设置为图 G 中 Sb 和 Sc 共享的公共节点数。最后，我们选择骨架图的最大生成树作为原始图 G 的最终骨架树 T。

映射矩阵 M ∈ {0, 1}^{|V|×|B|} 表示将每个节点 vi ∈ V 分配给每个包 bj ∈ B，其中 (i, j) 元素 Mi,j = 1 当且仅当 vi ∈ bj。树 T 的节点（包）特征矩阵由 M^⊤X 给出。此外，包的属性，如其类型（即边、环、团或单节点）和它包含的节点数，也可以通过将它们映射到可学习的嵌入来作为额外的包特征。

## B. 信息瓶颈的理论分析
在本小节中，我们从信息影响的角度深入理论分析了信息瓶颈的原因，并基于此分析结果，提供了打破这一瓶颈的独特见解，并在下一小节中展示了我们的实现。

先前的研究 [7] 已经启发式地指出，长距离是导致信息瓶颈的重要因素。由于长距离交互本质上可以被视为一个节点对另一个远端节点的影响，我们首先通过定量描述任意一对节点 u 和 v 之间的信息影响来开始我们的分析。基于先前的文献 [53]、[54]，我们将随机游走作为信息传播的一般规则，并定义节点影响如下。

定义 1（节点影响）：设 h(0)_v ∈ R^c0 是节点 v 的初始特征向量，h(k)_u 是节点 u 在 k 步随机游走后的表示向量。节点 v 对节点 u 的节点影响 I(u, v) 定义为雅可比矩阵 ∂h(k)_u / ∂h(0)_v 的范数：

$$
I(u, v) = \left\| \frac{\partial h(k)_u}{\partial h(0)_v} \right\|, 
$$

其中范数 ∥ · ∥ 是任意诱导范数。

节点影响量化了节点 v 的信息变化如何影响节点 u。下面定理给出了节点影响的分析表达式，以便进一步分析。

定理 1：给定一个图 G，节点 v 对节点 u 经过 k 步随机游走的节点影响 IG(u, v) 为

$$
IG(u, v) = \sum_{p=1}^{\Psi} \left( \prod_{i=1}^{k-1} \frac{1}{\sqrt{d_{v(p)_ i}}} \right) \left( \prod_{j=1}^{k-1} \frac{1}{\sqrt{d_{u(p)_j}}} \right),
$$  

其中 Ψ 是长度为 k 的路径总数，第 p 条路径表示为（u, v(p)_1, v(p)_ 2, ..., v(p)_(k-1), v）；avivj 是节点 vi 和 vj 之间的边权重，dv 是节点 v 的度数。

给定一个树 T，节点 y 对节点 x 经过 k 步随机游走的节点影响 IT(x, y) 为

$$
IT(x, y) = \prod_{i=1}^{k-1} \frac{1}{\sqrt{d_{y_i}}} \prod_{j=1}^{k-1} \frac{1}{\sqrt{d_{x_j}}}, 
$$

其中在节点 x 和 y 之间至多有一条长度为 k 的路径，表示为（x, y1, y2, ..., y(k-1), y）；ayiyj 是节点 yi 和 yj 之间的边权重，dy 是节点 y 的度数。

定理 1的详细证明在附录 A 中提供，可在线获取。本文考虑的是非加权图，其中所有边的权重统一为 1，这与先前文献 [5]、[53]、[54] 和许多现实世界场景一致。因此，定理 1 的结果可以进一步简化为

$$
IG(u, v) = \sum_{p=1}^{\Psi} \left( \prod_{i=1}^{k-1} \frac{1}{d_{v(p)_i}} \right),
$$

$$
IT(x, y) = \frac{1}{\sqrt{d_x d_{y_1} \cdots d_{y_{k-1}}}}.
$$

定理 1 及其简化形式 (1) 和 (2) 表明，随着节点间距离 k 的增加和节点度数的增加，节点影响减小，这与直觉一致。更具体地说，节点影响 I(u, v) 随着节点 u 和 v 之间距离的增加而呈指数级减小，并且随着连接 u 和 v 的路径上节点度数的增加而呈多项式级减小。因此，长距离（即长范围）和高度数是导致节点影响弱（即信息瓶颈）的关键因素。此外，在所有可能的路径中，当信息沿最短路径传播时，节点影响通常最大。因此，在后续分析中，我们将 k 设置为最短路径距离，即 k = dist(u, v)。接下来，我们从树分解和接收场的角度提供了打破信息瓶颈的独特见解。

考虑图G中的任意一对节点 $u_G$ 和 $v_G$ 。假设在树分解后， $u_G$ 和 $v_G$ 对应于树T中的节点 $u_T$ 和 $v_T$ 。设 $k_G = \text{dist}(u_G, v_G)$ 和 $k_T = \text{dist}(u_T, v_T)$ 。在树分解的过程中，我们通常有 $k_T \leq k_G$ 和 $d_{u_T} d_{v_T} \cdots d_{v_T, k_T - 1} \ll d_{u_G} d_{v(p)_ G, 1} \cdots d_{v(p)_ G, k_G - 1}, \forall p$ ，特别是在大型图上。因此，树分解通常导致 $I_T(u_T, v_T) \geq I_G(u_G, v_G)$ 。这表明，与其在原始图上传播信息，不如在骨架树上传播信息通常可以获得更强的信息影响，从而在一定程度上缓解信息瓶颈。
为了最大化骨架树中节点影响 $I_T(u, v)$ ，我们进一步限制接受域，使信息传播更加集中。这种限制是通过解决以下优化问题实现的：

$$
\max \frac{1}{\sqrt{d_u d_{v_1} \cdots d_{v_{k-1}}}}
$$
$$
\text{s.t. } d_u \geq 1, d_{v_i} \geq 2, \forall i = 1, 2, ..., k - 1
$$

显然，最优解是 $\hat{d}_ u = 1, \hat{d}_{v_i} = 2$ ，这对应于节点u和v之间的路径。由于树具有在任何一对节点之间恰好存在一条路径的独特属性，因此u和v之间的路径也是最短路径。因此，最优解激励我们将接受域从传统的k跳邻域限制为长度为k的最短路径，如图1(e)所示。

总之，本小节的理论分析表明，在骨架树上沿最短路径传播信息并限制接受域至该路径，有望克服信息瓶颈的挑战。在下一小节中，我们将展示相应的实现方式。

## C. 寻找图的主干
在自然界中，树干是树木最重要的部分。它储存水分和养分，将它们输送到整个树中，并为整棵树提供结构支持。受自然界中树干的启发，我们的目标是在骨架树中找到一个类似树干的子结构，并利用它来传输信息（类似于“养分”）到整个骨架树中。这样，这个子结构可以捕获并存储来自整个骨架树的重要信息，模仿自然界中树干的功能。为了避免与自然“树干”混淆，我们将在必要时将树干类子结构称为“图干”。
### 算法 1：寻找最长最短路径的算法。
输入：一个树 T。输出：树 T 中的最长最短路径。
1. 选择树 T 中的一个任意节点 s。
2. 从节点 s 开始，在树 T 上运行广度优先搜索（BFS），并将最后访问的节点标记为 u。
3. 初始化一个数组 pre[0, 1, ..., NT - 1] 为 (u, u, ..., u)。
4. 从节点 u 开始，在树 T 上运行另一个 BFS，访问节点 i 时更新 pre[i] 为 i 的直接前驱，并标记最后访问的节点为 v。
5. 使用数组 pre 从 v 回溯到 u，以确定节点 u 和 v 之间的路径 Puv。
6. 返回 Puv。

直观上，自然树中的树干类似于树数据结构中的关键路径。此外，我们之前的理论研究建议，沿着最短路径传播信息可以帮助缓解信息瓶颈。考虑到这些因素，图干可以作为骨架树中的一条特殊类型的路径。接下来，我们首先量化路径的整体信息影响力，然后使用它来在骨架树中所有可能的路径中找到图干。
### 定义 2（路径影响力）：
设 P 表示长度为 m 的路径，其顶点序列为 (v0, v1, ..., vm)。路径 P 的路径影响力 I(P) 定义为路径上每一对节点之间的节点影响力的总和： 

$$
I(P) = \sum_{i,j=0}^{m} I(vi, vj)
$$  

在自然界中，树干携带了树所需的大部分养分。受这一自然现象的启发，我们将图干视为骨架树中具有最大信息影响力的路径。通过这种方式，我们可以最大化树干携带的总信息量，使其能够维持整个骨架树。以下定理给出了具有最大信息影响力的路径。

**定理 2：** 给定一个树 T，最长的最短路径 ˆP 具有最大路径影响力

$$
I(\hat{P}) = \sum_{k=1}^{d} (d - k + 1) \cdot 2^{2k-1}
$$  

其中 d 是树 T 的直径。


定理 2 的详细证明在附录 B 中提供，可在线获取。定理 2 表明，在骨架树中所有可能的路径中，最长的最短路径具有最大的信息影响力。因此，我们取骨架树中的最长最短路径作为其主干。技术上，我们可以使用以下算法来找到最长的最短路径。

**命题 3：** 算法 1 输出的路径是树 T 中的最长最短路径。算法 1 的计算复杂度为 O(NT)，其中 NT 是树 T 中的节点数。

命题 3 的证明在附录 C 中提供，可在线获取。

## D. 寻找层次化的图主干
在自然界中，移除树干后树的剩余部分也被视为（子）树，每个（子）树都有自己的“（子）树干”。因此，自然树干在整个树中呈现出层次化的结构。鉴于这一有趣的自然现象，我们将整个骨架树分解为不同层次的树干，并使用所有层次的树干提供对整个骨架树的全面描述。更具体地说，第一层的树干 $t_1$ 作为整个骨架树的主干，通常只有一个主干在第一层。剩余的第 $\ell$ （ $\ell \geq 2$ ）层的树干 $t_\ell$ 被视为整个骨架树的一个分支，通常在第 $\ell$ （ $\ell \geq 2$ ）层有多个分支。我们表示第 $\ell$ 层树干的集合为 $T_\ell$。图 2(a) 和 (b) 提供了不同层次树干的插图。

在上一节中，算法 1 描述了如何在任意树中找到主干 $t_1$。一旦获得主干并从骨架树中移除，剩余部分变成一个森林。4 这个森林至少包含一个（通常是多个）连通分量，每个分量都是一个（子）树。要找到第 $\ell$ （ $\ell \geq 2$ ）层的树干，我们将算法 1 应用于移除所有先前 $\ell - 1$ 层树干后的森林的第 c 个连通分量。这个过程迭代进行，直到树中所有节点都被移除为止。假设总共有 L 层。以下算法详细描述了寻找层次化图主干的过程。

这里， $F_\ell$ 表示在移除所有先前 $\ell$ 层的树干和孤立节点后获得的森林。第 2 行将森林 $F_0$ 初始化为树 T = (B, R)。第 5 行考虑 $F_{\ell - 1}$ 中所有 $C_{\ell - 1}$ 个连通分量。第 6 行将算法 1 应用于 $F_{\ell - 1}$ 的第 c 个连通分量，以找到第 $\ell$ 层的树干 $t_\ell,c$ 。第 7-8 行从 $F_{\ell - 1}$ 中移除 $t_\ell,c$ 中的所有边，第 9-10 行移除所有孤立节点。第 13 行根据第 8 和 10 行的结果更新 $F_\ell$，这些结果移除了第 $\ell$ 层的所有树干和所有孤立节点。如果没有剩余节点，第 14 行终止循环，最后，第 15 行返回每个层次的树干集合。附录 D 中的图 4 提供了一个具体示例，说明了上述过程。


## E. 网络架构
在根据算法 2 获取了每个层次的所有树干之后，我们应用一个长短期记忆网络（LSTM）来学习每个树干沿着其对应路径的表示，公式化为：

$$
h^{(\ell)}_ T = \sum_{t^{(\ell)} \in T^{(\ell)}} LSTM \left( x^{(t^{(\ell)})}_ {v_0}, x^ {(t^{(\ell)})}_ {v_1}, \ldots, x^ {(t^{(\ell)})}_ {v_{k_{t^{(\ell)}}}} \right)
$$  

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f67ebf75ceac4274bd7125897b477c6b.png" width="70%" /> </div>


其中 $T^{(\ell)}$  是第 $\ell$  层次的树干集合， $t^{(\ell)}$  是 $T^{(\ell)}$  中的单个树干；让 $t^{(\ell)}$  的长度为 $k_{t^{(\ell)}}$ ； $x^{(t^{(\ell)})}_ {v_i} \in \mathbb{R}^{c_0}$  表示对应于树干 $t^{(\ell)}$  的路径上的节点 $v_i$  的特征向量， $h^{(\ell)}_ T \in \mathbb{R}^{c_\ell}$  表示第 $\ell$  层次的树干表示。然后，我们组合所有树干的表示来创建骨架树 $T$  的表示，并将其表示 $h_T$  作为原始图 $G$  的表示：

$$
h_G = h_T = \sum_{\ell=1}^{L} W^{(\ell)} h^{(\ell)}_T
$$

其中 $L$  是树干层次的总数， $W^{(\ell)}$  表示第 $\ell$  层次树干的可学习权重矩阵。对于图嵌入任务，我们把 $h_G$  视为图 $G$  的最终嵌入。对于图分类任务，我们设置 $W^{(\ell)} \in \mathbb{R}^{C \times c_\ell}$  并进一步将 $h_G$  输入到 softmax 函数来获得图 $G$  的预测类别。对于图回归任务，我们设置 $W^{(\ell)} \in \mathbb{R}^{1 \times c_\ell}$  并直接将 $h_G$  作为图 $G$  的预测结果。学习图表示的逐步过程在附录 D 中通过具体示例进行了演示。

在实践中，从原始图中提取骨架树并将其分解为不同层次的树干可以在预处理阶段完成。因此，公式 (3) 成为了模型训练过程中的主要操作。由于 (3) 恰好通过树 $T$  中的所有边一次，并且 LSTM 在通过每条边时需要 $O(c_\ell(c_0 + c_\ell))$  的矩阵运算，模型训练的计算复杂度为 $O(M_T c_\ell(c_0 + c_\ell)) = O(N_T c_\ell(c_0 + c_\ell))$ ，其中 $N_T$  和 $M_T = N_T - 1$  分别表示 $T$  中的节点数和边数。

# V. 实验
在本节中，我们在多个真实世界的跨领域数据集上评估了所提出的GTR模型。我们的代码可在 https://github.com/zhongyu1998/GTR 上公开获取。
## A. 数据集描述
我们在来自不同领域的九个真实世界数据集上评估了我们的模型，包括MUTAG（化学）、ENZYMES（生物学）、PROTEINS（生物学）、IMDB-B（社会学）、IMDB-M（社会学）、COLLAB（社会学）、ogbg-molhiv（化学）、Peptides-func（生物化学）和Peptides-struct（生物化学）。前六个数据集是经典TUDataset中的流行图分类任务，而ogbg-molhiv是Open Graph Benchmark（OGB）中广泛使用的图分类任务。Peptides-func和Peptides-struct是Long Range Graph Benchmark（LRGB）中的最新多标签图分类和图回归任务，以其显著较大的直径（更长的范围）而著称，比所有先前提出数据集都要大。我们专注于LRGB以测试模型捕获长距离依赖的能力。所有九个数据集的统计信息总结在表I中。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b1fb35bb78264bf084825133f41bc1f5.png" width="70%" /> </div>

## B. 实验细节
1) **评估协议：** 对于TUDataset，我们使用两种协议来分割数据并评估模型。第一种协议，评估协议1，遵循Karhadkar等人[23]的相同协议。具体来说，我们随机将数据分割为训练集、验证集和测试集，每个集合分别包含80%、10%和10%的图。我们重复实验100次，每次使用不同的随机分割，并根据这100次运行的测试准确率报告结果。第二种协议，评估协议2，遵循Xu等人[4]的相同协议，这在相关文献中广泛使用，但不太稳定。具体来说，我们执行10折交叉验证，并在最佳交叉验证准确率的时期报告结果。鉴于评估协议1的稳定性，我们在第V-E节的消融研究中在此协议下进行。
对于图基准测试，即ogbg-molhiv、Peptides-func和Peptides-struct，我们遵循原始数据集文献中使用的相同的数据分割和评估协议。

2) **评估指标：** 我们遵循原始数据集文献中使用的评估指标：TUDataset的分类准确率，ogbg-molhiv的ROC-AUC得分，Peptides-func的平均精度（AP）得分，以及Peptides-struct的平均绝对误差（MAE）。
## C. 预处理时间分析
我们首先测量GTR在每个数据集上完成预处理阶段所需的时间，并在表II中报告平均预处理时间。如表II所示，GTR可以在几秒钟内完成小型数据集的预处理阶段，即使是大型数据集（COLLAB、ogbg-molhiv、Peptides-func和Peptides-struct）也可以在大约10分钟内完成预处理。因此，GTR的预处理阶段在计算上是高效的。此外，结合表I中列出的统计属性，我们可以推断出每个图的预处理时间主要受其节点数和边数的影响。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0c2a5424e5474ad79027dd6e9c47fc17.png" width="70%" /> </div>

## D. 性能比较
接下来，我们将所提出的模型GTR与多个竞争基线进行比较，包括图卷积网络（GCN）[58]、Transformer [24]、门控图ConvNet（GatedGCN）[59]、深度图卷积神经网络（DGCNN）[60]、不变图网络（IGN）[61]、图同构网络（GIN）[4]、关系图同构网络（R-GIN）[23]、可证明强大的图网络（PPGN）[62]、图扩散卷积（GDC）[63]、层次化互信息传递（HIMP）[28]、自然图网络（NGN）[64]、Wasserstein嵌入图学习（WEGL）[65]、添加全邻接层（+FA）[7]、拉普拉斯位置编码（LapPE）[25]、单纯形同构网络（SIN）[45]、细胞同构网络（CIN）[46]、谱注意力网络（SAN）[26]、πGNN [51]、随机离散Ricci流（SDRF）[20]、随机游走结构编码（RWSE）[10]、带标记自我网络的子图联合网络（SUN）[EGO+] [49]和一阶谱重连（FoSR）[23]。

表III和IV分别总结了在TUDataset和图基准测试上的实验结果。为了确保公平比较，我们直接使用原始文献中在相同评估协议下报告的结果。表III中评估协议1下的基线结果取自Karhadkar等人[23]。表III中评估协议2下的基线结果取自它们的原始论文，除了IGN，它来自Maron等人[62]，以保持评估协议的一致性。表IV中的基线结果取自它们的原始论文和数据集的原始文献。“N/A”表示结果在原始文献中不可用。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/56292bea65a84f939bfbc71b024143cf.png" width="70%" /> </div>


实验结果表明，我们的模型在大多数情况下都取得了出色的性能，从而证明了其在图级任务中的优越性。此外，我们的模型在所有具有长范围（Peptides-func和Peptides-struct）和高度数（IMDB-B、IMDB-M，特别是COLLAB）的数据集上始终优于其他模型。这一观察结果与第IV-B节中提出的理论分析一致，该分析揭示了长距离和高度数是导致信息瓶颈和过度压缩问题的关键因素，从而突出了我们方法在克服这些挑战中的有效性。

值得注意的是，两个基线模型，SUN（EGO+）和FoSR，表现出色。SUN（EGO+）使用自我网络选择基于节点的子图，并将消息传递范式应用于这些子图。通过这种方式，SUN（EGO+）跨子图聚合和共享信息，从而改变了标准消息传递范式的“节点和边中心”思维。另一方面，FoSR使用图重连方法和谱间隙优化来缓解过度压缩问题。因此，它们的出色表现不仅展示了利用子图和子结构的潜力，而且强调了解决过度压缩问题的重要性。
## E. 消融研究
骨架树和类似树干的子结构是GTR模型的核心组件，它们在模型中起着关键作用。因此，我们对这两个组件进行了消融研究，以调查它们对模型性能的影响。具体来说，我们将通过树分解提取的骨架树替换为原始图的生成树，以评估骨架树的影响。此外，我们在骨架树上执行深度优先遍历，而不是将其分解为不同层次的类似树干的子结构，以评估类似树干的子结构的影响。相应的结果报告在表III和IV的底部，其中后缀“-SPT”和“-DFT”分别代表生成树和深度优先遍历。

与我们的预期一致，我们观察到每次消融都有明显的性能下降，这证实了骨架树和类似树干的子结构的有效性。尽管性能有所下降，但GTR-SPT和GTR-DFT在大多数情况下仍然优于传统的消息传递框架（GCN、GIN和GatedGCN）。此外，它们甚至与Transformer架构（Transformer + LapPE、SAN + LapPE和SAN + RWSE）和高级图重连策略（SDRF和FoSR）表现相当，从而证明了我们提出的框架的优越性。值得注意的是，GTR-SPT在大多数情况下都不如GTR-DFT，这突出了树分解和骨架树构建的重要性。

另一个有趣的现象是，GTR-DFT在长距离基准测试Peptides-func和Peptides-struct上表现出色。一个合理的解释是，深度优先遍历将骨架树转换为所有节点的线性序列，这使得LSTM更容易捕获所有节点之间的丰富长距离依赖。此外，在线性序列上学习时，信息量随着序列长度的增加而线性增加，这显著缓解了信息瓶颈和过度压缩问题。从某种意义上说，这一现象也证明了我们在第IV-B节中提出的策略的合理性，该策略将接收场限制为路径（一种特殊的线性序列）。然而，GTR-DFT的方差通常高于其他模型，这表明GTR-DFT稳定性较差，容易受到数据扰动的影响。因此，最好使用所提出的类似树干的子结构。
#
## F. 骨架树和图主干的可视化
为了进一步证明骨架树和类似树干的子结构的有效性和合理性，我们在图3中展示了原始图及其相应的骨架树，并突出显示了它们的不同层次的树干。由于肽是由氨基酸链构建的大分子，并包含丰富的长距离依赖[57]，我们将其作为典型示例来可视化各种类型的树干。如图3所示，原始图和骨架树中的绿色1层树干展示了传统GNN难以捕获的长距离依赖。相比之下，我们提出的GTR始终能够准确识别和捕获它们，无论图的大小如何，从而确认了我们方法的有效性。此外，我们方法在骨架树中捕获的主干对应于原始图中的重要子结构。例如，在肽的分子图中，主干代表了肽的主要结构，从生化角度提供了可解释性。鉴于这些事实，我们可以得出结论，我们提取的骨架树和捕获的类似树干的子结构是合理的并且可解释的。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/901d0a5b51fc4fe1826cddc7d2e3570d.png" width="70%" /> </div>


# VI. 讨论
在本节中，我们全面讨论几个重要话题，包括对树干层次的附加细节、本工作的局限性以及未来研究的有前景方向。

## A. 树干层次
在处理大量树干层次的情况时，例如在复杂网络中，可能并不理想地考虑所有 $L$  层，如公式 (4) 中的那样。因此，我们提供对树干层次的详细讨论。实际上，公式 (4) 中的权重矩阵 $W^{(\ell)}$  允许所提出的模型自动学习每个层次的重要性。如果第 $\ell$  层的树干展示了可区分或基本的特征， $W^{(\ell)}$  将突出相应的层次。否则， $W^{(\ell)}$  将减弱甚至忽略该层次。因此，公式 (4) 能够处理合理的树干层次数量。

在实践中，我们观察到通过关注较低层次的树干，通常就足以区分不同的树（以及它们的原始图）。因此，从经验上看，我们只考虑前 $(L - 1)$  层，并在学习树干表示时将后续的 $(L - L + 1)$  层合并为一个单独的层次，公式化为：

$$
h^{(L)}_ T = \sum_{t^L \in T^L \cup \ldots \cup T^L_L} LSTM \left( x^{(t^L)}_ {v_0}, x^{(t^L)}_ {v_1}, \ldots, x^{(t^L)}_ {v_{k_{t^L}}} \right)
$$

其中 $L$  表示我们考虑的树干层次的数量，并且被视为一个超参数。相应地，公式 (4) 变为：

$$
h_G = h_T = \sum_{\ell=1}^{L} W^{(\ell)} h^{(\ell)}_T
$$

因此，这种截断有效地防止了树干层次的过度数量，使我们的方法更加高效和普遍适用。

在同一连通分量中不同层次的树干之间，必须有共同的节点（们）连接它们。我们称这个共同节点为“树叉”节点。叉节点是不同层次树干的交点，因此决定了不同层次树干的相对位置。进一步的解释可以在在线提供的附录 D 中找到，通过具体示例进行了说明。

## B. 局限性和未来工作
为了方便理论分析，在定理 1 之后，我们假设图中所有边的权重为 1。这个假设导致我们在公式 (1) 和 (2) 中得到了简化的结果，这些结果仅依赖于图结构。在许多场景中，边权重、节点特征和边特征在后续的树干选择过程中扮演着重要角色，但当前被忽略了。为了将这些属性纳入考虑，我们可以探索将节点和边特征转换为边权重的方法，并重新审视定理 1 中给出的原始结果。考虑任意一对相邻节点 $v_i$  和 $v_j$ 。例如，我们可以计算节点特征 $x_{v_i} \in \mathbb{R}^{c_0}$  和 $x_{v_j} \in \mathbb{R}^{c_0}$  之间的皮尔逊相关系数作为节点 $v_i$  和 $v_j$  之间的边权重 $a_{v_i v_j}$ 。或者，我们可以使用由 Veličković 等人 [66] 提出的关注机制来计算关注系数作为边权重：

$$
a_{v_i v_j} = ATT \left( W_v x_{v_i}, W_v x_{v_j} \right)
$$

其中 $W_v \in \mathbb{R}^{c_a \times c_0}$  是节点之间共享的可学习权重矩阵，而 $ATT$  是一个共享的关注机制： $\mathbb{R}^{c_a \times c_a} \rightarrow \mathbb{R}$ 。对于节点 $v_i$  和 $v_j$  之间的边特征 $\epsilon_{ij} \in \mathbb{R}^{c_e}$ ，我们可以直接使用可学习的权重向量 $w_e \in \mathbb{R}^{c_e}$ （边之间共享）将其转换为标量 $w_e^T \epsilon_{ij}$ ，并在 $a_{v_i v_j}$  中包含 $w_e^T \epsilon_{ij}$  作为一项。

此外，当边权重可用时，树干选择方法需要相应地进行调整。实际上，当前的主树干，即最长的最短路径，可能并不总是唯一的。在这种情况下，我们简单地随机选择一个。一个更合理和有效的替代方案是基于每条路径中的边权重来选择树干。直观上，我们可以选择具有最大权重的路径作为主树干，从而在确保其唯一性的同时最大化其路径影响力。获得主树干后，我们迭代地执行此过程以找到层次化的图树干，如第 IV-D 节所述。然而，这个猜想应该通过更复杂的理论分析来确认。因此，未来的工作应当扩展当前选择树干的方法，以纳入边权重、节点特征和边特征。

另一方面，作为一种自然选择，公式 (3) 应用 LSTM 作为传播函数来学习每个树干的表示。实际上，任何能够捕获长期依赖性的函数都可以作为 LSTM 的替代品，从而引出灵活的模型变体。探索更先进的模型变体，如自注意力机制 [24]，在未来的工作中是有前景的。

一个有趣的现象是我们提出的模型在验证集和测试集上显示出相似的性能。根据 Hu 等人 [56] 的说法，所使用的数据集的数据分割具有挑战性，因为它们将结构不同的图分离到不同的子集中，并最大化测试图的多样性。结果，传统的图神经网络通常在这些验证集和测试集之间有较大的性能差距。相比之下，我们提出的模型在两个集合上展现出可比的性能，这表明了其强大的泛化能力。因此，研究所提出框架的泛化能力在未来的工作中是值得的。

# VII. 结论
在本文中，我们从信息影响力的视角提供了对信息瓶颈原因的深入理论分析，并提供了打破这一瓶颈的独特见解。基于理论结果，并从自然界中汲取灵感，我们为图级任务开发了一种新的框架，打破了传统的“节点和边为中心”的思维方式，并有效地克服了图级任务中的主要挑战。全面的实验评估和分析进一步证明了我们所提出方法的优越性。总之，我们为图级任务提供了独特的见解，并为图神经网络开发一个更合适的基础范式迈出了重要的一步。


# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
