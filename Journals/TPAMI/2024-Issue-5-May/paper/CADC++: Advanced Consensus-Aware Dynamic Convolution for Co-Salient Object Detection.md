[CADC++: Advanced Consensus-Aware Dynamic Convolution for Co-Salient Object Detection](https://ieeexplore.ieee.org/document/10339864/)
## 题目：CADC++: 用于共显著目标检测的先进共识感知动态卷积算法
**作者：Ni Zhang; Nian Liu; Fang Nan; Junwei Han**
****

# 摘要
在给定一组相关图像进行共同显著对象检测（Co-SOD）时，人类首先从整个组中总结共识线索，然后在每张图像中搜索共同显著对象。大多数先前的方法在总结阶段没有考虑鲁棒性、可扩展性或稳定性，并在搜索阶段采用简单的融合策略来融合共识和图像特征。我们的工作提出了一种新颖的共识感知动态卷积（CADC）模型，直接从“总结和搜索”的角度出发，明确有效地执行Co-SOD。在总结阶段，我们通过池化方法提取鲁棒的个体图像特征，并通过自注意力将它们整合以生成共识特征，从而建模可扩展性和稳定性。然后，我们同时学习两种类型的共识感知动态核，即通用核以捕获组内共同知识，以及自适应核以挖掘图像特定的共识线索。在第二阶段，我们采用动态卷积进行对象搜索。还开发了一种新颖的数据合成策略用于模型训练。尽管CADC已经获得了竞争性的性能，但我们认为，逐步学习动态核和表示比使用同时方案更直观和自然，因此提出了我们的CADC++，CADC的扩展。具体来说，我们首先采用基于通用核的动态卷积来捕获粗略的共同线索作为先验，然后使用基于自适应核的动态卷积来挖掘图像特定细节。我们还提出了一种递归指导策略，以进一步探索两种核和图像特征之间的深层交互。此外，我们为Co-SOD数据集注释了几个具有挑战性的属性，并进行基于属性的评估和鲁棒性分析，以促进Co-SOD领域对模型的全面评估。在四个基准数据集上的广泛实验结果验证了我们提出方法的有效性和鲁棒性。
# 关键词
- 共同显著对象检测
- 动态卷积
- 显著性检测
# I. 引言
共同显著对象检测（Co-SOD）的目标是在查看一组相关图像时识别和分割共同的显著对象。由于其宝贵的应用潜力，例如，它可以用作许多视觉任务的预处理技术，如对象共同分割[1]和图像检索[2]，因此它在计算机视觉界引起了越来越多的兴趣。最近，强大的深度学习模型已经引入到这个领域，并大大提高了Co-SOD的性能[3]、[4]、[5]、[6]。尽管从不同的角度提出了几种Co-SOD算法，让我们从人类的角度考虑这个问题。在查看一组相关图像时，我们人类不能直接捕获每张图像中的共同显著对象。相反，我们必须首先查看所有图像并总结共识知识，即这一组中的对象类别信息。接下来，我们回顾每张图像并利用这些知识搜索目标对象。我们称这个过程为“总结和搜索”，如图1所示。在本文中，我们遵循这一直观的思想，首先总结共识知识，然后在每张图像中搜索共同和显著的对象。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d51a0471950c43bf9bfd8b0d887a28d7.png" width="70%" /> </div>


先前的Co-SOD方法也可以从这个角度进行分析。对于共识知识总结，早期的传统方法依赖于聚类方法[8]、[9]或图模型[10]来获取共识信息。然而，这些模型并没有以端到端的方式学习，因此导致性能不佳。后来，一些深度端到端模型[11]、[12]、[13]直接将所有图像特征连接起来，然后使用简单的卷积操作获得组特征。这种策略只能整合不同图像中相同位置的线索。然而，共同显著对象在一组图像中通常在尺度和位置上表现出很大的变化。因此，这些模型在捕获丰富的共识知识方面可能受到限制。为了探索丰富的共识信息，[14]采用了非局部依赖性[15]来考虑成对图像之间的所有位置。然而，当扩展到处理大量图像时，这种方法将产生巨大的计算成本，因此缺乏模型可扩展性。其他一些工作[16]选择以递归方式从每张单独图像中生成共识特征。然而，它可能导致不稳定性问题，因为如果输入图像序列的顺序改变，预测可能会不稳定。

对于共识对象搜索，各种研究[5]、[6]、[11]、[12]、[13]、[16]、[17]、[18]、[19]、[20] 简单地采用了求和、连接或逐元素乘法操作来整合学习到的共识知识与图像特定特征。我们认为，上述线性融合策略在利用共识知识指导对象搜索方面存在局限性。此外，[7] 为每个图像特定特征基于其与共识嵌入的相似性生成了通道级权重，这可以被视为一种属性级对象搜索算法。我们认为，从空间视角进行搜索更自然，也更容易学习。

在我们2021年的ICCV版本工作中[21]，我们提出了一个新颖的共识感知动态卷积（CADC）模型，从“总结和搜索”的角度出发。具体来说，我们首先总结整个组的图像特征以获得共识知识，然后将其编码为动态核，这总结了共同显著对象的外观特征。接下来，我们通过采用动态核对图像特征进行卷积操作，从而获得Co-SOD结果，如图1所示。

然而，引入动态卷积来解决Co-SOD问题并非易事，需要精心设计模型。对于共识知识总结，我们提出了一个多尺度最大池化模块，首先对每张图像的特征进行总结，从而实现位置和尺度的鲁棒性。然后，通过自注意力机制[22]从个体图像特征中聚合跨图像的共识特征。因此，CADC能够满足可扩展性和稳定性的要求。对于共识感知动态核的生成，我们提出同时构建两种类型的核用于不同目的，即学习一个通用核来捕获整个组的粗略组内共同线索，并为每张图像构建一个自适应核以获得细粒度的图像特定知识。然后，我们并行使用这两种核进行动态卷积，并将它们的结果融合以利用它们的补充作用。我们还在多级特征上执行层次化搜索，以实现从粗到细的渐进式对象搜索。此外，我们提出了一种有效的数据合成策略，以缓解Co-SOD领域训练数据不足的问题。它以两种不同的方式将共同显著对象与无关显著对象融合，并模仿比以前的方法更具挑战性和现实性的真实世界场景。

基于CADC，我们发现逐步执行通用和自适应动态卷积更直观和有效，因此我们提出了改进的CADC++模型。具体来说，我们首先使用基于共识特征的通用核进行动态卷积，以捕获粗略的组内共同线索作为先验。一旦获得了粗略的共同信息，学习每张图像的细粒度图像特定细节就变得容易。因此，我们从通用图像特征中学习自适应核，以捕获个体图像的细粒度信息。此外，我们提出了一种递归指导策略，通过迭代更新图像特征和两种动态核，引入相互指导来探索它们之间复杂的相互作用，以促进学习过程。

我们还对我们的模型进行了全面的分析，与现有的最先进的深度Co-SOD方法进行了比较。一方面，我们采用了对抗性Co-SOD攻击[23]方法来评估模型的鲁棒性。另一方面，我们为常用的Co-SOD数据集注释了几个具有挑战性的属性，并提出了对Co-SOD研究社区进行全面的基于属性的性能评估。我们发现，我们的模型在对抗性攻击和具有挑战性的属性的情况下都优于以前的方法。

在我们之前的工作CADC[21]中，我们主要做出了以下四项贡献。首先，我们提出了从“总结和搜索”的角度对Co-SOD进行建模，即学习动态核以总结共识线索，然后应用动态卷积来执行对象搜索。其次，我们设计了一个组内通用核和图像特定自适应核，以提取每个图像组的互补共识知识。第三，我们提出了使用多尺度最大池化和自注意力模型来满足可扩展性和稳定性的需求。第四，我们提出了一种新颖而有效的数据策略，以模仿具有挑战性的真实世界场景。在这项工作中，我们做出了以下扩展。首先，我们探索了一种更直观的方式来逐步应用共识感知动态卷积，即使用基于通用核的动态卷积首先捕获粗略的组内知识，然后采用基于自适应核的动态卷积来学习图像特定线索。其次，我们提出了一种递归指导策略，以挖掘两种核和图像特征之间的复杂相互作用。第三，我们通过对流行的Co-SOD数据集注释具有挑战性的属性，进行了鲁棒性分析和基于属性的评估。实验结果证明了我们的模型在这两种评估实验中的优越性。第四，我们提出的CADC++模型在四个Co-SOD基准数据集上与现有的最先进方法相比，实现了最有效和最鲁棒的性能。
# III. 提出的方法
## A. 整体架构

我们首先介绍我们的CADC模型，它从“总结和搜索”的角度明确且有效地执行共同显著目标检测（Co-SOD）。具体来说，我们提出了共识特征聚合（CFA）模块，通过聚合鲁棒的个体图像特征来总结共识特征。使用CFA，我们的模型保持了可扩展性和稳定性。接下来，我们共同构建两种类型的动态卷积核，从共识特征中编码共识线索，即生成整个组的通用核和针对单个图像的自适应核。最后，我们通过并行使用这两种动态卷积明确执行目标搜索。

尽管CADC可以获得竞争性的性能，但我们认为增量学习表示更为直观和自然，因为通用核已经捕获了粗略的共同线索，这可以作为学习细粒度细节的先验。因此，在这项工作中，我们提出了改进的CADC++模型。具体来说，我们首先使用共识特征来学习整个组的通用核，通过提出的通用核构建（CKC）模块捕获粗略的组级信息。接下来，我们通过在原始特征上执行基于通用核的动态卷积来进行目标搜索，以获得通用特征映射。随后，我们转向从通用特征映射中为单个图像生成自适应核，通过提出的自适应核构建（AKC）模块捕获细粒度的图像特定线索。然后，我们使用自适应核进行目标搜索，获得细粒度的搜索结果。最后，我们连接使用两种核的结果，利用它们的互补作用进行更准确的共同显著目标搜索。此外，我们提出了一种递归引导策略，通过迭代执行CADC++ T次来深入探索两种核和图像特征之间的相互作用，从而引入相互引导以促进核的学习。我们将提出的CADC++嵌入到U形[40]网络中，将其编码器-解码器对在多个特征级别上连接，以过滤掉干扰和背景信息。整体框架如图2所示。

## B. 共识特征聚合

给定N个相关图像{In}Nn=1，我们遵循[41]使用它们的VGG-16[42]作为我们的编码器，带有修改的DASPP模块[43]来提取所有图像的特征映射X ∈ RN×H×W×C，其中H、W和C分别表示高度、宽度和通道数。之前的方法直接将这些图像特征X ∈ RN×H×W×C连接起来，并通过卷积来捕获共识知识，这忽略了不同图像之间的位置和尺度变化。为了解决这个问题，我们提出了一个多尺度最大池化模块来提取位置和尺度鲁棒的特征，如图3所示。具体来说，我们首先在X上使用一个自适应最大池化层，以三种不同的尺度，获得三个特征映射，空间尺寸分别为1×1、3×3和6×6。接下来，我们将这三个特征映射展平并连接在一起，产生一个特征F ∈ RN×46×C，它在多尺度级别上总结了每张图像中的主导对象，因此能够处理共同显著对象的位置和尺度变化。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0927f6bc0f184bc3b6e15da1fed797f8.png" width="70%" /> </div>


一旦总结了每张图像中的知识，下一步是使用F通过探索相关图像之间的关系来生成共识特征。由于我们提出的多尺度池化模块已经将特征维度从H×W大幅降低到46，因此我们可以采用自注意力机制来聚合所有图像的共识线索。具体来说，我们遵循[22]首先通过三个不同的线性变换将F投影到查询、键和值空间，具有C/2个通道。然后，我们计算一个亲和力矩阵A ∈ R46N×46N，基于查询和转置的键，公式为：

$$
A = Q(F)K(F)^⊤. \quad (1)
$$

这里A表示所有图像的46N个特征之间的成对相似性。Q(·)和K(·)分别表示查询和键的线性变换。接下来，我们的目标是基于A探索图像间相似性，但是，图像内相似性可能会主导A，因为同一图像中的一个特征通常比其他图像中的特征更相似。为了缓解这个问题，我们将A中的自相似性元素重置为最小值。

接下来，我们对A进行第二维度的归一化，然后通过值进行特征聚合：

$$
Y = \text{softmax}(A)V(F), \quad (2)
$$

其中V(·)表示值的线性变换，Y ∈ R46N×C/2。然后，我们采用另一个线性变换将Y重新投影到C通道，然后将其重塑回N × 46 × C的形状。最后，它被视为残差信号添加到原始特征映射F上，从而获得共识特征Z ∈ RN×46×C。

## C. 基于通用核的动态卷积

在总结了共识知识之后，以前的方法通常直接将共识知识与每个单独的图像特征连接起来以提供共识线索。我们认为这种策略无法有效地利用共识知识的指导。相反，我们提出生成共识感知核，它们直接扮演共识知识的代理角色。考虑到相关图像通常共享指示共同显著对象的共同知识，我们为整个组设计了一个通用核来捕获这种共享知识，该核可以通过我们提出的通用核构建模块从共识特征Z中构建。获得通用核后，我们使用它对原始特征映射进行动态卷积以进行目标搜索。

1) 通用核构建：对于实现，我们首先考虑一种直接的方法来生成1×1大小的动态核，正如[36]、[37]所建议的。接下来，我们详细说明如何生成具有更大尺寸的核以探索空间线索。

(1) **1×1大小的通用核**
给定共识特征Z ∈ RN×46×C，我们首先需要将Z中所有图像的特征聚合以获得组级共同特征Fc ∈ RC。之后，我们可以使用Fc来生成通用核。

为了获得Fc，我们依赖于公式(2)中的注意力矩阵softmax(A)。由于[44]发现不同查询计算的自注意力都倾向于关注同一组最具辨识力的关键元素，我们可以直接从注意力矩阵中找到最具辨识力的特征。具体来说，我们直接沿第一维对softmax(A)进行平均，从而获得权重矩阵W ∈ RN×46。接下来，我们使用W对Z沿第一和第二维进行加权求和，以生成Fc。

之后，我们使用Fc通过以下方式生成通用核：

$$
Kc = F_c(\text{PReLU}(F_c(F_c))), \quad (3)
$$

其中Kc ∈ RC1C，并进一步重塑为C1×C×1×1的形状作为通用核，C1是输出通道数。中间的FC层有C个节点。这里我们使用Parametric ReLU（PReLU）[45]激活函数来生成核，因为它通常具有正负激活。

(2) **3×3大小的通用核**
尽管之前的1×1大小的通用核可以在一定程度上编码共识知识，它们只关注通道信息并忽略空间线索。此外，它们只能在1×1范围内搜索对象，因此限制了搜索能力并降低了模型性能。为了考虑空间线索并扩大搜索范围，我们提出学习大空间动态核。然而，我们不使用与香草核相同的方法来生成大空间动态核，因为这将导致巨大的计算成本。例如，如果我们将动态核的空间尺寸从1×1扩大到3×3，Ka和Kc将增加九倍，用于生成核的FC层的参数也将有相同的增长。为此，我们引入深度可分离卷积[39]来缓解这个问题。受[39]的启发，我们首先将3×3通用核分解为深度通用核Kdc ∈ RC×3×3和点通用核Kpc ∈ RC1×C×1×1。Kpc的构建过程与Kc相同。因此，我们只解释如何生成深度通用核Kdc，整个过程如图4所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b26a16ccd31545408b7149575911fde7.png" width="70%" /> </div>


要学习Kdc，我们首先需要整合N个图像在共识特征Z中的线索。因此，我们提出学习一个注意力α3 ∈ RN来实现这一目标。

要学习α3，我们首先将Z展平为RN×46C，并使用它来学习两个注意力α1 ∈ RN×C和α2 ∈ RN×46，公式为：

$$
\alpha_1 = \text{FC}(\text{ReLU}(\text{BN}(\text{FC}(Z)))), \quad \alpha_2 = \text{FC}(\text{ReLU}(\text{BN}(\text{FC}(Z)))), \quad (4)
$$

其中，中间的FC层对α1和α2都有1024个节点，BN[46]表示批量归一化。之后，我们使用softmax对α1和α2的第二维度进行归一化。随后，我们采用α1和α2依次沿“C”和“46”维度聚合Z中的线索，从而获得α3。

一旦获得α3，我们就可以对α3使用softmax，然后使用它对Z进行加权求和以消除其第一维度，从而产生特征Fdc ∈ RC×46。最后，我们采用Fdc来生成Kdc，公式为：

   $$
   K_{dc} = FC(PReLU(BN(FC(F_{dc}))))
   $$
   
   其中K_dc∈R^(C×9)，并且进一步重塑为C×3×3的形状。中间FC层有46个节点。
   
2) *使用通用核进行对象搜索*：在生成通用核之后，我们对原始特征图{X_n}^N_1进行动态卷积以进行显式的对象搜索。具体来说，对于香草1×1通用核K_c，我们直接使用它对每个X_n进行卷积，以获得通用特征X_c∈R^(N×H×W×C_1)。对于大的通用核，我们遵循深度可分离卷积[39]首先采用深度核K_dc对每个通道独立进行3×3组卷积，然后使用点核K_pc执行标准1×1卷积，从而获得通用特征X_c。

## D. 基于自适应核的动态卷积
除了构建通用核，我们还提出为每张单独的图像学习自适应核，以编码图像特定的知识，因为目标对象在不同相关图像中往往呈现出不同的外观和尺度。与我们之前的CADC模型[21]同时构建两种类型的核并执行动态卷积不同，我们认为逐步执行通用和自适应动态卷积更自然和有效。由于存在显著的干扰对象，直接从原始特征学习自适应核可能会受到干扰。相比之下，获得的通用核通过利用组内共同信息有效地捕获了共识线索，因此可以过滤掉干扰，并为学习目标共同显著对象的信息提供强有力的指导。因此，如图3所示，我们首先应用学习到的通用核对原始特征图进行操作，获得一个通用图像特征X_c∈R^(N×H×W×C_1)，该特征从全局视角捕获了共同显著对象的判别模式。接下来，我们使用X_c作为先验，通过我们提出的自适应核构建模块为每张图像生成自适应核，以专注于图像特定的知识。

1) *自适应核构建*：我们首先使用一个标准的1×1卷积将X_c∈R^(N×H×W×C_1)投影到X'^c∈R^(N×H×W×C)，然后使用X'^c学习自适应核。与通用核类似，我们也解释了学习香草和大自适应核的过程。
 
   (1) **1×1大小的自适应核**
   给定通用特征X'^c∈R^(N×H×W×C)，我们首先使用CFA模块跨图像总结共识知识，从而获得增强的共识特征Z_c∈R^(N×46×C)。接下来，我们使用Z_c生成自适应核，如图5所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/3d8d8a5f63b24edfab82cf1537bd9f15.png" width="70%" /> </div>


   具体来说，我们首先旨在聚合Z_c∈R^(N×46×C)中的46维度。因此，我们将Z_c展平为R^(N×46C)，并通过以下方式生成注意力α∈R^(N×46)：
   
   $$
   \alpha = FC(ReLU(BN(FC(Z_c))))
   $$
   
   其中α通过沿第二维的softmax操作进一步归一化，中间FC层有1024个节点。然后，我们使用α对Z_c沿第二维进行加权求和，以整合每张图像所有46个特征中的判别特征，从而获得注意力特征F_a∈R^(N×C)。
   
   最后，我们采用F_a通过以下方式生成自适应核：
   
   $$
   K_a = FC(PReLU(BN(FC(F_a))))
   $$
   
   其中K_a∈R^(N×C_1×C×1×1)，并进一步重塑为N×C_1×C×1×1的形状作为自适应核。中间FC层有C个节点。
   
   (2) **3×3大小的自适应核**
   类似于生成大的通用核的过程，我们将3×3自适应核K_la分解为深度自适应核K_da∈R^(N×C×3×3)和点自适应核K_pa∈R^(N×C_1×C×1×1)。K_pa的构建过程与K_a相同。
   
   要生成K_da，我们直接使用Z_c中的46维特征来学习每个通道和每张图像的3×3核。具体来说，我们将Z_c排列为R^(N×C×46)，然后生成K_da如下：
   
   $$
   K_{da} = FC(PReLU(BN(FC(Z_c))))
   $$
   
   其中K_da∈R^(N×C×9)，并进一步重塑为N×C×3×3的形状。中间FC层有46个节点。
   
2) *使用自适应核进行对象搜索*：我们使用学习到的自适应核进行对象搜索的方式与应用通用核相同，这在第三节第2小节中有所解释。因此，我们可以获得自适应特征图X_a∈R^(N×H×W×C_1)。

## E. 递归引导策略

在我们之前的CADC模型中，我们直接融合了共同特征 $X_c$ 和自适应特征 $X_a$，然后产生Co-SOD显著性图。我们认为这种单步学习策略在探索两种核和图像特征之间的深层交互方面是有限的，这些交互对于预测Co-SOD结果是至关重要的。为此，我们提出了一种递归引导策略，引入相互引导以促进核的学习并提取更准确的特征。

给定图像特征 $X^t \in R^{N \times H \times W \times C}$ 在第 $t$ 步，我们提出的递归引导策略可以描述为：

$$
X_{t}^{c} = DConv(X^t; CKC(CFA(X^t)))
$$
$$
X_{t}^{a} = DConv(X_{t}^{c}; AKC(CFA(X_{t}^{c})))
$$
$$
X_{t+1} = Conv(X_{t}^{c}; X_{t}^{a})
$$

其中 $t \in [0, T - 1]$ 。 $DConv(X^* ;* )$ 表示通过动态卷积在特征 $X^*$ 上进行对象搜索。 $Conv( [ * ; * ])$ 表示先使用拼接，然后采用标准卷积。

在每次迭代中，图像特征 $X^t$ 首先用于指导共同核的学习，以生成共同特征 $X_{t}^{c}$ ，该特征进一步用于学习自适应核以获得相应的自适应特征 $X_{t}^{a}$ 。然后，通过融合 $X_{t}^{c}$ 和 $X_{t}^{a}$ 获得更新的图像特征 $X_{t+1}$ 。随着迭代过程的进行，由于在每一步中共享CKC和AKC，两种核的学习过程通过探索图像特征和学习到的核之间的深层交互而得到有效促进。结果，Co-SOD预测可以逐步改进。

## F. 解码器
如图 2 所示，我们采用所提出的 CADC++ 来连接编解码器对，以分层地在不同尺度上执行对象搜索，这可以大大提高模型性能。具体来说，我们在前四个解码器模块中执行上述过程。在每个解码器模块中，我们首先对编码器特征图执行 CADC++ 以过滤掉干扰并突出共同显著对象，从而获得搜索响应图。接下来，我们将该图与前一个解码器模块生成的特征图进行拼接，并应用两个标准的 3 × 3 卷积层，带有 BN（批量归一化）和 ReLU（修正线性激活函数）来融合它们。此外，考虑到内存限制，我们不对最后两个解码器模块采用 CADC++。相反，我们使用前一个解码器特征通过卷积和 sigmoid 函数生成空间注意力图。然后，我们使用注意力图通过逐元素乘法来细化当前编码器特征。最后，我们使用一个单通道的 3 × 3 卷积层对最后一个解码器特征进行卷积，并采用 sigmoid 激活函数来生成最终的共同显著图。

# IV. 计算成本分析
在本节中，我们分析了我们提出的模块的计算成本。对于共识特征聚合，我们提出的多尺度最大池化模块可以将特征维度从H×W大幅降低到46，从而使我们能够采用自注意力机制来总结所有图像的共识知识，而直接在原始特征图上应用自注意力将带来巨大的计算成本。例如，对于N个相关图像，使用自注意力在原始特征图上和我们提出的多尺度池化特征上的计算复杂度分别为O((NWH)^2)和O((46N)^2)，其中46远小于WH。对于共识感知核构建，我们的方法将核大小从1×1扩展到3×3，而没有大幅增加计算成本，这归功于引入了深度可分离卷积，即，将核参数数量从C_1×C×3×3减少到C×3×3+C_1×C。因此，(5)和(8)中用于生成核的FC层的可学习参数可以显著减少。

## V. 提出的数据合成策略
许多先前的方法 [3], [16], [47], [48] 提出了混合不同数据集来训练他们的深度模型进行共同显著对象检测（Co-SOD）。我们遵循 [31] 的做法，采用 COCO 数据集 [49] 的一个子集，该子集包含 9213 张图像，分为 65 组，用于模型训练。然而，该数据集只关注同一类别中的常见对象，并不考虑它们是否显著。因此，[31] 不得不采用一个现成的显著对象检测模型 [24]，该模型在 DUTS [50] 数据集上训练，以生成显著性图作为先验。因此，我们还在模型训练中使用 DUTS [50] 来补充显著性信息。

为了使 DUTS 适应 Co-SOD 任务，[7] 将其中的所有图像分类为 291 组，共有 8250 张图像，构建了用于 Co-SOD 的 DUTS 类数据集。但是，该数据集中的图像只包含共同显著的前景对象，没有任何分散注意力的显著对象，这是 Co-SOD 的一个主要挑战，并且在现实场景中普遍存在。为了模仿外部对象的存在，他们设计了一个拼图策略，将一个类别的图像与另一个类别的图像拼接，以合成新的训练图像，如图 6 的第二列所示。这种策略确实模拟了干扰，但仍然存在弱点，即拼接图像不自然，并且在调整大小以适应模型训练的固定形状时会出现严重的形状扭曲。

为此，我们提出了一种基于泊松混合 [52] 的复制和混合合成策略，以生成更自然的结果。具体来说，对于目标类别的每张图像，我们首先随机选择另一个类别的图像，并使用其显著对象作为干扰。然后，我们通过随机缩放和水平翻转变换所选图像中的显著对象，以增加干扰多样性。接下来，我们将变换后的显著对象复制并混合到目标图像的背景上，作为干扰，以获得合成图像，其显著性图与原始显著性图相同，如图 6 的第三列所示。我们将上述过程称为正常合成策略。然而，在正常合成的图像中，目标对象通常比复制的外部对象更显著。这种现象会导致在这些合成图像上训练的模型更多地关注显著性检测，而忽略了共同存在属性。为了缓解这个问题，我们进一步提出了一种反向合成策略，使用上述合成策略将共同显著对象复制并混合到外部图像的背景上，以获得反向合成图像，并执行将变换后的共同显著对象掩码粘贴到相应位置的全黑图上，以获得最终的显著性图。一个示例显示在图 6 的最后一列。我们称这种方法为反向合成策略。最后，我们采用正常和反向合成策略来合成训练我们模型的数据。

总体而言，如图 6 所示，我们提出的方法可以生成更自然的合成图像，并保持合理的对象形状，与 [7] 的方法相比，因此更适合于训练 Co-SOD 模型。此外，值得注意的是，我们提出的方法与混合方法 [53], [54], [55], [56], [57] 有相似之处。这些方法也是随机选择两张图像并将它们混合以创建一个新的训练样本。然而，与他们直接计算两个随机图像的加权平均值的方法不同，这通常会导致显著的对象重叠，我们的策略涉及将对象复制到其他图像的背景上。此外，他们通过混合两个原始图像来生成新的真值，而在我们的情况下，合成图像的真值仅来源于其中一个原始图像。此外，我们为 Co-SOD 任务设计了双向合成策略，以促进模型学习。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/4f973128de204bb3b3c2a4cd65fecf1c.png" width="70%" /> </div>


# VI. 实验
## A. 数据集和评估指标
我们使用四个共同显著对象检测（Co-SOD）基准数据集来评估我们提出的方法。MSRC [58] 最初是为了对象分类而收集的，我们遵循一些工作 [4]、[8] 的做法，从中选择了七组共233张图像来评估Co-SOD模型。CoSal2015 [4] 包含了50组的2015张图像，这些图像收集自ILSVRC2014检测集 [59] 和YouTube视频集 [60]。CoSOD3k [51] 包括160组的3316张图像，其中共同显著对象通常在语义或概念层面上具有相似性，而不是在视觉外观上。CoCA [7] 是最新且最具挑战性的Co-SOD数据集，包含80组的1295张图像。它提供了现实世界的Co-SOD场景，每张图像中至少包含一个显著的干扰对象，因此更适合评估Co-SOD方法。

按照 [61]、[62] 的做法，我们采用了以下四个广泛使用的指标进行全面评估。最大F度量（maxF）在最优阈值下，联合考虑二值化共同显著图的精确度和召回率。结构度量（Sm）[63] 计算对象级别和区域级别的结构相似性。增强对齐度量（Eξ）[64] 同时捕获图像级别和像素级别的信息。平均绝对误差（MAE）计算预测的共同显著图与相应真实标注之间的平均绝对差异。我们使用的评估代码可在 https://github.com/zzhanghub/eval-co-sod 上获取。

## B. 实现细节
对于数据增强，我们首先将每个训练图像调整至288×288像素，然后随机裁剪出256×256像素作为网络输入。同时采用了随机水平翻转。我们遵循 [31] 的做法，使用IoU损失作为我们的训练目标，并采用随机梯度下降作为我们的优化算法。为了简化网络训练，我们在每个解码器模块部署了深度监督。我们在每个组中随机选择最多14个样本来构建一个mini-batch，并总共训练我们的模型40000步。初始学习率设置为10^-4，并在第20000步和第30000步迭代时分别降低十倍。对于我们提出的数据合成策略，我们为每个原始DUTS类图像生成三个正常合成图像和三个反向合成图像。我们的CADC++模型基于Pytorch [65] 实现，并在GeForce RTX 3090 GPU上训练。代码将发布。

## C. 组件分析
由于在我们之前的论文 [21] 中，CADC模型的每个组件的有效性已经成功验证，这里我们只对CADC++网络中使用的扩展组件进行消融研究。我们遵循 [20] 的做法，使用三个具有挑战性的数据集，即CoCA [7]、CoSOD3k [51] 和 CoSal2015 [4] 对模型进行评估，并且不采用MSRC，因为MSRC中的大多数图像只包含一个显著对象。

1)* 不同模型组件的有效性*：我们在表I中报告了定量结果。首先，我们使用我们之前的版本，即CADC [21]，它采用同时方案（“SS”）来并行执行基于通用核和基于自适应核的动态卷积，如表I的第I行所示。接下来，我们遵循大多数最近的Co-SOD方法 [7]、[31] 采用IoU损失来重新训练我们的CADC模型，以进一步改进性能，并在表I的第II行报告了结果。我们可以发现，使用IoU损失实际上提高了我们的模型在大多数指标上的性能。然后，我们考虑用我们提出的增量方案（“IS”）替换CADC中采用的同时方案，其中我们首先执行基于通用核的动态卷积，然后基于获得的通用特征进行基于自适应核的动态卷积。结果报告在表I的第III行。我们可以看到，该模型在所有三个数据集上的性能都有了大幅提高，从而证明了应用基于通用核和基于自适应核的动态卷积的增量方案的有效性。最后，我们采用了我们提出的递归引导（“RG”）策略，并在表I的第IV行报告了结果。这里我们共享了CKC和AKC的学习参数，并且考虑到计算成本，我们通过递归引导两次来执行CADC++ T次。我们观察到这种策略与第III行模型相比，在所有三个数据集上都带来了性能提升，因此验证了探索两种核和图像特征之间深层交互作用的有效性。因此，我们采用这种模型设置作为我们最终的Co-SOD模型。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0006b0b997b642a787ee5a83a5838338.png" width="70%" /> </div>


此外，我们还在图7中展示了一些视觉比较示例。可以看出，使用同时方案会使模型（第(e)列）很容易受到复杂干扰显著区域的影响，而采用新提出的增量方案（第(d)列）和递归引导策略（第(c)列）可以逐步过滤掉这些干扰区域，从而获得清晰准确的检测结果。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0e0380bc881d4256b2a7ef42d68277ef.png" width="70%" /> </div>


2) *我们提出的数据合成策略的有效性*：为了进一步探索我们提出的数据合成策略的有效性，我们训练了两种最先进的方法，即GICD [7] 和 GCoNet [20]，以及我们的扩展模型CADC++，在不同数据上。实验结果显示在表II中。可以看出，我们提出的数据合成策略增强了模型性能，与原始三个模型相比有所提高，并且优于拼图策略。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/26091f8902ce4d1481f4a900f1d3c9dc.png" width="70%" /> </div>


3) *不同组件设置的消融研究*：我们进一步对不同组件设置进行消融研究，以全面探索最有效的模型设置。

**通用核首先还是自适应核首先？**我们提出的增量方案引发了一个重要问题，即我们应该首先使用基于通用核的动态卷积还是基于自适应核的动态卷积？为了回答这个问题，我们尝试首先执行基于自适应核的动态卷积以获得自适应特征，然后进行基于通用核的动态卷积。我们将所提出的增量方案中通用核先的设置称为“CA”，而将自适应核先的替代方案称为“AC”，并在表III的第I行和第II行中展示了结果。我们可以看到，“CA”设置（第I行）在所有三个数据集上的表现显著优于“AC”设置（第II行）。此外，我们发现“AC”设置甚至比原始的同时方案（表I中的第II行）更差。我们推测这是因为在“AC”设置中，最初学习的自适应核更关注图像特定线索，可能会激活太多与共识知识无关的噪声信息。因此，它不利于随后基于获得的自适应特征学习通用核。相比之下，我们采用的“CA”设置是Co-SOD的一种更直观和有效的方式，因为最初学习的通用核可以很容易地挖掘粗略的共同知识，然后作为提取各个图像详细信息的良好先验。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/ee9dde98946846b7856097c7bbdd55d8.png" width="70%" /> </div>


**递归引导中共享参数还是不共享参数？**我们还探索了我们在递归引导中进行的方式，即简单地堆叠两次CADC++模块（“Stack”）而不共享参数，或者共享可学习参数（“Share”）。通过比较表III的第I行和第III行，我们可以看到共享方式在所有三个数据集上都取得了更好的性能，特别是在CoCA数据集上。我们认为原因是参数共享策略减少了解码器中可学习参数的数量，从而降低了过拟合的风险。因此，我们采用共享策略作为我们最终的设置。

*递归引导次数*：我们进行实验以找到递归引导策略的最优次数T。具体来说，我们将T设置为1、2和3，并在表IV的第I行到第III行中报告相应的结果。我们可以看到，当我们将T从1增加到2时，模型性能得到了改进，而使用T=3会降低模型性能。因此，我们将T=2作为我们递归引导策略的最终设置。

4) *与原型模型的比较*：以前的原型方法 [31]、[32] 通常利用预测的显著图在图像特征上获得非参数原型，通过掩蔽平均池化。然而，这些原型对显著图的准确性很敏感。相比之下，我们提出的模型不依赖于任何预测的显著图。相反，我们直接从图像特征中学习参数化核，以数据驱动的方式编码共识线索，从而更好地处理Co-SOD中的干扰显著对象。此外，生成的原型只关注通道信息，并且仅限于在1×1范围内搜索共同显著对象。然而，我们提出的模型可以构建更大的空间核，扩展搜索能力。

为了支持上述陈述，我们对原型方法和我们提出的动态卷积模型进行了实验比较，并在表V中展示了实验结果。对于原型方法，我们探索了以下两个模块作为我们CADC++模块的替代品。1) 遵循ICNet [31]，我们使用预计算的显著图为所有图像获得原型，并将它们平均化以获得用于卷积图像特征的通用原型，我们称之为“Mean Proto”。2) 我们采用了ICNet [31]中的CFM模块，它将每个图像特征与所有生成的原型进行卷积，并执行加权融合以获得每个图像的最终输出。对于我们基于动态卷积的方法，我们展示了使用我们的1×1通用核（“1×1 CK”）和3×3通用核（“3×3 CK”）获得的结果。我们可以看到，“1×1 CK”模型的结果是比原型方法更好的，从而证实了以数据驱动方式学习共识线索的有效性。此外，“3×3 CK”模型与“1×1 CK”模型相比的优越性能证明了扩展搜索范围的有效性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0877197d887744bab478ee60cee82317.png" width="70%" /> </div>


## D. 与最先进方法的比较
我们将我们最终的CADC++ Co-SOD模型与11种最近发布的方法进行了比较，即CODW [66]、DIM [67]、CBCS [8]、CSMG [5]、GICD [7]、ICNet [31]、GCoNet [20]、Co-EGNet [6]、DeepACG [68]、DCFM [69]和我们之前的CADC [21]。此外，我们重新训练了三种突出的Co-SOD方法，即GICD [7]、GCoNet [20]和DCFM [69]，使用与我们相同的训练数据，从而获得了模型GICD*、GCoNet和DCFM。为了报告这些模型的实验结果，我们使用了与我们的CADC和CADC++模型一致的最终周期模型。此外，遵循DCFM [69]的做法，我们还使用CoCA数据集从不同周期训练的各种模型中选择最佳模型，从而获得了选定的最佳模型版本，即GICDs、GCoNets、DCFM*s和CADC++s。

*定量结果*：我们在表VI中报告了我们的CADC++与以前最先进方法之间的定量比较结果。一般来说，我们可以看到在相同设置下，我们的CADC++模型实现了优越的性能。此外，值得注意的是，CADC++在所有三个数据集和所有指标上都比我们之前的CADC模型有大幅度的性能提升，从而证明了我们扩展模型设计的有效性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f9730893205f463386dde06c845f7626.png" width="70%" /> </div>


*定性结果*：我们还展示了我们的模型和其他竞争模型生成的一些显著图，用于定性比较，如图8所示。这些样本非常具有挑战性，因为它们通常包含一些显著的干扰对象或具有混乱的背景。在这些具有挑战性的场景中，我们的模型可以准确地搜索和分割共同出现的显著对象，而其他方法要么丢失目标，要么容易被干扰的显著对象分散注意力，从而证明了我们提出方法的有效性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/4126d55568ea4b60bc097873a096badd.png" width="70%" /> </div>


# VII. 鲁棒性研究
为了分析先前Co-SOD方法和我们提出的模型的鲁棒性，我们使用了一个名为联合对抗性暴露和噪声攻击（Jadena）[23]的Co-SOD攻击方法，该方法旨在误导Co-SOD方法预测不准确的共同显著对象。它被设计为扰动原始图像组，使其高层特征在所有图像中趋于一致，从而降低目标对象的显著性程度并减少可区分性。结果，这些不区分性的特征使得Co-SOD模型难以从一组对抗性图像中捕获共识知识，可能会突出显示干扰区域或丢失目标对象。Jadena方法主要包括两种关键变体，即Jadena group和Jadena augment。前者使用Co-SOD数据集中的每个原始完整图像组作为输入，后者则采用每个单独图像及其增强版本来形成一个新的组。有关更多详细信息，请参阅[23]。

在我们的实验中，我们首先采用Jadena group方法和Jadena augment方法对来自四个Co-SOD基准数据集的图像进行操作，即CoCA、CoSOD3K、CoSal2015和MSRC，以生成对抗性版本的图像。接下来，我们在这些对抗性图像上预测了先前Co-SOD方法和我们的共同显著图。最后，我们计算了定量比较结果，并在表VII中进行了报告。我们观察到，对于大多数Co-SOD方法，Jadena augment方法实现了比Jadena group方法更强大的攻击。然而，无论我们采用哪种变体，我们的CADC++和CADC在大多数指标上都实现了最佳和第二佳的性能，验证了我们的模型在对抗性图像上与先前方法相比具有最佳的鲁棒性。此外，我们还在图9中展示了一些视觉示例。我们可以看到，我们的CADC++和CADC模型能够有效地防御两种类型的攻击，而其他模型往往会突出显示不准确的区域或丢失目标对象。与CADC相比，我们扩展的模型CADC++能够更准确地检测共同出现的显著对象，从而证明了我们提出的CADC++模型的优越鲁棒性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/55951d882dbf4831a75ae05ba869eb19.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d6625555fc9e4ac4902502e22964d04a.png" width="70%" /> </div>


# VIII. 基于属性的评估
为了促进对最新 Co-SOD 方法的全面评估，我们在使用的 Co-SOD 基准数据集上标注了一组属性，以代表现实世界中潜在的具有挑战性的情况。对于 Co-SOD 任务，个体图像和整个组包含具有挑战性的条件。因此，我们同时考虑了图像级和组级属性。
## A. 图像级属性
我们首先遵循 [72], [73] 的做法，考虑了一些针对 SOD 任务的常见属性，即前景和背景相似 (SFB)、背景混乱 (BC)、遮挡 (OC)、小目标 (SO)、大目标 (BO)、触摸图像边界 (TIB)、形状复杂性 (SC) 和多个显著目标 (MSO)。此外，正如 [7] 所指出的，外部显著对象的存在可能会分散 Co-SOD 算法准确搜索目标对象的注意力，从而使 Co-SOD 任务更具挑战性。考虑到当前 Co-SOD 数据集的这一特性，我们进一步考虑了外部显著对象带来的挑战。我们首先考虑一个具有挑战性的情况，即如果外部显著对象的大小大于共同显著对象的大小，那么干扰将会非常严重。因此，我们提出了一个新的属性，作为大外部对象 (BEO)。然而，由于外部显著对象的真值并不可用，我们提出了一种估计它们大小的方法。具体来说，我们首先采用一个最先进的 SOD 方法 [74] 为每张图像预测一个显著性图，该图通常突出显示所有显著对象。然后，我们计算这些单独的显著性图与真值共同显著图之间的差异，以生成残差图，突出显示大多数外部对象的区域。因此，我们可以使用残差图来计算外部显著区域的大小。一些具有上述定义的图像级属性的示例图像可以在图 10 中找到。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/7847c2030320487181cd8ac85be35188.png" width="70%" /> </div>

## B. 组级属性
接下来，基于 Co-SOD 任务的特性，我们考虑了四个组级属性，即大规模变化 (LSV)、大外观变化 (LAV)、大干扰比例 (LIP) 和强同类别干扰 (SCI)。我们在图 11 中展示了一些示例图像。LSV 考虑了共同显著对象在一组图像中显示出大规模变化的情况。LAV 关注不同图像中共同显著对象外观多样化的图像组。LIP 定义为包含外部显著对象的图像占组内总相关图像的一半以上。基于 LIP 属性，我们进一步观察到，组中的一些外部显著对象可能属于同一类别，例如，人物经常出现在乐器图像中。这些同类别外部显著对象将大大分散 Co-SOD 方法正确识别共同目标类别的注意力。因此，我们进一步定义 SCI 以满足图像中包含同类别外部显著对象的图像占总数一半以上的情况。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/dded9e05eaa54c939b46d2a87016f5bc.png" width="70%" /> </div>

## C. 基于属性的性能分析
我们采用了四种最先进的深度 Co-SOD 模型和我们提出的 CADC 和 CADC++ 进行基于属性的评估。首先，我们发现一些属性，即 BO、TIB、SC、MSO、LSV，实际上对于 Co-SOD 来说并不具有足够的挑战性，因为我们没有观察到大多数模型在这些属性上的性能下降。其中，LSV 和 BO 属性甚至可以在表 VI 中显示的平均结果上带来更好的性能，即在整体数据上计算的结果。我们认为这是因为这样的挑战可以被具有强大和区分性特征的现代基于深度学习的模型很好地处理。其次，我们观察到 MSRC 数据集并不包含大多数提出的挑战性属性。进一步考虑到这个数据集只包含少量图像，我们得出结论，由于其简单性和不足，它不适合进行全面的模型评估。最后，我们仅报告了在三个具有挑战性的数据集上，即 CoCA [7]、CoSOD3k [51] 和 CoSal2015 [4] 上，相对具有挑战性的属性的结果，即 SFB、BC、OC、SO、BEO、LAV、LIP 和 SCI。

我们首先在这三个数据集上计算所选挑战性属性的图像百分比或组百分比，并在表 VIII 中显示它们。然后，我们报告了六个模型在表 X 中的定量比较结果。由于空间限制，我们这里只显示了两个广泛使用的指标的结果，即 Sm 和 maxF。通常，从表 VIII 可以看出，CoCA [7] 数据集包含大多数属性的最高百分比，与其它数据集相比，反映了其难度和复杂性。此外，我们可以发现大多数模型在 CoCA [7] 数据集上的大多数属性上表现出最差的性能。这种现象也可以在表 VI 中找到。所有这些结果都证明了 CoCA [7] 目前是最具挑战性的 Co-SOD 基准数据集。此外，可以看出我们提出的 CADC++ 模型在大多数挑战性属性上与其他方法相比可以实现更优越的性能，从而验证了我们提出的方法的有效性和鲁棒性。我们还评估了 Co-SOD 模型的 FLOPs、参数和 FPS，如表 IX 所示。可以看出，我们的模型与大多数以前的模型相比具有可比的计算效率。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/9d31b3bc97104a5da80c504184775b72.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/946ade93f75743debccd3f06f5cebfb6.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/1f850ca54fcf45a8aa57379f5ce6d06a.png" width="70%" /> </div>


在图像级属性中，我们可以发现 SFB、BC 和 SO 比 BEO 和 OC 在 CoCA [7] 中更具挑战性，而 BEO 是 CoSOD3k [51] 和 CoSal2015 [4] 中最具挑战性的属性。原因可能是 CoCA [7] 中的大多数 SFB、BC 和 SO 图像也可能属于 BEO，因为该数据集大约 80% 的图像具有 BEO 属性。因此，这三个属性表示更复杂的情况，例如同时具有 SFB 和 BEO 属性的图像。然而，对于 CoSOD3k [51] 和 CoSal2015 [4]，它们分别只包含不到 32% 和 20% 的 BEO 图像。其他属性可能只在没有大外部显著对象干扰的情况下引起有限的干扰。

在组级属性中，SCI 对于 CoCA [7] 和 CoSOD3k [51] 来说是最具有挑战性的属性。这是合理的，因为当超过 50% 的图像包含与共同显著对象相同类别的外部显著对象时，模型可能会错误地将该类别视为共识知识，从而对学习共同显著对象的准确模式造成强烈干扰。此外，我们发现 LAV 在 CoCA [7] 中比 LIP 稍微具有挑战性，而在其他两个数据集 [4], [51] 中，LIP 比 LAV 更具挑战性。这是因为 CoCA 中的每个组都有外部对象，而 CoSOD3k 和 CoSal2015 的大多数组没有这个特性。因此，对于这两个数据集 [4], [51]，即使目标对象显示出大的外观变化（即 LAV），当前的深度模型仍然可以很好地处理它们，因为这些对象在没有外部显著对象干扰的情况下相对显著。

# IX. 结论
在我们的工作中，我们提出了一个从“总结和搜索”角度出发的共识感知动态卷积（CADC）模型，以明确执行共同显著对象检测（Co-SOD）。我们联合学习两种类型的动态核，分别捕获组内和图像特定的共识线索。然后，我们通过动态卷积执行对象搜索。我们还开发了一种数据合成方法来训练我们的模型。此外，我们扩展了CADC，并提出了一个改进的模型CADC++，它采用了更直观的方式来执行两种类型的动态卷积，即首先应用基于通用核的动态卷积来捕获粗略的共同线索，并使用输出特征作为先验来学习自适应核，以挖掘细节。我们还提出了一种递归引导策略，以探索两种核和图像特征之间的深层交互作用，以促进学习过程。此外，我们为Co-SOD数据集注释了几个具有挑战性的属性，并在现有最先进方法和我们提出的模型之间进行了基于属性的评估和鲁棒性分析。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
