# [DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation](https://ieeexplore.ieee.org/document/10345691/)
## 题目：DaGAN++: 用于生成会说话的头部视频的深度感知生成对抗网络
**作者：Fa-Ting Hong; Li Shen; Dan Xu**  
**源码：https://github.com/harlanhong/CVPR2022-DaGANGitHub**  
****

# 摘要
目前，关于生成会说话的头像视频的主要技术大多依赖于2D信息，包括输入面部图像中的面部外观和运动。然而，密集的3D面部几何信息，如像素级深度，在构建精确的3D面部结构和抑制复杂背景噪声方面起着关键作用。然而，获取面部视频的密集3D注释成本极高。在本文中，首先，我们提出了一种新的自监督方法，用于从面部视频学习密集的3D面部几何（即深度），无需在训练中使用相机参数和3D几何注释。我们进一步提出了一种策略，通过学习像素级不确定性来感知更可靠的刚体运动像素，以便于几何学习。其次，我们设计了一个有效的几何引导的面部关键点估计模块，为生成运动场提供准确的关键点。最后，我们开发了一个3D感知的跨模态（即外观和深度）注意力机制，可以应用于每个生成层，以粗到细的方式捕获面部几何信息。我们在三个具有挑战性的基准测试（即VoxCeleb1、VoxCeleb2和HDTF）上进行了广泛的实验。结果表明，我们提出的框架可以生成高度逼真的再现会说话的视频，在这些基准测试上建立了新的最先进性能。
# 关键词
- 会说话的头像生成
- 自监督面部深度估计
- 几何引导的视频生成
# I. 引言
会说话的头像视频生成，其目标是通过给定的一张静态源图像和一段动态驱动视频，生成一个逼真的会说话的头像视频，近年来受到了学术界的迅速关注。一个优选的会说话的头像生成模型应该能够使静态源图像准确地模仿目标驱动视频中呈现的丰富面部表情和复杂的头部运动。这自然触发了许多实际应用，例如基于AI的人类对话、数字人类广播和电影中的虚拟主播。

现有的会说话的头像生成方法大致可以分为两类：基于模型的和非基于模型的方法。基于模型的方法[1]、[2]、[3]、[4]、[5]、[6]通常利用预训练的面部模型（例如3DMM[7]或地标检测器[8]）来获取人脸或包含面部形状和表情信息的地标的解耦参数。通过这种方式，它们可以使用这些解耦的参数或地标获得与外观无关的运动信息。然而，这类方法不可避免地会因为预训练模型的不准确而遭受误差累积的问题。当面部3D模型的训练数据分布与会说话的头像模型的数据分布不一致时，这个问题就更加严重了。因此，非基于模型的方法[9]、[10]、[11]不依赖于预训练的第三方模型，并将它们的方法扩展到任意对象，甚至是卡通对象。例如，FOMM[9]提出了一种使用自监督检测到的关键点来估计两个面部之间的运动流的一阶近似方法。基于一阶方法，Face-vid2vid[10]将运动估计扩展到3D特征空间，以可控地控制头部运动。尽管取得了有希望的性能，但这些现有的工作主要关注于学习有效的面部外观和运动表示以执行会说话的头像生成。然而，3D密集面部几何信息是面部生成的关键线索，因为它本质上捕获了丰富的3D面部结构和细节。

直观上，整合面部几何信息可以为会说话的头像视频生成提供明显的好处。首先，考虑到视频捕获了3D物理环境中的头部运动，3D几何信息可以大大有助于准确重建3D面部结构，因此保持逼真的3D面部结构对于生成高保真的面部视频至关重要。其次，密集的几何信息可以方便模型在生成过程中区分嘈杂的背景信息，特别是在存在复杂背景的情况下。最后，密集的几何信息在识别微妙的、与表情相关的面部运动方面特别有用。然而，使用3D密集几何信息来增强生成的一个重大挑战是，这项特定任务的3D几何注释通常是不可用的，而对视频数据进行3D标记的成本极高。

基于上述动机，我们提出了一种使用几何变形和光度一致性以自监督方式学习像素级深度图的方法。我们的提议允许通过训练视频中连续的面部帧自动重建密集的3D面部几何，无需昂贵的3D几何注释。更具体地说，我们开发了一个新的面部深度估计网络来产生准确的面部深度图（见图1）。在会说话的头像生成中，移动的面部包含刚体和非刚体运动。非刚体运动通常与表情相关的运动有关，而大多数面部像素与刚体运动相关。为了更准确地学习刚体运动，我们引入了一种方案，通过学习不确定性图来发现指示更显著和重要刚体运动的可靠刚体运动像素，这些像素对于面部深度估计更为重要。然后，不确定性图在光度一致性损失中受到约束，以便于几何学习。基于学习到的密集面部深度图，我们进一步设计了两种有效的策略来利用面部几何信息增强会说话的面部合成。第一种策略是几何引导的面部关键点检测。由于面部关键点被用来计算面部运动以进行姿态转移，因此准确估计关键点至关重要。由于面部深度图明确表示了3D面部结构信息，我们将深度图的几何表示和面部图像的外观表示结合起来，以预测更准确和结构一致的面部关键点。第二种策略是设计的跨模态注意力机制，旨在通过抑制背景中的噪声运动并增强与表情相关的微观运动来提升面部运动场的学习。所提出的跨模态注意力是一种几何引导的注意力，它在运动场上施加密集的3D几何约束。跨模态注意力模块也应用于生成过程的每个阶段，以实现粗到细的几何引导生成。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/123347ebeca24b63896c8ee219cceea0.png" width="70%" /> </div>


本文显著扩展了我们之前的CVPR版本，即DaGAN[12]。具体来说，我们提出了一个新的增强面部深度估计网络，设计了一个更健壮的几何增强的多层生成过程，进一步阐述了相关工作，提供了不同组件的额外技术细节，并大幅扩展了实验和分析，以验证其在几个会说话的头像数据集上的有效性，包括一个额外的新的高分辨率会说话的头像数据集HDTF[13]。我们将这个新的性能增强框架命名为DaGAN++。与DaGAN相比，改进的面部深度网络使用不确定性作为几何学习的指导，可以生成更准确的面部深度图，这有助于捕获和再现面部表情的微小变化，从而产生更具表现力和逼真的会说话的头像动画。通过替换DaGAN中的普通生成模块，具有粗到细几何引导的几何增强多层生成也可以推进面部生成细节。基于所有新设计，DaGAN++在DaGAN上取得了明显的改进，并在不同的挑战性基准测试上建立了新的最先进结果。

总之，本文的贡献可以概括为三个方面：
- 据我们所知，我们的工作是第一个引入自监督几何学习方法，用于从面部视频中恢复密集的3D面部几何信息（即面部深度图），用于会说话的头像视频生成，无需相机参数或昂贵的3D注释。我们展示了学习到的面部深度图可以有效地增强面部视频生成。
- 我们提出了一个新颖的会说话的头像生成框架DaGAN++，该框架通过两种设计策略将学习到的密集面部深度整合到生成网络中：几何引导的面部关键点估计以捕获人脸的准确运动，以及从面部几何引导的信息传递中受益的几何增强的多层生成，以粗到细的方式进行。
- 我们在三个具有挑战性的数据集上进行了广泛的实验，证明了所提出的模型通过整合更精确的几何信息，并在所有数据集上与最先进的技术相比展现出优越的生成性能，显著超越了以前的方法。

我们按照以下方式组织本文的其余部分。第二节介绍了密切相关文献的综述。第三节介绍了DaGAN++框架用于会说话的头像视频生成的全面说明。第四节展示了实验结果及其解释。本文的结论在第五节中作出。

# III. 方法论

在本文中，我们使用源图像和驱动视频作为输入来生成合成视频，该视频在保持提供源图像的身份信息的同时，模仿驱动视频中个体的面部运动。我们首先设计了一个基于自监督学习框架的鲁棒面部深度估计技术，仅使用训练面部视频，而不依赖任何昂贵的3D几何注释。通过整合准确的几何信息，我们提出的DaGAN++能够有效地将面部几何信息整合到生成过程中，产生更高质量的视频，更好地捕捉面部结构和与表情相关的微观运动。

## A. 概述

如图2所示，给定输入源图像 $I^S$ 和包含一系列帧 $\{I^D_1, I^D_2, ..., I^D_T\}$ 的驱动视频 $V^D$ ，所提出的深度生成框架DaGAN++旨在产生一系列合成图像 $\{I^1_{rst}, I^2_{rst}, ..., I^T_{rst}\}$ 。我们提出的DaGAN++主要由三个主要部分组成：(i) 密集面部深度估计网络 $F_d$ 。将面部视频中的两个连续帧视为两个不同视图，我们通过采用自监督学习方法来估计深度。然后，我们的DaGAN++被联合训练，保持 $F_d$ 固定。(ii) 基于几何的面部关键点检测。给定源图像 $I^S$ 和目标图像 $I^t_D$ （在后续表示中省略上标 $t$ 以简化），我们使用学习到的面部深度网络 $F_d$ 为源和目标图像生成深度图 $(D^S和D^D)$ 。接下来，我们将面部几何（深度图）和外观信息（RGB图像）结合起来，以检测面部关键点（即 $\{x^S_k\}^K_{k=1}和\{x^D_k\}^K_{k=1}$ ）；(iii) 然后我们使用检测到的关键点通过泰勒近似方法[9]计算两个面部图像之间的运动场。然后，几何增强的多层生成过程采用估计的运动场、源图像的深度图 $D^S$ 以及通过CNN编码器提取的源图像 $I^S$ 的N个编码特征图 $\{F^e_i\}^N_{i=1}$ ，并输出增强的特征图以生成最终输出图像。在每一层中，为了鼓励模型保持面部结构细节，我们还使用源深度图 $D^S$ 和通过运动流 $T^S_{\leftarrow D}$ 变形的源特征图 $F^w_i$ 学习密集的跨模态注意力。通过这种方式，我们可以将几何信息整合到生成过程中，产生高质量的结构保持输出。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/795e8e944d8e4ca8a5c2558942bae6bc.png" width="70%" /> </div>

## B. 密集面部深度学习

本节中，我们介绍了我们提出的面部深度估计网络，该网络能够自动恢复人脸的深度信息。虽然SfM-Learner[59]先前提出了一种无监督方法，用于从户外场景的视频中学习场景深度，但在数据分布显著不同于户外场景，并且相机通常是静态的视频中如何有效地学习面部深度仍然不清楚。此外，SfM-Learner在学习中仍然需要提供的相机内参。在我们的会说话的头像生成任务中，面部视频可能直接来自互联网，因此相机内参不可用。因此，我们开发了一种基于普通面部视频的方法来学习面部深度，用于会说话的头像生成，无需任何相机参数和3D注释。

1) 自监督面部深度估计：如图3所示，我们的面部深度学习网络有两个主要模块，即面部深度子网络 $F_d$ 和姿态子网络 $F_p$ 。面部深度网络使用可用的训练面部视频进行优化。具体来说，我们首先提取两个连续的视频帧 $I^i$ 和 $I^{i+1}$ ，并将前者视为目标视图，后者视为源视图。我们的面部深度学习网络旨在预测几个几何元素，包括目标图像 $I^i$ 的深度图 $D^i$ ，每对输入图像的相机内参矩阵 $K^i$ ，两个图像之间的相对相机姿态 $R^i$ 和 $t^i$ ，以及指示每个像素位置发生运动的概率的不确定性图 $U^i$ 。在我们的方法中， $K^i$ 是针对每对图像学习，因此输入只需要视频帧。为了简化，我们在后续表示中将下标“Ii → Ii+1”替换为“i”。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/5296d1a1376044ce91b910a71892e8e5.png" width="70%" /> </div>


深度图 $D^i$ 可以通过面部深度网络 $F_d(\cdot)$ 生成。运动矩阵（包括旋转 $R^i$ 和平移 $t^i$ ）、相机内参矩阵 $K^i$ 和不确定性图 $U^i$ 分别由姿态网络 $F_p(\cdot)$ 中的三个不同的头预测得出：

$$
D^i = F_d(I^i), 
$$

$$
[R^i, t^i], K^i, U^i = F_p(I^i || I^{i+1}),
$$

其中符号||表示沿通道维度的连接操作。因此，利用上述估计的几何元素，我们执行光度投影以通过变形目标视图来获得重建的源视图，如下所示：

$$
q^{\mu} \sim K^i[R^i | t^i]D^i(p^j)K^{-1}, 
$$

$$
\hat{I}^i = B^I(I^{i+1}, \{q^{\mu}\}^M_{\mu=1}),
$$

其中 $q^{\mu}$ 和 $p^j$ 分别代表源图像 $I^{i+1}$ 中的变形像素和目标图像 $I^i$ 中的原始像素；M表示图像中的像素数量； $B^I(\cdot)$ 是可微分的双线性插值函数； $\hat{I}^i$ 是重建图像。因此，我们可以建立 $\hat{I}^i$ 和 $I^i$ 之间的光度一致性误差 $L^P_e(\cdot, \cdot)$ ，这使得在没有额外3D注释的情况下训练面部深度网络成为可能。

2) 不确定性引导的面部深度估计：在会说话的头像视频生成中，目标视频通常由静止相机拍摄，因此视频中的背景保持不动。图像中面部的移动像素也可以被划分为刚体运动和非刚体运动像素。通常，刚体运动像素占据整个面部，并呈现出更可靠和有效的运动用于几何学习。为了更好地学习两个视图之间的刚体运动，我们提出了一种机制，通过估计像素级不确定性图来指示每个像素位置发生可靠运动的概率。然后，我们在最终目标函数中使用不确定性图来规范具有高运动发生概率的像素。更具体地说，按照[55]，我们首先构建如下的光度一致性误差 $L^P_e$ ：

$$
L^P_e(I^i, \hat{I}^i) = \alpha(1 - SSIM(I^i, \hat{I}^i)) + (1 - \alpha)||I^i - \hat{I}^i||,
$$

其中 $SSIM(\cdot, \cdot)$ [62]衡量两个图像的外观相似性。在这项工作中，将 $\alpha$ 设置为0.8以获得最佳结果。为了生成平滑的深度图，我们还通过以下方式在深度学习中采用平滑约束[60]：

$$
L^D = \sum_{x,y} |\nabla_x D(x,y)|e^{-|\nabla_x I(x,y)|} + |\nabla_y D(x,y)|e^{-|\nabla_y I(x,y)|}.
$$

如前所述，为了利用占主导地位且更可靠的刚体像素运动进行几何学习，我们考虑估计的像素级不确定性图 $U^i$ 作为引导，以过滤出不确定的非刚体像素区域：

$$
L^{\text{depth}} = \lambda_1 M^a U^i L^P_e(I^i, \hat{I}^i) + \lambda_2 L^D + \lambda_3 \log U^i,
$$

其中超参数 $\lambda_1$ 、 $\lambda_2$ 和 $\lambda_3$ 控制每个优化目标的平衡。由于我们的面部视频中相机是静止的，背景始终保持静止，只有面部显示运动。因此，引入自动掩码 $M^a$ [55]以忽略图像中的静止像素，并使我们的几何学习模型专注于运动区域。最后一项 $\log U^i$ 是不确定性图的正则化。在这项工作中，我们采用一系列上采样块堆叠来上采样姿态网络的输出，然后使用sigmoid函数产生软不确定性图（见图5(f)）：

$$
U^i = \text{Sigmoid}(F^U(F^p(I^i, I^{i+1}))),
$$

其中 $F^U(\cdot)$ 是不确定性头， $F^p(\cdot, \cdot)$ 代表图3中的姿态网络。"Sigmoid(\cdot)"表示sigmoid函数。值得注意的是，会说话的头像生成任务专注于前景，并旨在生成具有良好驱动运动对齐的高质量和逼真的面部视频。因此，会说话的头像数据集的背景通常在会说话的头像生成的基本设置下被视为静止。重要的是，应该承认面部深度网络的性能可能会受到训练期间多帧中动态背景对象或噪声的影响。如果训练期间背景和前景都变得动态，那么估计可靠的几何元素（即相对相机姿态 $R^i$ 和平移 $t^i$ ）就会变得复杂，因为这种情况将与[59]中概述的SfM的假设相冲突，可能导致不准确的结果。然而，由于在这项特定任务中，训练数据的背景通常保持稳定，因此这个问题在我们的工作中得到了缓解。

### 3) 基于几何引导关键点的运动建模：

面部关键点对于从目标驱动视频中估计运动场非常重要。为了生成更准确的运动场，我们提出利用学习到的面部深度图提供几何信息，以便于学习关键点。一旦从面部深度网络获得面部深度图，关键点检测器就会接受一个组合输入，包括面部RGB图像及其相关联的深度图，并预测一组稀疏关键点（ $x^S_k, x^D_k \in R^{1 \times 2}$ ）及其对应的雅可比度量（ $J^S_k, J^D_k \in R^{2 \times 2}$ ）。

$$
\{x^\tau_k, J^\tau_k\}^K_{k=1} = F_{kp}(I^\tau || D^\tau), \tau \in \{S, D\},
$$

其中下标 $\tau$ 表示检测到的面部关键点是来自源图像还是驱动图像，K代表检测到的关键点总数。基于检测到的面部关键点，我们可以估计两个面部图像之间的运动场。具体来说，我们采用泰勒近似方法来计算两个面部图像之间的运动流。然后，我们使用两幅面部图像的配对关键点来估计一组稀疏运动场 $\{T^S_{\leftarrow D,k}(z)\}^K_{k=1}$ ，如下所示：

$$
T^S_{\leftarrow D,k}(z) = x^S_k + J^S_k(J^D_k)^{-1}(z - x^D_k),
$$

其中 $z \in R^2$ 是驱动面部上的任意一点。之后，我们采用FOMM[9]中的密集运动模块，使用输入的稀疏仿射运动场 $\{T^S_{\leftarrow D,k}(z)\}^K_{k=1}$ 来估计密集的2D运动场 $T^S_{\leftarrow D}$ 。

$$
T^S_{\leftarrow D} = F^M\left(\{T^S_{\leftarrow D,k}(z)\}^K_{k=1}, I^S\right),
$$

其中 $F^M(\cdot, \cdot)$ 是密集运动模块，估计的遮挡图M用于遮盖由于面部的较大旋转导致变化和相对较大旋转的区域。

## C. 几何增强的多层生成

为了有效地整合获得的面部深度图以提高生成质量，我们引入了一个跨模态（即深度和图像）注意力机制，以更好地保留面部结构并生成与表情相关的微观面部运动。深度信息提供了密集的3D面部几何信息，在维护面部结构和识别关键运动期间，这被证明是根本上的优势。在这项工作中，我们通过在多层上实现它，扩展了DaGAN的跨模态注意力机制[12]，使模型能够在不同的外观特征层次上捕获面部几何信息。这最终有助于提高生成性能。如图4所示，每个跨模态几何引导注意力模块包括两个阶段，即特征变形步骤和跨模态注意力计算步骤。我们在以下部分详细介绍这两个步骤。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/323be6d1b7bf478792fb00537b849aac.png" width="70%" /> </div>


1) 特征变形：在生成过程中，我们考虑使用编码器-解码器架构来生成图像。首先，我们使用编码器产生一个L层编码的外观特征图 $\{F^e_i\}^N_{i=1}$ 。同时，我们使用深度编码器 $E^d$ （见图5(b)）预测深度特征图 $F^{depth}$ ，输入源深度图 $D^s$ 。第i个跨模态注意力模块采用外观特征 $F^e_i$ 、深度特征 $F^{depth}$ 、遮挡图M和运动场 $T^S_{\leftarrow D}$ 作为输入，产生几何增强的特征图 $F^e_{\text{en}}$ 。如图4(a)所示，我们首先使用运动场 $T^S_{\leftarrow D}$ 对源视图特征图 $F^e_i$ 进行变形，以对齐目标视图。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/3fdb3ec641f647f78c24ec27c32dfe9a.png" width="70%" /> </div>


这种大致的对齐随后通过乘以遮挡图进行，该遮挡图指示由于大运动可能引起的歧义，如下所示：

$$
F^w_i = M \times W^p(F^e_i, T^S_{\leftarrow D}),
$$

其中×表示逐元素乘法操作， $W^p$ 表示变形函数。通过这种方式，变形后的特征 $F^w_i$ 可以保留源图像的身份，同时保留源和目标面部之间的头部运动信息。

2) 跨注意力几何增强：我们通过在变形后的特征图 $F^w_i$ 和深度特征图 $F^{depth}$ 之间执行跨模态注意力，将面部几何信息整合到生成过程中。如图4所示，我们在 $F^{depth}$ 和变形的源图像特征 $F^w_i$ 上执行线性投影，得到 $F^q_i$ 、 $F^k_i$ 和 $F^v_i$ ，使用三个不同的1×1卷积层，分别带有核 $W^q_i$ 、 $W^k_i$ 和 $W^v_i$ 。这些图， $F^q_i$ 、 $F^k_i$ 和 $F^v_i$ ，分别对应于跨注意力机制中的查询(query)、键(key)和值(value)。我们使用外观特征图 $F^w_i$ 生成查询 $F^q_i$ ，其形状与输出相同。因此，可以通过跨注意力从几何特征中查询与几何相关的信息。然后，通过残差连接将与几何相关的信息通过加法操作添加到外观特征图中。这样，深度特征就可以为面部生成提供密集的指导。具体来说，生成的第i层几何增强特征 $F^e_{\text{en},i}$ 如下所示：

$$
F^e_{\text{en},i} = \text{Softmax}(F^q_i \otimes (F^k_i)^T) \otimes F^v_i + F^w_i,
$$

其中 $\text{Softmax}(\cdot)$ 是softmax操作， $\otimes$ 表示矩阵乘法操作。由于3D几何引导，我们的模型可以更好地感知面部结构和驱动面部的微观运动。

3) 多层几何增强：为了使模型在整个生成过程中对面部几何信息保持感知，我们还在生成过程的每一层应用了上述跨模态注意力机制。如图4(b)所示，最终特征图 $F^N_d$ 通过几何增强特征 $F^N_{\text{en}}$ 和 $F^{N-1}_ d$ 的连接生成，在上采样块之后。在第i层（i > 1），我们将解码特征图 $F^i_d$ 输入到上采样块，上采样结果与几何增强特征 $F^{i+1}_ {\text{en}}$ 连接，通过卷积层产生下一层解码特征 $F^{i+1}_ d$ 。最后，我们将 $F^N_d$ 输入到卷积层，然后使用Sigmoid单元生成最终的面部图像 $I_{\text{rst}}$ 。通过这种方式，我们的模型可以在每一层利用几何信息实现面部结构，以产生逼真的人脸。

## D. 优化

根据之前的研究[9]、[10]，我们通过最小化以下损失来学习我们的DaGAN++：

$$
L = \lambda_P L_P(I_{GT}, I_{rst}) + \lambda_E L_E(\{I_S||D_S\}) + \lambda_D(L_D(\{x_{S,k}\}^K_{k=1}) + L_D(\{x_{D,k}\}^K_{k=1})),
$$

其中：

- **感知损失 $L_P$ **：我们考虑使用预训练的VGG-19[63]模型，在多分辨率下最小化真实面部图像 $I_{GT}$（训练阶段的驱动帧 $I_D$ ）和生成面部图像 $I_{rst}$ 之间的外观特征差异。

$$
L_P = \sum_{l,i} |V^l_i(I_{GT}) - V^l_i(I_{rst})|,
$$
  
  其中 $V^l_i$ 是VGG-19模型的第i层特征，l表示输入被下采样了l次。

- **等变性损失 $L_E$ **：与FOMM[9]类似，我们使用等变性损失来保证识别出的关键点的一致性。

$$
L_E = |F_{kp}(T_{\text{random}}(I_\tau||D_\tau)) - T_{\text{random}}(F_{kp}(I_\tau||D_\tau))|,
$$
  
  其中 $T_{\text{random}}$ 是一个随机的非线性变换。在本工作中，我们应用了类似于FOMM[9]的随机TPS变换。通过使用这个损失，关键点检测器可以对任何输入产生稳定且一致的关键点。

- **关键点距离损失 $L_D$  **：当两个关键点之间的距离低于一个预定阈值时，我们对模型进行惩罚，以防止关键点聚集在一个受限的邻域内。对于面部图像 $I_\tau$ 的任意一对关键点 $x_{\tau,i}$ 和 $x_{\tau,j}$ ，我们建立了以下正则化损失：

$$
  L_D = \sum_{i=1}^K \sum_{j=1, j \neq i}^K (1 - \text{sign}(||x_{\tau,i} - x_{\tau,j}||_1 - \beta)),
  $$
  
  其中 sign函数表示符号函数，β代表距离阈值。在本工作中，我们将β设置为0.2，这在我们的实验中表现出了令人满意的性能。超参数 $\lambda_P$ 、 $\lambda_E$ 和 $\lambda_D$ 有助于平衡这些损失的学习。

## E. 网络架构细节

我们介绍了DaGAN++中不同网络组件的更多细节。我们在图5中展示了每个组件的实现，并在下面详细说明：

- **特征编码器 $E_I$ **：如图5(c)所示，我们的特征编码器 $E_I$ 由三个下采样块组成，产生四种不同尺度的特征图。因此，我们可以获取包含详细面部纹理的低级和包含语义信息的高级面部特征图。

- **深度编码器 $E_d$ **：我们在图5(b)中展示了面部深度编码器 $E_d$ 的结构。其结构与 $E_I$ 相同，确保从两种模态学习的特征具有等价的表示能力。

- **特征解码器**：在图5(e)中，我们在生成网络的中间插入了ResBlock来提升网络的容量。如第三节C中所述，我们应用跨模态注意力作为跳跃连接，将几何信息嵌入到生成过程中的不同层。

- **不确定性头**：在图5(f)中，我们使用一系列UpBlock来上采样姿态网络的输出，其尺寸为[1024 × 2 × 2]。我们在不确定性头的末端使用sigmoid来预测不确定性图，其值在[0,1]范围内。

# IV. 实验

为了验证我们提出方法的有效性，我们在多个公开可用的会说话头像生成基准数据集上进行了全面的实验。在本节中，我们将首先介绍实验设置，然后分析实验结果。

## A. 数据集描述

在这项工作中，我们在三个不同的面部再现数据集（即VoxCeleb1[64]、VoxCeleb2[65]和HDTF[13]数据集）上进行了实验。

*VoxCeleb1 数据集：* VoxCeleb1 数据集是一个最初用于说话人识别和识别任务的大型收集的音视频数据集。该数据集包括不同年龄、性别和种族的多样化说话人，涵盖各种口音和语言。它包括开发集和测试集。开发集包含1,211个身份，而测试集包括40个身份。重要的是，两套之间没有说话人重叠，确保了模型在未见过的数据上的正确评估。视频没有针对面部检测或对齐进行预处理，要求模型处理原始数据。这使得该数据集对会说话头像生成具有挑战性。

*VoxCeleb2 数据集：* VoxCeleb2[65] 在原始的 VoxCeleb1 数据集的基础上构建。它旨在用于说话人识别、面部识别和会说话头像生成等任务。该数据集拥有来自6,000多名名人的100多万个视频片段，收集自YouTube，提供了跨越年龄、性别、种族、口音和语言的多样化说话人。两套之间没有重叠，确保了模型在未见过的数据上的正确评估。VoxCeleb2 视频带有说话人身份的注释，并且在背景噪声、照明和头部姿势方面不受限制。这提供了一个更具挑战性的数据集，更代表真实世界场景，适用于各种计算机视觉和语音处理任务。

*HDTF 数据集：* HDTF 数据集[13]包含大约362个不同的视频，总共15.8小时的内容。原始视频的分辨率为720P或1080P。最初使用地标检测器来隔离面部区域，裁剪窗口在整个视频中保持一致。随后，每个裁剪的视频被调整大小为512×512的尺寸，以保持视觉质量。

## B. 指标描述

在这项研究中，我们采用多种措施来评估生成图像的质量。我们使用结构相似性（SSIM）、峰值信噪比（PSNR）和LPIPS来评估生成图像与真实图像之间的视觉相似性。此外，我们还包括三个额外的指标，即L1、平均关键点距离（AKD）和平均欧几里得距离（AED），如[66]中建议的，用于基于关键点的方法评估。按照DaGAN的方式，我们还引入了CSIM、AUCON和PRMSE来评估跨身份再现的质量。

## C. 实施细节

在面部深度网络学习过程中，姿态网络和深度网络的架构与DaGAN[12]中的一致。然而，我们在姿态网络的末端添加了一个不确定性预测头，用于估计不确定性图。不确定性头的实现如图5(f)所示。在这项工作中，我们的关键点检测器与DaGAN[12]中的类似，而我们使用了与[60]中相同的结构来实现面部深度网络和姿态网络。关于优化损失，我们分配了λP = 10, λE = 10, 和 λD = 10。DaGAN++中的关键点数量设置为15。值得注意的是，我们的DaGAN++包含85.2 M可训练参数，而DaGAN有63.1 M参数。这种增加归因于我们在生成过程的每一层采用的几何增强，自然引入了额外的参数。然而，与具有相似数量可训练参数（85.1 M）的最先进方法如TPSM[48]相比，我们的方法仍然可以获得更优越的性能。在训练阶段，我们最初使用VoxCeleb1视频中连续的帧来训练我们增强的不确定性引导面部深度网络，并在整个深度生成框架的训练过程中保持其固定。在这项工作中，我们使用了公开代码[1]来处理VoxCeleb1数据集的原始数据。

[1] https://github.com/AliaksandrSiarohin/video_preprocessing

## D. 与最先进方法的比较

自我再现：首先，我们进行了自我再现实验，以定量评估我们的方法。自我再现的结果在表I、III和II中报告。我们可以观察到，在所有数据集上，DaGAN++与其他方法相比都取得了最佳性能。与DaGAN相比，我们的DaGAN++在大多数指标上都有显著改进，例如，在HDTF数据集上的SSIM为86.7%对82.3%。这验证了我们对DaGAN改进设计的有效性。与其他无模型方法（FOMM[9]、MRAA[11]和TPSM[48]）相比，我们的DaGAN++模型能够准确捕获头部运动。该方法在VoxCeleb2数据集上的AKD指标上取得了1.675的得分（越低越好），而比较方法中取得的最佳分数为1.703。这一结果证实了我们的几何引导面部关键点估计可以更有效地生成人脸之间的运动场。此外，我们的方法在VoxCeleb1上以82.6%的SSIM得分继续取得最佳成绩。这表明该提议可以恢复更细微的面部细节，如表情和运动。此外，我们的方法在其他指标上也取得了最高分，即LPIPS、L1误差和PSNR，表明我们的方法与竞争方法相比可以生成更逼真的图像。除了定量比较外，我们还在图6中可视化了一些样本。从图6中生成的面部可以看出，我们的方法产生了更准确的结果。此外，我们的方法生成的伪影较少，如图6第一行所示。这主要是因为稳健的面部深度信息有助于更好地学习运动表示，抑制背景噪声，并保留3D面部结构。总体而言，DaGAN++在不同数据集上一致的优越性能突显了其与竞争方法相比在处理同一身份再现任务中的有效性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/a4963ad26d6c4b5c8420f323d3e95128.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/7281dc512ce1487692acd9e4d0ee32a5.png" width="70%" /> </div>


跨身份再现：我们还在源图像和驱动图像来自不同个体的情况下进行了实验，以研究跨身份运动转移。我们的方法与其他方法进行了定性比较，并在图7和图8中显示了可视化结果。对于图7和图8中的面部，我们的方法相比其他方法生成了具有更精细细节的面部图像，如图7中三行的眼区所示。这证实了使用深度图允许模型识别微妙的面部表情运动。我们的方法还可以为以前未见过的目标生成视觉上吸引人的结果。值得注意的是，TPSM由于没有在面部动画中结合相对运动转移[9]，因此产生了较差的结果。与DaGAN相比，增强的DaGAN++在保留源图像原始背景信息的同时明显减少了伪影，如图8第二行所示。此外，我们还在表IV中报告了定量结果，我们的方法在所有指标上都优于所有比较方法。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f4c868ad020c459989b0c21926baad6e.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/a600d44aa1854baea83c1cb1fd531b95.png" width="70%" /> </div>

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e7b90021e1024fe5b6dfacc275a86ee2.png" width="70%" /> </div>

## E. 消融研究

我们还进行了消融研究，以验证我们工作的每个组件。在本节中，我们首先评估我们生成的深度图的质量，然后研究学习到的深度图对会说话头像生成的影响。我们在图9、10、11、12、13和表V、VI中展示了结果。在表V中，“w/ GMG”指的是使用多层跨模态注意力机制将几何信息整合到生成过程中的使用；“w/ FDN++”表示应用改进的面部深度网络来预测面部深度图，用于几何引导的稀疏关键点估计。在此背景下，我们的基线对应于未使用面部深度图和跨模态注意力模块训练的基本模型。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/7ff578ad761f47089a2ed6433855182c.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/3728470f87eb47f98ca01dc4fd47b6f2.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f1b67baf073b497cbf1c543f887c79f2.png" width="70%" /> </div>


1) 自监督面部深度学习：深度结果：在这项研究中，我们展示了与DaGAN相比学习更准确面部深度估计网络的改进设计。由于缺乏真实数据，我们采用了面部深度估计的自监督方法。由于缺乏真实深度数据，很难定量评估深度估计性能，因此深度图的质量只能通过可视化进行定性评估。我们在图9中展示了DaGAN和DaGAN++的估计深度图。显然，与DaGAN相比，DaGAN++在深度图中产生了更多的面部结构细节。此外，DaGAN++的深度图成功地消除了背景，如图9第五列所示。这些发现表明，我们的自监督面部深度学习方法可以有效地恢复人脸的密集3D几何形状，这对于后续的会说话头像生成任务是有利的。

不确定性图学习：在我们的自监督深度学习方法中，我们提出学习一个不确定性图，以指示面部图像中可靠和占主导地位的刚体运动像素区域的概率。我们在图10中报告了几个样本。回顾深度学习目标的形式（见(7)），它将尝试将不确定性U设置为规范化光度一致性误差。如图10所示，我们可以观察到，发生更可靠刚体运动的像素将被学习为赋予较小的不确定性，然后为这些像素的光度一致性误差产生较大的权重。它通过学习不确定性图来表示具有不同重要分数的刚体和非刚体运动像素，验证了我们的动机。

改进深度图的有效性：如前所述，准确的面部几何信息可以极大地促进面部结构的保持。在本文中，我们提出了一种改进的面部深度学习方法，与DaGAN相比，其性能有了显著提高，如图3所示。为了定量验证我们改进的面部深度网络的有效性，我们将原始面部深度网络替换为增强型网络，在DaGAN中。如表V所示，带有我们增强面部深度网络的DaGAN组件（即“FDN”和“CAM”）即“FDN++”和“CAM++”，与原始DaGAN相比取得了显著改进，证实了增强面部深度估计网络的有效性。这些结果表明，准确的面部几何信息确实可以提高会说话头像生成的质量。

深度引导关键点估计的有效性：如文中所讨论的，面部深度对于关键点估计至关重要。我们首先定量研究深度图对关键点检测的影响，并在表V中展示了相应的结果。从表V中可以看出，与基线模型相比，深度引导的关键点在我们模型的所有评估指标上稳定地贡献了性能，证实了深度图对于面部关键点估计的重要性。从图11中，“Baseline w/ FDN++”也显示出比“Baseline”更准确的面部表情生成，这表明几何引导的面部关键点可以有效帮助生成更精确的人脸运动场。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/a085ea7e60664f12aa3649b4bc11b8cd.png" width="70%" /> </div>


2) 几何增强的多层生成：几何增强多层模块的有效性：通过比较“基线”和“Baseline w/ CAM”之间的性能，我们可以验证跨模态注意力机制在会说话头像生成中的有效性。在这项工作中，我们进一步在生成过程的每一层部署了跨模态注意力单元，以便我们可以在不同层次的特征中引入几何信息，以实现粗到细的几何引导生成。从表V和图11中，几何增强的多层生成（GMG）可以明显提高输出人脸的质量。此外，得益于面部几何信息，我们的方法可以保留与表情相关的微观运动。与“基线”相比，“Baseline w/ GMG”可以产生更逼真的表情（例如，在图11中的眼区）。从表V和图11中，我们可以验证我们提出的几何增强多层生成过程可以有效将面部几何信息嵌入到生成过程中，以捕获人脸的关键运动（例如，表情），从而实现更高质量的面部生成。

深度图对生成特征的影响：我们还在生成器中可视化了中间生成特征图，以验证我们的几何增强的影响。我们展示了应用几何增强前后的生成特征。如第三节C中所述，几何增强使中间特征图能够保留全局面部结构并识别与表情相关的微观面部运动。通过利用密集的深度信息，我们可以在整个特征图上提供密集的（像素级）几何增强。与仅突出特定局部区域的区域聚焦注意力相比，这改善了全局面部结构。因此，具有几何增强的特征图在整个图像中提供了更多的纹理细节，如图12所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/fd8547b32bdf4e2e9ea6783a3f1598fa.png" width="70%" /> </div>


跨域结果：除了真实的人脸，我们还在图13中展示了一些跨域样本的生成结果，以验证我们模型的跨域生成能力。如图13所示，我们的方法可以有效地修改油画或卡通中人脸的表情。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/387dd9aa0c604b2baab9e9a9ba602466.png" width="70%" /> </div>


关键点数量的选择：从表VI中，我们将关键点数量设置为15，因为它可以实现最佳性能。使用少于15个关键点会导致面部运动表示更稀疏，性能下降，而将关键点数量增加到15个以上不会带来显著的性能提升，却会导致模型尺寸增大。一个可能的原因是，随着关键点数量的增加，它们的集体约束可能导致特定区域的运动更加不稳定。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/bd6cfc6bd7a6440e9b37b726fd4c3a70.png" width="70%" /> </div>


时间一致性评估：此外，我们还采用了时间一致性度量来衡量和评估生成视频的帧间一致性（即TCM[68]）。具体来说，TCM定义为：

$$
TCM = \frac{1}{T} \sum_{t=1}^{T} \exp \left(-\left\| I_t^{GT} - Wp\left(I_{t-1}^{GT}, T_{t-1 \leftarrow t}\right) \right\| \right) \left\| I_t^{rst} - Wp\left(I_{t-1}^{rst}, T_{t-1 \leftarrow t}\right) \right\| - 1
$$


其中 $I_t^{GT}$ 和 $I_t^{rst}$ 分别是真实视频和生成视频的第 t 帧。 $Wp(\cdot, \cdot)$ 表示变形函数， $T_{t-1 \leftarrow t}$ 是 $I_t^{GT}$ 和 $I_{t-1}^{GT}$ 之间的运动流。我们使用 OpenCV [69] 中 Gunnar Farneback 的算法来计算运动 $T_{t-1 \leftarrow t}$ 。通过这个公式，生成视频被鼓励与真实视频的变化保持时间上的一致性。结果展示在表 IV 中。我们的方法获得了最佳的 TCM 值，因为连续帧几乎有相同的由我们的人脸深度网络估计的深度图，且深度图足够稳定，能够在生成过程中约束面部结构，从而实现更好的时间一致性。

# V. 结论
在本文中，我们提出了一个名为DaGAN++的深度框架，用于生成会说话的头像，它通过大幅改进面部深度估计网络，并在生成过程中应用多层跨模态注意力机制，扩展了深度感知生成对抗网络（DaGAN）。首先，我们引入了一个不确定性头，以预测更可靠和占主导地位的刚体运动像素区域，这些可以用于更有效地学习几何信息。然后，我们提出了一个跨模态（外观和深度）注意力机制，并将其插入到生成过程的每一层中，以实现几何引导的粗到细生成。我们在三个不同的公共基准上评估了我们的DaGAN++，并且我们的提议在所有具有挑战性的数据集上与最先进的对手相比展示了更优越的性能。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
