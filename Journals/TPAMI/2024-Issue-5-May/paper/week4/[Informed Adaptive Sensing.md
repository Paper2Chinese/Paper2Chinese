
# [Informed Adaptive Sensing](https://ieeexplore.ieee.org/document/10349951/)
## 题目：信息驱动的自适应感知
**作者：Amr Morssy , Marcus R. Frean , and Paul D. Teal , Senior Member, IEEE**  
****

# 摘要
对于许多反问题，求解所基于的数据是按顺序获取的。我们提出了一种求解此类反问题的方法，其中传感器可以在飞行中被定向（或以其他方式重新配置）以获取特定测量值。一个示例问题是磁共振图像重建。我们使用由生成模型提供的经验条件分布推导出的互信息估计来指导我们在已获取的测量值的基础上进行测量获取。条件生成的数据是一组样本，代表满足已获取测量值的可能解。我们对玩具数据集和真实世界数据集进行了实验。我们专注于图像数据，但我们证明了该方法适用于更广泛的类问题。我们还展示了如何利用深度神经网络等学习模型来实现对未见数据的泛化。我们的知情自适应感知方法优于随机采样、基于方差的采样、基于稀疏性的采样和压缩感知。

# 关键词
- 实验设计
- 反问题
- 迭代求解技术
- 机器学习
- 医学成像
- 相位恢复
- 概率算法

# I. 引言
反问题涉及从观察到的效果推断潜在原因。这类问题出现在许多科学学科中，研究者从不完整或损坏的观察数据中推断生成观察数据的原因。观察值在某些情况下也称为测量值。示例包括医学图像重建、超声波、雷达观察中的图像 [1]，以及地球物理应用中的图像 [2]。在许多这些应用中，观察值的获取可能成本高昂，并且可能数量和质量有限。例如，在计算机断层扫描（CT）中，测量涉及对患者的辐射损害，而在磁共振成像（MRI）中，测量则耗时—因此通常希望尽量减少获取的观察值数量 [3]。

反问题求解是根据已知的前向模型 $A$ 从可能被噪声 $\eta$ 污染的观察值 $y$ 中估计数据 $x$ 的任务。这可以表述为：

$$
y = Ax + \eta
$$

通常无法使用最大似然方法来求解 (1) 中的 $x$，因为问题是不适定的 [4]。因此，在 $x$ 上施加稀疏先验以约束解 [5], [6]，但稀疏先验相比学习的先验过于通用。这促使使用深度学习模型作为学习先验来解决反问题。近年来，使用深度学习解决反问题（如压缩感知 [7] 和 MRI 重建 [8]）迅速增加，性能优于基于稀疏性的先验 [1]。深度模型可以通过直接从观察值求解 [6], [7], [9]，或通过后处理初步重建结果 [10]，或作为指导求解过程的先验来解决反问题 [4]。使用模型作为先验允许将其应用于不同的反问题设置而无需重新训练。其他选择是使用递归神经网络模型以端到端训练方式学习迭代优化过程 [5], [11]。

深度学习方法存在两个缺点。首先，在许多应用中，观察值的获取是一个按顺序进行的过程，可以从自适应地选择未来的观察值中受益 [12], [13]。需要以可解释的方式优化观察值的获取，这是许多方法所忽视的。其次，在大多数当前方法中，反问题的单一解被提供。理想的反问题求解器应生成满足给定有限观察值的一系列输出，以表示问题的内在不确定性 [14]。通过使用输出分布来表示不确定性，我们可以希望避免深度学习模型输出质量高但与观察值相关性存疑的结果 [15]。

我们提出了一种自适应优化方法，用于在给定预期数据分布的情况下指导反问题的观察获取。我们的方法不考虑分布外数据，并且是在测试时应用，而不是任何形式的主动或自适应训练 [16]。我们的方法：
- 能够使用不同的生成模型，唯一的限制是模型必须是隐式和可微分的。此外，改变前向模型 $A$ 不需要重新训练。
- 适用于广泛的反问题，包括涉及非线性前向模型的问题。
- 能够通过一组与观察值 $y$ 一致的可能样本 $x$ 表示解的不确定性。

第 II 节总结了基于互信息的相关工作以及我们的方法的不同之处，第 III 节介绍了开发的方法，第 IV 和 V 节展示了实验结果。第 VI 节陈述了限制，第 VII 节是结论。

# III. 方法

## A. 数据重建的知情传感

香农的信息理论为量化与概率分布相关的事件不确定性提供了一种原则性的方法。离散随机变量X的熵H表征了与观察相关的预期信息增益。两个离散随机变量X和Y之间的互信息（MI）是Y对X熵的减少。由于在观察Y时我们旨在增加对X的确定性，因此最大化互信息的观察将是预期中最有利的。我们的方法基于有关互信息的以下定理。

**定理 1**：假设X的先验分布由数据集的样本xj表示，j = 1, ..., M。对于所讨论的数据实例X，我们有先前的观察y0。假设已知测量的前向模型f(y|x)，并且与y0独立。那么，当对先前的观察y0进行条件化时，一些新观察Y和未知数据实例X之间的互信息可以近似为：

$$
\begin{split}
I(X; Y |y0) = HY X - \sum_{m=1}^{M} w_{0m} f(y|x_m) \log \left(\frac{1}{M} \sum_{j=1}^{M} w_{0j} f(y|x_j) \right) dy
\end{split}
$$

其中HY X是条件分布的熵，权重w0j是f(y0|xj)/ $\sum_{k=1}^{M} f(y0|xk)$ 。

**证明**：连续随机变量X和Y的互信息由下式给出：

$$
I(X; Y ) = H(X) - H(X|Y )
$$

其中H()表示微分熵[27] (8.48)。对于在先前观察y0的条件下的情况，我们可以使用信息链规则[27] (2.60)。写出X, Y的联合分布为f(x, y)，并将Z设为y0，可以展开第二项，得到：

$$
\begin{split}
I(X; Y |y0) = H(X|y0) + \int f(y|x, y0)f(x|y0) \log \left(\frac{f(y|x, y0) f(x|y0)}{f(y|x', y0) f(x'|y0)}\right) dx' dx dy
\end{split}
$$

其中，形式为f(y|x, y0)的项在假设观察Y的分布完全由x单独确定的条件下简化为f(y|x)（“朴素贝叶斯”假设），而形式为f(x|y0)的项可以如下近似。由大量样本形成的实证分布可以作为近似先验f(x)，在这种情况下，条件（后验）密度f(x|y0)简单地是加权狄拉克函数的混合，即：

$$
f(x) \approx \frac{1}{M} \sum_{m} \delta(x = x_m)
$$

$$
f(x|y0) \approx \sum_{m} w_{0m} \delta(x = x_m)
$$

其中第m个样本的归一化权重为：

$$
f(x_m|y0) \approx \frac{f(y0|x_m)}{\sum_{k=1}^{M} f(y0|x_k)}
$$

这只需要前向模型，并且通过应用贝叶斯定理，注意到从(5)中f(xm) = 1/M。

(4)中的第一项是给定先前观察y0的X的熵，我们可以像上面一样进行经验近似：

$$
H(X|y0) = -\int f(x|y0) \log f(x|y0) dx \approx -\sum_{m=1}^{M} w_{0m} \log w_{0m}
$$

将上述识别插入(4)中，将x的积分替换为样本xm的求和并重新排列，条件互信息I(X; Y |y0)变为：

$$
\begin{split}
HY X - \sum_{m=1}^{M} w_{0m} f(y|x_m) \left( \frac{1}{M} \sum_{j=1}^{M} w_{0j} f(y|x_j) \right) dy
\end{split}
$$

第二项的积分计算结果为1，因此前两项被抵消。第三项的积分是给定输入x的前向模型的熵（负熵）。对xm的条件化不一定改变y的熵——例如，x可能合理地确定y的正态分布的均值，但不是其方差。在这种情况下，H(y|xm)只是噪声的熵，表示为HY X，这给出了第三项，因为w0m的总和为1。因此，条件互信息I(X; Y |y0)简化为：

$$
\begin{split}
HY X - \sum_{m=1}^{M} w_{0m} f(y|x_m) \left( \log \left(\frac{1}{M} \sum_{j=1}^{M} w_{0j} f(y|x_j)\right) \right) dy
\end{split}
$$

现在，通过从混合分布中抽取样本，可以进行蒙特卡洛近似，这很直接。首先，通过从具有概率w0m的分类分布中抽取样本xm，然后使用前向模型从f(y|xm)中抽取样本ym。因此，互信息I(X; Y |y0)随后被(2)近似。□

在图1中总结了用于实例确定的互信息查找的伪代码。我们将获得的观察/测量表示为Y，候选测量为y，条件模型由p(x|Y)表示。我们用空集初始化算法的Y，以便p(x|Y)最初将从p(x)产生M个样本 $\hat{X}$ 。我们使用 $\hat{X}$ 为每个候选测量生成k个MC样本p(yj| $\hat{x}$ )。从这个我们计算如果我们观察这个测量后的后验熵的预期估计。最后，我们观察最小化估计的预期条件熵并因此最大化MI的测量。测量被添加到集合Y，这个过程重复s次迭代。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/88e7d16d347e4103a0defff3798e27e9.png" width="70%" /> </div>


对于实现，我们并行生成MC样本，并行计算每个测量的熵。我们的算法在Tesla K20 GPU上运行，每次迭代大约需要6毫秒，用于28×28像素图像。我们的方法可以在获取40到60个像素后（取决于噪声方差），从3通道28×28像素数据集中以信心识别实例，这意味着所有迭代的运行时间为0.3秒。对于更大的数据集，如128×128像素MRI数据集，整个重建（200次测量）大约需要一分钟。

## B. 确定数据标签的知情传感

在标记数据的情况下，我们希望从观察y中最大化对给定x的类别或标签ℓ的信心。我们从之前的表达式开始，只是将X替换为L：

$$
I(L; Y ) = h(L) + \iint f(ℓ, y) \log f(ℓ|y) dℓ dy
$$

其中f(ℓ, y)定义为：

$$
f(ℓ, y) = \int f(y, ℓ|x)f(x) dx
$$

假设观察Y和标签L在给定X的条件下是独立的，并且使用f(ℓ|y) = f(ℓ, y) / f(y)，我们有：

$$
f(ℓ|y) \approx \sum_{m=1}^{M} \frac{f(y|x_m)f(ℓ|x_m)f(x_m)}{\sum_{j=1}^{M} f(y|x_j)f(x_j)}
$$

与之前一样，如果我们还有先前的测量y0，条件化对y0不会改变f(ℓ|xm)或f(y|xm)，但是f(xm)必须被f(xm|y0)替换，我们再次用w0m表示。如果L是分类标签，并且没有噪声，f(ℓ|xm)由指示器Imℓ ∈ {0, 1}给出，仅当ℓ是样本xm的正确标签时为1。

因此我们有：

$$I(L; Y |y0) \approx h(L|y0) + \sum_{ℓ} \sum_{m} f(y|x_m)Imℓw_{0m} \left( \log \frac{\sum_{j} f(y|x_j)Ijℓw_{0j}}{\sum_{k} f(y|x_k)w_{0k}} \right) dy$$

类似于(2)，这也很容易从条件样本中估计。

## C. 关于推导的评论

在知情传感算法的每一步中，我们所做的额外计算只是新测量的可能性。这种方法的一个吸引人的方面是f(x|y0)可以被解释为条件生成模型pθ(x|y)。这意味着我们可以轻松地将该方法扩展到使用在给定数据分布上训练的条件模型。我们在第三节讨论了这一点。尽管我们假设观察到的是实数值像素，但该方法也适用于其他测量类型和领域。例如，在MRI[25]和射电天文学中的频域观测。在第五节C2中，我们将展示该方法应用于复值傅里叶数据作为应用于不同测量领域的示例。

我们提出的方法也可以很容易地扩展，正如我们在第五节C3中展示的，指导解决非线性逆问题的测量获取。我们实验的一个例子是相位恢复，它与频域数据相关，并在许多领域中出现，如电子显微镜、晶体学、天文学和光学成像。相位恢复在天文学中也扮演着重要的角色，可以用来校正光学和大气畸变[28]。
## D. 条件生成

为了知情传感，我们希望产生满足迄今为止获得的测量或观察y0的数据样本x。如果我们假设一个基于记忆训练数据X的模型，那么可以通过首先根据y0给每个x ∈ X称重，然后从归一化分布中替换抽样来产生条件分布。使用我们的条件模型类似于决策树[29]，除了它有大量类别相当于训练实例的数量，并且这个条件模型不能推广到看不见的数据。

我们可以通过训练深度生成模型来克服这一点。

深度生成模型学习假设数据位于一个流形上，该流形可以映射到潜在空间分布p(z)上。分布p(z)通常设置为正态分布，以允许轻松生成新的样本。已经提出了解决逆问题的条件生成模型（如[30]），但这些模型通常需要固定大小的测量。这对于像我们这样的自适应方法来说是不适合的，因为我们的测量集是可变的。我们通过将条件生成建模为一个优化目标来解决这个问题，我们的目标是找到一个根据测量看起来合理的x，同时根据深度生成模型也是一个合理的数据点。从随机z向量开始，我们通过梯度下降来优化这个向量以满足目标。这成为了一个流形优化问题上的反向传播问题[31]，我们可以写成：

$$
\min_z \|AG(z) - y\| + \lambda \phi(z)
$$

其中A是前向算子，可以被非线性算子替换（第五节C3）。G(z)是给定潜在空间输入的生成模型的输出，λ是一个超参数，φ(z)是在深度模型下使数据的合理性最大化的惩罚项。模型和范数必须是可微的。在高斯测量噪声的情况下，L2范数是合适的。

我们测试了另一种创建条件样本的方法，即使用Metropolis算法在潜在空间中采样点[32]，以满足测量。假设测量的条件独立性，我们可以将给定测量的潜在空间的条件分布p(z|y)写成：

$$
p(z|y) = \frac{\sum_x p(y|x)p(x|z)p(z)}{\sum_z \sum_x p(y|x)p(x|z)p(z)}
$$

如果我们有两个潜在向量z1, z2，我们可以写出一个似然比：

$$
\frac{p(z1|y)}{p(z2|y)} = \frac{\sum_x p(y|x)p(x|z1)p(z1)}{\sum_x p(y|x)p(x|z2)p(z2)}
$$

这个似然比现在可以用来从所需的分布中采样。这种方法不会受到局部最小值的影响，也不需要像梯度下降那样保存梯度的内存，但它需要更长的时间，并且需要不断地拒绝在高斯下变得不太可能的样本（∥z∥ > 1）。

## E. 向量测量的扩展

我们可以将我们的方法扩展到向量测量的情况，例如，当一次观察多个测量时，例如一次观察图像的一整行，通过将(3)重写为：

$$
I(X; Y ) = h(Y ) − h(Y |X)
$$

假设已知噪声模型，例如高斯噪声，那么对于每个测量Y，项h(Y |X)是已知的。需要估计测量熵的估计值。如果我们假设噪声是高斯的，那么我们可以估计这个项的界限。在这种情况下，我们建议将p(Y)的分布估计为以样本X为中心的高斯混合模型。问题随后变成了估计高斯混合模型的熵。为了减少我们需要获取的观测数量，我们应该要求最具信息量的观测。因此，我们对(13)感兴趣，并使用一个下限估计h(Y)将保证我们获得最小的信息增益。我们使用[33]中提出的下限估计，假设我们有N个条件样本，得到的估计为：

$$
I(X; Y ) \geq -\frac{1}{N} \sum_{i=1}^{N} \log \left( \frac{1}{N} \sum_{j=1}^{N} N(yi; yj, 2\sigma^2) \right) - h(Y |X)
$$

其中yi表示从样本i获得的测量，σ是噪声方差。这使得该方法能够自适应地优化任意多变量向量测量（而不是单个测量）。


# IV. 实验

所有的实验都是使用PyTorch编写的，并在Tesla K20 GPU上运行。我们在玩具集、数值和图像数据集上展示了实验结果。此外，我们还展示了不同测量领域中的结果。在这一部分中，我们展示了使用训练数据作为模型的结果。

## A. 玩具实验和结果

我们首先在图2所示的4像素玩具集上说明我们的方法，其中每个像素的值为0或1。假设我们想要确定在观察下的是A到D中的哪一个示例，我们应该获取哪些像素？任何像素的观察都带有加性高斯噪声。或者，这可以被看作是一个任务，即使用最少数量的顺序噪声像素观察来推断观察实例的标记A到D。在这个集合中，第一个像素是最有信息量的，其次是第二个或第三个，这取决于观察到的第一个像素的值。我们在不同观察噪声水平下将我们的方法与随机采样像素进行了比较。我们通过绘制在观察到的像素数量与条件分布的可能候选项的熵之间的关系图来监控进展。图3显示了在不同噪声水平下，100次运行的平均条件熵与获得的测量数量之间的关系。图3中虚线表示知情感知，实线表示随机采样。不同的线条颜色对应于0.3、0.5和0.9的三种不同的观察噪声水平标准差。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f58b17a8b80f470584d5541354ca4aef.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/50b444a2f4a943b3b8dbab7ff312a6db.png" width="70%" /> </div>


正如预期的那样，在高噪声水平下，条件熵下降得很慢，这意味着需要更多的测量才能对我们正在观察的内容有更多的信心。两种技术之间的差异在更高的噪声水平下更为明显。在噪声标准差为0.9的情况下，平均需要4次知情测量与10次随机测量才能将熵减半。在所有熵降至零的运行中，随机和知情感知都收敛于真实解。

## B. 图像数据集上的实验

对于图像数据集的实验，我们使用知情感知来指导像素的获取。我们根据(2)计算观察图像中任何像素的预期熵，并对像素进行带噪声的观察。这导致在观察后的条件分布得到更新。这个过程一直重复，直到达到所需的条件分布的确定性。请注意，每获取一个测量，我们都有一个完整且合理的样本分布，即没有涉及部分重建。样本是由代表已获得测量（包括重复测量）的集合y0的条件模型p(x|y0)生成的。因此，每一步我们都有一个完整的重建，即在先验下合理且满足测量（由可能性表示）的样本。由于我们知道真实情况，我们可以找到条件分布中每个样本与真实情况的均方根误差（RMSE）。我们使用分布的平均RMSE作为比较指标。

我们还使用了样本的条件熵的近似值作为另一个度量。由于我们使用样本来模拟我们条件分布，真实的样本熵无法获得以监控进展，因此我们使用了[21]中概述的方法，该方法允许从经验样本中闭式估计2阶的Rényi熵。我们称这为归一化的Rényi熵。然而，重要的是要注意，与RMSE度量不同，如果条件生成算法存在偏差，或者使用的模型没有学习到数据中的所有变化，则归一化熵度量可能无法很好地代表收敛性。这是因为在某些情况下，分布熵可能很低，但RMSE很高，即模型很有信心但是错误的。

我们首先展示了在MNIST [34]上应用该方法的结果，使用50,000个图像训练集D作为模型。这被建模为具有均匀先验概率1/50,000的分类分布。给定一组获得的观察结果，我们根据观察可能性对每个MNIST数字进行加权。现在我们从这个测量加权分类分布中替换N个索引，并检索相应的MNIST数字作为由索引表示的样本。给定真实图像，我们计算样本的平均RMSE。我们不计算样本索引之间的MRMSE，而是首先检索从分类分布中替换得到的对应于获得的索引的样本。

表I显示了在0.5的噪声标准差下，获取60个像素后，知情和随机感知在图像数据集上的条件分布p(x|y)样本的归一化Rényi熵和样本相对于真实情况的平均均方根误差MRMSE。我们选择了10个训练集数字和10个测试集数字，并在0.5的噪声标准差下平均了40次运行的结果。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/298db5ffebbd438f8a5dcb164fac3600.png" width="70%" /> </div>

我们还测试了其他图像数据集CIFAR-10 [35], [36], HAM-10000 [37]（常见色素性皮肤病变的皮肤镜图像）、FashionMNIST [38]（时尚产品图像）、SVHN[39]（街头门牌号码）以及患有胶质瘤的患者的大脑MRI数据集 [40], [41], [42]。所有像素都归一化到[-1,1]范围内，测量噪声的标准差为0.5。我们将我们的方法与随机采样进行了比较，并使用归一化熵度量和均方根误差在表I中提供了结果摘要。

我们在测试集上看到了类似的结果，除了熵不收敛于零。这是因为这里的模型是基于记忆训练数据的，并没有推广到训练集。在第五节中，我们使用生成模型来解决这个问题。

对于MNIST的示例，在5个示例中，通过知情感知观察到的像素如图4中显示为白色。我们可以看到，该方法可以用来解释为什么我们选择了信息像素。例如，没有观察到跨MNIST个数字的一致的图像的外部边界，而焦点是在图像的中心。数字0的模式可以看作是识别一个具有空核心的圆形对象。这可以与[43]的图4进行比较，图4显示了有助于实例识别的像素。

我们在表二中展示了应用确定数字类（标签）的方法的结果，该方法显示了条件分布中属于真实类的样本的比例与观察到的像素数量。对于观察了60个像素（噪声SD = 0.5）后的测试集，条件分布中81%的样本属于正确的类。仅在这种情况下，我们的方法类似于基于熵的二叉树[29]进行分类，同时能够解释任何参数噪声模型。
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/1bd272cd567044119b2b18ae2d54d9a4.png" width="70%" /> </div>

## C. 与压缩感知的比较

压缩感知（CS）是一种广泛用于重建稀疏信号的技术[44]。压缩感知使用随机投影矩阵（感知矩阵），该矩阵保持数据点之间的距离，并满足受限等距性质（RIP）[44]。所需的测量数量取决于信号的稀疏程度。我们的方法与压缩感知不直接可比，因为我们的方法是自适应的（在线的），而压缩感知考虑全局数据（稀疏程度）。旨在学习感知矩阵的方法[45]是离线的且非自适应的，与我们的方法不同。我们之所以与压缩感知进行比较，是因为它是在许多领域中用于数据压缩的已建立基准。请注意，压缩感知的每个测量是图像中所有像素的加权求和，而我们方法中的测量只是1个像素。表III显示了基于MNIST和大脑MRI数据集[40], [41], [42]产生的样本的MRMSE，比较了压缩感知和知情感知。“像素数量”列显示了实际获得的像素数量。我们看到，由于在线自适应的特性，我们的方法需要显著较少的像素数量就能产生与压缩感知相似的结果。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e989b56c8e1d4ee6bb2f8cc02dbe07ac.png" width="70%" /> </div>


## D. 数值数据集上的结果

在这一部分中，我们通过将其应用于众所周知的数值数据集来展示我们的方法，包括鸢尾花、威斯康星州乳腺癌（诊断）、蛋白质三级结构和伽马望远镜数据集[46]。数据集的特征独立地归一化到0-1范围内，我们将噪声标准差设置为0.03。我们在表IV中比较了我们的方法与随机采样，在进行了15次带噪声的观察后。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/82d403b0e5774fae905c33bb8e23481f.png" width="70%" /> </div>


# V. 使用生成模型的实验

在这一部分中，我们展示了使用生成模型的结果。这允许我们推广到测试数据。我们将自己与压缩感知、随机采样、基于方差的像素采样以及[13]中的基于互信息的方法进行了比较。基于方差的像素采样涉及在离线确定训练集的像素方差，并在推理时从具有最高方差的像素开始采样。我们在不同的测量领域展示了实验。

## A. 图像领域实验

在前一节中，我们确立了需要一个概括模型以在测试数据上获得可接受的性能。我们展示了使用基于神经网络的条件模型会导致在与训练数据相同分布的未见过测试数据上取得良好的结果。我们训练了无条件生成模型，并使用第III-D节中提到的条件生成技术进行条件生成。我们在各种数据集上训练了生成对抗网络（GAN）和变分自编码器（VAE）。我们还测试了单层和双层的无监督RBM模型[47]，在MNIST上进行条件生成，但没有获得令人满意的结果。对于GAN，我们使用了[48]的架构，适应了28×28的图像大小。模型架构的详细信息可以根据要求提供。

我们在MNIST、FashionMNIST、皮肤癌HAM-10000数据集和CIFAR-10上训练了模型。对于每个模型，我们在5个训练集和5个测试集数据输入上运行了20次知情感知模拟。我们为MNIST和FashionMNIST添加了标准差为0.3的高斯噪声。HAM-10000和CIFAR-10集的噪声标准差为0.2。所有考虑的数据集的图像大小均为28×28像素，除了CIFAR-10为32×32像素。我们比较了在训练集和测试集上的结果，并在获取40个带噪声像素后，使用生成样本的RMSE进行了总结。表V总结了在三个图像数据集上，知情感知和随机采样生成样本的平均RMSE。随机采样的值显示在括号中。我们观察到，对于所有模型和数据集，训练集和测试集的平均性能是相似的。这表明该方法可以通过使用生成模型推广到未见过的数据。我们还注意到，VAE通常比GAN表现得更好。这是由于训练GAN模型的困难。请注意，由生成模型产生的样本具有比使用训练集作为数据时更高的RMSE，因为神经模型的训练误差不会降至零，这有助于知情和随机感知之间低差异。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/2d964a6f34d5447ab7881a752e28e543.png" width="70%" /> </div>


我们使用在MNIST上训练的VAE进一步实验，将知情感知与基于方差的像素选择进行比较。对于基于方差的感知，我们根据训练数据中各个像素的方差对像素进行排序。在这些实验中，我们使用了100个测试集数字，并在表VI中展示了20次运行的均值和1个标准差RMSE，用于获取5、10、20和40个像素。我们的方法优于基于方差的采样。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/6c33c15003004c08b7e756353a739bd6.png" width="70%" /> </div>


我们展示了使用VAE进行知情感知生成的样本结果在图5中，以及使用随机感知在图6中。第一列显示了真实的测试集数字，其余列是样本。从上到下排列的行是在观察了0、10、20和40个带噪声像素之后的样本。我们可以看到，在这个例子中，知情感知在20个像素之后收敛，而随机感知即使在40个像素之后也显示出变化。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b20ba39406cd4349be51ff26090776a5.png" width="70%" /> </div>


我们在FashionMNIST上获得了与MNIST相似的结果。

图7显示了使用在HAM-10000数据集上训练的VAE进行知情感知的结果。第一行显示真实情况，第二行显示观察到的像素，最后一行是VAE重建的完整图像，其余是样本。前三列是训练集示例，后三列是测试集示例。我们可以看到，在观察了80个像素之后，样本中仍然有一些变化，表明需要采样更多的像素。在这个数据集上训练生成模型是具有挑战性的，因为它展示了大量不同形状和颜色的病变，同时总共只有10,000个示例。我们使用9000个示例进行训练，留下1000个用于测试和验证。此外，请注意该数据集对红色的偏见，这被模型捕获了。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/14542f9af00940358f24eb4eab4eb1e1.png" width="70%" /> </div>


在第III-E节中，我们展示了如何使我们的方法适应向量测量。我们在CIFAR-10上使用向量测量进行了实验。在这里，我们限制了图像中的任何行或列的测量。因此，算法被允许采样任何完整的32像素行或列，并添加噪声。我们比较了使用知情感知与随机感知的情况，并在表VII中展示了结果。我们发现，知情和随机感知的性能接近，知情感知只是稍微好一点。这是因为每个像素都传达了关于对象或背景颜色的信息。此外，请注意VAE模型的表现优于GAN。通过知情感知和随机感知产生的样本示例分别在图8和图9中展示。第一列是真实情况，从上到下排列的行是在获取0、5、10和15次测量之后的样本。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/cd0d071e61d545e99e287729749d87ea.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e3120e2243e94264ab89b59b9bfb7c89.png" width="70%" /> </div>


## B. 使用生成模型与压缩感知的比较

我们使用生成模型在两个数据集上将我们的方法与压缩感知进行了比较。表VIII显示了两种方法产生的样本的RMSE。使用VAE的压缩感知结果在图10中展示了三个训练示例（前三列）和三个测试示例。第一行是真实情况，其余是样本。与知情感知的比较见图7。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/497a97e173cd481ea55d6eed659f1eae.png" width="70%" /> </div>


图11展示了使用GAN在HAM-10000上进行知情感知的灰度像素演化。第一列是真实情况，从上到下的行是在观察了0、10、20和40个像素之后的样本。噪声标准差是0.3。

图12展示了使用在HAM-10000上训练的GAN进行随机感知的灰度像素样本。行和列与图11相同。

## C. 不同测量领域

在本节中，我们展示了我们的方法可以应用于不同的测量领域。例如，如果观察是在傅里叶域中，或者是RGB像素的单通道值，并且需要检索颜色，即颜色检索。我们强调，我们使用的是相同的在图像域数据上训练的神经网络模型，而无需针对特定测量领域进行重新训练。

1) 颜色检索：在这个实验中，我们通过在通道上平均，将RGB HAM-10000数据集转换为灰度集。前向模型是RGB像素的平均强度。图11展示了使用先前训练的GAN进行知情感知的颜色检索结果，用于1个测试集图像。左列是真实情况，从上到下的行是在测量了0、20、40和80个像素的平均RGB强度之后的样本。噪声标准差是0.2。我们可以看到，即使在观察了80个带噪声的像素之后，可能的解决方案之间仍然存在一些变化，表明恢复缺失信息的挑战性。随机感知的相同结果在图12中展示。我们可以看到，与随机感知相比，知情感知更好地表示了真实的颜色分布。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/2cd4e75adb084fc5a4f1529245a99bbf.png" width="70%" /> </div>


2) 频率域实验：在这个子部分中，我们将我们的方法应用于频率域数据（在MRI的背景下是k空间）。提出的方法允许使用在图像域数据上训练的相同生成模型，而无需重新训练。我们在图像域生成样本，然后对样本应用二维快速傅里叶变换（FFT）。我们假设每个测量都带有加性不相关的高斯噪声。我们在MNIST和MRI大脑数据集上测试了我们的方法。在频率域中，(1)中的欠采样矩阵A是傅里叶算子F，然后是逐元素乘以掩码M（即哈达玛德乘积）。因此(1)变为：

$$
y = M \odot Fx + \eta
$$

图13展示了使用在图像域数据上训练的GAN的MNIST频率域结果。第一列是真实情况，其余列是获取0、10、20和40个测量后的10个最可能的样本。我们看到了与图像域数据相似的性能。

我们使用QIN BRAIN MRI数据集[40], [41], [42]来训练VAE，用于我们在图像域中的实验，并在FFT域中应用知情感知。结果在图14中展示。第一列是真实情况，行是在获取0、20、40和80个带噪声测量后的样本。保持条件分布的好处是，我们可以决定是否需要更多的测量，样本显示了重建中的变化程度和置信度。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/ca2efa614d374531beeaff6ada62cae6.png" width="70%" /> </div>


3) 相位恢复：在这一部分中，我们将我们的方法应用于相位恢复问题。相位恢复发生在频率域中，我们观察到数据的幅度|Ax|，需要重建实部和虚部x[28]。我们可以选择性地有一个欠采样矩阵A，即，我们不观察所有像素的幅度。对于相位恢复，公式(1)变为：

$$
y = M \odot |Fx| + \eta
$$

其中 $|.|$ 表示复数的幅度。使用变分自编码器(VAE)进行相位恢复的结果展示在图15中。详细信息与图14相似。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/77add1cd7ed04b18a5357e007dc66bb2.png" width="70%" /> </div>


## D. 与基于互信息方法的比较

我们比较了我们的在线优化方法[13]，因为它与我们的工作相关。首先我们讨论[13]中的方法。该方法基于两个循环。外循环是优化循环，内循环是变分循环。外循环与知情传感中的观察选择步骤大致相似，只是使用变分近似后验来进行熵估计。

在我们的情况下，我们使用来自生成模型的条件样本来估计后验熵，而在他们的情况下，内循环通过拟合稀疏解的高斯变分分布来完成此任务。这个拟合循环会为每个候选观察重复执行。相比之下，我们的熵计算直接使用样本而无需拟合迭代。在[13]中，图像首先通过基于边缘提取器的核库B进行处理，以及离散小波变换特征。这个变换的值τ被视为未知，并需要估计，以便得到的图像满足观测。先验被选择为图像上的超高斯/拉普拉斯分布，它本身被近似为一个有界它的高斯分布。通过解决[13]附录中的(A1)来找到一个在先验和观测下最大化可能性的解。在他们的工作中，他们迭代求解方程，因为不存在封闭形式的解。他们的方法涉及矩阵求逆，这对于大型数据来说是昂贵的，因此他们提出了计算可行的近似方法。然后使用来自高斯变分分布的结果方差来封闭形式计算熵。

[13]中的方法适用于任何图像，因为没有假设数据特定的先验。不幸的是，如果未给出一组信息丰富的初始测量，他们的方法表现不佳。因此，在他们的工作中，通常为256×256图像提供256个可能行中的中央32行的k空间。对于28×28图像，这个比例对应于在频域观察中央3行。

我们在频域使用VAE对MNIST测试集数字进行实验，并使用知情传感和[13]的方法。根据他们的实现，我们允许他们的方法获得k空间中的中央3行。此外，他们的方法产生MAP估计，这意味着作为单一输出返回平均解。我们的方法产生样本，因此，对于我们的方法，我们在观测下返回最可能的样本。

图16显示了两种方法在10个MNIST测试集数字上的比较结果。真实的数字用红框突出显示。两个子图的列分别是每个测试集数字的输出。

表IX显示了知情传感和[13]方法产生的样本的平均均方根误差统计数据，以及95%的置信区间。每种方法观察到的k空间点数也显示在左列中，格式为IS/[13]的数量。

这表明我们的方法更有效，重要的是能够像[13]的方法一样很好地泛化。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/9c98906fd0fe4b1f8e83abc5e64424e2.png" width="70%" /> </div>


# VI. 局限性、假设和缺点

我们假设观察到的数据x属于已知的分布，并且不是分布之外的数据（ODD）。我们假设神经生成模型没有偏差（测试集和训练集的性能相似），并且充分涵盖了数据中观察到的变化，即没有遭受模式崩溃。最后，我们假设每个测量的噪声是独立的。

我们方法的一个缺点是需要计算所有候选测量的互信息。然而，我们发现知情传感部分应用起来非常快，对于128×128像素大小的图像，在Tesla K20 GPU上需要0.3秒。此外，知情传感的步骤很容易并行化。主要的时间复杂度来自于使用生成模型生成条件样本。

我们的方法与[13]类似，在某种程度上是短视的，因为我们在没有比单一测量更进一步的情况下采样预期最信息丰富的测量，但该方法并不贪婪，因为我们考虑的是每个观测的预期互信息，而不是最大值。我们的方法在稀疏数据集上表现最佳。尽管(2)中的期望项是有偏的（由于对数项），我们发现对于每个迭代独立进行条件生成，并使用200个样本可以得到良好的经验结果。实验表明，如果模型流形中存在真实示例，我们就会收敛到该示例。

我们的方法可以看作是通过维护解决方案的分布来解决逆问题。这个分布是由条件模型生成的，因此模型中的任何偏差都会导致错误的后验分布。我们在实验中展示了这一点，其中我们使用训练数据作为先验，并在测试数据上运行我们的算法。即使它们不准确地类似于测试输入，我们也获得了一致的样本。因此，这种偏差可能会导致过度自信和错误的不确定性估计。未来的工作将调查如何通过使用条件模型的集成来减轻这种偏差。

# VII. 结论

在本文中，我们提出了一种基于互信息的知情自适应传感算法，用于顺序采样应用。我们在不同的数据集和不同的前向模型上测试了我们的算法。我们展示了使用生成模型，我们的算法可以推广到看不见的数据。未来的工作将解决讨论方法的缺点。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
