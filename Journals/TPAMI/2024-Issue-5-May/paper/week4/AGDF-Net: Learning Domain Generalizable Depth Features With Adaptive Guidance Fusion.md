# 题目：[AGDF-Net: Learning Domain Generalizable Depth Features With Adaptive Guidance Fusion](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10356721)  
## AGDF-Net: 学习具有自适应引导融合的领域泛化深度特征
**作者：Lina Liu；  Xibin Song；Mengmeng Wang；Yuchao Dai；Yong Liu；Liangjun Zhang** 

****
# 摘要
跨领域泛化深度估计旨在使用在源域（即合成域）上训练的模型来估计目标域（即现实世界）的深度。以往的方法主要使用额外的现实世界域数据集来提取深度特定信息以进行跨领域泛化深度估计。不幸的是，由于存在较大的域间差距，难以获得足够的深度特定信息，并且难以去除干扰，从而限制了性能。为了缓解这些问题，我们提出了一种具有自适应引导融合的领域泛化特征提取网络（AGDF-Net），以在多尺度特征级别上充分获取深度估计所需的基本特征。具体来说，我们的AGDF-Net首先通过重建和对比损失将图像分离为初始深度和弱相关深度成分。随后，设计了一种自适应引导融合模块，以充分增强初始深度特征，从而获取领域泛化的强化深度特征。最后，以强化的深度特征作为输入，可以使用任意深度估计网络进行现实世界的深度估计。仅使用合成数据集，我们的AGDF-Net可以应用于各种现实世界的数据集（即KITTI、NYUDv2、NuScenes、DrivingStereo和CityScapes），并具有最先进的性能。此外，在半监督设置下使用少量现实世界数据的实验也证明了AGDF-Net相对于最先进方法的优越性。

# 关键词
- 深度估计
- 领域泛化
- 领域泛化深度特征
- 自适应引导融合

# 引言
单目深度估计是一项重要的感知任务，已广泛应用于许多应用中，如自动驾驶，3D场景重建和增强现实等。基于深度卷积神经网络（DCNNs）的监督深度估计方法已取得了令人鼓舞的成果，这些方法主要使用注释的深度真实值来监督DCNNs以估计良好的深度图。然而，监督学习需要大量由传感器获取的深度并与图像对齐作为真实值，这既昂贵又耗时。因此，提出了使用视频序列或立体图像以自监督方式估计深度的方法。然而，当前数据集中并不总是有连续的视频序列和立体图像，结果通常限于单个训练数据集，难以泛化到不同的未见场景。

为了解决这些问题，提出了几种解决方案，使用合成数据，深度注释可以直接用于训练，并在真实数据中进行测试（从合成到真实），以推广到多个真实场景，我们总结为：

1. **直接方法：** 一些方法尝试使用由易于获得真实注释的合成数据集训练的模型来估计现实世界的深度。然而，合成与现实世界域之间存在巨大差距，实质上限制了现实世界深度估计的性能。

2. **基于领域适应的方法：** 为了缩小从合成到真实的差距，一个直观的考虑是直接将合成域图像转换为真实图像，包括在图像级别或特征级别上减少域的差距。上述方法是基于领域适应的方法，要求同时使用合成域和真实域图像进行训练以减少域间差距。然而，很难获得大量不同场景的真实世界图像。

3. **基于领域泛化的方法：** 为了解决上述限制，合成到真实的领域泛化方法仅使用标注的合成数据进行训练，并直接在多个真实数据场景中进行测试，这是一项更具挑战的任务，因为在训练过程中无法获得真实数据的风格。这些方法旨在使用仅合成数据集来学习深度特定特征图，用于合成和现实世界图像的深度估计，并通常在深度特定特征图上进行增强操作，从而在真实数据集上获得良好的泛化结果。然而，这些方法通常需要使用同时包含合成和额外真实世界图像的预训练编码器来获得深度特定特征，这带来了新的域问题，限制了深度特定特征图的性能。此外，简单的注意力操作通常用于增强深度特定特征图，效果有限。

为了缓解上述限制，我们期望找到深度估计的本质信息表达。我们提出了一种新的基于领域泛化的框架，从特征级别学习多尺度领域可推广深度特征进行深度估计，并且仅使用合成数据进行训练，而不使用任何真实数据。先前的工作证明了图像的结构和纹理在观察深度时起着关键作用，而风格和照明等是深度感知的干扰项。因此，我们旨在减轻干扰项的影响，并从图像中提取领域不变的组件用于深度估计。为了获得具有更好泛化能力的领域不变组件，我们首先通过重建和对抗损失从图像中提取初始深度特征，并使用自适应引导融合模块增强初始深度特征，以加强受先前工作启发的领域不变信息，最终获得领域可推广的增强深度特征作为后续深度估计网络的输入。需要注意的是，领域可推广的深度特征对于不同的域是不变的。我们期望在多尺度特征级别上学习领域可推广的深度特征，从而获得更好的泛化深度图，并进一步缩小从合成到真实的领域差距。

为了获得用于深度估计的领域可推广特征，我们提出了一个框架，即AGDF-Net，通过两个网络分支受重建损失和对抗损失约束将图像分离为多尺度初始深度和弱相关深度部分。初始深度部分应该是与深度相关的信息，而弱相关深度部分应包含深度估计的干扰项。然后，使用自适应引导融合模块增强提取的初始深度特征以获得增强的深度特征。具体来说，在每个尺度上，使用前一个尺度的增强深度特征来增强提取的初始深度特征，并使用自适应引导融合模块。最大的尺度由从彩色图像中提取的特征引导。自适应引导融合模块的目的是使网络更多地关注领域不变的部分。该模块可以增强初始深度特征，进一步消除干扰信息并恢复领域不变信息。最终，获得用于后续任意深度估计网络的领域可推广的增强深度特征，并受深度损失约束。总体而言，我们的框架的实际设计（初始深度分支，弱相关深度分支和自适应引导融合模块）和用于约束特征提取的不同损失（对抗损失，重建损失和深度损失）有助于获得领域可推广的深度估计特征。


![](https://img-blog.csdnimg.cn/direct/b997bfcdb3464674b682ff5506de4dcc.png)


本文的主要贡献可以总结为：
- 我们提出了一种有效的领域可推广深度特征提取框架（AGDF-Net），通过将图像分离为初始深度和弱相关深度组件，以有效提取跨领域可推广深度估计的深度相关信息；
- 设计了一个自适应引导融合模块，以在多尺度级别上充分重用和增强提取的初始深度特征，从而获得领域可推广的增强深度特征。该模块可以进一步增强深度估计的领域不变组件。最终，仅在合成域上训练后，可以很好地推广到未见的真实域；
- 无需使用任何真实世界数据集，我们的AGDF-Net可以很好地应用于各种深度估计数据集（即KITTI，NYUDv2，NuScenes，DrivingStereo和CityScapes），并实现最先进的性能，更适用于实际场景。此外，在半监督设置中使用少量标注的真实世界数据的实验也证明了我们的AGDF-Net相对于最先进方法的优越性。


![](https://img-blog.csdnimg.cn/direct/95a2b65c3c844e658d17dd84066a2d20.png)


## III. 方法
在单目深度估计中取得了显著进展，大多数方法的训练和测试过程都在同一域中。然而，当训练和测试在不同域中时，性能受到了严重限制。为了解决这一问题，我们提出了一种通过学习具有自适应引导融合的领域可推广深度特征来实现可推广深度估计的框架，即AGDF-Net，并在本节中提供了更多关于所提出网络的细节。

### A. 概述
先前的工作证明，结构信息与深度更相关，而风格和照明等是深度感知的干扰项。因此，我们的方法旨在提取用于跨领域可推广深度估计的领域不变表示。为了初步提取与深度估计相关的特征，以彩色图像 $I$ 为输入，我们的AGDF-Net首先利用初始分支（初始深度编码器和解码器： $E_{in}$ 和 $D_{in}$ ）和弱相关分支（弱相关深度编码器和解码器： $E_{wr}$ 和 $D_{wr}$ ）从彩色图像中提取初始深度特征。目的是提取与深度估计相关的初始深度特征，同时可以初步去除干扰信息。在此过程中，使用对抗损失和重建损失将两种初始深度特征和弱相关深度特征分离和提取。具体来说，初始分支和弱相关分支从同一图像中提取两种特征，并通过对抗损失约束这些特征以获得相互排斥的特征。为了避免丢失信息，使用重建损失将这两个特征重建回原始图像，以确保两个分支提取的信息不同但互补。

为了进一步增强初始深度特征以获得领域可推广的深度特征，增强分支（增强深度编码器和解码器： $E_{it}$ 和 $D_{it}$ ）重用彩色图像以进一步引导初始深度特征进行特征增强，从而获得更有效的领域可推广特征用于深度估计。具体来说，在获得两种完全不同的特征后，将其中一个特征（初始分支提取的特征）发送到自适应引导融合模块，以增强多尺度初始深度特征，获得用于后续跨领域深度估计的多尺度增强深度特征（领域可推广特征）。

最后，可以应用任意深度估计网络以获得领域可推广的深度估计结果。在此过程中，训练时施加深度损失以约束预测的深度结果。

通过初始分支和自适应引导融合模块可以获得更好的深度结果，领域可推广特征用于跨领域深度估计，剩余的信息可以分离到弱相关分支（具有对抗损失和重建损失）。需要注意的是，初始和弱相关分支用于初步特征解耦。获得的初始深度特征并不是完全用于深度估计的领域不变特征。可以使用提出的自适应引导模块进一步增强初始深度特征，进一步消除小纹理信息并加强足够的信息。最终获得完全解耦的领域可推广增强深度特征用于后续深度估计。

### B. 初始特征提取
此过程旨在从不同域的图像中提取公共信息。受先前工作的启发，使用两个编码-解码分支将不同域的图像分离为初始深度特征和弱相关深度特征。这两个分支定义为初始分支（ $E_{in}$ 和 $D_{in}$ ）和弱相关分支（ $E_{wr}$ 和 $D_{wr}$ ）。

**初始分支：** 初始分支由初始深度编码器 $E_{in}$ 和初始深度解码器 $D_{in}$ 组成。 $E_{in}$ 旨在从不同域的图像中提取初始深度编码特征 $f_{E_{in}}$ 。然后将 $f_{E_{in}}$ 输入到 $D_{in}$ 以提取初始深度解码特征 $f_{D_{in}}$ ，并重建初始深度图 $I'_{in}$ 以进行后续图像重建。请注意，这里使用常用的ResNet18编码器，过程可以表示为：

$$
f_{E_{in}} = E_{in}(I)
$$

$$
f_{D_{in}}, I'_ {in} = D_{in}(f_{E_{in}})
$$

其中 $f_{E_{in}} = \{ f_{E_{in}}^1, \dots, f_{E_{in}}^n \}$ ， $f_{D_{in}} = \{ f_{D_{in}}^1, \dots, f_{D_{in}}^n \}$ ， $n$ 是在不同编码/解码尺度上提取的特征数量。

**弱相关分支：** 弱相关分支由弱相关深度编码器 $E_{wr}$ 和弱相关深度解码器 $D_{wr}$ 组成。 $E_{wr}$ 旨在从不同域的图像中提取作为深度估计干扰的弱相关深度编码特征 $f_{E_{wr}}$ 。然后将 $f_{E_{wr}}$ 输入到 $D_{wr}$ 以提取弱相关深度解码特征 $f_{D_{wr}}$ ，并重建弱相关深度图 $I'_{wr}$ 以进行后续图像重建。过程可以表示为：

$$
f_{E_{wr}} = E_{wr}(I)
$$

$$
f_{D_{wr}}, I'_ {wr} = D_{wr}(f_{E_{wr}})
$$

其中 $f_{E_{wr}} = \{ f_{E_{wr}}^1, \dots, f_{E_{wr}}^n \}$ ， $f_{D_{wr}} = \{ f_{D_{wr}}^1, \dots, f_{D_{wr}}^n \}$ ， $n$ 是在不同编码/解码尺度上提取的特征数量。

**初始深度特征提取：** 初始分支和弱相关分支的目的是将图像分离为初始深度和弱相关深度部分。根据先前的工作，当神经网络观察深度时，结构和纹理起着重要作用，这是深度估计的领域不变项。这也符合人类通过素描和结构观察3D几何（深度）的结论。因此，受图像分解的启发，从同一图像中分离的初始深度信息（与深度感知相关）和弱相关深度信息（剩余干扰信息）应该是独立且互补的。在此过程中，初始深度编码特征 $f_{E_{in}}$ 和弱相关深度编码特征 $f_{E_{wr}}$ 受对抗损失约束，因此两个分支可以独立地提取不同的信息，其中初始分支用于领域可推广深度估计。从两个分支提取的初始深度和弱相关深度特征重建的图像（ $I'_ {in}$ 和 $I'_{wr}$ ）可以共同重建到输入图像，这可以表示为：

$$
I' = I'_ {in} + I'_{wr}
$$

其中 $I'$ 是重建图像，应与输入图像 $I$ 相同。重建图像 $I'$ 和输入图像 $I$ 受重建损失约束。


![](https://img-blog.csdnimg.cn/direct/e1b56e80b51f40deb6b4e9a6cfbcd2f8.png)


![](https://img-blog.csdnimg.cn/direct/0eb2bc4a07fb4aeba84a7d6e8952d6ff.png)

![](https://img-blog.csdnimg.cn/direct/6445aaf1c5a047e4b23b0e6054c25aba.png)

![](https://img-blog.csdnimg.cn/direct/5028cf5b0f0d4b3e9460fc5dfa9fe9a7.png)


### C. 增强特征提取
我们的目标是学习用于跨领域可推广深度估计的领域可推广深度特征。为了进一步消除深度估计的干扰信息，例如一些详细的纹理，在此过程中，我们设计了增强深度分支，以进一步优化提取的初始深度特征，消除残留的干扰信息，并增强深度估计的领域不变信息。图像包含大量纹理信息，并且与深度图在结构和纹理上有一定的对应关系。因此，在增强特征提取过程中，重用图像信息来学习自适应卷积参数，以引导领域不变信息的优化。


![](https://img-blog.csdnimg.cn/direct/000864b233484229b8e8c7259db94a04.png)



**增强分支：** 增强分支由增强深度编码器 $E_{it}$ 和增强深度解码器 $D_{it}$ 组成。 $E_{it}$ 旨在通过自适应引导融合增强初始深度解码特征 $f_{D_{in}}$ ，获得增强深度编码特征 $f_{E_{it}}$ 。然后将 $f_{E_{it}}$ 输入到 $D_{it}$ 以提取多尺度增强深度图 $f_{D_{it}}$ ，用于后续的领域可推广深度估计。

### D. 自适应引导融合
自适应引导融合的详细信息如图3(a)所示。在神经网络的每个尺度上，结合图像信息通过自适应引导模块（AG）优化初始深度信息，以获得多尺度增强深度特征。整个过程可以表示为：

$$
f_{E_{it}}^0 = E_{it}^1(I)
$$

$$
f_{E_{it}}^1 = E_{it}^2(AG_1(f_{E_{it}}^0, f_{D_{in}}^1))
$$

$$
f_{E_{it}}^2 = E_{it}^3(AG_2(f_{E_{it}}^1, f_{D_{in}}^2))
$$

$$
\dots
$$

$$
f_{E_{it}}^{n-1} = E_{it}^n(AG_{n-1}(f_{E_{it}}^{n-2}, f_{D_{in}}^{n-1}))
$$

$$
f_{E_{it}}^n = AG_n(f_{E_{it}}^{n-1}, f_{D_{in}}^n)
$$

$$
f_{D_{it}} = D_{it}(f_{E_{it}})
$$

其中 $f_{E_{it}} = \{ f_{E_{it}}^0, f_{E_{it}}^1, \dots, f_{E_{it}}^n \}$ ， $f_{E_{it}}^0$ 是提取的图像特征， $f_{D_{it}} = \{ f_{D_{it}}^1, \dots, f_{D_{it}}^n \}$ ， $E_{it} = \{ E_{it}^1, \dots, E_{it}^n \}$ ， $n$ 是在不同编码/解码尺度上提取的特征数量。 $AG_i$ 表示在第 $i$ 个特征尺度上的自适应引导操作。对于 $f_{E_{it}}^i$ 和 $f_{D_{in}}^i$ ，特征尺度从大到小随着 $i$ 增加。


![](https://img-blog.csdnimg.cn/direct/83ef07aa9b404171a07e5001dffa07df.png)

![](https://img-blog.csdnimg.cn/direct/f49852d245e94763a73d519e144e6cdf.png)


![](https://img-blog.csdnimg.cn/direct/c39b2431392b422a90f27cb86ec5f3a4.png)

![](https://img-blog.csdnimg.cn/direct/f66ff60bd6b34a319b02615668bbe746.png)

![](https://img-blog.csdnimg.cn/direct/3414662dee974f0aa2eaa7a4ae5e71a9.png)



**自适应引导模块（AG）：** 卷积作为特征增强的自适应形式被应用。AG 旨在学习结合图像信息的卷积核参数，并使用卷积优化初始深度解码特征（ $f_{D_{in}}$ ）以获得增强的深度特征（ $f_{it}$ ）。自适应引导融合可以表示为：

$$
f_{it} = AG(f_{in}, f_{guidance})
$$

其中 $f_{guidance}$ 代表从引导图像中提取的特征， $f_{in}$ 代表初始深度特征。整体引导融合过程如图所示，包括通道内和跨通道卷积以生成自适应卷积核。


### E. 领域泛化深度估计
在获得具有领域泛化特性的增强深度图 $f_{D_{it}}$ 后，可以使用任意深度估计网络进行随后的领域泛化深度估计，获得跨域泛化深度结果 $d_{it}$ ，如下所示：

$$
d_{it} = DepthNet(f_{D_{it}})
$$

### F. 损失函数
1. **相反损失：** 为了将同一图像的初始深度特征和弱相关深度特征分离，相反约束施加于初始深度编码特征 $f_{E_{in}}$ 和弱相关深度编码特征 $f_{E_{wr}}$ 上，以从两个分支（初始和弱相关）中提取独立且互补的信息，这被定义为相反损失。具体来说，为了确保初始深度和弱相关深度特征尽可能独立，相反损失约束这两个特征在向量空间中是正交的，可以表达为：

$$
\upsilon_{in} = \frac{\Theta(f_{E_{in}})}{||\Theta(f_{E_{in}})||_ 2 + \gamma}
$$

$$
\upsilon_{wr} = \frac{\Theta^T(f_{E_{wr}})}{||\Theta^T(f_{E_{wr}})||_ 2 + \gamma}
$$

$$
L_{con} = \upsilon_{in} \cdot \upsilon_{wr} + \lambda_ 1 + \lambda_ 2
$$

其中 $\Theta$ 是卷积操作层，并将特征拉直为一维向量。 $|| \cdot ||_ 2$ 是2-范数操作， $\gamma$ 设置为 $1e-6$ ，以避免分母为零。 $\lambda_ 1$ 和 $\lambda_ 2$ 是正则化项。目的是避免特征项为零，其中 $\lambda_ 1 = abs(||\upsilon_{in}||_ 2 - 1)$ ， $\lambda_ 2 = abs(||\upsilon_{wr}||_ 2 - 1)$ ，abs(·) 是绝对操作。

2. **重建损失：** 为了使初始深度分支和弱相关深度分支学习互补信息，重建约束施加于重建图像 $I'$ 和原始输入图像 $I$ 上，称为重建损失，定义为：

$$
L_{rec} = ||I' - I||_ 2 + ||I' - I||_ 2 + \sigma_ 1 + \sigma_ 2
$$

其中 $|| \cdot ||$ 是1-范数操作。 $\sigma_ 1$ 和 $\sigma_ 2$ 是正则化项，其中 $\sigma_ 1 = -||I'_ {in}||$ ， $\sigma_ 2 = -||I'_ {wr}||$ 。由于相反损失避免了编码器的特征项收敛到零， $I'_ {in}$ 和 $I'_{wr}$ 整合了编码器的特征，因此在所有损失的约束下，也可以防止解码器最终重建的输出为零。

3. **深度损失：** 预测的多尺度跨域泛化深度图 $d_{it}$ 和相应的深度真值图 $d_{gt}$ 受到深度损失的约束，表述为：

$$
L_d = w_ 1 \sum_{i=1}^n ||d_{gt}^i - d_{it}^i|| + w_ 2 \sum_{i=1}^n SSIM(d_{gt}^i，d_{it}^i)
$$

其中 $w_ 1$ 和 $w_ 2$ 是加权参数，经验设定为1和5。 $SSIM$ 表示结构相似性损失。

总损失定义为：

$$
L_{total} = a_ 1 \cdot L_{con} + a_ 2 \cdot L_{rec} + a_ 3 \cdot L_d
$$

其中 $a_ 1$ 到 $a_ 3$ 是加权参数，经验设定为0.1，1.0和1.0。

上述三种损失都用于将图像分离为初始深度和弱相关深度特征。相反损失用于尽可能分离图像的两个特征，重建损失用于约束分离的特征不同但互补，并且可以组合回原始图像而不丢失信息。深度损失保证最终的深度估计结果可以获得，具有领域泛化特征的更好深度估计结果，剩余信息可以分离到弱相关分支中（通过相反损失和重建损失）。


### G. 实现细节
对于初始分支、弱相关分支和增强分支，我们框架中使用的所有编码器都基于ResNet18。编码器-解码器骨干与先前的工作相同，编码器和解码器之间有跳跃连接。对于DepthNet，应用了两种网络结构，以证明我们AGDF-Net的泛化性，包括先前工作中的深度估计网络架构和广泛使用的ResNet18深度估计网络。

整个网络端到端训练20个周期。训练过程以 $1e-4$ 的初始学习率开始，每5个周期减少50%。我们使用Adam优化器（ $\beta_ 1 = 0.9，\beta_ 2 = 0.999$ ）的分步学习率衰减，批量大小设置为24。


## IV. 实验
在本节中，我们首先介绍我们的实验设置的细节。然后，我们验证了我们的方法在户外和室内场景中进行从合成到现实的深度估计的有效性。还提供了半监督设置的比较，以进一步证明网络的能力。然后，在更多数据集上的泛化结果进一步证明了我们方法的泛化性。最后，提供了消融研究，以分析我们架构每个部分的有效性。

### A. 实验设置
1. **户外数据集：**
  - **Virtual KITTI (vKITTI)：** vKITTI 是一个照片真实感的合成视频数据集，旨在学习和评估用于多种视频理解任务的计算机视觉模型，包含在不同成像和天气条件下从五个不同虚拟世界生成的21,260帧。在本文中，该数据集用作户外场景源域数据集进行训练。按照先前的工作，随机选择20,760对图像-深度作为我们的训练数据集。图像分辨率从375x1242降采样到192x640。按照先前的工作，深度真值的范围被裁剪到80米。
  - **KITTI：** KITTI 是一个大型现实户外自动驾驶数据集，包含由Velodyne HDL64收集的彩色图像和深度。在本文中，该数据集用作现实世界的户外数据集进行评估。按照先前的工作，使用697帧测试帧进行评估，并将帧降采样到192x640。

2. **室内数据集：**
  - **SUNCG：** SUNCG 是一个大型室内合成3D场景数据集，包含45,622个具有各种房间类型的3D房屋。在本文中，该数据集用作室内源域数据集进行训练。按照先前的工作，选择130 k对图像-深度进行训练。由于我们网络的输入分辨率需要为16的倍数，与先前的工作类似，原始分辨率为480x640像素的图像被降采样并裁剪到224x304像素作为输入。
  - **NYUDv2：** NYUDv2 是一个现实世界的室内数据集，包含由Microsoft Kinect捕获的大量视频帧，包含1,449帧测试帧。在本文中，该数据集用于评估。按照先前的工作，从1,449帧测试帧中选择654帧进行评估。原始分辨率为480x640的图像被降采样并裁剪到224x304作为输入。

3. **评估指标：** 我们方法的所有结果都使用先前工作中描述的标准评估指标进行评估，包括RMSE、Abs Rel、RMSElog、Sq Rel、log10和Threshold $\delta$ 。设 $d_i$ 和 $\hat{d}_i$ 分别表示在像素位置 $i$ 处的深度真值和估计深度， $i \in [1，N]$ ， $N$ 是深度真值中的有效像素数量。评估指标具体如下：
  - **RMSE：** $\sqrt{\frac{1}{N} \sum_{i=1}^N (\hat{d}_i - d_i)^2}$
  - **Abs Rel：** $\frac{1}{N} \sum_{i=1}^N \frac{|\hat{d}_i - d_i|}{d_i}$
  - **RMSElog：** $\sqrt{\frac{1}{N} \sum_{i=1}^N (\log(\hat{d}_i) - \log(d_i))^2}$
  - **Sq Rel：** $\frac{1}{N} \sum_{i=1}^N \frac{|\hat{d}_i - d_i|^2}{d_i}$
  - **log10：** $\frac{1}{N} \sum_{i=1}^N |\log10(\hat{d}_i) - \log10(d_i)|$
  - **Threshold $\delta$：** 百分比 $\hat{d}_i$ ，满足 $\max \left(\frac{\hat{d}_i}{d_i}，\frac{d_i}{\hat{d}_i} \right) < \delta$ ， $\delta \in \{1.25，1.25^2，1.25^3 \}$ 。


### B. ADGF-Net分析
我们的AGDF-Net仅使用合成数据集进行训练，并在户外和室内场景中直接测试现实世界数据。请注意，训练过程中未使用现实世界数据。为了获得领域泛化的深度估计结果，我们的AGDF-Net分为三个步骤，包括：（1）初始特征提取；（2）增强特征提取；和（3）领域泛化深度估计。请注意，我们的方法以端到端的方式进行训练。

1. **初始特征提取：** 对于初始特征提取，我们的AGDF-Net通过两个编码-解码分支将图像分解为初始深度和弱相关深度部分，仅前者用于随后的跨域泛化深度估计。网络获得的每个部分的最终图被相加以获得完整图像。对于户外场景，初始深度和弱相关深度特征的可视化结果显示在图4(b)和(c)中，分别表示最大分辨率下的 $f_{D_{in}}$ 和 $f_{D_{wr}}$ ，在通道级别上求和进行显示。如图4所示，初始深度特征包含更多与结构相关的信息。弱相关深度特征是从图像中分离出的剩余信息。这与先前的工作一致，证明了深度图主要从图像的结构信息中恢复。此外，我们还提供了从初始深度特征 $D_{in}$ 重建的重建图像（ $I'_ {in}$ ）、从弱相关深度特征 $D_{wr}$ 重建的重建图像（ $I'_ {wr}$ ）、最终重建的图像 $I'$ 、输入图像 $I$ 和重建误差的可视化结果，如图6所示。结果表明， $I'_ {in}$ 和 $I'_ {wr}$ 是最终重建图像的组成部分，两者相加形成最终重建图像，与输入图像几乎相同。重建误差颜色越暗，误差越小，总体重建误差保持在较低水平。对于室内场景，如图5(b)和(c)所示，分别表示初始深度和弱相关深度特征，使用最大分辨率下的 $f_{D_{in}}$ 和 $f_{D_{wr}}$ 在通道级别上求和显示。与户外场景一致，初始深度特征包含更多的整体结构信息，而弱相关深度特征包含从图像中分离出的剩余信息，如瓷砖网格、棋盘格、被子和地毯的纹理等。

为了进一步证明提取的初始深度和弱相关深度特征，这些预训练的两个特征分别作为输入直接用于使用相同的深度估计网络在KITTI上预测深度图。图7显示了预训练的初始深度和弱相关深度特征的结果，以及使用相应特征作为输入的深度图的重新训练结果。请注意，预训练的初始深度特征和弱相关深度特征分别直接传递给相应的新DepthNet模型。在训练期间，初始深度特征（ $E_{in}$ 和 $D_{in}$ ）和弱相关深度特征（ $E_{wr}$ 和 $D_{wr}$ ）被冻结，相应的DepthNet模型从头开始训练。如图7所示，初始深度预测图（ $d_{in}$ ）在物体边界上更加清晰，而弱相关深度预测图（ $d_{wr}$ ）的结果边界模糊，甚至预测错误。为了进行定量比较，我们给出了 $d_{in}$ 和 $d_{wr}$ 的Abs_Rel比较，其中 $d_{wr}$ 远不如 $d_{in}$ （0.344对0.104等）。 $d_{in}$ 的清晰边界表明初始深度特征与深度估计相关。相反，使用从图像中分离出的缺少深度细节信息的弱相关深度特征无法正确估计物体的深度，导致估计结果较差。

2. **增强特征提取：** 对于增强特征提取，应用增强分支进一步优化提取的初始深度特征，增强领域不变信息。最终，通过自适应引导融合获得领域泛化的增强深度特征，并输入最终的深度估计网络。对于户外场景，如图4(d)所示，其中(d)是最大分辨率下的增强深度特征 $f_{D_{it}}$ ，车道线和建筑表面纹理等几乎被去除，形成不随图像领域变化的领域泛化深度特征图。值得注意的是，我们的增强深度特征在天空中有很高的贡献，这与S2R-DepthNet的深度特定图在天空区域的响应更强相同，也与先前的工作相似。这种现象的原因是，具有最远深度值的天空代表消失点，这是深度估计的一个重要线索，并且在不同领域图像中具有相同的无限深度表示，可以视为一种强领域不变深度信息。对于室内场景，如图5(d)所示，瓷砖网格、棋盘格和被子及地毯的纹理等干扰信息几乎被消除。一个有趣的观察是，室内增强深度特征具有更明显的线条和边缘。室内场景的景深比户外场景小，结构信息比户外场景丰富。此外，户外场景的天空区域具有大值，图像中深度范围的分布是非线性的。先前的工作证明，图像结构对深度估计有更强的贡献，因此我们的增强深度特征在景深小的室内场景中比景深大的户外场景中具有更明显的线条和边缘。由于非线性深度范围，户外场景的结构线在可视化中不够明显，但建筑物等整体结构边缘在可视化中清晰可见。

此外，这些领域泛化的增强深度特征在合成领域和真实领域之间几乎一致。然后，增强深度特征输入到随后的深度估计网络中，以估计跨域深度图，并获得出色的结果。

3. **不同风格图像的特征稳定性：** 为了进一步证明提取的初始深度、弱相关深度和增强深度特征，我们提供了同一场景中不同风格图像的三种提取特征，这些图像具有相似的初始深度特征和几乎相同的增强深度特征。如图所示，对于在同一场景中捕获的不同风格的图像，我们的方法可以生成相似的初始深度特征和几乎相同的增强深度特征，而弱相关深度特征对于不同风格具有不同的强度，并且深度细节信息丢失。我们还提供了初始深度特征和增强深度特征的差异图，其中差异图表示相应列的初始深度特征与第一列的初始深度特征之间的差异图，差异图表示相应列的增强深度特征与第一列的增强深度特征之间的差异图。结果表明，相对于自身，第一列的差异图为零，其他列的差异图在初始深度特征上相对较小，在增强深度特征上几乎为零。这些相似的初始深度特征进一步证明了所提出的框架可以提取与深度估计相关的特征。几乎相同的增强深度特征证明了我们提出的增强特征提取过程（具有自适应引导模块）可以获得完全解耦的领域泛化深度特征，用于随后的深度估计。

### C. 实验结果
按照先前的工作，我们仅在合成数据集上进行训练，并在现实世界数据集上进行测试。我们在户外数据集vKITTI到KITTI和室内数据集SUNCG到NYUDv2上进行了实验比较，包括从合成到真实的实验和半监督学习实验。为了展示我们的AGDF-Net的泛化能力，我们提供了从vKITTI到NuScenes、DrivingStereo和CityScapes的比较结果。此外，为了进一步证明提取的领域泛化深度特征的泛化能力，我们还提供了我们的AGDF-Net与使用不同深度估计网络的最先进方法在现实世界深度估计中的比较结果。

1. **vKITTI到KITTI：从合成到真实：** 表I展示了vKITTI到KITTI的实验结果。按照先前的工作，我们与最先进的无监督领域适应方法和领域泛化方法进行了比较。同时，我们还与现实世界的监督深度估计方法进行了比较，其中后者在KITTI数据集上进行了训练，MiDaS在MIX 5数据集上进行了训练，DPT-Hybrid在包含约140万张图像的MIX 6数据集上进行了训练，并在KITTI数据集上进行了微调。请注意，MiDaS的结果在官方预训练模型上使用与我们相同的输入进行评估，DPT-Hybrid的结果来自官方论文，这些结果在KITTI上进行了微调。如表I所示，我们的方法在所有评估指标上优于现有的在vKITTI上训练的最先进方法，在50米和80米的深度范围内，而我们的方法在没有任何现实世界数据的情况下进行训练，结果仍显著优于现有的无监督领域适应方法。同时，我们的方法除vKITTI外，不使用任何外部图像进行训练，并且仍然优于使用外部现实世界数据集的领域泛化方法。具体来说，与先前的方法相比，在80米范围内，Abs_Rel降低了6.06%，Sq_Rel降低了22.35%。此外，图中展示了定性比较，与先前的方法相比，我们的方法可以在深度中获得更明显的结构和细节。

**半监督学习：** 按照先前的方法，我们选择了现实世界捕获的KITTI的前1,000帧进行进一步训练，这仅包含总数据集的4.42%。这更符合实际使用场景，可以视为一种半监督设置。在相同设置下，这里比较了半监督方法和基于领域泛化的半监督学习方法，如表II所示。我们的方法在所有指标上优于比较方法。具体来说，我们的方法在领域泛化训练过程中不使用vKITTI之外的任何额外数据集，并且仍然优于使用外部数据集的先前方法在所有指标上的表现。此外，尽管另一种方法使用更多的KITTI帧和立体对进行训练，我们的方法仍在大多数评估指标上超过了另一种方法。

2. **SUNCG到NYUDv2：** 接下来，我们报告了室内场景中的SUNCG到NYUDv2传输实验，这意味着网络在SUNCG上训练，并直接在NYUDv2上测试。按照先前的工作，我们使用Eigen 654分割作为测试数据集。为了公平比较，我们在相同设置下重新训练了先前的方法，输入大小为224x304。结果表明，我们的方法在室内场景的所有指标上优于先前的方法，Abs_Rel降低了2.93%。此外，我们还提供了基于深度学习的现实世界监督方法的比较结果以供参考，其中前两种方法在NYUDv2数据集上进行了训练，MiDaS是在MIX 5数据集上训练的强监督方法，DPT-Hybrid是另一种在包含约140万张图像的MIX 6数据集上训练并在NYUDv2数据集上进行微调的强监督方法。请注意，MiDaS的结果在官方预训练模型上使用与我们相同的输入进行评估，DPT-Hybrid的结果来自官方论文，这些结果在NYUDv2上进行了微调。为了评估我们方法在室内场景中的有效性，我们在图中报告了NYUDv2上的定性结果，表明我们的方法可以比先前的方法捕获更完整的物体边界，如椅子、台灯等。


### D. 更多数据集上的泛化
然后，我们验证了在不同数据集上的跨数据集泛化性能，包括从vKITTI到NuScenes、DrivingStereo和CityScapes。换句话说，网络在合成的vKITTI数据集上进行训练，并直接在NuScenes、DrivingStereo和CityScapes上进行评估。

**NuScenes：** NuScenes数据集是一个大型自动驾驶数据集，包含3D对象注释，包括1,000个驾驶场景，其中850个场景用于训练和验证，150个场景用于测试。在本文中，我们随机选择了测试集中的174帧进行评估。在评估中，原始图像分辨率首先从900x1600裁剪到480x1600，然后降采样到192x640。

**DrivingStereo：** DrivingStereo数据集是一个大型立体数据集，包含超过180 k张图像，涵盖多种驾驶场景。我们从测试数据中随机选择463帧进行评估。原始图像分辨率首先从800x1762裁剪到528x1762，然后降采样到192x640进行评估。

**CityScapes：** CityScapes数据集是一个大型数据集，包含在50个不同城市记录的街景立体视频序列。我们从测试数据中随机选择495帧进行评估。原始图像分辨率首先从1024x2048裁剪到614x2048，然后降采样到192x640进行评估。

为了公平比较，所有方法在相同设置下进行评估，与vKITTI到KITTI一致。请注意，先前的方法在不同数据集上使用官方提供的预训练模型进行评估。如表IV所示，我们的方法在所有指标上优于无监督领域适应方法和领域泛化方法。图中显示了定性结果，从中可以看出，我们的方法可以更好地估计物体边界，如人、车、树干、标志等。



![](https://img-blog.csdnimg.cn/direct/0e14cee001df4734a3870a9b518d08a0.png)


![](https://img-blog.csdnimg.cn/direct/2970edc98e31405ab5b83e9b0070c0bd.png)



### E. 不同深度估计网络上的泛化
为了进一步证明我们方法的泛化性，我们展示了使用不同深度估计网络的AGDF-Net和最先进方法的实验结果，包括先前工作中使用的DepthNet网络（表V中的DepthNet），以及不使用ImageNet预训练的常用ResNet深度估计网络（表V中的RB-Net）和使用ImageNet预训练的常用ResNet深度估计网络（表V中的RB-Net（预训练））。请注意，使用RB-Net作为深度估计网络时，重新训练了先前方法的结果，使用官方提供的代码。此外，如表V所示，我们的方法在使用预训练模型和不使用预训练模型时都可以获得更好的结果，其中使用和不使用预训练模型的Abs_rel分别为0.164和0.168。如表V所示，使用DepthNet和RB-Net，先前方法的Abs_rel结果分别为0.165和0.186，而我们方法的结果分别为0.155和0.168。我们的方法在不同深度估计网络下所有评估指标上都有更稳定的结果。此外，学习到的领域泛化深度特征与随后的深度估计网络完全解耦。因此，这些特征可以与任何其他深度估计网络级联，以提高深度估计的泛化性，并进一步缩小合成领域和现实领域之间的差距。





### F. 推理时间
我们的方法实现了实时性能，使用GTX 1080 GPU在KITTI数据集上的平均推理时间为0.037秒。输入图像的大小为192x640。


### G. 消融研究

我们在户外场景vKITTI到KITTI中提供了消融研究，如表VI、VII、VIII和IX所示。所有结果均在80米深度范围内获得。

**初始特征提取（INE）：** 我们网络的基线与[16]、[20]相同，结果如表VI的第一行所示。初始特征提取（INE）过程包括初始分支（INB）和弱相关分支（WRB），其中初始分支用于提取初始深度特征，弱相关分支用于提取弱相关深度特征。由INE提取的初始深度特征直接输入到基线网络进行深度估计，结果如表VI的第二行所示。结果表明，使用我们框架提取的初始深度特征进行深度估计可以显著改善Abs_rel，从0.230提高到0.173，相比于直接将图像输入到DepthNet[20]。比较显示，将图像分离为初始深度和弱相关深度成分，可以初步提取与跨域深度估计相关的深度信息，并从合成场景泛化到真实场景。

具体来说，为了分析INE过程中的每个模块及其损失函数的有效性，消融研究结果如表VII所示，其中INB和WRB分别表示初始深度分支和弱相关深度分支。当将INB和WRB都添加到基线时（第二行），Abs_rel从0.184降至0.179，相比于仅添加INB（第一行）。这表明，将图像分离为初始深度和弱相关深度成分对跨域泛化深度估计是有效的，可以提高泛化性。请注意，将INB和WRB添加到基线后，使用重建损失（ $L_{rec}$ ）约束两个分支的图像重建回输入图像。表VII的第三行显示了添加相反损失（ $L_{con}$ ）的结果，Abs_rel从0.179降至0.173。这表明，相反损失可以有效地解耦初始分支和弱相关分支的特征，获得更多泛化的初始深度特征。

另一个有趣的结果是仅将初始分支（沙漏架构）添加到基线的改进（Abs_Rel 0.230对0.184）。如表VIII所示，我们提供了在深度估计网络前级联多个（1-4）沙漏架构的实验。随着从2到4的沙漏结构的堆叠，对深度估计结果的影响很小。证明了在深度估计网络前堆叠沙漏网络的结果是有限的，仅通过级联沙漏架构无法提取与深度估计相关的特征。此外，我们的方法通过初始和弱相关分支可以将性能从0.184提高到0.155，这也证明了所提出的框架可以从图像中分离出与深度估计相关的有效特征。

**增强特征提取（ITE）：** 增强特征提取过程（ITE）旨在进一步优化提取的初始深度特征，去除剩余的干扰信息，并增强领域不变信息以进行深度估计。为了分析ITE过程中所提出的自适应引导融合模块的有效性，消融研究结果如表VI所示，其中ITE表示整个增强特征提取过程。结果显示，将ITE添加到初始特征提取（INE）可以显著提高Abs_rel，从0.173提高到0.155。这表明，所提出的自适应引导融合模块可以有效地增强深度估计的领域不变信息，进一步提高网络从合成领域到真实领域的泛化能力。

**自适应引导卷积核分析：** 我们设计了不同大小的卷积核在AG中进行特征融合。表IX提供了不同大小的卷积核特征融合性能。行1-3显示了仅学习1×1、3×3和5×5卷积核参数并引导特征融合的结果。第4行显示了在引导特征融合前添加三种大小卷积核的结果。结果显示，与学习单个卷积核参数相比，多种卷积核的融合可以增强不同感受野中的特征，从而获得更好的深度估计结果。



![](https://img-blog.csdnimg.cn/direct/9c64d02b040c4a00ac887535930dfe2a.png)




### H. 失败案例

图12展示了我们方法（行(f)）和S2R-DepthNet[20]（行(e)）的一些失败示例。结果表明，我们的方法和S2R-DepthNet在极端恶劣条件下表现不佳，如雾（第一列）和黑暗（第二、三列）。紫色圈出的区域特别指出了图像中能见度低和远处物体（标志或房屋）的不完美深度估计结果，我们的方法无法估计出完整的圈出标志或房屋，而S2R-DepthNet无法恢复它们。由于在极端恶劣条件下（如雾或黑暗）图像收集信息有限，一些物体被雾遮挡或由于黑暗轮廓不清楚。这限制了所提出的框架分离深度相关信息和提取领域泛化特征，最终导致深度估计失败。极端恶劣条件一直是深度估计领域未解决的问题之一。当前大多数深度估计方法，如S2R-DepthNet和我们的方法，都无法在这种条件下很好地估计深度。未来的研究可以集中在低能见度图像的深度估计。通过设计恶劣条件下的分离模块，可以获得更普遍的深度估计结果，或通过预图像处理改善能见度后实现统一的图像深度估计。




![](https://img-blog.csdnimg.cn/direct/038dcf26371946a9beef0803e0707c1b.png)


## V. 结论

在本文中，我们提出了一种具有自适应引导融合的领域泛化深度特征提取网络，即AGDF-Net，以更充分地获取在多尺度特征级别上具有领域泛化性的基本深度特征。我们的方法可以很好地泛化到仅在合成数据上训练的模型的未见现实世界图像。我们通过重建损失和相反损失将图像分
# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
