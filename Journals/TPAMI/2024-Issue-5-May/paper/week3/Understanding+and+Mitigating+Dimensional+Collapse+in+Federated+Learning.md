# [Understanding and Mitigating Dimensional Collapse in Federated Learning](https://ieeexplore.ieee.org/document/10336535/)
## 题目：理解并缓解联邦学习中的维度塌缩问题
**作者：Yujun Shi , Jian Liang , Member, IEEE, Wenqing Zhang , Chuhui Xue ,
Vincent Y. F. Tan , Senior Member, IEEE, and Song Bai**  
**源码：https://github.com/bytedance/FedDecorr.**  
****

# 摘要
联邦学习旨在在不同客户端之间协作训练模型，而无需共享数据以考虑隐私问题。然而，这种学习范式的一个主要挑战是数据异质性问题，这指的是不同客户端之间本地数据分布的差异。为了解决这个问题，我们首先研究了数据异质性如何影响全局聚合模型的表示。有趣的是，我们发现异质性数据导致全局模型遭受严重的维度塌陷，其中表示倾向于位于较低维度的空间而不是整个空间。这种维度塌陷现象严重限制了模型的表达能力，导致性能显著下降。接下来，通过实验，我们进行了更多的观察，并提出了导致这种现象的两个原因：1）局部模型的维度塌陷；2）全局模型参数的全局平均操作。此外，我们理论上分析了梯度流动态，以阐明数据异质性如何导致维度塌陷。为了解决由数据异质性引起的这个问题，我们提出了FEDDECORR，一种新的方法，可以有效减轻联邦学习中的维度塌陷。具体来说，FEDDECORR在本地训练期间应用了一个正则化项，鼓励表示的不同维度是不相关的。FEDDECORR易于实现且计算效率高，在包括CIFAR10、CIFAR100、TinyImageNet、Office-Caltech10和DomainNet在内的五个标准基准数据集上，一致地改进了各种基线。关键词包括：联邦学习、表示学习、分布偏移、维度塌陷。
# 关键词
- 联邦学习
- 表示学习
- 分布偏移
- 维度塌陷

# 引言
随着深度学习的快速发展和大量数据的可用性，对数据隐私的担忧越来越受到工业界和学术界的关注。为了解决这一问题，[1]提出了联邦学习——一种分散式训练范式，允许在不同客户端之间协作训练，而无需共享数据。

联邦学习的一个主要挑战是客户端之间本地训练数据分布的潜在差异，这被称为数据异质性问题。特别是，本文的分析重点关注标签分布的异质性（见图1(a)示例）。这种差异可能导致客户端的局部最优解与期望的全局最优解之间存在巨大差异，从而导致全局模型性能严重下降。先前尝试解决这一挑战的工作主要集中在模型参数上，无论是在本地训练[2]、[3]期间，还是在全球聚合[4]期间。然而，这些方法通常会导致计算负担过重或通信成本高昂[5]，因为深度神经网络通常严重过参数化。与这些工作不同，我们专注于模型的表示空间，研究数据异质性的影响。

首先，我们在第III-A节中研究了异质性数据如何影响联邦学习中的全局模型。具体来说，我们比较了在不同数据异质性程度下训练的全局模型产生的表示。由于协方差矩阵的奇异值提供了高维嵌入分布的全面描述，我们使用它来研究每个全局模型输出的表示。有趣的是，我们发现随着数据异质性程度的增加，更多的奇异值趋向于零。这一观察表明，更强的数据异质性导致训练的全局模型遭受更严重的维度塌陷，其中表示偏向于位于较低维度的空间（或流形）。图1(b)-(c)以图形方式说明了异质性训练数据如何影响输出表示。我们的观察表明，维度塌陷可能是联邦学习方法在数据异质性下表现不佳的主要原因之一。本质上，维度塌陷是模型的一种过度简化，其中表示空间没有被充分利用来区分不同类别的多样化数据。

基于对全局模型的观察，我们继续探索这种现象的原因。首先，由于全局模型是通过对每个客户端上本地训练的模型进行聚合而得到的，我们推测全局模型的维度塌陷背后的一个原因是局部模型的维度塌陷。为了验证这一猜想，我们在第III-B节中通过表示协方差矩阵的奇异值可视化了局部模型。我们观察到与全局模型中类似的维度塌陷现象。

接下来，在第III-C节中，我们研究了全局平均操作本身是否会导致维度塌陷。具体来说，我们比较了全局模型和局部模型产生的表示，就其表示协方差矩阵的奇异值而言。令人惊讶的是，我们发现全局模型的奇异值始终低于局部模型的奇异值。这表明，将局部模型的参数平均到一个全局模型中的全局平均操作，导致与原始局部模型相比，全局模型上发生了维度塌陷。

为了进一步发展对维度塌陷的更严格理解，我们在第III-D节中分析了本地训练期间的梯度流动态。有趣的是，我们理论上展示了异质性数据如何驱动局部模型的权重矩阵偏向低秩，这进一步导致表示的维度塌陷。

受到上述观察和分析的启发，我们在第IV节中开发了一种在本地训练期间减轻维度塌陷的方法。具体来说，我们提出了一种名为 FEDDECORR 的新型联邦学习方法。FEDDECORR 在本地训练期间增加了一个正则化项，以鼓励表示的 Frobenius 范数的相关性矩阵保持较小。我们从理论和实证上展示了这一提出的正则化项可以有效减轻维度塌陷（见图1(d)示例）。接下来，在第V节中，通过对包括 CIFAR10、CIFAR100、TinyImageNet、Office-Caltech10 和 DomainNet 在内的标准基准数据集进行广泛的实验，我们展示了 FEDDECORR 一致性地改进了基线联邦学习方法。此外，我们发现 FEDDECORR 在更具挑战性的联邦学习设置中，例如更强的异质性或更多的客户端数量下，取得了更显著的改进。最后，FEDDECORR 具有极低的计算开销，并且可以构建在任何现有的联邦学习基线方法之上，这使其具有广泛的适用性。

我们的贡献总结如下。首先，我们通过实验发现，在联邦学习中，更强的数据异质性导致全局模型的维度塌陷更大。其次，我们发现了全局模型维度塌陷背后的两个潜在原因，即局部模型的维度塌陷和对局部模型参数进行全局平均操作。第三，我们发展了一种理论理解，将数据异质性与维度塌陷联系起来。第四，基于减轻维度塌陷的动机，我们提出了一种名为 FEDDECORR 的新方法，在各种数据异质性设置下均取得了一致的改进，同时易于实现且计算效率高。

这项工作扩展了我们在 ICLR2023 上提出的会议论文“Towards Understanding and Mitigating Dimensional Collapse in Federated Learning”。与会议版本相比，我们做出了以下实质性改进：
- 我们发现了一个新的原因，可以导致全局模型上的维度塌陷，即平均局部模型参数的操作。我们还实证展示了我们提出的方法 FEDDECORR 可以减轻由此新发现的原因引起的维度塌陷的不利结果。
- 与会议版本不同，我们实证展示了 FEDDECORR 甚至可以用于标签异质性之外的情况，并在域异质性下的联邦学习中改进分类性能。
- 我们增加了更多的实验来更好地验证我们提出的方法 FEDDECORR。这包括：1) 在一种新的标签异质性分区类型：病理非iid分区上展示 FEDDECORR 的有效性的实验；2) FEDDECORR 与先前方法在计算效率方面的比较；3) FEDDECORR 与其他去相关方法的比较；4) 在不同的神经网络架构上的实验。
- 我们对摘要、介绍、方法、实验部分进行了完善，并对相关作品进行了更深入的讨论。


# III. 数据异质性导致的维度崩溃

在本节中，我们首先在第III-A节中实证地可视化并比较了在不同数据异质性程度下训练的全局模型的表示。接下来，在第III-B节和第III-C节中，我们提供了实证分析，并揭示了全局模型维度崩溃背后的两个不同原因。最后，在第III-D节中，为了理论上理解我们的观察结果，我们分析了梯度流动力学。

## A. 全局模型的实证观察

我们首先实证地证明更强的数据异质性会导致全局模型的维度崩溃更为严重。具体来说，我们首先将CIFAR100的训练样本分为10个分割，每个分割对应于一个客户端的本地数据。为了模拟客户端之间的数据异质性，如之前的研究[6]、[15]、[52]，我们采样一个概率向量pc = (pc,1, pc,2, ..., pc,K) ∼ DirK(α)，并将类别c ∈ [C] = {1, 2, ..., C}的实例的pc,k比例分配给客户端k ∈ [K]，其中DirK(α)是具有K个类别的狄利克雷分布，α是集中参数；α越小表示数据异质性越强（α = ∞对应于同质设置）。我们设置α ∈ {0.01, 0.05, 0.25, ∞}。

对于由不同α生成的每个设置，我们应用FedAvg [1]来训练CIFAR100上的MobileNetV2 [53]（在补充材料第C节中提供了其他联邦学习方法、模型架构或数据集的观察结果）。接下来，对于每个训练的全局模型，我们计算CIFAR100上N个测试数据点的表示的协方差矩阵Σ = 1 N ∑Ni=1(zi − ¯z)(zi − ¯z)⊤。这里的zi是第i个测试数据点，¯z = 1 N ∑Ni=1 zi是它们的平均值。

最后，我们对每个协方差矩阵应用奇异值分解（SVD），并在图2中可视化前100个奇异值。如果我们定义一个小值τ作为奇异值显著性的阈值（例如，log τ = −2），我们观察到在同质设置中，所有的奇异值都是显著的，即它们超过了τ。然而，随着α的减小，超过τ的奇异值数量单调减少。这意味着随着本地训练数据之间异质性的增强，训练出的全局模型产生的表示向量倾向于驻留在更低维的空间中，对应于更严重的维度崩溃。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/3e970b90e8414b7ab707e3f4b250803e.png" width="70%" /> </div>


## B. 本地模型的实证观察

由于全局模型是通过聚合每个客户端上本地训练的模型获得的，我们推测全局模型维度崩溃的一个原因是本地模型的维度崩溃。为了进一步验证这一猜想，我们继续研究增加数据异质性是否也会导致本地训练模型的维度崩溃更为严重。

具体来说，对于不同的α，我们可视化了一个客户端的本地训练模型（在补充材料第D节中提供了其他客户端本地模型的可视化）。按照第III-A节中的相同程序，我们绘制了本地模型产生的表示的协方差矩阵的奇异值。我们从图3中观察到，本地训练模型表现出与全局模型相同的趋势——即更强的数据异质性导致更严重的维度崩溃。这些实验证实了全局模型从本地模型继承了不利的维度崩溃现象。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/7b9064ff64714b688a7969e113238c05.png" width="70%" /> </div>


## C. 本地模型与全局模型之间的比较

为了进一步理解为什么全局模型在数据异质性下会遭受维度崩溃，我们提供了本地模型和全局模型之间表示的比较。具体来说，在不同的α下，我们在图4中绘制了本地和全局模型输出表示的协方差矩阵的奇异值。我们还定义了以下度量R来定量比较本地和全局模型的曲线：

$$
R = \frac{1}{K} \sum_{k=1}^{K} \log \frac{\lambda^{(l)}_k}{\lambda^{(g)}_k},
$$

其中K是奇异值的总数， $\lambda^{(l)}_k$ 和 $\lambda^{(g)}_k$ 分别是本地模型和全局模型曲线的第k个奇异值。较小的R表示两曲线之间的差距较小。

从结果中，我们观察到在同质情况（图4(d)）下，本地和全局模型的曲线几乎相同，表明在这种情况下全局平均不会对全局模型造成维度崩溃。

然而，随着异质性的加剧（图4(a)–(c)），全局模型的曲线始终低于本地模型的曲线，它们之间的差距增加。因此，我们已经证明了在数据异质性下，本地模型的全局平均操作也会导致全局模型的维度崩溃。

尽管我们已经确定了全局模型维度崩溃的两个不同原因，但根据我们的可视化，本地模型的维度崩溃是更突出的一个因素。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/66d7aaba11384f43b6e86c96f73e2239.png" width="70%" /> </div>

## D. 维度崩溃的理论解释

基于之前对本地模型维度崩溃的实证观察，我们现在发展了一种理论理解，以解释为什么异质性训练数据会导致学习到的表示发生维度崩溃。

由于我们已经观察到本地模型的维度崩溃是导致全局模型维度崩溃的主要因素，我们在本节中将注意力集中在本地模型上。不失一般性，我们研究一个任意客户端的本地训练。具体来说，我们首先分析本地训练期间模型权重的梯度流动力学。这一分析显示了异质性本地训练数据如何驱动模型权重趋向于低秩，从而导致表示的维度崩溃。

*1) 设置和符号：*我们表示训练样本的数量为N，输入数据的维度为din，类别总数为C。第i个样本表示为Xi ∈ Rdin，其对应的独热编码标签为yi ∈ RC。所有N个训练样本的集合表示为X = [X1, X2 ..., XN] ∈ Rdin×N，N个独热编码的训练标签表示为y = [y1, y2, ..., yN] ∈ RC×N。

为了简化表述，我们遵循[44]、[45]和[40]的工作，分析线性神经网络（没有非线性激活层）。我们考虑一个(L + 1)层的线性神经网络（其中L ≥ 1），使用交叉熵损失在梯度流（即具有无限小学习率的梯度下降）下进行训练。第i层（i ∈ [L + 1]）的权重矩阵（在优化时间步t）表示为Wi(t)。动态可以表示为：

$$
\dot{W}_ i(t) = -\frac{\partial}{\partial W_i} \ell(W_1(t), ..., W_{L+1}(t)),
$$

其中ℓ表示交叉熵损失。

此外，在优化时间步t和给定输入数据Xi时，我们表示zi(t) ∈ Rd为输出表示向量（d是表示的维度），γi(t) ∈ RC为输出softmax概率向量。我们有：

$$
\gamma_i(t) = \text{softmax}(W_{L+1}(t)z_i(t)) = \text{softmax}(W_{L+1}(t)W_L(t) ... W_1(t)X_i).
$$

我们定义μc = Nc/N，其中Nc是类别c的数据样本数量。我们表示ec为C维的独热向量，其中只有第c个条目是1（其他为0）。此外，让¯γc(t) = (1/Nc) ∑Ni=1 γi(t)1{yi = ec} 和 ¯Xc = (1/Nc) ∑Ni=1 Xi1{yi = ec}。

*2) 梯度流动力学分析：*由于我们的目标是分析模型表示zi(t)，我们专注于直接产生表示的权重矩阵（即前L层）。我们表示前L层权重矩阵的乘积为Π(t) = WL(t)WL−1(t) ... W1(t)，并分析Π(t)在梯度流动力学下的行为。特别是，我们推导出Π(t)的奇异值的以下结果。

*定理1（非正式）：*假设如补充材料第A.3节所述的温和条件成立。让σk(t)对于k ∈ [d]是Π(t)的第k个最大奇异值。那么，

$$
\dot{\sigma}_ k(t) = N \frac{(\sigma_k(t))^2 - \frac{2}{L} \sum_{(\sigma_k(t))}^2}{L} + M (u_{L+1,k}(t))^\top G(t)v_k(t),
$$

其中uL+1,k(t)是WL+1(t)的第k个左奇异向量，vk(t)是Π(t)的第k个右奇异向量，M是一个常数，G(t)定义为

$$
G(t) = \sum_{c=1}^{C} \mu_c (e_c - \bar{\gamma}_c(t)) \bar{X}_c^\top,
$$

其中μc, ec,  $\bar{\gamma}_c(t)$ ,  $\bar{X}_c$ 在 (3) 后定义。

定理 1 的精确版本的证明提供在补充材料第 A 节。

基于定理 1，我们可以解释为什么更大的数据异质性会导致 $\Pi(t)$ 偏向于成为低秩矩阵。注意到强数据异质性会导致一个客户端的局部训练数据在每个类别的数据样本数上高度不平衡（回想图 1(a)）。这意味着 μc，即类别 c 数据的比例，对某些类别将接近 0。

接下来，基于 (5) 中 G(t) 的定义，更多的 μc 接近 0 会导致 G(t) 偏向于低秩矩阵。如果是这样，那么 (4) 中的项 $(u_{L+1,k}(t))^\top G(t) v_k(t)$ 将只为较少的 k 值显著（幅度大）。这是因为 $u_{L+1,k}(t)$ 和 $v_k(t)$ 都是奇异向量，在不同的 k 中是正交的。这进一步导致 (4) 左边的 $\dot{\sigma}_k(t)$ ，即 σk 的演化速率，在训练过程中对大多数 k 来说都很小。这些观察结果意味着 $\Pi(t)$ 的只有相对较少的奇异值在训练后会显著增加。

此外， $\Pi(t)$ 偏向于成为低秩矩阵将直接导致表示的维度崩溃。为此，我们只需将表示的协方差矩阵表示为 $\Pi(t)$ 的函数：

$$
\Sigma(t) = \frac{1}{N} \sum_{i=1}^{N} (z_i(t) - \bar{z}(t)) (z_i(t) - \bar{z}(t))^\top = \Pi(t) \left( \frac{1}{N} \sum_{i=1}^{N} (X_i - \bar{X})(X_i - \bar{X})^\top \right) \Pi(t)^\top.
$$

从 (6) 中，我们观察到如果 $\Pi(t)$ 演化成低秩矩阵，Σ(t) 也将倾向于成为低秩矩阵，这对应于图 3 中观察到的更强的维度崩溃。

# IV. 使用 FEDDECORR 缓解维度塌陷

## A. 方法开发

受到上述观察和分析的启发，我们探讨了如何缓解由数据异质性引起的过度维度塌陷问题。由于局部模型的维度塌陷是导致全局模型维度塌陷的主要原因，我们提出在局部训练期间缓解这一问题。一种自然的方式是在训练期间对表示添加以下正则化项：

$$
L_{singular}(w, X) = \frac{1}{d} \sum_{i=1}^{d} \left( \lambda_i - \frac{1}{d} \sum_{j=1}^{d} \lambda_j \right)^2
$$

这里 $\lambda_i$ 是表示协方差矩阵的第 $i$ 个奇异值。本质上， $L_{singular}$ 对奇异值之间的方差进行惩罚，从而阻止尾部奇异值塌陷至 0，缓解维度塌陷。然而，这个正则化项并不实用，因为它需要计算所有的奇异值，这在计算上是昂贵的。

因此，为了得到一个计算成本低廉的训练目标，我们首先对所有的表示向量 $z_i$ 应用 z 分数标准化，如下所示：

$$
\hat{z}_i = \frac{z_i - \bar{z}}{\sqrt{\text{Var}(z)}}
$$

这使得 $\hat{z}_i$ 的协方差矩阵等于其相关矩阵（即相关系数矩阵）。下面的命题提出了一个更便捷的成本函数来规范：

**命题 1**：对于一个 $d \times d$ 的相关矩阵 $K$ 与奇异值 $(\lambda_1, ..., \lambda_d)$，我们有

$$
\sum_{i=1}^{d} \left( \lambda_i - \frac{1}{d} \sum_{j=1}^{d} \lambda_j \right)^2 = \| K \|_F^2 - d
$$

其中 $\| K \|_F$ 是矩阵 $K$ 的弗罗贝尼乌斯范数。

**证明** 可以在补充材料的 B 节中找到。这个命题表明，通过规范相关矩阵的弗罗贝尼乌斯范数 $\| K \|_ F$ 可以达到与最小化 $L_{singular}$ 相同的效果。与奇异值不同， $\| K \|_F$ 可以高效计算。

利用这个命题，我们提出了一种新的方法，称为 FEDDECORR。FEDDECORR 在每个客户端的局部训练期间规范表示向量的相关矩阵的弗罗贝尼乌斯范数。正式地，所提出的正则化项定义为：

$$
L_{FedDecorr}(w, X) = \frac{1}{d^2} \| K \|^2_F
$$

这里 $w$ 是模型参数， $K$ 是表示的相关矩阵。每个局部客户端的总体目标是

$$
\min_w \ell(w, X, y) + \beta L_{FedDecorr}(w, X)
$$

这里 $\ell$ 是交叉熵损失， $\beta$ 是 FEDDECORR 的正则化系数。计算 FEDDECORR 损失的伪代码在算法 1 中提供。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/f4668051c3e84b14b1ca923b447eb981.png" width="70%" /> </div>


## B. FEDDECORR 的有效性

为了见证 LFedDecorr 在缓解维度塌陷中的效力，我们在异质设置下实现该方法，并在 $\alpha \in \{0.01, 0.05\}$ 的情况下进行比较。

首先，我们可视化了局部训练模型的表示。我们在图 5 中绘制了结果。如预期，应用 FEDDECORR 鼓励尾部奇异值不塌陷至 0，从而有效地缓解了局部模型的维度塌陷。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/d4a3dfa91f7f42f89da27d24128caa27.png" width="70%" /> </div>


接下来，我们比较了局部模型和全局模型的表示，以验证 FEDDECORR 是否可以缓解由全局平均化引起的维度塌陷。我们在图 6 中显示了结果。我们观察到应用 FEDDECORR 有助于有效地减少定义在 (1) 中的 $R$ ，表明局部和全局模型的奇异值曲线之间的差距较小，从而缓解了全局平均化引起的维度塌陷。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/469da333a5a043b4962addb82df2e8f1.png" width="70%" /> </div>


最后，我们可视化了全局模型的表示。结果在图 7 中显示，这表明应用 FEDDECORR 确实可以最终缓解全局模型的维度塌陷。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/8446581be1294c62aa2a6a848e682c45.png" width="70%" /> </div>


在本节中，我们已经展示了应用 FEDDECORR 可以帮助缓解在第 III 节 B 和 C 中研究的全局模型维度塌陷背后的两个因素。

# V. 实验
## A. 实验设置
**数据集**。在我们的实验中，我们模拟了具有多个客户端进行本地训练和一台参数服务器执行全局聚合的联合学习场景。我们采用了三个数据集，即 CIFAR10 [54]、CIFAR100 [54] 和 TinyImageNet [55]，以评估标签异质性设置下的性能。CIFAR10 和 CIFAR100 都有 50,000 个训练样本和 10,000 个测试样本，每个图像的大小为 32×32。TinyImageNet 包含 200 个类别，有 100,000 个训练样本和 10,000 个测试样本，每个图像的大小为 64×64。

我们使用两种方案为每个客户端生成本地数据。第一种方法涉及采样一个概率向量 $( p_c = (p_{c,1}, p_{c,2}, ..., p_{c,K}) \sim \text{DirK}(\alpha)$ 并为类别 $c \in [C] = \{1, 2, ..., C\}$ 的实例分配 $p_{c,k}$ 的比例给客户端 $k \in [K]$ ，其中 $\text{DirK}(\alpha)$ 是具有 K 类别的狄利克雷分布，α 是集中参数；α 越小表示异质性水平越高（α = ∞ 对应于同质设置）。该方法遵循[6]，[15]，[52]。第二种方法遵循[1]中的病理non-iid分区，并将M个类的数据分配给每个客户端。在这种划分下，M越小，异质性就越强。在下面的章节中，我们将第一种分区方案称为狄利克雷分区，第二种方案称为病态non-iid分区。

我们使用两个数据集，即 Office-Caltech10[56]和DomainNet[57]，来评估我们和竞争的方法在域异质性下的性能。Office-Caltech10是一个在不同的相机或环境中获取的数据集，总共包含4个域。DomainNet是一个具有来自不同样式的图像的数据集，总共包含6个域。我们使用一个域的数据作为一个客户端的本地数据。

**实现细节**：对于标签异质性实验，我们使用 MobileNetV2 [53]。我们在 CIFAR10/100 数据集上的实验运行了 100 轮通信，在 TinyImageNet 数据集上运行了 50 轮通信。我们在每个通信轮中进行 10 轮本地训练，使用 SGD 优化器，学习率为 0.01，动量为 0.9，批量大小为 64。对于 CIFAR10，权重衰减设置为 10^-5，对于 CIFAR100 和 TinyImageNet，权重衰减设置为 10^-4。我们在 CIFAR100 和 TinyImageNet 实验中应用了 [58] 中的数据增强。FEDDECORR 的 β（即公式 (10) 中的 β）调整为 0.1（见第 V-E 节的消融研究）。

在域异质性实验中，我们遵循 [28] 中的设置。具体来说，我们为所有实验使用带有批量归一化的 AlexNet [59]。我们在 Ofﬁce-Caltech10 和 DomainNet 上的所有实验中运行了 300 轮通信。我们在每个通信轮中进行 1 轮本地训练，使用 SGD 优化器，学习率为 0.01，SGD 动量参数为 0.9，批量大小为 64。权重衰减设置为 10^-5。当将 FEDDECORR 应用于 FedBN 时，我们也将 FedDecorr 损失直接添加到本地训练中。

对于所有实验，FedProx [2] 的正则化系数 μ 在 {10^-4, 10^-3, 10^-2, 10^-1} 中调整，并选择为 μ = 10^-3；MOON [6] 的正则化系数 μ 在 {0.1, 1.0, 5.0, 10.0} 中调整，并选择为 μ = 1.0；FedAvgM [13] 的服务器动量参数 ρ 在 {0.1, 0.5, 0.9} 中调整，并选择为 ρ = 0.5。

## B. FEDDECORR 在标签异质性下提高基线方法的性能
为了验证我们方法的有效性，我们将 FEDDECORR 应用于四个基线方法，即 FedAvg [1]、FedAvgM [13]、FedProx [2] 和 MOON [6]。此外，我们还与另外两个基线方法进行了比较，即 Scaffold [3] 和 FedNova [4]。我们首先应用狄利克雷分割（在第 V-A 节的第二段中描述），并将三个基准数据集（CIFAR10、CIFAR100 和 TinyImageNet）分成 10 个客户端，α ∈ {0.05, 0.1, 0.5, ∞}。由于 α = ∞ 对应于同质设置，在这种设置下训练的模型应该不受过度维度崩溃的不利影响，我们只期望 FEDDECORR 在这种设置下与基线方法表现相当。狄利克雷分割的实验结果如表 I 所示。接下来，我们应用病理性非iid分割（也在第 V-A 节的第二段中描述）并将三个基准数据集分成 10 个客户端，对于 CIFAR10，M ∈ {3, 4, 5}，对于 CIFAR100，M ∈ {20, 30, 40}，对于 TinyImageNet，M ∈ {80, 90, 100}。基于病理性非iid分割的实验结果如表 II 所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/026b23c14feb4fc1be9e8ccb96de9da3.png" width="70%" /> </div>


我们观察到，在所有数据集上的所有异质性设置中，通过在某种基线方法上添加 FEDDECORR 实现了最高的准确率。特别是，在 α ∈ {0.05, 0.1} 或每个数据集的最小 M 的强异质性设置中，添加 FEDDECORR 带来了大约 2% 到 9% 的显著改进。另一方面，在异质性较小的设置中（即较大的 α 或 M），FEDDECORR 带来的改进较小。这是因为在异质性较小的设置中，维度崩溃问题不太明显；这一现象已在第 III 节中讨论。此外，令人惊讶的是，在 α = ∞ 的同质设置中，FEDDECORR 仍然在 TinyImageNet 数据集上产生了约 2% 的改进。我们推测这是因为 TinyImageNet 比 CIFAR 数据集要复杂得多，除了标签的异质性之外，联合学习设置中可能还有其他因素导致不希望的维度崩溃。因此，即使在同质设置中，TinyImageNet 上的联合学习也可以从 FEDDECORR 中受益。

为了进一步展示 FEDDECORR 的优势，我们将 FEDDECORR 应用于 FedAvg 并绘制了全局模型的测试准确率随着通信轮数增加的变化，如图 8 所示。在这个图中，如果我们将某个测试准确率的值设为阈值，我们看到添加 FEDDECORR 显著减少了达到给定阈值所需的通信轮数。这进一步表明 FEDDECORR 不仅提高了最终性能，而且还大大提高了联合学习中的通信效率。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/2c735c900e104a5788b77a1ff14d98bd.png" width="70%" /> </div>


## C. FEDDECORR 在域异质性下提高基线方法的性能
接下来，我们在域异质性设置下评估 FEDDECORR 的有效性，使用两个基准数据集，即 Ofﬁce-Caltech10 和 DomainNet。在这种设置下，我们将一个域的数据分配为一个客户端的本地训练数据。
我们在五个基线方法上应用 FEDDECORR，即 FedAvg [1]、FedProx [2]、FedAvgM [13]、MOON [6] 和 FedBN [28]。域异质性的实验结果如表 III 所示。这些结果表明，在所有基线和所有数据集上，通过添加 FEDDECORR 在平均准确率方面带来了一致的改进，提高了大约 1% 到3%。值得注意的是，FEDDECORR 在应用于如 FedBN 这样的个性化方法时也展示了其有效性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/6ac9dcfa05614139adbfef9bd302b211.png" width="70%" /> </div>


尽管我们对 FEDDECORR 的分析主要集中在标签异质性设置上，但我们通过实验展示了其有效性可以推广到域异质性。我们推测这是因为域异质性也会导致训练出的模型遭受不希望的维度崩溃，而 FEDDECORR 可以有效缓解这种情况。开发对 FEDDECORR 如何在联合学习中对抗域异质性的坚定理论理解是未来研究的课题。

## D. 关于客户端数量的消融研究
接下来，我们研究 FEDDECORR 带来的改进是否会随着客户端数量的增加而保持。我们将 TinyImageNet 数据集分成 10、20、30、50 和 100 个客户端，并根据不同的 α 运行 FedAvg，有和没有 FEDDECORR。对于 10、20 和 30 个客户端的实验，我们运行了 50 轮通信。对于 50 和 100 个客户端的实验，我们在每轮中随机选择 20% 的客户端参与联合学习，并运行 100 轮通信。结果如表 IV 所示。从该表中，我们看到 FEDDECORR 带来的性能改进从大约 3% 到 5% 增加到大约 7% 到 10%，随着客户端数量的增长。因此，有趣的是，我们通过实验表明 FEDDECORR 带来的改进在客户端数量更多的更具挑战性的设置下甚至更加明显。此外，我们在随机客户端参与下的实验结果表明，FEDDECORR 的改进对此类不确定性是稳健的。这些实验展示了 FEDDECORR 应用于现实世界中具有大量客户端和随机客户端参与的联合学习设置的潜力。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0c56789472304773a646d1ced118de54.png" width="70%" /> </div>


## E. 关于正则化系数 β 的消融研究
接下来，我们通过在集合 {0.01, 0.05, 0.1, 0.2, 0.3} 中变化它来研究 FEDDECORR 对 (10) 中 β 的鲁棒性。我们将 CIFAR10 和 TinyImageNet 数据集分成 10 个客户端，α 设置为 0.05 和 0.1 以模拟异质性设置。结果如图 9 所示。我们观察到，一般来说，随着 β 的增加，FEDDECORR 的性能首先增加，然后趋于平稳，最后略有下降。这些结果表明 FEDDECORR 对 β 的选择相对不敏感，这意味着 FEDDECORR 是一种易于调整的联合学习方法。此外，在所有实验设置中，将 β 设置为 0.1 一致地产生（几乎）最好的结果。因此，在没有关于数据集的先验信息时，我们建议选择 β = 0.1。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/10564478ce3f4ba0acc52bec2835db4e.png" width="70%" /> </div>


## F. 关于每轮通信的本地轮数的消融研究
最后，我们对每轮通信的本地轮数 E 进行消融研究。我们将本地轮数 E 设置为集合 {1, 5, 10, 20} 中的值。我们运行有和没有 FEDDECORR 的实验，并使用 CIFAR100 和 TinyImageNet 数据集，α 为 0.05 和 0.1 进行这项消融研究。结果如表 V 所示，其中观察到随着 E 的增加，FEDAVG 的性能首先增加然后减少。这是因为当 E 过小时，本地训练在每轮通信中不能正确收敛。另一方面，当 E 过大时，本地客户端的模型参数可能会偏离全局最优太远。然而，FEDDECORR 在不同的本地轮数 E 选择下始终提高了基线方法的性能。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/ef2ecede7623401ca63ad2b556a456ec.png" width="70%" /> </div>



## G. 计算效率
我们展示了 FEDDECORR 相对于一些竞争对手在计算效率方面的优势。我们比较了 FEDDECORR 与在本地训练期间应用额外正则化项的其他一些方法，如 FedProx 和 MOON。我们将 CIFAR10、CIFAR100 和 TinyImageNet 分割成 10 个客户端，α 设置为 0.5，并报告了一轮训练所需的总计算时间，包括 FedAvg、FedProx、MOON 和 FEDDECORR。具体来说，对于 FedAvg，在本地训练期间只应用了简单的 SGD。结果展示在表 VI 中。所有结果都是使用 NVIDIA Tesla V100 GPU 产生的。我们看到 FEDDECORR 在原始 FedAvg 的基础上仅增加了微不足道的计算开销，而 FedProx 和 MOON 需要大约 0.5 到 1 倍的额外计算成本。FEDDECORR 在效率方面的优势主要是由于它只涉及计算矩阵的 Frobenius 范数，这是一个极其廉价的操作。实际上，这种正则化作用于模型的输出表示向量，既不需要像 FedProx 那样计算参数级正则化，也不需要像 MOON 那样的额外前向传递。

## H. 与其他去相关方法的比较
一些去相关正则化方法，如 DeCov [49] 和 Structured-DeCov [51]，被提出是为了提高标准分类任务中的泛化能力。这两种方法都是直接在表示的协方差矩阵上操作，而不是像我们提出的 FEDDECORR 方法那样在相关矩阵上操作。为了比较我们的 FEDDECORR 与现有的去相关方法，我们遵循 FEDDECORR 的相同程序，在本地训练期间应用 DeCov 和 Structured-DeCov。我们的实验基于 TinyImageNet 和 FedAvg。TinyImageNet 根据各种 α 值分割成 10 个客户端。结果展示在表 VII 中。令人惊讶的是，我们看到与我们稳步提高基线的 FEDDECORR 不同，添加 DeCov 或 Structured-DeCov 都降低了联合学习中的性能。我们推测这是因为直接正则化协方差矩阵可能非常不稳定，导致对表示的不希望的修改。这个实验表明，我们设计的相关矩阵而不是协方差矩阵的正则化对于确保稳定性至关重要。

## I. 在其他模型架构上的实验
在本节中，我们展示了 FEDDECORR 在不同模型架构下的有效性。在这里，除了前几节中使用的 MobileNetV2，我们还对 ResNet18 和 ResNet32 [61] 进行了实验。注意，ResNet18 是 ResNet 的更宽版本，其表示维度为 512；而 ResNet32 是较窄的版本，其表示维度仅为 64。异质性参数 α 设置为 {0.05, 0.1}，我们使用 CIFAR10 数据集。我们的结果展示在表 VIII 中。可以看到，FEDDECORR 在不同的神经网络架构中都取得了一致的改进。一个有趣的现象是，FEDDECORR 带来的改进在更宽的网络（例如，MobileNetV2、ResNet18）上比在较窄的网络（例如，ResNet32）上要大得多。我们推测这是因为更宽网络的周围空间的维度明显高于较浅网络。因此，相对而言，由数据异质性引起的维度崩溃对于更宽的网络来说会更加严重。

# VI. 结论
在本项工作中，我们研究了在客户端数据异质性的联合学习环境下训练出的模型的表示。通过广泛的实证观察和理论分析，我们展示了更强的数据异质性导致了全局和局部表示的更严重的维度崩溃。基于此，我们提出了 FEDDECORR，这是一种新颖的方法，用于缓解维度崩溃，从而在异质数据设置下改进联合学习。在基准数据集上的广泛实验表明，FEDDECORR 取得了比现有基线方法更一致的改进。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
