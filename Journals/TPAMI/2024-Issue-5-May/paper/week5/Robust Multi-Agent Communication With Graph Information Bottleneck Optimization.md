# 题目：[Robust Multi-Agent Communication With Graph Information Bottleneck Optimization](https://ieeexplore.ieee.org/document/10334015)  
## 具有图信息瓶颈优化的鲁棒多Agent通信
**作者：Shifei Ding; Wei Du; Ling Ding; Jian Zhang; Lili Guo; Bo An** 



****

# 摘要

最近关于多智能体强化学习（MARL）的研究表明，通过引入通信学习机制可以显著增强多智能体的动作协调。同时，图神经网络（GNN）为MARL的通信学习提供了一个有前景的范式。在这一范式下，智能体和通信信道可以被视为图中的节点和边，智能体可以通过GNN从邻近智能体聚合信息。然而，这种基于GNN的通信范式容易受到对抗性攻击和噪声扰动的影响，如何在扰动下实现鲁棒的通信学习在很大程度上被忽视了。为此，本文探讨了这个问题，并引入了一个具有图信息瓶颈优化的鲁棒通信学习机制，该机制可以最优地实现通信学习的鲁棒性和有效性。我们引入了两个信息论正则化器来学习多智能体通信的最小充分消息表示。这些正则化器旨在最大化消息表示和动作选择之间的互信息（MI），同时最小化智能体特征和消息表示之间的MI。此外，我们提出了一个可以与现有价值分解方法集成的MARL框架。实验结果表明，所提出的方法比现有的基于GNN的MARL方法更具鲁棒性和效率。

# 关键词

- 图神经网络
- 多智能体强化学习
- 图信息瓶颈优化
- 通信学习

# Ⅰ. 引言

增强学习在解决单智能体复杂任务方面取得了显著进展，例如机器人导航[1]和车辆控制[2]。同时，许多现实世界场景，例如智能交通系统[3]，不仅包含一个智能体，通常涉及多个智能体参与学习合作任务。这些场景自然导致了多智能体强化学习（MARL）的普及，其中关键挑战包括可扩展性和非平稳性（由部分可观测性限制引起）。最近，集中训练与分散执行（CTDE）的范式被广泛采用来应对这些挑战，在该范式中，智能体仅基于分散的本地信息执行操作，但其策略可以使用额外的全局信息进行训练[4]，[5]。价值函数分解方法[6]，[7]，[8]，[9]进一步探索了这一范式，它将全局价值函数分解为一组个体价值函数。然而，这些方法在需要动作协调的场景中仍然表现不佳，因为在执行期间的部分可观测性会增加一个智能体对其他智能体动作的不确定性，导致动作协调困难。

允许智能体学习有效通信可以加强动作协调，最终提高学习策略的质量。为此，提出了许多多智能体通信强化学习方法（MACRL）[10]，[11]，[12]，[13]，[14]，[15]，这些方法允许智能体在分散执行阶段交换消息，例如局部个体观测或智能体特征嵌入。最近，图神经网络（GNN）作为一种高效的表示学习方法得到了发展，它可以处理图结构数据的拓扑信息和属性信息，以特征表示学习为最终任务。GNN已被用于构建MACRL的通信学习机制，通常将智能体和通信信道视为图中的节点和边，动作选择对应于节点标记。许多最先进的MACRL方法都属于这种基于GNN的通信范式，例如TarMAC[16]。

基于GNN的MACRL的关键任务是学习有效的信息表示，该信息表示携带了智能体特征信息和拓扑关系信息，用于动作选择和协调。然而，最近的基于GNN的MACRL方法在信息表示学习上仍然遇到了一些挑战。一方面，信息表示聚合了邻近智能体的特征信息，其中可能包含对动作选择有负面影响的无用信息。另一方面，基于GNN的MACRL依赖于图的边来实现智能体之间的信息交换，这使其容易受到智能体特征和拓扑结构上的对抗性攻击和噪声扰动的影响。

如Fig. 1所示， $D = (A, H)$ 携带了来自图拓扑结构A和智能体特征嵌入H的信息。如果通信信息表示携带了来自A和H的无关信息，它容易受到模型超参数变化、对抗性攻击和D上的噪声扰动的影响。MACRL方法的性能在对抗性攻击下趋于下降，这可能使许多基于MACRL模型的实际应用面临高风险。例如，研究人员已经表明，在多智能体自动驾驶系统中，对多智能体车辆之间通信过程的对抗性攻击可以诱使自动驾驶车辆做出异常判断，例如驶入对面车道[17]。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fee54f9d5b234ecd9cff7f610f2d99c8.png#pic_ center" width="70%" />
</div>

因此，在本文中，我们重新思考了什么是以及如何获得一个“最优”的通信信息表示，它包含足够的信息以促进动作协调，同时避免对抗性攻击的影响。对于一般的表示学习，信息瓶颈（IB）[18]引入了一个重要原则：最优表示应包含对最终任务有用且最少的信息。受这一原则的启发，我们定义最优消息表示为包含足够且最少信息的表示，用于图1所示的动作选择任务。然而，将IB原则扩展到基于GNN的MACRL方法中以获得最优消息时遇到了两个主要挑战。1）图中包含的拓扑结构信息对通信信息表示至关重要，但这些信息是离散的，因此难以优化。2）以前的基于IB的表示学习方法通常限制输入数据满足独立同分布条件（i.i.d.）。因此，IB原则在MACRL中难以实现，因为不支持智能体特征的条件。

为了解决这些挑战，我们提出了一种具有图信息瓶颈优化的多智能体通信机制（MAGI），用于通信信息表示学习。为了解决第一个挑战，我们提出了两个信息论正则化器来派生最小充分通信消息：一个用于限制来自图拓扑结构和智能体特征嵌入的信息，另一个用于最大化消息表示中的动作选择和协调的信息。有了这两个正则化器，我们可以确保学习到的通信消息既充分（有效促进智能体的动作选择）又最小（消息表示不包含不必要的信息）。此外，为了解决由智能体特征的非i.i.d属性引起的第二个挑战，我们利用智能体特征的局部依赖假设来分层捕获图拓扑结构A和智能体特征嵌入H的信息。我们工作的主要贡献总结如下：

1）据我们所知，我们的工作是第一次尝试将图信息瓶颈原则[19]扩展到基于GNN的MACRL方法中，实现了高效且鲁棒的多智能体通信学习。
2）我们提出了两个信息论正则化器来获得最优的消息表示，该表示包含足够且最少的信息，用于动作选择和下游任务的动作协调。
3）我们提出了一个通用的MARL框架，可以灵活地将所提出的通信学习机制与任何价值函数分解方法集成。
4）我们在几个MARL环境上评估了所提出的方法，包括SMAC[20]和MAgent[21]。实验结果表明，MAGI比现有的MACRL方法更加稳健和高效。

# Ⅲ. 方法论

## A. 问题公式化

多智能体强化学习(MARL)问题通常可以被建模为一个分散部分可观测马尔可夫决策过程(Dec-POMDP)，其特征是元组 $\langle I, S, U, P, R, O \rangle$ 。I代表从1到n索引的智能体/代理的有限集合。S代表状态的有限集合。O表示联合观测，其中 $o_ i \in O$ 是代理i的局部观测。U代表联合动作的有限空间。在每个时间步，代理i根据其局部观测 $o_ i$ 采取动作 $a_ i$ 。 $a = (a_ 1, ..., a_ n) \in U$ 表示联合动作。状态根据马尔可夫转移 $P : S \times U \rightarrow S$ 变化。 $R : S \times U \rightarrow R$ 代表奖励函数。代理i的总体任务是最大化其总折扣奖励 $R_ i = \sum_ {t=0}^{T} \gamma^t r_ {t}^i$ ，其中 $\gamma \in [0, 1]$ 表示折扣因子。代理的目标是学习一个最优联合策略 $\pi(\tau, a)$ 来最大化全局价值 $Q_ {\pi}^{tot}(\tau, a) = \mathbb{E}_ {s,a}[\sum_ {t=0}^{\infty} \gamma^t R(s, a)]$ ，其中 $\tau$ 表示联合观测历史。

在我们的工作中，我们使用图 $G = (V, E, H)$ 对多智能体系统进行建模，其中有n个节点，其中 $V = \{1, 2, ... n\}$ 表示代理/节点集合， $E \subseteq V \times V$ 表示边集合， $H \in \mathbb{R}^{n \times f}$ 表示代理特征/节点属性。我们使用 $A \in \mathbb{R}^{n \times n}$ 表示图G的邻接矩阵，如果 $(i, j) \in E$ ，则 $A_ {ij} = 1$ ，否则 $A_ {ij} = 0$ 。我们利用 $d(i, j)$ 表示两个代理i, j( $\in V$ ）之间的最短路径距离。因此，GNN基通信学习机制的输入特征信息数据可以被整体表示为 $D = (A, H)$ 。我们专注于从D中提取代理级消息表示 $M_ H \in \mathbb{R}^{n \times f'}$ ，以便MH可以被用来促进代理选择动作Y。下标带有代理i ∈ V被用来表示与代理i的关系。例如，代理i的通信消息表示由 $M_ {H,i} = m_ i$ 表示，其相应的最优动作选择由 $Y_ i = a_ i$ 表示。

## B. 整体框架

所提出方法的整体框架如图2所示。对于代理i，它获取局部观测 $o_ i$ 并使用门控循环单元(GRU)和多层感知器(MLP)生成代理特征 $h_ i$ 。然后我们将特征 $h_ i$ 输入到通信学习模块，在那里图神经网络(GNN)被用来产生通信消息表示 $m_ i$ 。在我们的工作中，我们使用图注意力网络(GAT)[36]作为通信学习模块中的GNN结构。我们通过堆叠多个GNN层来融合邻居代理的特征信息和拓扑结构信息。每个代理的高层通信消息表示可以通过多轮通信提取出来。此外，我们使用图信息瓶颈优化进一步获得最优的消息表示，这些表示对于决策是最小且充分的。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/69db39d0ce024d188c3aed2caef1b23a.png#pic_ center" width="70%" />
</div>

然后，对于代理i，消息 $m_ i$ 与当前局部历史 $\tau_ i$ 连接，作为输入提供给局部动作价值函数 $Q_ i(\tau_ i, a_ i, m_ i)$ 。如图2所示，我们将所有代理的局部动作值输入到混合网络，并最终获得全局动作价值 $Q_ {tot}$ 的估计。在这项工作中，我们选择了VDN[7]、QMIX[8]和QPLEX[9]提出的混合网络，它可以灵活地被其他现有价值分解方法的混合网络替换。例如，在QMIX的情况下，混合网络由MLPs组成。然而，混合网络的权重和偏差来自超网络，以确保遵守单调性约束。在下一部分中，我们将详细介绍这个框架中的多智能体通信通过图信息瓶颈优化(MAGI)模块。

## C. 通过图信息瓶颈优化的多智能体通信

多智能体通信通过图信息瓶颈(MAGI)模块的目标是获得最优的通信消息表示，这是最小且充分的。为了实现这一目标，MAGI需要通信消息MH最大化选择最优动作Y的互信息，并最小化来自特征D = (A, H)的信息。MAGI的目标在(1)中表示，其中 $I(\cdot; \cdot)$ 表示互信息，

$$
\min_ {MAGI} \beta(D, Y; M) \triangleq -I(Y; M) + \beta I(D; M)
$$

在MAGI中，我们为图结构化代理特征引入了一个局部依赖假设[19]：对于每个代理i，给定在一定跳数内的与邻居相关的代理，其他代理特征与代理i的特征是独立的。我们利用这个假设来限制最优通信消息表示的空间Ω，这使得MAGI目标的优化更加可行。具体来说，我们使用 $P(M_ H | D)$ 来层次化地迭代通信消息表示，以模拟代理特征的相关性，其中 $P(\cdot)$ 表示联合概率分布函数。每个GNN层对应于代理与其邻居之间的一轮通信消息交换。

在每一轮消息交换l中，我们利用局部依赖假设。每个代理的通信消息可以通过聚合其邻居代理的代理特征来细化，相对于图结构 $M^l_ A$ 。因此， $(M^l_ A)_ {1 \leq l \leq L}$ 是通过局部调整原始图拓扑结构A获得的，这本质上可以控制来自图拓扑结构A的消息流。最后，我们使用高层通信消息表示 $M^L_ H$ 来实现动作选择和动作协调。因此，MAGI优化的目标可以简化为以下表示：

$$
\min_ {P(M^L_ H|D) \in \Omega} \beta(D, Y; M^L_ H) \triangleq -I(Y; M^L_ H) + \beta I(D; M^L_ H)
$$

在这个方程中，我们只需要优化这两个序列的分布： $P(M^l_ H | M^{l-1}_ H, M^l_ A)$ 和 $P(M^l_ A | M^{l-1}_ H, A)$ 。这些分布更容易优化，因为它们在代理之间具有局部依赖性。我们利用简单的MAGI原理和一些适当的参数化 $P(M^l_ H | M^{l-1}_ H, M^l_ A)$ 和 $P(M^l_ A | M^{l-1}_ H, A)$ ，然而，(2)中的 $I(Y; M^L_ H)$ 和 $I(D; M^L_ H)$ 的计算仍然不可行。因此，我们应该在(2)的两个项上引入变分界限正则化器，这有助于优化最终目标。正如[18]中所示，我们可以推导出项 $I(Y; M^L_ H)$ 的下界和项 $I(D; M^L_ H)$ 的上界，分别显示在(3)和(4)中。对于任何代理i ∈ V的任何分布 $V_ 1(Y_ i | M^L_ H,i)$ 和 $V_ 2(Y)$ ，项 $I(Y; M^L_ H)$ 的下界显示在(3)中，

$$
I(Y; M^L_ H) \geq 1 + \mathbb{E} \left[ \sum_ {i \in V} \log \frac{V_ 1(Y_ i | M^L_ {H,i})}{V_ 2(Y)} \right] + \mathbb{E}_ {P(Y)P(M^L_ H)} \left[ \sum_ {i \in V} \frac{V_ 1(Y_ i | M^L_ {H,i})}{V_ 2(Y)} \right]
$$

我们选择两组索引 $S_ H, S_ A \subseteq [L]$ ，使得 $D \perp M^L_ H | (M^l_ H)_ {l \in S_ H} \cup (M^l_ A)_ {l \in S_ A}$ ，取决于马尔可夫依赖性，其中 $M^1 \perp M^2 | M^3$ 表示给定M3时M1和M2条件独立。对于任何分布 $V(M^l_ H), l \in S_ H$ 和 $V(M^l_ A), l \in S_ A$ ，

$$
I(D; M^L_ H) \leq I \left( D; (M^l_ H)_ {l \in S_ H} \cup (M^l_ A)_ {l \in S_ A} \right) \leq \sum_ {l \in S_ A} AIBl + \sum_ {l \in S_ H} HIBl
$$

$$
AIBl = \mathbb{E} \left[ \log \frac{P(M^l_ A | A, M^{l-1}_ H)}{V(M^l_ A)} \right]
$$

$$
HIBl = \mathbb{E} \left[ \log \frac{P(Z^l_ X | M^{l-1}_ H, M^l_ A)}{V(M^l_ H)} \right]
$$

方程(4)说明我们应该选择一组满足(4)中的条件独立的随机变量的索引SH和SA。SH和SA具有以下属性：1)如果SH中的最大索引是l，SA包括所有[l + 1, L]中的整数。2)SH ≠ ∅。为了使用MAGI原理，我们应该模拟 $P(M^l_ A | M^{l-1}_ H, A)$ 和 $P(M^l_ H | M^{l-1}_ H, M^l_ A)$ 。

接下来，我们选择某些变分分布 $V(M^l_ H)$ 和 $V(M^l_ A)$ 来估计相应的 $AIBl$ 和 $HIBl$ 的正则化，以及一些 $V_ 1(Y_ i | M^L_ {H,i})$ 和 $V_ 2(Y)$ 来获得(3)中的下界。因此，我们可以通过将(3)和(4)插入(2)来获得优化目标的上界。

在这项工作中，我们利用图注意力网络(GAT)[36]作为通信学习模块中的GNN结构。在图注意力网络的每一层中，MAGI首先需要利用注意力权重来细化代理的图结构，以获得 $M^l_ A$ ，接下来通过在 $M^l_ A$ 上传播 $M^{l-1}_ H$ 来细化通信消息表示 $M^l_ H$ 。对于邻居代理采样，我们利用分类分布，并将注意力权重视为分类分布的参数，这可以采样细化图的拓扑结构，以捕获拓扑结构信息。接下来，我们为代理i从代理池Vic中提取k个邻居代理，其中Vic包含与代理i的最短路径距离为c的代理。我们利用C作为c的上限约束，以保证局部依赖的假设。接下来，我们对邻居代理进行求和池化，并使用输出来计算高斯分布的参数，其中将对细化的消息表示进行采样。

然后，我们旨在优化MAGI模块的参数。需要指定(3)中 $I(Y; M^L_ H)$ 的项的界限和(4)中 $I(D; M^L_ H)$ 的项的界限，进一步需要计算(2)中MAGI优化目标的界限。为了表征(4)中的 $AIBl$ ，我们假设 $V(M^l_ A)$ 满足非信息分布。具体来说，我们利用均匀分类分布： $MA \sim V(MA), MA_ i = \bigcup_ {d=1}^{T} \{ j \in Vic | j \sim \text{Cat}(\frac{1}{|Vic|}) \}$ 并且如果 $i \neq j$ ，则 $MA_ i \perp MA_ j$ 。

我们利用 $\text{Cat}(\phi)$ 表示具有参数 $\phi$ 的分类分布，它对应于不同类别的概率，因此 $\|\phi\|_ 1 = 1$ 。计算 $\phi^l_ {id}$ 后，我们可以获取 $AIBl$ 的经验估计，

$$
\text{AIB}_ l = \mathbb{E}_ {P(M^l_ A|A,M^{l-1}_ H)} \left[ \log \frac{P(M^l_ A | A, M^{l-1}_ X)}{V(M^l_ A)} \right]
$$

这被实例化为如下，

$$
\text{AIB}_ l = \sum_ {i \in V, d \in [T]} \text{KL} \left( \text{Cat}(\phi^l_ {id}) || \text{Cat} \left( \frac{1}{|Vid|} \right) \right)
$$

为了进一步估计 $HIBl$ ，我们进行 $V(M^l_ H)$ 作为高斯分布的混合。具体来说，对于任何代理i， $M_ H \sim V(MH),$ 我们设置 $M_ {H,i} \sim \sum_ {u=1}^{m} w_ u \text{Gaussian}(\mu_ {0,u}, \sigma^2_ {0,u})$ ，其中 $w_ u, \mu_ {0,u}, \sigma_ {0,u}$ 意味着所有代理共享的可学习参数，并且如果 $i \neq j$ ，则 $M_ {H,i} \perp M_ {H,j}$ 。我们可以利用采样的 $M^l_ H$ 估计 $HIBl$ ：

$$
\text{HIB}_ l = \sum_ {i \in V} \left[ \log \Phi \left( \frac{M^l_ {H,i}; \mu_ i, \sigma^2_ i}{\sqrt{2\pi}} \right) - \log \left( \sum_ {u=1}^{m} w_ u \Phi \left( \frac{M^l_ {H,i}; \mu_ {0,u}, \sigma^2_ {0,u}}{\sqrt{2\pi}} \right) \right) \right]
$$

因此，我们可以选择适当的索引集SH, SA，以保证(4)中的假设，并使用替换：

$$
I(D; M^L_ H) \rightarrow \sum_ {l \in S_ A} \text{AIB}_ l + \sum_ {l \in S_ H} \text{HIB}_ l
$$

为了表征(3)，我们可以简单地设置 $V_ 2(Y) = P(Y)$ 和 $V_ 1(Y_ i | Z^L_ {H,i}) = \text{Cat}(M^L_ {H,i})W_ {\text{out}}$ 。因此，(3)可以简化为以下形式的交叉熵损失，不包含常数项，

$$
I(Y; M^L_ H) \rightarrow -\sum_ {i \in V} \text{Cross-Entropy}(M^L_ {H,i}W_ {\text{out}}; Y_ i)
$$

将(10)和(11)插入(2)，可以获得训练我们提出的通信学习模块的MAGI优化目标。

除了MAGI优化约束在通信模块中的通信消息表示学习之外，框架中其他模块的所有参数都通过最小化TD损失 $L_ {TD}$ 来更新。最后，TD损失显示在(12)中，

$$
L_ {TD} = \left( r + \gamma \max_ {a'} Q_ {\text{tot}}(\tau', a'; \theta^-) \right)^2 - Q_ {\text{tot}}(\tau, a; \theta)
$$

其中θ表示提出方法中的所有参数，而θ^-表示目标网络的参数。提出方法的总体优化目标显示如下，

$$
L = L_ {TD} + \lambda L_ {IB}
$$

其中λ表示可以微调的超参数，以实现图信息瓶颈优化损失 $L_ {IB} = \beta(D, Y; M^L_ H)$ 和TD损失 $L_ {TD}$ 之间的权衡。超参数λ被设置为λ = 0.1，取决于实验结果。

提出方法的实例化细节显示在算法1中。步骤1-4显示了特征处理阶段。对于每个时间步t ∈ T，每个代理i ∈ N获取其观测oi，并使用GRU和MLP生成其初始代理特征。步骤5-6描述了使用代理特征构建图的过程。消息表示优化的核心过程在步骤7-18中实例化。在每一层中，MAGI首先利用注意力权重细化图结构以获得 $M^l_ A,i$ （步骤8-13），然后通过在 $M^l_ A,i$ 上传播 $M^{l-1}_ H,i$ 来细化消息表示 $M^l_ H,i$ （步骤14-18）。

步骤14是邻居代理的求和池化操作，其输出将被用来计算高斯分布的参数，其中将对细化的消息表示进行采样。MAGI松散地依赖于图结构，因为A只用于决定每个代理的潜在邻居代理，并且根据MA执行消息传递。在步骤19-22中，代理i根据获得的消息mi计算局部价值，然后选择动作 $a^t_ i$ 。步骤23-27描述了集中训练阶段，该阶段使用(13)中的损失函数。

# IV. 实验

在本节中，我们选择StarCraft II Multi-Agent Challenge (SMAC)[20]和MAgent[21]作为我们的基准。我们在这些复杂的基准上进行了各种实验，以回答以下问题：Q1：所提出的方法和其他基于GNN的MACRL方法是否容易受到敌意攻击和噪声扰动的影响？Q2：MAGI优化能否在敌意攻击和噪声扰动下增强通信学习的鲁棒性？Q3：MAGI能否实现有助于动作协调的高效通信？Q4：MAGI能否应用于大规模多智能体场景？Q5：所提出框架的哪个组件有助于性能提升？Q6：MAGI能否灵活地与现有的价值函数分解方法集成？Q7：超参数如何影响MAGI的性能？Q8：MAGI在更复杂的敌意攻击方法下表现如何？我们选择了QMIX[8]、TarMAC[16]和MAGIC[15]作为基线。QMIX提出了一个混合网络，通过非线性组合个体动作值来估计联合动作值。TarMAC采用具有软注意力机制的GAT来确定处理消息的程度。MAGIC构建了一个通信图，并利用GAT进行多轮通信。值得注意的是，TarMAC、MAGIC和MAGI都利用GAT作为通信学习模块中的GNN结构。所有实验都在配备有Pytorch的Nvidia RTX 2080Ti GPU上进行。

## A. 环境

SMAC：StarCraft Multi-Agent Challenge (SMAC)是建立在流行策略游戏StarCraft II上的。SMAC由各种StarCraft II微场景组成，示例如图3所示。这些场景非常复杂，盟军代理需要学习至少一种微管理技术才能击败敌方代理。在SMAC的场景中，所有盟军代理都通过MARL方法进行训练，所有敌方代理都由内置AI控制。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/42f5ee03abdb448a84827e913864fa58.png#pic_ center" width="70%" />
</div>

代理的动作空间包含4个动作：移动、攻击、无操作和停止。攻击范围设置为6。在每个时间步，盟军代理可以采取攻击行动向敌方代理开火并获得全局奖励。此外，盟军代理还可以通过杀死敌方代理并赢得游戏获得额外奖励。特别是，我们微调了默认的实验设置，使盟军代理更难以协调行动。我们将盟军代理的视野范围从9减少到2。此外，我们选择了几个复杂场景，如表I所示。提供了5个场景的详细信息如下。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/440b1d9a629c415a904bf4319352f424.png#pic_ center" width="70%" />
</div>

MMM2和MMM3是两个超级困难的场景，包含海军陆战队员、土匪和医护兵。这些场景中的每种代理都有自己的独特属性和技能。盟军只有在每个代理都发挥最大的能力和协调其他代理的行动时才能成功。医护兵具有治疗能力，可以为受伤的盟军代理提供治疗。为了赢得战斗，盟军代理必须学会与其他代理通信，例如向医护兵发送他们的健康状况。

1o2r对4r场景包括1个监管者和2个火箭虫作为盟军代理。监管者的目标是找到敌方单位的4个收割者，而2个火箭虫的目标是到达收割者的位置并尝试击败收割者。监管者和收割者在场景中是随机生成的，敌方火箭虫在其他点随机生成。考虑到只有监管者可以获得有关敌人位置的信息，火箭虫必须学会聚合邻居监管者的信息，以有效赢得战斗。

2c3s5z场景包含盟军和敌方代理的巨像、追猎者和狂热者。盟军代理必须学会许多战术，例如利用狂热者拦截敌方狂热者，以保护追猎者免受严重伤害。在有效的信息互动下，代理更容易学习这些策略并协调行动。在8m对9m场景中，盟军和敌方代理都是海军陆战队员。

MAgent：MAgent是一个MARL平台，旨在支持需要数百个代理的任务。我们选择MAgent的战斗场景进行实验。战斗场景包含K个盟军代理和Z个敌方代理。盟军代理的目标是学会击败所有敌方代理。每个代理可以采取两个动作：移动或攻击，行动范围为4。每个敌方代理的能力都比每个盟军代理强，因此盟军代理必须发展与其他盟军代理合作的战略。

由于战斗场景在代理死亡后往往会失去平衡，我们随机添加新的盟军代理或敌方代理以保持平衡。MAGI和其他基线首先在K = 40和Z = 24的相同设置下进行训练。在战斗场景中，代理在攻击敌人时可以获得+5的正奖励。当代理被敌方代理杀死或击中空白网格时，代理可以获得-2和-0.01的负奖励，分别。

## B. 性能

鲁棒性(Q1，Q2)：为了验证MAGI和现有的基于GNN的MACRL方法是否容易受到敌意攻击和噪声扰动的影响，我们在邻接矩阵A和代理特征H中产生随机扰动，并将它们注入。我们向代理特征H添加独立的高斯噪声，并增加振幅。具体来说，我们使用每个代理的最大特征的均值作为参考振幅φ。然后我们向每个代理特征添加高斯噪声η·φ·ϵ，其中ϵ ∼ N(0, 1)，η表示特征噪声比率。我们使用η = 1.5验证MAGI和其他基于GNN的MACRL方法的鲁棒性。在消融部分，我们使用η ∈ {0.5, 1, 1.5}验证不同参数下的鲁棒性。

为了在邻接矩阵A上产生敌意攻击，我们首先尝试在代理之间随机删除边缘并绘制图结构。然而，实验结果表明，这种简单策略不能显著影响基于GNN的MACRL方法的通信效率。因此，我们使用投影梯度下降(PGD)[29]来产生攻击，而不是随机删除边缘。图4显示了MAGI和其他基于GNN的MACRL方法在SMAC的四个场景中的学习曲线。实线显示了没有敌意攻击和噪声扰动的方法的平均胜率，相应的阴影区域表示95%的置信区间。虚线说明了在敌意攻击和噪声扰动下方法的平均胜率。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/57cd19689c6b4ebcac2ec38893794228.png#pic_ center" width="70%" />
</div>

我们可以从图4中得出以下几个结论。1) MAGI在所有场景中的表现明显优于其他基线方法。2)通过比较QMIX和基于GNN的MACRL方法，我们可以看到这些方法的基于GNN的通信学习机制确实可以促进代理之间的合作并提高性能。3)此外，没有敌意攻击的情况下，通过比较MAGI与其他基于GNN的MACRL方法(TarMAC和MAGIC)，MAGI的表现优于其他基线，这证明了所提出的框架将通信学习机制与价值分解方法融合在一起的有效性。4)此外，TarMAC+A和MAGIC+A的性能与TarMAC和MAGIC相比显著下降，这证明了其他现有的基于GNN的MACRL方法容易受到敌意攻击的影响。5)与MAGI相比，MAGI+A的性能只有轻微下降，这证明了MAGI在敌意攻击下是鲁棒的，因为图信息瓶颈优化可以提高通信学习的鲁棒性。

有效性(Q3)：不同方法在战斗场景下敌意攻击下的性能如表II所示。为了方便起见，在后续实验中，没有“+A”后缀的方法(如MAGI)也默认为在敌意攻击下该方法。如图5所示，MAGI在击杀数量(击杀敌方代理的数量)、击杀死亡率(击杀敌方代理的数量除以盟军代理的死亡数量)和平均奖励方面的表现优于其他基线方法。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0eab969ab2ef4893bae2ee7cdf7c5d2c.png#pic_ center" width="70%" />
</div>

图5展示了不同方法训练的代理在战斗场景中的代表性行为。如图5所示，在噪声扰动和敌意攻击下，通过MAGI训练的代理学习了一些动作协调技术，例如包围。对于单个敌方代理，通过MAGI训练的代理可以学会协调他们的行动来包围并击败它。对于敌方代理群体，通过MAGI训练的代理可以学会协调对群体一侧的攻击。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/33a16b11069f45aaa0b4608aa6fc3d95.png#pic_ center" width="70%" />
</div>

相比之下，其他基于GNN的MACRL方法在敌意攻击和噪声扰动下学习了次优策略，例如聚集在角落里以避免被敌方代理攻击。这种行为表明，这些方法的通信学习在扰动下变得无效。这可能是因为MAGI采用了图信息瓶颈来学习鲁棒且最小充分的消息表示，从而实现了促进代理动作协调和策略学习的高效通信。

可扩展性(Q4)：为了验证MAGI可以扩展到大规模多智能体场景，我们在不同数量的代理(K ∈ {20, 30, 40, 50})的战斗场景下，对MAGI和其他基线在扰动下进行了比较。如表III所示，MAGI的可扩展性表现在随着代理数量的增加，MAGI始终优于基线。我们认为，通信学习和价值分解的结合可以作为一个通用范式，用于解决大规模多智能体问题。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c1496339254b47818265ceee1671ee3d.png#pic_ center" width="70%" />
</div>

## C. Ablations

 Contribution（Q5）进一步地，我们评估了MAGI的每个组成部分的贡献。MAGI包含两个关键组件：价值分解（VD）组件和信息瓶颈（IB）优化组件。因此，我们设计了MAGI的两个变体：1）没有VD组件的MAGI称为MAGI-VD；2）没有IB组件的MAGI称为MAGI-IB。如表IV和表V所示，通过比较MAGI和MAGI-IB，我们可以看到在对抗性攻击和噪声干扰下，移除IB组件会导致性能下降。此外，当比较MAGI和MAGI-VD时，我们可以看到移除VD模块也会导致性能略微下降。这些实验结果表明，IB优化可以显著提高在对抗性攻击和噪声干扰下的通信学习的鲁棒性和有效性，VD模块可以进一步促进动作协调和策略学习。
 
<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/376a82ac46b7497a8ec0aa3fccae335a.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5a74ecb8fc8b46768eeac2fc7b8f916d.png#pic_ center" width="70%" />
</div>

为了进一步评估两个正则化项的贡献，我们使用 $IB_ {r1} = I(Y ; M^L_ H)$ 来表示正则化器1的信息瓶颈损失，使用 $IB_ {r2} = I(D; M^L_ H)$ 来表示正则化器2的信息瓶颈损失。因此，MAGI w/ IBr1表示只包含正则化器1的MAGI，MAGI w/ IBr2表示只包含正则化器2的MAGI，MAGI-IB表示没有正则化器的MAGI。如表VI和表VII所示，在MMM2、8 m versus 9 m和1o2r versus 4r场景中，正则化器1比正则化器2更有效。在其他场景中，正则化器2更有效。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/36ccaed989064fa5bd386013e18a836e.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c24066e7ca4145048320a527b72bd529.png#pic_ center" width="70%" />
</div>

灵活性（Q6）：为了展示MAGI可以灵活地与任何价值分解方法融合，我们通过将其与其他流行的价值分解方法VDN和QPLEX集成进行实验。融合方法MAGI (VDN)和MAGI (QPLEX)在SMAC的各种场景中进行了评估。如表VIII所示，MAGI (VDN)和MAGI (QPLEX)的表现优于原始的价值分解方法VDN和QMIX，这表明了所提出方法的灵活性。此外，所提出的方法可以为更一般的任务（如混合合作和竞争任务）移除价值分解模块。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0fb377b839544c7ba9e359948279991f.png#pic_ center" width="70%" />
</div>

参数（Q7）：为了探索不同超参数对性能的影响，我们首先在SMAC的2c3s5z和MMM3场景中对特征噪声比进行了消融实验。

如图6所示，采用不同的特征噪声比（η ∈ {0.5, 1.0, 1.5}），MAGI一直表现得比其他基于GNN的MACRL方法更好。特别是，当η较大时（η = 1.5），其他方法的性能受到显著影响，而MAGI保持稳定，这表明MAGI的IB优化使通信学习在对抗性攻击和噪声干扰下更加稳健。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/65d0355034fb4156bbcd186b2317d2c3.png#pic_ center" width="70%" />
</div>

为了评估不同λ对性能的影响，我们在SMAC的不同场景下，在对抗性攻击和噪声干扰下进行了消融实验。如表IX所示，不同的λ（λ ∈ {0.05, 0.10, 0.15}）中，MAGI在MMM3场景中以λ = 0.15表现最佳，在其他场景中以λ = 0.10表现最佳。因此，为了保持一致性，我们为SMAC的所有场景设置了λ = 0.10。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0915cc545ecb478b83ab56eebf1f7658.png#pic_ center" width="70%" />
</div>

此外，我们评估了不同变体在逐渐增加的特征噪声比（η ∈ {1.5, 2.0, 2.5}）下的稳健性和有效性。如图7所示，从三种变体的比较结果来看，MAGI-IB的性能随着特征噪声比η的增加而急剧下降。相比之下，MAGI在稳定性方面表现优异。MAGI的效果最好，MAGI-IB的方差最大，同时MAGI-IB的性能也最差。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a3cc7c769ab7459186c303a9f1d7362e.png#pic_ center" width="70%" />
</div>

一般性（Q8）：对抗性攻击通常可以被视为修改特征和修改边缘。因此，在之前的实验中，我们选择了两种对抗性攻击（GN+PGD）来同时生成机会：添加高斯噪声（修改特征）和PGD[29]（修改边缘）。此外，我们利用另外两种复杂的对抗性攻击（IG-JSMA[28]和GUA[30]）进行实验，以验证所提出的方法的通用性，结果如表X所示。我们利用IG-JSMA[28]在代理特征和边缘上添加对抗性扰动。我们利用GUA[30]通过翻转锚代理之间的连接来改变边缘。有关更多详细信息，请参阅[28]、[29]、[30]。如表X所示，在各种对抗性攻击方法下，与其他基线方法相比，MAGIal方法实现了最佳性能，这证明了所提出方法的通用性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1240bf26f55e47379820f3b6a3d9f158.png#pic_ center" width="70%" />
</div>

## D. Details of Model Hyper-Parameters

在本节中，为了方便重现性，我们提供了实验设置的详细信息。表XI描述了所有实验基准的固定超参数。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9e7907924f26430abb03ad3f0e3cce91.png#pic_ center" width="70%" />
</div>

# V. 结论

在本文中，我们将图信息瓶颈优化引入基于GNN的MACRL方法。所提出的方法通过两个信息论正则化器实现鲁棒和高效的通信学习，这些正则化器最小化通信消息与代理特征之间的互信息，并同时最大化消息表示与动作选择之间的互信息。在各种多智能体场景中的实验结果表明，所提出的方法显著优于其他基线，并且可以灵活地与现有的价值分解方法融合，以促进动作协调。

据我们所知，这项工作是首次在MACRL领域通过图信息瓶颈优化学习鲁棒通信的尝试。我们相信这是建立大规模多智能体系统在敌意攻击和噪声扰动下高效通信的有希望的途径。在未来，将所提出的方法应用于现实世界的大规模多智能体系统是值得的。需要承认的是，我们当前的方法专门针对离散动作和完全合作场景设计。它不适用于连续动作环境或混合竞争合作场景。然而，我们未来的工作旨在开发更具通用性和鲁棒性的通信模型，以适应更广泛的应用场景。
# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
