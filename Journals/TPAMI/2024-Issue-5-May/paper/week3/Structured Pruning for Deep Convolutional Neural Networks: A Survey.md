# [Structured Pruning for Deep Convolutional Neural Networks: A Survey](https://ieeexplore.ieee.org/document/10330640/)
## 深度卷积神经网络的结构化剪枝：综述
**作者：Yang He; Lingao Xiao**  
**源码：https://github.com/he-y/Awesome-Pruning**  
****


# 摘要
深度卷积神经网络（CNNs）的卓越性能通常归因于它们更深更宽的架构，这可能会带来显著的计算成本。因此，修剪神经网络已经引起了人们的兴趣，因为它有效地降低了存储和计算成本。与导致非结构化模型的权重修剪不同，结构化修剪提供了通过产生对硬件实现友好的模型来实现现实加速的好处。结构化修剪的特殊要求导致了众多新挑战的发现和创新解决方案的开发。本文综述了深度CNNs结构化修剪的最新进展。我们总结了并比较了最先进的结构化修剪技术，这些技术涉及滤波器排序方法、正则化方法、动态执行、神经架构搜索、彩票假设以及修剪的应用。在讨论结构化修剪算法时，我们简要介绍了非结构化修剪的对应部分，以强调它们之间的差异。此外，我们还提供了对结构化修剪领域潜在研究机会的见解。神经网络修剪论文的精选列表可以在 https://github.com/he-y/Awesome-Pruning 找到。一个提供更互动的结构化修剪方法比较的专用网站可以在 https://huggingface.co/spaces/heyang/Structured-Pruning-Survey 找到。
# 关键词
- 计算机视觉
- 深度学习
- 神经网络压缩
- 结构化修剪
- 非结构化修剪

# I. 引言
深度卷积神经网络（CNNs）在各种应用中表现出色，包括图像分类[1]、目标检测[2]和图像分割[3]等[4]。已经提出了多种CNN结构，包括AlexNet[5]、VGGNet[6]、Inceptions[7]、ResNet[8]和DenseNet[9]。这些架构包含数百万参数，需要大量的计算能力，使得在资源有限的硬件上部署变得具有挑战性。模型压缩是解决这一问题的方法，旨在减少参数数量、计算成本和内存消耗。因此，其研究已经变得重要。

为了生成更高效的模型，已经提出了包括修剪[10]、量化[11]、分解[12]和知识蒸馏[13]在内的模型压缩技术。"修剪"一词指的是移除网络的组件以产生用于加速和压缩的稀疏模型。修剪的目标是在不显著影响模型性能的情况下最小化参数数量。大多数关于修剪的研究都是在图像分类任务上的CNNs进行的，这是其他计算机视觉任务的基础。

修剪可以分为非结构化[10]和结构化修剪[14]。非结构化修剪移除神经网络的连接（权重），导致非结构化稀疏。非结构化修剪通常会导致高压缩率，但需要特定硬件或库支持以实现现实的加速。结构化修剪移除神经网络中的整个滤波器，并可以通过利用高效的库（如基本线性代数子程序（BLAS）库）与标准硬件一起实现现实的加速和压缩。

从结构化修剪的角度重新审视CNNs的属性在Transformer[15]时代是有意义的。最近，将CNNs的架构设计融入基于Transformer的模型的趋势日益增加[16]、[17]、[18]、[19]、[20]。尽管Transformer中的自注意力[21]在计算序列的表示方面是有效的，但由于Transformer通常缺乏归纳偏差[18]、[22]、[23]，因此仍然需要大量的训练数据。相比之下，CNNs的结构对权重施加了两个关键的归纳偏差：局部性和权重共享，以影响学习算法的泛化，并且与数据无关[18]。本调查提供了对CNNs的更好理解，并为未来高效设计架构提供了见解。

在本次调查中，我们专注于结构化修剪。现有的相关压缩研究调查如表I所示。一些调查涵盖了包括量化[24]、知识蒸馏[25]和神经架构搜索[26]等正交领域。一些调查[27]提供了更广泛的概述。尽管一些调查专注于修剪，但它们更多地关注非结构化修剪，并且涵盖了少量关于结构化修剪的研究。在[28]、[29]、[30]、[31]、[32]、[33]、[34]中引用的结构化修剪论文数量分别为1、11、15、55、38、10和20。我们提供了一个更全面的调查，涵盖了200多篇结构化修剪论文。例如，[31]可以被第II-A、II-B、II-C、II-D1、II-G1和III-A节涵盖。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/17c08a35482745df9225d24e1f2df7f6.png" width="70%" /> </div>


调查安排如下。在分类法（图1）中，我们将结构化修剪方法分为不同的类别。第II节的每个小节对应于一类结构化修剪方法。大多数方法最初是以非结构化方式开发的，然后扩展以满足结构化约束。虽然一些研究跨越了多个类别，我们将它们放置在最适合本调查的类别中。第III节然后介绍了一些潜在的和有希望的未来方向。由于篇幅限制，只有最具代表性的研究在详细讨论。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/c7407709c79f426086eae0643ac05d44.png" width="70%" /> </div>

# II. 方法
初步知识：一个深度卷积神经网络N可以由 ${Wl \in \mathbb{R}^{N_{l+1} \times N_l \times K_l \times K_l}, 1 \leq l \leq L}$ 参数化。在第l层，输入张量Il具有形状 $N_l \times H_l \times W_l$ ，输出张量Ol具有形状 $N_{l+1} \times H_{l+1} \times W_{l+1}$ 。Nl和Nl+1分别表示第l层输入和输出张量的通道数。Wl表示输入张量Il和输出张量（特征图）Ol之间的连接（权重）。权重矩阵Wl由Nl+1个3-D滤波器Fl组成。具体来说，第l层的第i个滤波器可以表示为 $\{F^l_i \in \mathbb{R}^{N_l \times H_l \times W_l}, 1 \leq i \leq N_{l+1}\}$ 。在本文中，我们称Kl $\in \mathbb{R}^{K_l \times K_l}$为2-D核，因此一个滤波器有Nl个核，核大小为Kl。为了表示一个单独的权重，我们使用 $w = F^l_i(n, k_1, k_2), 1 \leq n \leq N_l, 1 \leq k_1, k_2 \leq K_l$ 。第l层的卷积操作可以表示为：

$$
O^l_i = I_l * F^l_i, 1 \leq i \leq N_{l+1}
$$

其中*表示卷积运算符。
结构化修剪，如滤波器修剪，旨在：

$$
\min_{F} L(F; D) = \min_{F} \frac{1}{N} \sum_{i=1}^{N} L(F; (x_i, y_i)), \text{ s.t. } \text{Card}(F) \leq \kappa,
$$

其中 $L(\cdot)$ 是损失函数（例如交叉熵损失）， $D = \{(x_i, y_i)\}^N_{i=1}$ 是一个数据集。 $Card(\cdot)$ 是过滤器集的基数， $\kappa$ 是目标稀疏性水平，例如剩余非零过滤器的数量。

## A. 基于权重的方法
基于权重的标准专门设计用来评估神经网络中过滤器的重要性。这是通过评估这些过滤器的权重来完成的，以确定哪些过滤器和/或通道对模型性能至关重要。与基于激活的方法相比，基于权重的方法不涉及输入数据。因此，基于权重的方法被认为是直接的，并且需要较低的计算成本。基于权重的标准有两个子类别：过滤器范数和过滤器相关性。计算一个过滤器的范数是独立于其他过滤器的范数进行的，而计算过滤器相关性涉及多个过滤器。

*1) 过滤器范数：*与使用权重大小作为度量的非结构化剪枝不同，结构化剪枝计算过滤器范数值作为度量。一个过滤器的 $\ell_p$ -范数可以写成：

$$
\|F^l_i\|_ p = \left( \sum_{n=1}^{N^l} \sum_{k^1=1}^{K^l} \sum_{k^2=1}^{K^l} |F^l_i(n, k^1, k^2)|^p \right)^{1/p},
$$

其中 $i \in N^{l+1}$ 表示第 $l$ 层的第 $i$ 个过滤器， $N^l$ 是输入通道大小， $K^l$ 是核大小。 $p$ 是范数的阶，两个常见的范数是 $\ell_1$ -范数（曼哈顿范数）和 $\ell_2$ -范数（欧几里得范数）。

Pruning Filter for Efficient ConvNets (PFEC) [14]基于 $\ell_1$ -范数计算过滤器的重要性。Li等人认为范数较小的过滤器具有较弱的激活，对最终分类决策的贡献较小[14]。

Soft Filter Pruning (SFP) [115]的经验发现 $\ell_2$ -范数比 $\ell_1$ -范数略好。这将在第II-E1节中进一步讨论。

*2) 过滤器相关性：*Filter Pruning via Geometric Median (FPGM) [52]揭示了“范数越小越不重要”的假设并不总是正确的，这是基于神经网络的真实分布。它不是剪除不重要的过滤器，而是通过利用同一层中过滤器之间的关系来寻找冗余过滤器。He等人认为接近几何中值的过滤器是冗余的，因为它们表示同一层中所有过滤器共享的共同信息[52]。这些冗余过滤器可以在不显著影响性能的情况下被移除。

RED [53]使用无数据的结构化压缩方法。它包括三个步骤。首先，对每层的权重进行标量哈希。其次，基于过滤器的相对相似性合并冗余过滤器。第三，使用一种新颖的不均匀深度分离技术来剪除层。RED++ [54]将第三步替换为输入分割技术，以去除乘法和加法等冗余操作。这是因为数学运算比内存分配更成为瓶颈。

与FPGM [52]不同，后者在层内测量过滤器的重要性，Correlation-based Pruning (COP) [55]比较跨层过滤器的重要性。为了确定同一层内过滤器的冗余性，COP [55]首先进行皮尔逊相关性测试。接下来，使用逐层最大归一化来解决基于相关性的重要性度量的缩放效应，以便在层之间对过滤器进行排名。最后，将一个成本感知的正则化项添加到全局过滤器重要性计算中，使用户可以更精细地控制预算。

Structural Redundancy Reduction (SRR) [56]通过寻找最冗余的层而不是所有层中排名最低的过滤器来利用结构冗余。首先，为每层中的过滤器建立一个图。图的冗余性可以通过其两个相关属性来评估，即商空间大小和 $\ell$ -覆盖数。具有大值的两个属性表示一个复杂且因此冗余度较低的图。在最冗余的层内，可以应用过滤器范数来剪除最不重要的过滤器。最后，重新建立层的图，并重新评估层的冗余性。

## B. 基于激活的修剪方法

与通过权重确定滤波器重要性的方法不同，基于激活的修剪方法利用激活图来进行修剪决策。激活图在（1）中详细说明，是通过输入数据和滤波器之间的卷积过程产生的。通道修剪是滤波器修剪的另一个名称，因为移除激活图的通道等同于移除滤波器。除了当前层的影响外，滤波器修剪还通过特征图影响下一层的滤波器。

为了评估第l层的滤波器，我们可以利用以下信息：

1) 当前层 - 可以通过使用重建误差[59]、激活图的分解[60]、通道独立性的利用[62]和后激活[63]、[64]来评估通道重要性；
2) 相邻层 - 通过利用当前层和下一层之间的依赖关系，可以有效地识别冗余通道[67]、[68]。此外，前一层的激活图也可以用于指导修剪决策[123]、[124]；
3) 所有层 - 通过最小化最终响应层[70]的重建误差，并考虑所有层的区分能力[71]，可以评估移除一个滤波器的整体效应。


*1) 当前层：*通道修剪（CP）[59]使用第l层（当前层）的激活图来指导第l层的滤波器修剪。它将层级通道修剪建模为一个优化问题，该问题最小化稀疏激活图的重建误差。解决优化问题涉及两个交替步骤。(1) 为了找到要修剪的通道，CP明确解决了LASSO回归问题，而不是在训练损失上施加稀疏正则化。(2) 为了最小化第l层特征图的重建误差，权重在固定的修剪决策下进行微调。

HRank [60] 使用当前层激活图的平均排名作为滤波器重要性。一个重要的发现是，不管接收到的数据如何，单个滤波器产生的激活图具有相同的平均排名。为了找到平均排名，采用了奇异值分解（SVD）。这里进行的分解是为了找到排名，而不是为了降低计算成本。确定平均排名后，然后提出了一个层级修剪算法来保留top-k滤波器。

Coreset-Based Compression (CBC) [61] 采用滤波器修剪来预处理基于coreset的压缩[226]中的滤波器。滤波器的评分基于整个训练集上激活规范的平均值。然后使用二分搜索来找到满足准确度约束的最少数量的滤波器。修剪后，讨论了三种基于coreset的压缩技术，包括k-Means、结构化稀疏PCA和激活-加权coresets。利用深度压缩[1]，激活-加权coresets优于其他方法。

CHannel Independence (CHIP) [62] 是Sui等人用来评估通道重要性的。通道独立性是通过跨通道相关性来确定的，这表明一个通道是否线性依赖于其他通道。通道的独立性越大，其重要性越高。通过测量激活图的核范数变化来确定通道重要性。

Average Percentage of Zero (APoZ) [63] 利用当前层的后激活图，即激活函数（如ReLU）之后的激活图。在后激活图中零的平均百分比（APoZ）用来衡量通道的重要性。一个较小的APoZ值意味着大部分激活图被激活，因此这些激活图对最终结果的贡献更大，也更重要。

DropNet [64] 利用后激活图的平均幅度作为指标。在这个指标下，APoZ [63] 认为重要的小非零激活值，在DropNet [64] 中不再重要。使用这个指标有两个原因。首先，一个小的平均幅度表明存在许多非活动节点。其次，小幅度也意味着这些节点对学习不太适应。

*2) 相邻层：*ThiNet [67] 使用第l+1层（下一层）的激活图来指导第l层（当前层）的修剪。主要思想是使用第l层的激活图的子集来近似第l+1层的激活图。在这些子集之外的通道被修剪。为了找到这些子集，提出了一个贪婪算法。具体来说，该算法贪婪地将通道添加到一个最初为空的集合中，并测量重建误差。选择具有最小重建误差并满足稀疏性约束的子集。

Approximated Oracle Filter Pruning (AOFP) [68] 使用第l+1层的激活图，目标是在不需要Oracle修剪方法[14]、[63]、[95]通常所需的启发式知识的情况下进行修剪。首先，引入了损伤隔离的概念，以避免使用启发式重要性指标。损伤隔离意味着通过第l+1层隔离了由修剪第l层引起的损伤，使损伤对l+2不可见。其次，使用多路径框架来从并行评分和微调中受益。第三，使用二进制滤波器搜索方法来解决多路径框架的问题。

除了使用下一层的激活图，Runtime Neural Pruning (RNP) [123] 和 Feature Boosting and Suppression (FBS) [124] 还利用第l-1层（前一层）的激活图来指导当前层的修剪。两种方法都使用前一层的全局平均池化结果作为滤波器重要性。这将在第II-E2节中进一步讨论，因为这两种方法在推理期间进行动态修剪。

*3) 所有层：*尽管现有方法取得了成功，Neuron Importance Score Propagation (NISP) [70] 的支持者认为大多数方法没有考虑重建误差传播。NISP提出使用最终响应层（FRL）来确定神经元重要性，因为从所有先前层的重建误差最终都会传播到FRL。最初，FRL的重要性分数可以通过任何特征排名技术确定，例如Inf-FS [227]。然后，从FRL向后传播到前一层的神经元重要性。最后，修剪掉该层中重要性分数低的神经元。被修剪的神经元将不再向前一层反向传播分数。

Discrimination-aware Channel Pruning (DCP) [71] 的目标是保留在它们缺席时显著改变最终损失的区分通道。然而，修剪浅层通常由于长传播路径而触发的最终损失的减少较小。为了解决这个问题，Zhuang等人在中间层的每个最后层引入了区分感知损失。然后，使用基于区分感知损失和基线网络与修剪网络之间的重建损失的贪婪算法来选择通道。

### C. 正则化

正则化可用于通过添加不同的稀疏正则化器 $R_s(\cdot)$ 来学习结构化稀疏网络。如果网络包含批量归一化层，则可以将稀疏正则化器应用于 BN 参数。为了实现结构化稀疏性，使用 BN 参数来指示诸如通道或滤波器等结构的修剪决策。引入了额外的参数作为可学习门控，以指导修剪。有了这些额外参数，网络不再需要批量归一化层。稀疏正则化器也可直接应用于滤波器。常用的方法是使用 Group Lasso 正则化以结构化方式稀疏化滤波器。

一般 Group Lasso 定义为解决以下凸优化问题的解： 

$$
\min_{\beta \in \mathbb{R}^p} \left\\{ \frac{1}{2} \sum_{i=1}^{n} (y_i - \mathbf{G} \mathbf{x}_ i)^2 + \lambda \sum_{g=1}^{G} \sqrt{n_g} \|\beta_g\|_2 \right\\}
$$

其中特征矩阵被分为 $G$ 组，形成矩阵 $\mathbf{X}_g$，其中只包含组 $g$ 的示例以及相应的系数向量 $\beta_g$。 $n_g$ 表示组 $g$ 的大小， $\lambda \geq 0$ 是一个调整参数。在滤波器修剪的背景下，第一项可以被视为特征图的重建误差，第二项可以重写为：

$$
\lambda \sum_{i=1}^{N_{l+1}} \|\mathbf{F}^l_i\|_{2,1}
$$

其中组 $G$ 被输出通道 $N_{l+1}$ 替换，系数向量 $\beta_g$ 被滤波器替换。此外，使用 $\ell_1$ -范数、 $\ell_2$ -范数和 $\ell_{2,1}$ -范数作为惩罚函数存在差异（见图 2）。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/ff6cda0640aa45b59d01432fde2d1ea1.png" width="70%" /> </div>


*1) 在 BN 参数上的正则化：*批量归一化（BN）层已在许多现代 CNN 中广泛使用，以改善模型泛化。通过归一化每个训练小批量，解决了内部协变量偏移问题。BN 层的操作由以下等式描述：

$$
\hat{\mathbf{z}}_ in = \frac{\mathbf{z}_ in - \mu_B}{\sigma_B^2 + \epsilon}; \quad \mathbf{z}_{out} = \gamma \hat{\mathbf{z}}_in + \beta
$$

其中 $N_{l+1}$ 维的 $\mathbf{z}_ {in}$ 和 $\mathbf{z}_ {out}$ 分别是 BN 层的输入和输出， $\mu_B$ 和 $\sigma_B$ 是当前小批量 $B$ 的均值和标准差。 $\epsilon$ 是一个小数，以防止除以零。 $\gamma \in \mathbb{R}^{N_{l+1}}$ 和 $\beta \in \mathbb{R}^{N_{l+1}}$ 是可学习参数，分别表示尺度和偏移。BN 参数被用作滤波器修剪的门控，因为可学习参数的数量等于特征图和滤波器的数量。

Network Slimming (NS) [74] 直接使用 BN 中的缩放参数 $\gamma$ 来控制输出通道。然后引入了通道级稀疏诱导的 $\ell_1$ 正则化来与 $\gamma$ 一起训练权重。训练完成后，剪除对应通道，这些通道的 $\gamma$ 接近零。为了优化非平滑的 $\ell_1$ 惩罚项，使用次梯度下降方法 [229]。

与 NS [74] 的情况类似，Gated Batch Normalization (GBN) [75] 使用 $\gamma$ 作为通道门控， $\gamma$ 的 $\ell_1$ -范数作为正则化项。提出了一个 Tick-Tock 修剪框架，通过迭代修剪来提高准确性。Tick 阶段用少量数据训练网络，并且只允许门控和最终线性层在训练期间更新一个周期。同时，通过全局滤波器排名的一阶泰勒展开计算通道重要性。Tock 阶段然后对稀疏网络进行微调，施加稀疏性约束。

Polarization Regularization (PR) [76] 提供了一种基于 $\ell_1$ 的正则化变体，以极化缩放因子 $\gamma$ 。它认为在大多数稀疏正则化方法中，如 NS [74]，一个简单的 $\ell_1$ 正则化器会不加选择地将所有缩放因子收敛到零。更合理的方法是将不重要神经元的缩放因子推向零，将重要神经元的缩放因子推向一个较大的值。为了实现极化效应，向简单的 $\ell_1$ 项中添加了另一个惩罚项，尽可能地将 $\gamma$ 从它们的均值分开。与 NS [74] 类似，使用次梯度 [229] 在非可微点上解决非平滑正则化器。

Rethinking Smaller-Norm-Less-Informative (RSNLI) [77] 是从先前方法的基础上发展起来的，这些方法倾向于受到模型重参数化问题和变换不变性问题的干扰。由于对小范数参数是否不太信息量存在疑问；Ye 等人提出使用 ISTA [230] 来强制 $\gamma$ 上的稀疏性。将 $\gamma$ 等于零的通道剪除。然后使用 $\gamma$ -W 重新缩放技巧在 $\gamma$ 和权重上快速启动稀疏化过程。

Operation-aware Soft Channel Pruning (SCP) [78] 同时考虑了 BN 和 ReLU 操作。与 NS [74] 仅根据通道缩放 $\gamma$ 做决策不同，Kang 等人还考虑了偏移参数 $\beta$。具体来说，具有大负 $\beta$ 和大 $\gamma$ 的通道被认为不重要，因为这些通道在 ReLU 之后将变为零。为了考虑 BN 的大负均值，使用由 $\beta$ 和 $\gamma$ 参数化的高斯分布的累积分布函数（CDF）作为指标函数。为了优化 $\beta$ 和 $\gamma$ ，设计了一个稀疏损失诱导的大 CDF 值，以鼓励网络更加稀疏。

EagleEye [79] 提出了一个三阶段流水线。首先，通过简单随机抽样生成修剪策略（即，逐层修剪比例）。其次，根据修剪策略和滤波器的 $\ell_1$ -范数生成子网络。第三，使用基于自适应 BN 的候选评估模块来评估子网络的性能。Li 等人认为，过时的 BN 统计数据对子网络评估不公平，每个候选的 BN 统计数据应该在训练数据集的一小部分上重新计算 [79]。在评估了具有自适应 BN 统计数据的子网络后，选择性能最好的作为最终修剪模型。

*2) 在额外参数上的正则化：*尽管一些研究 [75]、[77] 为没有 BN 层的网络进行了特殊调整，引入额外参数是更一般的解决方案。额外参数 $\theta$ 是可训练的，并且参数化了门控 $g(\theta)$ 以确定修剪结果。

为了找到一个稀疏结构，Sparse Structure Selection (SSS) [80] 试图通过将结构的输出强制为零来实现。在每个结构之后引入了一个缩放因子 $\theta$，即神经元、组或残差块。当 $\theta$ 小于一个阈值时，相应的结构被移除。门控函数 $g(\theta)$ 定义为：

$$
g(\theta) =
\begin{cases}
0, & \text{if } \theta < \text{Threshold} \\
1, & \text{otherwise}
\end{cases}
$$

它采用了凸松弛 $\ell_1$-范数作为额外参数 $\theta$ 上的稀疏正则化。为了更新 $\theta$，使用了修改后的加速近端梯度 [231]。

Generative Adversarial Learning (GAL) [81] 联合修剪结构，并采用生成对抗网络（GAN）实现无标签学习。在生成器中，在每个结构之后引入了额外的缩放因子，形成软掩码。在训练过程中，提出了一个特殊的正则化项，其中包含三个正则化器：1) 生成器上的 $\ell_2$ 权重衰减正则化器；2) 掩码上的 $\ell_1$ 稀疏正则化器；3) 鉴别器上的对抗性正则化器。此外，使用 FISTA [230] 迭代更新生成器和鉴别器，掩码与生成器一起更新。

Discrete Model Compression (DMC) [82] 明确地在特征图之后引入了离散（二进制）门控，以精确反映修剪通道对损失函数的影响。首先，它采用随机离散门控采样子网络：

$$
g(\theta) =
\begin{cases}
1, & \text{以概率 } \theta \\
0, & \text{以概率 } 1 - \theta
\end{cases}
$$

其中 "w.p." 代表 “以概率”（with probability）。门控的随机性质确保了如果 $\theta \neq 0$，则每个通道都有机会被采样，从而可以产生不同的子网络。为了更新非可微的二进制门控，采用了直通估计器（Straight-Through Estimator）[232]。

类似于 PR [76]，Gates with Differentiable Polarization (GDP-Guo) [83] 旨在极化门控。设计具有极化效应的门控，具有平滑 $\ell_0$ 形式的性质：

$$
g(\theta) = \frac{\theta^2}{\theta^2 + \epsilon}
$$

这里 $\epsilon$ 是一个小的正值，以防止零除。这个门控本身是可微的。然而，对 $\theta$ 的稀疏正则化涉及 $\ell_0$-范数，使目标函数不可微。因此，使用近端随机梯度下降（proximal-SGD）[234] 来更新 $\theta$。

Convolutional Re-parameterization and Gradient Resetting (ResRep) [84] 将 CNN 重新参数化为两部分。第一部分是 “记忆部分”，它学习维持模型性能并且不会被修剪。第二部分是 “遗忘部分”，在 BN 层之后插入 1x1 CONV 层，或者称为紧凑器。在训练过程中，修改的 SGD 更新规则仅更新紧凑器。因此，只有紧凑器被允许遗忘（遗忘），而其他 CONV 层保持不变（记忆）。

Scientific Control Pruning (SCOP) [85] 认为，滤波器的重要性可能会受到潜在因素的干扰，例如输入数据。例如，如果输入数据略有变化，对于依赖数据的方法，滤波器重要性排名可能会变化。为了最小化潜在因素的影响，它通过创建仿制副本 [235] 在科学控制下进行修剪。仿制特征与真实特征相同，除了不知道真实标签。然后引入两个缩放因子 $\theta$ 和 $\tilde{\theta}$，分别控制真实和仿制特征的参与度。两个参数是互补的，即 $\theta + \tilde{\theta} = 1$。如果 $\theta$ 不能抑制 $\tilde{\theta}$，则认为真实特征与真实输出几乎没有或没有关联。因此，滤波器重要性分数定义为 $I = \theta - \tilde{\theta}$，并且具有小重要性分数的滤波器被认为是多余的。

为了直接控制模型预算，Budget-Aware Regularization (BAR) [86] 使用先验损失并引入可学习的丢弃参数 $\theta$ [236]。先验损失是两个函数的乘积。第一个函数是关于 $\theta$ 的不同iable近似预算。第二个函数是一个变体的对数障碍函数 [237]，它采用sigmoidal 时间表。新颖的目标函数由先验损失组成，使得根据预算同时进行训练和修剪成为可能。然后使用知识蒸馏来提高准确性。

*3) 在滤波器上的正则化：*Structured Sparsity Learning (SSL) [91] 使用 Group Lasso 修剪通道。移除第 l 层的通道将导致移除第 l 层的滤波器和第 l+1 层的输入通道。因此，它为滤波器修剪和通道修剪添加了两个独立的正则化项：

$$
\lambda \sum_{n_l=1}^{N_{l+1}} \|\mathbf{W}^{(l)}_ {:,n_l,:,:}\|_ 2, \quad \lambda \sum_{c_l=1}^{N_l} \|\mathbf{W}^{(l)}_{:,c_l,:,:}\|_2
$$

其中 $\mathbf{W}^{(l)} \in \mathbb{R}^{N_{l+1} \times N_l \times K_l \times K_l}$。

Out-In-Channel Sparsity Regularization (OICSR) [92] 使用 Group Lasso 联合正则化协同工作的滤波器。正则化项是：

$$
\lambda \sum_{i=1}^{N} \|\mathbf{W}^{(l)}_ {i,:} \oplus \mathbf{W}^{(l+1)}_{:,i}\|_2
$$

其中 $\oplus$ 表示第 l 层的输出通道滤波器 $\mathbf{W}^{(l)}_ {i,:} \in \mathbb{R}^{N_{l+1} \times (N_l K_l \times K_l)}$ 和第 l+1 层的输入通道滤波器 $\mathbf{W}^{(l+1)}_ {:,i} \in \mathbb{R}^{(N_{l+2} K_{l+1} \times K_{l+1}) \times N_{l+1}}$ 的连接。它使用的前提是第 l 层的输出通道滤波器与第 l+1 层的输入通道滤波器相互依赖，因此这些滤波器应该一起正则化。

Only Train Once (OTO) [93] 主张即使所有滤波器权重都为零，由于三个参数：1) 卷积偏置，2) BN 均值，和 3) BN 方差，激活图将非零。与仅对滤波器进行分组的方法不同，这种方法将所有导致非零激活的参数分组到一个名为零不变组的组中。通过应用混合 $\ell_1/\ell_2$-范数，将结构化稀疏性引入到这个组中。为了解决非平滑混合范数正则化问题，使用了一种名为半空间随机投影梯度的随机优化算法。

Growing Regularization (GREG) [94] 在增长惩罚下利用正则化，并使用两种算法。第一种算法专注于修剪时间表，并采用 $\ell_1$-范数 [14] 来获得修剪的掩码。与立即移除不重要的滤波器不同，使用逐渐增长的 $\ell_2$ 惩罚将它们逐渐推向零。第二种算法使用增长正则化来利用底层的 Hessian 信息。作者观察到，随着正则化参数的增加，权重差异增加，权重自然会分离。如果差异足够大，即使是简单的 $\ell_1$-范数也可以是准确的标准。

D. 优化工具

优化工具被集成到修剪过程中，以找到或诱导神经网络中的结构化稀疏性。例如，Taylor Expansion 通过在特定滤波器变为零时近似损失函数的变化来找到滤波器的重要性。变分贝叶斯方法通过利用先验和后验分布来确定滤波器的重要性。基于 SGD 的方法修改梯度更新规则以检测和解决冗余滤波器。基于 ADMM 的方法通过使用 ADMM 优化算法强加结构化稀疏性约束并找到解决方案。贝叶斯优化有助于减少在学习最优稀疏结构时遇到的 “维数诅咒问题” [238]。

*1) Taylor Expansion: *Taylor Expansion [239] 将函数扩展为泰勒级数，这是一个无限项的总和。函数 $f(x)$ 的泰勒展开在某个点 $a$ 处为：

$$
f(x) = f(a) + f'(a) \frac{1}{1!} (x - a) + f''(a) \frac{1}{2!} (x - a)^2 + R_2(x)
$$

其中 $f'(\cdot)$ 和 $f''(\cdot)$ 分别是关于 $x$ 的一阶导数和二阶导数。

在结构化修剪中，Taylor Expansion 用于近似移除某些权重时损失 $\Delta L$ 的变化，并且由于修剪后的权重设置为 0，可以使用泰勒展开在 $a = 0$ 处评估权重 $w$ 的损失函数 $L(w)$。通过操作 (12)，我们可以得到：

$$
\Delta L = \frac{\partial L}{\partial w} \Delta w + \frac{1}{2} \Delta w^T H \Delta w + R_2(w)
$$

其中 $\frac{\partial L}{\partial w}$ 是损失函数关于权重的一阶梯度， $H$ 是包含二阶导数的 Hessian 矩阵。与基于正则化的修剪方法不同，使用 Taylor Expansion 进行修剪不需要等待激活训练到足够小 [96]。

一阶和二阶泰勒展开具有各自的特点。二阶展开包含了更多的信息，但它需要计算二阶梯度，这在计算上是昂贵的。相比之下，一阶展开可以从反向传播中获得，而不需要额外的内存，但提供的信息较少。

*First-Order-Taylor:* Mol-16 [95] 使用一阶信息来估计修剪激活图时损失的变化。由于高阶余项，包括二阶项，在计算上不可行，因此被丢弃，并通过广泛使用的 ReLU 激活函数被鼓励为小。因此，使用一阶项近似的绝对损失变化被用作特征图重要性的度量：

$$
\text{Importance} = \sum_{m=1}^{M} \left| \frac{\partial L}{\partial z_m} \right| z_m
$$

其中 M 是展平特征图的长度，而 z 是特征图中的一个激活。确定特征图的重要性后，排名最低的图将被修剪。

与 Mol-16 [95] 相比，Mol-19 [96] 提出了一种更通用的方法，使用泰勒展开来近似最终损失的平方变化。与 Mol-16 [95] 使用增加内存消耗的激活不同，Mol-19 [96] 基于权重计算重要性 I：

$$
I_S(W) \triangleq \sum_{s \in S} \left( \frac{\partial L}{\partial w_s} \right)^2 w_s
$$

其中 S 是参数的结构集，例如一个卷积滤波器， $w_s$ 是滤波器中的个体权重， $\frac{\partial L}{\partial w_s}$ 表示梯度。一阶展开的计算速度明显快于二阶展开，但准确度略有下降。

由于其简单和高效，一阶展开被许多方法广泛采用，如 GBN [75] 和 GDP-Lin [116]，这些在其他部分讨论。

*Second-Order-Taylor:* 在本小节中，利用包含二阶信息的 Hessian 矩阵 H（如公式 (14) 中的）被利用。首先，简要介绍在无结构修剪中使用二阶泰勒展开的开创性研究。其次，解决结构化修剪方法。

*开创性无结构研究：*大多数当前使用二阶泰勒展开的结构化修剪方法基于两个开创性的无结构修剪研究：Optimal Brain Damage (OBD) [240] 和 Optimal Brain Surgeon (OBS) [241]。OBD [240] 假设 H 为对角线以简化计算。然后使用对角线 H 来计算参数重要性。然而，OBS [241] 发现大多数 Hessian 矩阵 H 都是强非对角线的。因此，使用完整的 H 并计算参数重要性 H^{-1}。

*结构化修剪方法：*由于 OBD [240] 和 OBS [241] 在无结构修剪中的成功，二阶展开被应用于结构化修剪。由于深度 CNN 有数百万参数，计算和存储 Hessian H 变得具有挑战性 [97]。最近的方法旨在为结构化修剪近似 Hessian 矩阵。

Collaborative Channel Pruning (CCP) [97] 通过仅使用预训练模型的一阶导数来近似 Hessian 矩阵。一阶信息可以从反向传播中检索，无需额外存储。此外，Peng 等人利用移除多个通道而不是单个通道的效果。H 中的非对角元素反映了两个通道之间的相互作用，因此利用了通道之间的依赖性。CCP 将通道选择问题建模为一个受约束的 0-1 二次优化问题，以评估修剪和未修剪通道的联合影响。

Eigen Damage (ED) [98] 引入了一个基线方法，即 OBD [240] 和 OBS [241] 的简单结构化扩展。然后应用了两种算法来改进基线方法。基线方法将参数的个体变化求和到滤波器级别。然而，计算和存储 Hessian H 是不可行的。Wang 等人提出了第一种算法 [98]，该算法应用 K-FAC [242] 近似来分解滤波器。由于简单扩展和第一种算法都未能捕捉到滤波器之间的相关性，因此应用了第二种算法，在修剪之前采用 K-FAC [242] 将权重空间投影到 Kronecker-Factored eigenspace (KFE) [243]，其中相关性较小。

Group Fisher Pruning (GFP) [99] 解决了其他修剪方法面临的困难，当多层的通道耦合并需要同时修剪时。首先，使用层分组算法自动识别耦合通道。其次，使用 Hessian 信息作为单个通道和耦合通道的统一重要性标准。在 Fisher 信息的帮助下，将 Hessian 矩阵转换为一阶梯度的平方。

*2) 变分贝叶斯：*贝叶斯推断[244]是一种推断参数θ的后验概率分布p(θ|x)的方法，已知参数θ的先验概率分布p(θ)和观测数据x。计算θ的后验分布在公式中给出：

$$
\text{Posterior} \propto \frac{\text{Likelihood} \cdot \text{Piror}}{\text{Evidence}}
$$

其中， $p(x) = \int p(x|θ)p(θ)dθ$ 是证据项。然而，在涉及大量数据时，计算证据通常需要进行计算上不可行的积分。变分贝叶斯(VB)方法[245]通过一个变分分布q(θ)来近似后验分布p(θ|x)。具体来说，q(θ)通过最小化Kullback–Leibler (KL)散度来优化，该散度衡量q(θ)和p(θ|x)之间的“相似性”。由于KL散度的计算涉及不可行的后验分布p(θ|x)，优化问题通过等价地转换为最大化证据下界(ELBO)来解决。

变分剪枝(VP)[102]基于通道重要性由随机变量指示，因为通过确定性通道重要性进行剪枝本质上是不恰当和不稳定的。因此，使用BN的参数γ来指示通道显著性并将模型γ建模为高斯分布N(μ, σ)。为了引入稀疏性，VP利用高斯分布N的中心性质，从N(μ=0, σ)采样γ作为稀疏先验分布。在优化ELBO后，具有接近零均值和小方差的分布被认为是安全的剪枝对象，因为这样的分布不太可能有显著的参数。

递归贝叶斯剪枝(RBP)[103]针对冗余的后验，它假设通道之间存在层间依赖性。首先，每个输入通道都通过一个dropout噪声θ[105], [236]进行缩放，dropout率为r。其次，dropout噪声被建模为一个马尔可夫链，以利用通道之间的层间依赖性。为了获得冗余的后验，选择了一个诱导稀疏性的狄拉克式先验。此外，RBP[103]采用了重新参数化技巧[236]来缩放对应通道上的噪声θ，以考虑数据拟合度。因此，dropout率r可以与权重一起通过基于梯度的方式进行更新。当r大于阈值时，通过将相应通道设置为零来进行剪枝。

还有其他一些研究也采用了贝叶斯观点。VIBNet[104]使用变分信息瓶颈，这是衡量相邻层之间冗余的信息化理论度量。Louizos等人[105]有效地近似了通道冗余，使用了马蹄铁先验。Neklyudov等人[106]使用了一个对数正态先验，得到了一个可处理且可解释的对数正态后验。

*3) 其他方法：SGD-based（随机梯度下降法基础的）*：与将滤波器置零不同，Centripetal SGD（C-SGD）[107] 使得冗余滤波器彼此相同，并将相同的滤波器合并为一个滤波器。Regularized Modernized Dual Averaging (RMDA) [108] 添加了动量到 RDA 算法[246]，并确保训练出的模型与原始模型具有相同的结构。为了剪枝模型，RMDA 采用了 Group Lasso 来促进结构化稀疏。

*ADMM-Based（基于乘子法的）：*交替方向乘子法（ADMM）[247] 是一种优化算法，用于将初始问题分解为两个更小、更易处理的子问题。StructADMM [111] 研究了不同类型的结构稀疏，如滤波器级和形状级。Zhang 等人使用了一个渐进式和多步骤的 ADMM 框架：在每一步中，它使用 ADMM 来剪枝并遮蔽零权重，留下剩余权重作为下一步的优化空间。

*贝叶斯优化：*贝叶斯优化（BO）[248] 是一种序列设计策略，用于全局优化不假设任何函数形式的黑盒函数。当剪枝一个涉及极大设计空间的模型时，就会发生维度的诅咒问题[249]。Rollback [113] 采用了基于 RL（强化学习）风格的自动通道剪枝[130]，并使用 BO 来确定最优的剪枝策略。

*软阈值：*软阈值[250] 是数字信号处理领域的去噪操作。Soft Threshold Reparameterization (STR) [114] 采用了这个操作器来学习每层优化的非均匀稀疏预算。

## E. 动态剪枝

结构化剪枝可以在训练期间和推理期间以动态方式进行。训练期间的动态剪枝旨在通过在训练期间保持动态剪枝掩模来保持模型的表示能力。这也被称为软剪枝，以确保错误的剪枝决策可以稍后恢复。另一方面，硬剪枝则使用固定掩模永久移除权重。在推理期间的动态剪枝表明，根据不同的输入样本动态地剪枝网络。例如，一个包含清晰目标的简单图像与一个复杂图像相比，需要较少的模型容量[108]。因此，动态推理提供了更好的资源-准确性权衡。

*1) 训练期间的动态剪枝：*权重级动态：训练时动态剪枝的概念最初在 Dynamic Network Surgery (DNS) [251] 中引入，这是一种非结构化剪枝方法。为了阐明与其他方法的区别，其公式为：

$$
W_{l}^{i,j} \leftarrow W_{l}^{i,j} - \eta \frac{\partial L(W_l \odot T_l)}{\partial (W_l^{i,j} T_l^{i,j})}
$$

其中 η 是学习率，⊙ 表示哈达玛积运算符。Wl 是第 l 层的权重矩阵，其中所有核的权重展开并连接在一起。Tl 是与权重矩阵形状相同的二进制掩模，指示权重的重要性。L(·) 是损失函数。Wl^{i,j} 和 Tl^{i,j} 表示 Wl 和 Tl 中的单个元素。W 和 T 交替更新。所有权重都更新，因此错误剪枝的参数有机会重新生长。

*滤波器级动态：*Soft Filter Pruning (SFP) [115] 采用了结构化方式的动态剪枝思想。其使用基于这样一个前提，即使用固定掩模的硬剪枝会减少优化空间。因此，它在每个时期基于滤波器的 ℓ2-范数动态生成掩模。软剪枝意味着将滤波器的值设置为零，而不是移除滤波器。之前软剪枝的滤波器在下一个时期可以更新，在这个时期掩模将根据新的权重重新形成。更新规则为：

$$
W_l \leftarrow W_l - \eta \frac{\partial L}{\partial (W_l \odot m_l)}
$$

其中 Wl ∈ RNl+1×NlKK 和 ml ∈ RNl+1。

全局动态剪枝 (GDP-Lin) [116] 也在训练期间保持一个基于滤波器重要性的二进制动态掩模。GDP-Lin 采用了一阶泰勒展开来近似每个滤波器的全局判别能力。此外，作者认为频繁改变掩模不能有效地指导剪枝。因此，掩模每 e 个迭代更新一次，其中 e 设置为一个递减值以加速收敛。

除了保持稀疏动态掩模，Dynamic Pruning with Feedback (DPF) [117] 同时保持一个密集模型。其背后的前提是稀疏模型可以被认为是带有压缩误差的密集模型。这个误差可以用作反馈，以确保梯度更新的正确方向。为此，使用从稀疏模型计算的梯度来更新密集模型。应用梯度于密集模型的优势在于它有助于权重从误差中恢复。

CHannel EXploration (CHEX) [118] 使用两个过程动态调整滤波器重要性。第一个过程是通道剪枝过程，使用列子集选择标准[252]。第二个过程是基于正交投影[253]的重新生长过程，以避免重新生长冗余通道并探索通道多样性。重新生长的通道恢复到最近使用的（MRU）参数，而不是零。为了更好地保持模型准确性，剩余通道在所有层之间动态重新分配。

Dynamic Sparse Graph (DSG) [119] 在每次迭代中通过构建的稀疏图动态激活少量关键神经元。DSG 是从这样一个论点发展而来的：直接根据输出激活计算以寻找关键神经元的成本非常高。降维搜索被制定为通过在较低维度上计算输入和滤波器来预测激活输出。此外，为了防止 BN 层破坏稀疏性，Liu 等人引入了双重掩模选择，使用相同的选择掩模在 BN 层之前和之后[119]。

其他方法，如 SCP[78] 和 DMC[82]，也保持掩模动态。这些在第 II-C 节中讨论。

*2) 推理期间的动态剪枝：*Runtime Neural Pruning (RNP) [123] 基于静态模型未能利用输入图像的不同属性，对简单模式和复杂模式图像使用相同权重的前提。它使用一个框架，以 CNN 作为主干和 RNN 作为决策网络。网络剪枝被建模为一个马尔可夫决策过程，并通过强化学习（RL）进行训练。RL 代理评估滤波器重要性，并根据任务的样本相对难度执行通道级剪枝。因此，当图像对任务更容易时，会生成一个更稀疏的网络。

Feature Boosting and Suppression (FBS) [124] 通过引入一个微小且可微的辅助网络来预测输出通道重要性。在辅助网络中，使用 2D 全局平均池化来将前一层的激活通道降采样为标量。这样做的优点是它减少了辅助网络中的计算开销。然后，一个显著性预测器使用这些降采样的标量通过一个全连接层和激活函数生成预测显著性分数。最终，只有前 k 个显著通道参与推理时计算。

Deep Reinforcement Learning pruning (DRLP) [126] 学习通道的运行时（动态）和静态重要性。运行时重要性衡量特定于输入的通道重要性。相比之下，静态重要性衡量整个数据集的通道重要性。与 RNP [123] 类似，DRLP 使用强化学习（RL）。RL 框架包含两个部分：一个静态运行时和一个运行时。每个部分包含一个重要性预测器和一个代理，分别提供静态/运行时重要性和分层剪枝比率。此外，一个权衡修剪器根据这两个部分的输出生成一个统一的修剪决策。

Dynamic Dual Gating (DDG) [127] 使用两个独立的门控模块（空间门控和通道门控）来根据输入确定推理时的重要性。空间门控模块由一个自适应平均池化组成，后跟一个 3x3 卷积 (CONV) 层，用于提取信息丰富的空间特征。通过空间掩模，只有信息特征被允许传递到下一层。通道门控模块的工作方式类似于现有的方法，如 Feature Boosting and Suppression (FBS) [124]，通过使用全局平均池化后跟全连接 (FC) 层的应用。为了使梯度流动，DDG 为这两个门控模块都使用了 Gumbel-Softmax 重新参数化 [254]。

Fire Together Wire Together (FTWT) [128] 将动态剪枝任务建模为一个自监督的二元分类问题。所提出的框架使用一个预测头部来生成可学习的二进制掩模，以及一个真实掩模来指导每个卷积层之后的剪枝学习。预测头部由一个全局最大池化层、一个 1x1 卷积层和一个 softmax 层组成。然后，预测头部的输出被四舍五入以生成二进制掩模。为了实现自监督学习，需要真实掩模；通过排名输入激活的范数，做出真实的剪枝决策以指导学习。

同时，Contrastive Dual Gating (CDG) [129] 实现了另一个自监督动态剪枝框架，使用对比学习 [255]。对比学习通过训练模型来利用两个对比分支的高层特征的潜在对比性。因此，CDG 使用基于相似性的对比损失 [256] 进行基于梯度的学习，而无需标记数据。实证研究表明，剪枝决策不能在对比分支之间转移。因此，引入了双重门控，两个分支具有不同的剪枝掩模。CDG 使用类似于 CGNet [257] 的框架为每个对比分支。与 DDG [127] 一样，CDG 使用一个 3x3 卷积层来提取未剪枝特征的空间特征。

## F. NAS-Based Pruning
鉴于手动确定与剪枝相关的超参数（例如，层级剪枝比率）是繁琐的，因此提出了基于神经架构搜索（NAS）的剪枝方法，以自动发现剪枝结构。根据对神经架构搜索（NAS）[26]的调查，我们将剪枝中的 NAS 分为三种方法。NAS 可以被建模为：
1) 强化学习（RL）问题，其中 RL 代理通过搜索动作空间（如剪枝比率）来寻找稀疏子网络。
2) 梯度基础方法，这些方法修改梯度更新规则，使得具有稀疏性约束的优化问题对权重可微。
3) 进化方法，采用进化算法探索和搜索稀疏子网络。

*1) 基于强化学习的：*AutoML for Model Compression (AMC) [130] 使用 RL 自动选择适当的层级剪枝比率，而无需手动敏感性分析。为了在连续动作空间中搜索，采用了深度确定性策略梯度（DDPG）算法[258]。演员接收 11 个依赖于层的状态（如 FLOPS）作为输入，并输出关于剪枝比率的连续动作。为了确保准确度保证的剪枝，DDPG 代理的奖励函数从（20）修改为（21）：

$$
R_{err} = -\text{Error} \quad (20)
$$

$$
R_{FLOPs} = -\text{Error} \cdot \log(\text{FLOPs}) 
$$

$$
\quad R_{Param} = -\text{Error} \cdot \log(\\# \text{Param}) \quad (21)
$$

Automatic Graph encoder-decoder Model Compression (AGMC) [131] 采用了这样一个前提，即 AMC [130] 仍然需要手动选择输入到 RL 代理的固定 11 个状态。此外，固定的环境状态忽略了计算图中的丰富信息。因此，DNN 被建模为计算图，并输入到基于图卷积网络（GCN）的图编码器-解码器[259]中，自动学习 RL 代理的输入状态。与 AMC [130] 相比，AGMC 重新调整剪枝比率以补偿任何未满足的稀疏性约束，而不是将 FLOPs 应用到奖励函数（21）中。

DECORE [132] 采用了多代理学习。多个代理的存在来自于为所有通道分配专用代理，每个代理仅学习一个参数，代表一个二进制决策。与 AMC [130] 和 AGMC [131] 相比，DECORE 对通道本身的状态进行建模。为了学习动作参数，当准确度保持任务和压缩任务都很好地完成时，给出更高的奖励：

$$
R_{lcomp.} = \prod_{j=1}^{N_{l+1}} (1 - a_j^l) \quad R_{acc} = 
\begin{cases} 
1 & \text{if } \hat{y} = y \\
-\lambda & \text{else}
\end{cases} 
$$

$$
\quad R_l = R_{lcomp.} \cdot R_{acc} \quad (22)
$$

其中 $N_{l+1}$ 表示第 l 层的输出通道大小， $a_j^l$ 是第 l 层的动作向量，y 和 $\hat{y}$ 分别是真实标签和预测标签， $-\lambda$ 是错误预测的大惩罚。奖励函数不考虑 FLOPs；相反，压缩任务的奖励仅考虑剩余的通道数量。

GNN-RL [133] 首先将 DNN 建模为多阶段图神经网络（GNN）以学习全局拓扑。然后，生成的层次计算图被用作代理的环境状态。与 AMC [130] 和 AGMC[131] 都使用 DDPG 不同，这种方法使用近端策略优化（PPO）算法[260] 作为策略，因为 PPO 给出了更好的实验结果。此外，由于特别设计的图环境，RL 系统的奖励不需要包含稀疏性约束。准确度是奖励的唯一指标，由（20）描述。

*2) 基于梯度的：*为了搜索层级稀疏性，Differentiable Markov Channel Pruning (DMCP) [136] 将通道剪枝建模为一个马尔可夫过程。这里，状态意味着在剪枝过程中保留通道，状态之间的转移代表剪枝过程。DMCP 通过可学习的架构参数参数化转移和预算损失。因此，最终损失相对于架构参数是可微的。使用两个阶段以基于梯度的方式完成剪枝。第一阶段使用变体三明治规则[261]更新未剪枝网络的权重。第二阶段更新架构参数。

Differentiable Sparsity Allocation (DSA) [137] 使用基于梯度的方法在连续空间中优化稀疏性分配，这种方法比离散搜索更有效。验证损失被用作可微的替代品，使得验证准确性的评估是可微的。接下来，使用概率不同的剪枝过程作为不可微的硬剪枝过程的替代品。最后，在预算约束下获得稀疏性分配。使用受 ADMM 启发的优化方法[247]来解决受约束的非凸优化问题。

Differentiable Hyper Pruning (DHP) [138] 使用超网络为骨干网络生成权重。设计的超网络由三层组成。1) 潜在层首先取两个可学习的潜在向量 $\{z_l \text{ and } z_{l-1}, z_l \in \mathbb{R}^{N_{l+1}}\}$ 并形成潜在矩阵。然后 2) 嵌入层将潜在矩阵投影到嵌入空间，形成嵌入向量。最后，3) 显式层取嵌入向量并输出可用作 CONV 层权重的权重。剪枝通过对潜在向量进行 $\ell_1$ 稀疏性正则化来完成，并且使用近端梯度算法[237]来更新潜在向量。由于近端操作步骤具有封闭形式解，这种方法大约是可微的。

Learning Filter Pruning Criteria (LFPC) [140] 搜索层级剪枝标准而不是剪枝比率。其使用的基础是，滤波器对于提取粗略和细粒度特征具有不同的分布。这使得对所有层使用相同的剪枝标准是不恰当的。为了探索标准空间，引入了标准参数来指导标准的选择。为了使损失对标准参数可微，这种方法使用了 Gumbel-Softmax 重新参数化[254] 来使损失可微。

Transformable Architecture Search (TAS) [141] 搜索网络的宽度和深度。引入了两个可学习参数 $\alpha$ 和 $\beta$ 来指示可能的通道数和层数的分布。可以根据 $\alpha$ 和 $\beta$ 采样子网络。应用 Gumbel-Softmax 重新参数化[254] 使采样过程可微。搜索目标是在鼓励较小的计算成本和惩罚未满足的资源预算的同时，最小化验证损失。进一步使用知识蒸馏来提高剪枝网络的性能。

Exploration and Estimation (EE) [142] 通过两步基于梯度的方法实现通道剪枝。第一步是探索子网络以允许更大的搜索空间，这一步骤采用了快速采样技术（随机梯度哈密顿蒙特卡洛[262]）。为了引入稀疏性，该方法在探索过程中应用了感知计算量（FLOPs）的先验分布。第二步是估计步骤，用于指导生成高质量的子网络。

*3) 基于进化算法的：*MetaPruning [148] 旨在找到最优的逐层通道数，采用两阶段框架实现。第一阶段训练一个名为 PruningNet 的元网络来生成不同的结构。PruningNet 接收随机采样的编码向量，这些向量代表结构，并以端到端的方式进行学习。在第二阶段，部署了一个进化搜索算法来在约束条件下寻找最优结构。由于 PruningNet 为所有剪枝后的网络预测权重，因此在搜索时不需要进行微调。

与 MetaPruning [148] 相比，ABCPruner [149] 寻找最优的层级通道数，采用了一次性方法，并且不需要额外的支持网络。此外，它通过限制保留的通道到一个给定的空间，大幅减少了剪枝结构的组合数。为了搜索最优的剪枝结构，应用了基于人工蜂群算法（Artificial Bee Colony algorithm, ABC）[263]的进化算法。

与在完整网络上使用进化算法（Evolutionary Algorithms, EA）不同，Cooperative Coevolution algorithm for Pruning (CCEP) [150] 使用合作共同进化的方法，将网络按层分组，并对每层应用 EA。通过将网络分解为多个组，显著减少了搜索空间。整体框架采用了迭代剪枝和微调策略。在每次迭代中，通过随机剪枝滤波器生成多个个体候选项（父代）。然后通过在父代上进行按位变异生成后代，其中每个位代表滤波器的存在与否。之后，使用准确度和浮点运算次数（FLOPs）来评估个体，并且只有排名靠前的 k 个个体被保留到下一次迭代中。

## G. 扩展
结构化剪枝可以通过彩票假设（Lottery Ticket Hypothesis）、正交网络压缩技术以及不同粒度的结构稀疏性进行扩展。

*1) 彩票假设：*彩票假设（Lottery Ticket Hypothesis, LTH）[265] 主张“密集的、随机初始化的前馈网络包含子网络（中奖彩票），这些子网络在独立训练时能够在相似数量的迭代中达到与原始网络相当的测试准确率。” 权重回退通常在基于 LTH 的论文中使用。在 Frankle & Carbin [265] 的研究中，权重和学习率计划被回退到第 k = 0 个时期的值，以找到初始化时的稀疏子网络。Frankle 等人 [266] 提出将权重和学习率计划回退到 k > 0 的时期，以处理 SGD 的随机性（噪声）。已经提出了不同的方法来实现非结构化的 LTH：SNIP [267] 保留训练损失；GraSP [268] 保留梯度流；神经切核（Neural Tangent Kernel, NTK）[269] 捕获训练动态；GF [270] 剪除导致梯度流变化最小的权重。

Renda 等人 [271] 提出只回退学习率计划而不是权重值（学习率回退）。实验表明，回退技术一致性地优于微调，其中学习率回退在所有场景中要么优于要么匹配权重回退。

重新思考网络剪枝的价值（Rethinking the Value of Network Pruning, RVNP）[152] 重新评估了网络剪枝的价值。该论点是传统的微调技术并不比从头开始剪枝更好。在大规模数据集上扩展 LTH 到结构化剪枝设置中未能找到中奖彩票。

Early Bird, EB[153] 提出在训练的早期阶段找到中奖彩票，而不是完全训练密集网络。为了找到 EB 彩票，基于从同一模型剪枝得到的两个子网络之间的汉明距离计算掩模。

前景剪枝（Prospect Pruning, ProsPr）[154] 认为应该考虑剪枝后网络的可训练性。可训练性存在于模型将在接受剪枝后进行训练时。因此，元梯度（梯度的梯度）被提出作为可训练性的度量。与在初始化时估计损失变化不同，ProsPr 估计了剪枝对损失的影响，这是通过在训练开始时的几个梯度下降步骤来完成的。

通过梯度流保护实现早期压缩（Early Compression via Gradient Flow Preservation, Early CroP）[155] 通过解决三个关键问题实现早期剪枝。问题 1）为什么要剪枝，通过将基于 GF 的剪枝标准 [270] 扩展到结构化剪枝来解决。问题 2）如何剪枝，通过使用梯度流和 NTK [269] 之间的联系来保持训练动态。最后，问题 3）何时剪枝，通过找到懒惰核区域来解决，这是剪枝对训练动态影响很小的阶段。

剪枝感知训练（Pruning-aware Training, PaT）[156] 试图解决早期剪枝应该何时开始的问题。PaT 基于这样一个前提，即具有相同剩余神经元数量的子网络可以具有非常不同的架构。为了确定何时架构稳定，提出了一种新的度量标准，称为早期剪枝指标（Early Pruning Indicator, EPI），该指标计算两个子网络的结构相似性。

*2) 联合压缩：*一些流行的技术，如架构搜索、分解和量化，与剪枝方法正交，可以用于神经网络压缩。按顺序应用这些技术似乎是自然而然的扩展，但由于不同的优化目标，可能会导致次优解决方案。因此，研究人员提出了将这些技术联合应用。

*剪枝和 NAS：*NPAS [161] 开发了一个联合网络剪枝和架构搜索的框架。该方法对编译器感知，并将计算激活函数（如 sigmoid 函数）替换为编译器友好的函数。

*剪枝和量化：*DJPQ [162] 以基于梯度的方式联合执行结构化剪枝和混合位宽量化。这种结构化剪枝基于变分信息瓶颈 [104]，并将混合位宽量化扩展到仅限 2 的幂次量化。Bayesian Bits (BB) [163] 统一了混合精度量化和剪枝的观点，并在滤波器上引入了混合精度门控，以实现结构化剪枝。IODF [164] 利用基于 8 位量化的仅整数算术，并具有可学习的二进制门控，以在推理期间消除冗余滤波器。

*剪枝、NAS 和量化：*APQ [165] 联合执行这些任务。首先训练一个支持大搜索空间的通道数的一次性网络 [272]。接下来，设计了一个量化感知的准确度预测器，以评估所选结构与混合精度量化的准确度 [273]。然后，使用基于进化算法的架构搜索 [274] 在延迟或能量约束下找到最佳准确度模型。知识蒸馏进一步用于提高剪枝网络的性能。

*剪枝和分解：*一些研究在统一框架内利用结构化剪枝和低秩分解技术。Hinge [166] 的作者认为，剪枝技术无法处理残差块中的最后一个 CONV 层，而低秩分解可以做到。这两种技术通过在滤波器后引入稀疏性诱导矩阵而相互结合。对稀疏性诱导矩阵的列和行施加组稀疏性，分别实现了滤波器剪枝和分解。协同压缩（Collaborative Compression, CC）[167] 同时学习模型的稀疏性和低秩性，以联合实现通道剪枝和张量分解。

*3) 特殊粒度：*除了在权重级别进行非结构化剪枝和在滤波器级别进行结构化剪枝外，还有在其他粒度上的剪枝方法。为了对这些方法进行分类，我们定义了三个基本维度：输出维度 $N_{l+1}$、输入维度 $N_l$ 和核维度 $K^2$。滤波器剪枝可以被视为沿维度 $N_l \times K^2$ 进行剪枝，而通道剪枝是沿维度 $N_{l+1} \times K^2$。它们是相似的，因为剪除第 l 层的滤波器将移除第 l+1 层的相应通道。图 3 显示了更多不同的剪枝粒度。

*沿着基本维度的组：*GBD [170] 和 SSL [91] 引入了组级剪枝（图 3(c)），这种剪枝方式是沿着输出维度 $N_{l+1}$ 进行的，即在所有滤波器中位于相同位置的权重被剪除。SWP [171] 以条纹级（图 3(d)）的方式沿着输入维度 $N_l$ 进行剪枝。与组级剪枝相比，条纹级剪枝保持了滤波器的独立性，因为在滤波器之间没有模式。PCONV [172] 使用核级剪枝（图 3(e)）沿着核维度 $K^2$ 剪枝权重。

*不同在基本维度上：*一些研究人员发现，沿着基本维度进行相同的剪枝决策可能不是最优的，他们提出要深入研究基本维度。GKP-TMI [173] 使用分组核剪枝（图 3(f)）。在核级剪枝的基础上，这种方法对不同的滤波器进行分组，并在输出维度 $N_{l+1}$ 上做出不同的剪枝决策。剩下的内核被重新排列，以输出一个从并行计算中受益的密集结构化剪枝网络。1xN [174]（图 3(f)）使用 1xN 剪枝模式，将共享相同输入通道索引的 N 个输出内核视为基本剪枝粒度。PCONV [172] 通过查看内核并沿着内核维度 $K^2$ 做出不同的剪枝决策，使用基于模式的剪枝（图 3(g)）。块级剪枝 [161]（图 3(h)）将基于模式的剪枝 [172] 推广到共享相同模式的内核组，其中内核在输出维度 $N_{l+1}$ 和输入维度 $N_l$ 上都被分组。

*层级：*除了将滤波器分解为更小的粒度外，Shallowing Deep Networks (SDN) [175] 使用逐层剪枝来剪除整个层。具体来说，SDN 在每层之后放置一个线性分类器探针 [275] 来评估该层的有效性。在剪枝不重要的层后，使用知识蒸馏技术重新训练网络。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/9a2337e9154d4752a1dc359460806023.png" width="70%" /> </div>


# III. 未来方向

## A. 剪枝主题
每个在第II节中的小节都有在未来进一步发展的潜力。此外，结构化剪枝将继续从非结构化剪枝中借鉴思想。在本节中，我们将讨论一些直接与剪枝相关的有前景的主题。

*剪枝理论：*除了在第II-D节中使用优化工具进行剪枝，一些工作从突触流[180]、信号传播[181]和图论[182]的角度审视剪枝。此外，剪枝过程可以通过利用模型的解释[183]、损失景观[110]、泛化-稳定性权衡[184]和模型的熵[185]来指导。此外，对LTH（彩票假设）背后的理论进行了研究，提出了对数剪枝[186]。此外，不同的训练方法[187]、[188]、[189]可以与剪枝结合使用。上述方向在结构化剪枝中有潜力。

*剪枝机制：*研究人员对当前的训练-剪枝-重训练三阶段机制有了新的看法。首先，彩票假设是在非结构化剪枝中提出的，预计将扩展到结构化剪枝。其次，单次剪枝[190]、[191]只剪枝一次以获得剪枝模型。结构化剪枝也有可能从这种机制中受益。第三，AC/DC训练[192]能够共同训练密集和稀疏模型。因此，在剪枝和训练期间处理多个模型是结构化剪枝的另一个有前景的方向。

*剪枝率：*结构化剪枝也可以扩展当前的权重剪枝策略，研究每层的剪枝比率[144]、[193]、[194]。

*剪枝领域：*利用频域[65]、[195]中的表示来指导剪枝是另一个有趣的方向。

## B. 针对特定任务的剪枝
剪枝技术可以应用于其他任务，以实现高计算效率。以下是一些直接的例子：超分辨率[203]、个人重识别[204]、[276]、医学成像诊断[205]、面部属性分类[206]和集成学习[207]、[208]。除了上述任务，一些新兴的方向仍处于早期阶段，但在未来是有前景的。

*针对联邦学习的剪枝：*联邦学习[277]旨在解决在不将数据传输到中央位置的情况下训练模型的问题。剪枝[196]、[197]有助于减轻设备和服务器之间所需的通信成本。Zhang等人[196]认为，每个设备和服务器的非独立同分布（non-IID）程度是不同的，因此提出了计算每个设备的不同的剪枝比率和服务器的聚合预期剪枝比率。

*针对持续学习的剪枝：*持续学习解决了灾难性遗忘[198]的问题。一些开创性的工作[199]、[200]、[201]使用剪枝后的稀疏网络的滤波器来训练新任务，因此训练过程不会对以前任务的性能造成恶化。

*有限数据集：*数据集压缩和剪枝[278]是新兴的。这个想法是利用训练数据集中的部分图像来训练网络。结构化剪枝[202]在这个新趋势中有很多研究主题。

## C. 剪枝特定网络
除了流行的CNN之外，剪枝对其他类型的神经网络也是有益的，如MLPs[217]、矩形神经网络[218]、脉冲神经网络[219]、[220]和生成对抗网络（GAN）[209]、[210]、[211]、[212]。

剪枝基于CNN的Transformer：将CNN的架构设计纳入Transformer模型[16]、[17]、[18]、[19]、[20]的趋势日益增长。因此，采用这些结构化剪枝技术来压缩这些新架构设计是有意义的[15]、[213]。

*剪枝基于Transformer的架构：*Transformer架构的注意力机制基本上使用全连接层操作，当将核大小设置为1时，可以被视为卷积层的独特表现[279]。近年来，基础模型[214]，如GPT-3[215]和通用代理[216]，是通往人工通用智能（AGI）的可能途径。这些巨大模型的注意力组件的效率可以从结构化剪枝研究中受益。

## D. 剪枝目标
在过去的几年中，剪枝的目标已经从减少非结构化剪枝中的参数数量转变为最小化结构化剪枝中的FLOPs。最近，剪枝的目标一直在演变，以满足实际场景的实际需求。

*硬件：*将剪枝纳入硬件过程是一种新兴趋势。硬件编译器感知剪枝[221]根据编译器调整期间构建的子图的结构信息进行剪枝。

*能源：*随着AI模型部署的扩展，AI模型的不断增长的能源消耗需要更多的关注。在剪枝期间考虑能源应该是研究的方向。能源感知剪枝[222]贪婪地剪枝消耗最多能源的层，因为最小化MACs可能并不一定减少最多的能源消耗。

*鲁棒性：*网络的鲁棒性描述了在攻击下网络被愚弄做出错误预测的容易程度[223]。在模型设计过程中同时考虑模型的鲁棒性和计算成本是一个新的趋势。研究人员发现，剪枝带来的稀疏性可以提高对抗性鲁棒性[280]。线性嫁接（Grafting）[224]基于网络鲁棒性有利于线性函数的前提，并提出了线性嫁接方法。ANP-VS[225]识别具有高脆弱性的潜在特征，并提出了一个贝叶斯框架，通过最小化对抗性损失和特征级脆弱性来剪枝这些特征。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
