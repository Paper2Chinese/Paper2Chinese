# 题目：[Enhancing Visual Grounding in Vision-Language Pre-Training With Position-Guided Text Prompts](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10363674)  
## 增强视觉-语言预训练中的视觉定位与位置引导文本提示
**作者：Alex Jinpeng Wang; Pan Zhou; Mike Zheng Shou; Shuicheng Yan** 

****
# 摘要
视觉-语言预训练（VLP）在图像和文本对齐方面展示了巨大的潜力，为广泛的跨模态学习任务铺平了道路。然而，我们观察到 VLP 模型在视觉定位和局部化能力方面经常表现不佳，而这些能力对许多下游任务，如视觉推理，至关重要。为此，我们提出了一种新颖的位置引导文本提示（PTP）范式，以增强使用 VLP 训练的跨模态模型的视觉定位能力。在 VLP 阶段，PTP 将图像划分为 $N \times N$ 块，并使用广泛使用的目标检测器来识别每个块中的目标。PTP 然后将视觉定位任务重新定义为填空问题，鼓励模型预测给定块中的目标或回归给定目标的块，例如填充“[P]”或“[O]”在 PTP 句子中，如“ The block [P] has a [O]。” 这种策略增强了 VLP 模型的视觉定位能力，使其能够更好地处理各种下游任务。此外，我们整合了目标之间的二阶关系，以进一步增强我们提出的 PTP 范式的视觉定位能力。将 PTP 纳入几种最先进的 VLP 框架，导致代表性跨模态学习模型架构和多个基准测试的一致显著改进，例如 ViLT 基线的零样本 Flickr30k 检索（平均 recall@1 提高 5.6）和最先进的 BLIP 基线的 COCO 标注（CIDEr 提高 5.5）。此外，PTP 在推理过程中丢弃其目标检测器，与其他方法不同，从而达到与基于目标检测器的方法相当的结果和更快的推理速度。


# 关键词
- 填空式
- 位置引导的文本提示
- 视觉-语言预训练
- 视觉基础



## I. 引言


视觉-语言预训练（VLP）模型，如 CLIP [40]、ALIGN [20] 和 CoCa [58]，显著改善了跨模态任务，如视觉问答 [4]、自然语言视觉推理 [46] 和图像标注 [1]，[9]。这些模型的成功归因于一个两阶段的学习过程：在大量图像-字幕数据上进行预训练以增强泛化能力，然后在下游任务上进行微调以实现无缝适应。这种高效的预训练和微调范式已确立了 VLP 模型在多模态研究领域的主导地位，凸显了它们在各种跨模态应用中进一步发展和持续性能改进的潜力。


![](https://img-blog.csdnimg.cn/direct/616191be86f6499d8a86e45eac4328cf.png)



在 VLP 中，视觉定位在各种任务中起着至关重要的作用，正如之前的研究 [3]，[56] 所证实的那样。传统的 VLP 模型 [3]，[31]，[62]，如图 1(a) 顶部所示，利用预训练在 1600 类 Visual Genome [23] 上的 Faster R-CNN [42] 提取显著区域特征和边界框。这些模型然后将边界框和目标特征作为输入，使其能够识别显著区域内的目标并确定其位置。然而，通过使用区域特征作为输入，模型选择性地关注边界框内的信息，同时忽略其边界之外的上下文数据 [17]。这种限制可能导致下游任务的性能不佳，需要使用额外的目标检测器来提取目标，进而导致推理速度显著变慢。

为了摆脱区域特征以提高效率，最近的研究 [17]，[22]（图 1(a) 中间）采用原始图像作为输入，而不是区域特征，并使用图像-文本匹配 [10] 和掩码语言建模 [12] 损失进行端到端训练。尽管速度更快，这些模型无法很好地学习目标位置及其关系。如图 1(b) 所示，一个训练良好的 ViLT 模型 [22] 可以成功识别图像中的目标。然而，它未能准确学习目标位置。例如，它错误地预测“这张图片中的狗在右边”。在微调评估期间，下游任务需要目标位置信息以全面理解图像。这一差距显著阻碍了下游任务的性能，强调了改进目标位置和关系学习的必要性。

在这项工作中，我们的目标是在保持下游任务快速推理时间的同时，解决端到端（e2e）模型中的位置学习问题。借鉴最近的提示学习方法 [21]，[33]，[41]，[57]，我们提出了一种新颖且有效的位置引导文本提示（PTP）范式（如图 1(a) 底部所示）用于 VLP。核心见解是通过在图像和文本中结合基于位置的共指标记，视觉定位可以转化为填空问题，从而显著简化目标信息的学习。为了在语言表达和图像之间建立联系，PTP 包括两个组件：（1）块标签生成，将图像划分为 $N \times N$ 块并识别每个块中的目标，以及（2）文本提示生成，将查询文本嵌入基于位置的文本查询模板中。这种创新方法在保持 e2e 模型效率优势的同时，促进了更准确的位置学习。

将位置信息整合到预训练阶段，我们的 PTP 显著增强了 VLP 模型的视觉定位能力。我们还研究了目标之间的二阶关系，以提高推理能力。重要的是，我们的方法保持了快速的推理时间，因为我们在下游任务中不依赖目标检测器。实验结果表明，我们的方法在零样本设置中显著优于其对手。我们提出的模型 PTP-BLIP 在 COCO 数据集上的零样本图像到文本检索 Recall@1 任务中表现出色，相较于 CoCa [58] 提高了 6.9% 的绝对准确度，同时使用了显著更少的训练数据（4 M 对 3B）和更小的模型规模（220 M 对 2.1B）。此外，PTP 的有效性不仅限于检索，如视觉定位和图像标注等其他视觉语言任务也证明了其成功。这些结果强调了我们基于位置的方法在推进跨模态学习领域最先进技术的潜力。


![](https://img-blog.csdnimg.cn/direct/19d34971961d43e4b6c4a0edfac61874.png)




我们的贡献可以总结如下：

1. 我们引入了一种新颖的视觉语言模型预训练范式，称为跨模态提示预训练，该方法在提示中明确加入了位置信息。据我们所知，这是首次尝试将这种方法用于预训练视觉语言模型。
2. 我们为所提出的 PTP 模型设计和评估了多种高质量的跨模态提示配置，展示了我们方法的多功能性和适应性。
3. 我们进行了全面的实验，使用四种主干模型，展示了 PTP 在一系列视觉语言任务中的有效性。我们将我们的方法扩展到数十亿规模的数据，并展示了其在强大的大语言模型中的有效性，进一步强调了其在推进跨模态学习领域最先进技术方面的潜力。

这篇期刊论文在几个方面扩展了我们之前的工作 [50]：

- 首先，我们提出了一种新颖的二阶提示，以进一步增强对目标关系的理解。这一创新在各种下游任务中带来了改进的最先进结果，如视觉-文本检索、视觉问答和图像标注。
- 其次，我们通过包括更多下游任务来扩展我们的评估，特别是 RefCOCO [59] 和 RefCOCO+ [59] 数据集上的视觉定位。此外，我们还探索了 MSVD [55]、TVQA [25] 和 TGIF [19] 上的视频问答，以提供与 VLP 相关工作的更全面比较。
- 第三，我们提供了对视觉定位能力的深入分析、丰富的掩码块可视化和详尽的消融研究。这些元素共同展示了我们二阶 PTP 方法在推进跨模态学习领域最先进技术方面的有效性和鲁棒性。
- 最后，我们在大规模 DataComp-1B [14] 数据集上训练了 PTP，并将其与流行的大语言模型整合，从而展示了 PTP 的多功能性和广泛适用性。




![](https://img-blog.csdnimg.cn/direct/3eb0b790e3d04ad4bea8b4b05a1a3ed6.png)


## III. 位置引导文本提示

在本节中，我们首先详细解释我们提出的位置引导文本提示范式（简称 PTP）。随后，我们展示如何将其集成到现有的视觉-语言预训练（VLP）框架中，以增强其视觉定位能力。我们使用经典和流行的模型 VILT [22]、CLIP [40] 和 BLIP [27] 作为示例展示集成。

### PTP 范式

为了增强使用 VLP 训练的跨模态模型的视觉定位能力，我们提出了一种新颖且有效的位置引导文本提示（PTP），帮助跨模态模型感知目标并将其与相关文本对齐。PTP 不同于传统的视觉语言对齐方法，如 [3]，[10]，[31]，[62]，它们连接目标特征和边界框作为输入，以学习目标与相关文本之间的对齐。这种替代方法提供了几个优势，如第 III-B 节所述。如图 3 所示，PTP 包括两个步骤：1）块标签生成，将输入图像划分为多个块并识别每个块中的目标；2）文本提示生成，将视觉定位任务重新格式化为基于步骤 1 的目标位置信息的填空问题。通过解决 PTP 中的填空问题，可以轻松将 PTP 集成到 VLP 模型中。以下是这些步骤的详细信息。

1. 块标签生成：如图 3 所示，对于训练阶段的每个图像-文本对，我们均匀地将输入图像划分为 $N \times N$ 块。然后我们使用以下两种方法之一识别每个块中的目标：

    1. 目标检测器：首先，我们采用 VinVL [62] 中使用的强大的 Faster-RCNN [42] 提取每个图像的所有目标。这个 Faster-RCNN 版本基于 ResNeXt152，并在 1600 类 Visual Genome [23] 上训练。然后我们选择预测置信度最高的前 K 个目标，记为 $O = {o_i}^K_{i=1}$，其中 $o_i = (z_i, q_i)$ 表示具有 4 维区域位置向量 $z$ 和目标类别 $q$ 的目标。对于每个块，我们选择区域中心位于该块中的目标。最后，该块的块标签为所选目标的 $q$。在这项工作中，我们默认使用目标检测器生成目标标签。

    2. CLIP 模型：作为使用沉重目标检测器的替代方法，最近的工作 [64]，[65] 尝试基于 CLIP [40] 生成区域监督，因为其效率和效果。受这些工作的启发，PTP 还可以使用 CLIP（ViT-B）模型生成块级目标监督。

首先，我们从文本语料库中提取最频繁的 $M$（默认为 3000）个关键词/短语。这些关键词/短语形成了我们的词汇 $V$。然后，我们使用 CLIP 文本编码器提取所有这些 $M$ 个文本嵌入的文本特征 $e_i$， $i \in [1, ..., M]$。

此外，我们从每个块中获取图像嵌入 $h$，并计算每个文本特征的相似度。具有最高相似度得分的关键词/短语被选为该特定块的目标标签。形式上，每个块的目标标签索引计算为：

$$
I = \arg\max_{y \in [1, ..., M]} \left( \frac{\exp(h^T e_y)}{\sum_{w \in V} \exp(h^T e_w)} \right),
$$

其中 $h$ 是所选块的视觉特征嵌入。与目标检测器相比，CLIP 模型有两个优点。首先，不是预定义的目标类别，产生更多样化的目标标签。其次，使用 CLIP 模型生成块标签比使用目标检测器更快，例如，比 Faster-RCNN（ResNeXt152）模型快 40 倍。

2. 一阶文本提示生成：对于每个训练对的输入图像，第 III-A1 节已经生成了目标标签和位置，这使我们能够设计一个简单的文本提示，如下所示：

“The block [P] has a [O].”

这里， $P \in 1, ..., N^2$ 表示所选块的索引， $O$ 表示为该块生成的目标标签。如果一个块中有多个目标，我们为每个提示随机选择一个目标标签。这样，每个提示包含目标位置和文本，可以帮助模型更好地理解目标和文本之间的关系。更多提示设计将在第 IV-D 节中探讨。

3. 二阶文本提示生成：一阶关系主要集中在识别图像中各个目标的位置。虽然这种方法为理解目标放置提供了基础，但它在捕捉目标之间更复杂、更具挑战性的关系方面有所不足。例如，在图 3 中展示的例子中，一面旗帜位于一个女孩的上方，展示了一种一阶提示无法捕捉到的更高阶关系。

为了应对这一限制并进一步增强学习过程，我们提出通过引入更复杂的提示来探索二阶关系：

“The block [P] has a [O] and a [O2] in [R] of this block.”

在这种格式中， $O2$ 是另一个随机选择的目标， $R$ 表示相对位置， $R \in \{top, bottom, left, right\}$。通过采用这种结构，模型不仅面临识别个体目标的挑战，还需要理解它们在图像中的相互关系和相对位置。

我们将这种方法命名为 PTP2R，其中 2R 表示二阶关系。这种方法要求模型对图像中的所有目标进行推理，使其能够更全面地理解视觉场景。通过深入研究这些更高阶关系，我们旨在提高模型识别和解释复杂目标交互的能力，从而提升其在 VLP 任务中的性能。

### 带有 PTP 的预训练

在这项工作中，我们将我们的 PTP 集成到主流 VLP 框架中，导致 PTP-ViLT [22]、PTP-CLIP [40] 和 PTP-BLIP [27]。收到 PTP 后，我们有两种训练这些模型的选项：

- 集成到现有任务中：使用文本提示的最简单方法是更改文本输入。如图 3 所示，提示的文本和原始字幕只是简单地填充在一起。形式上，我们方法的输入字幕 $x$ 表示为：

$$
x = [w, q],
$$

其中 $w$ 是文本， $q$ 是我们生成的文本提示。然后我们使用常规目标对 VLP 模型进行端到端训练。根据 [22]，[27]，[40]，我们采用语言建模（LM）损失、图像-文本匹配（ITM）和图像-文本对比（ITC）损失来训练我们的 PTP-BLIP；我们使用 ITM 和掩码语言建模（MLM）损失来训练我们的 PTP-ViLT；我们仅使用 ITC 损失来训练我们的 PTP-CLIP。我们在所有实验中默认使用这种方法，因为它具有良好的性能。

- 作为新的预文本任务：另一种方法是将位置预测作为额外的语言建模任务进行探索。形式上，如果 $D$ 是预训练数据， $y_1, ..., y_T$ 是我们生成的文本提示 $q$ 的训练标记序列，那么在时间步 $t$，我们设计我们的模型以预测概率分布 $p(t) = p(*|y_1, ..., y_{t-1})$。然后我们递归尝试最大化正确标记的概率。目标预测损失计算为：

$$
\mathcal{L}_ {\mathrm{PTP}}(\theta) = -\mathbb{E}_ {\mathbf{y} \sim D} \left[ \sum_{t=1}^{T} \log P_{\theta} \left( \mathbf{y}_ {t}  \mid \mathbf{y}_{<t} \right) \right],
$$

其中 $\theta$ 是模型的可训练参数。通过这种方式，模型被要求预测哪个块 $P$ 有目标以及哪个目标 $O$ 在这个块中。

讨论：值得注意的是，我们的方法不需要修改基础网络，可以应用于任何 VLP 模型而无需繁琐。该模型设计用于从原始像素图像中学习位置信息。请注意，我们只在预训练阶段需要目标的位置信息；然而，在下游任务中，我们在没有目标信息的情况下以正常的端到端方式评估模型，以摆脱繁重的目标特征提取。

## IV. 实验

在本节中，我们对 PTP 在多个下游任务中的表现进行实证评估，并进行全面研究。


![](https://img-blog.csdnimg.cn/direct/956ea58bb87e4bfcadceb7275e923a07.png)


### 实验设置

我们首先描述预训练实验条件，包括数据集、训练配置、评估程序和我们研究中使用的基线模型。

预训练数据集统计：在这项工作中，我们从 4 M（小）和 14 M（基）设置开始进行探索。与早期研究 [31]，[62] 一样，我们首先使用一个由四个流行的预训练数据集（COCO [32]、VG [23]、SBU [37] 和 CC3M [44]）组成的 4 M 设置开始。14 M 设置是 4 M 设置和 CC-12 M 的组合。根据最近的工作 [27]，我们还探索了 14 M 设置，其中包括一个额外的 CC12M [8] 数据集（实际上只有 1000 万个图像 URL 可用）除了 4 M 数据集外。我们在表 I 中报告了数据统计信息。由于 CC3M 和 CC12 M 数据集的 URL 是从互联网上获得的，考虑到部分 URL 不再有效，我们总共下载了 280 万个 CC3M 数据实例和 1020 万个 CC12 M 数据实例。需要注意的是，BLIP 基线为 CC3M 使用了 300 万个数据实例，稍多于我们的模型。就包含边界框的图像数量而言，CC3M 有 269 万个图像，CC12 M 有 700 万个图像。这些边界框在我们的 PTP 中使用。为了快速评估，我们对 BLIP 模型进行了 50000 步预训练，而不是 [22]，[61] 中的 200000 步。

此外，为了评估我们的方法在大规模下的有效性，我们对包括数十亿数据点的广泛数据集进行了实验，这些数据集包括 DataComp-1B [14]、MMC4 [67] 和 LAION400M [43]。

训练设置：我们的模型在 PyTorch [38] 中实现，并在 8 个 NVIDIA A100 GPU 上进行预训练。为了确保公平比较，我们采用基线工作中的原始实现中的优化器和训练超参数。此外，我们研究了 RandAugment [11] 用于数据增强，并利用所有原始策略，除了颜色反转，因为颜色信息对我们的任务至关重要。此外，我们应用仿射变换以类似于图像旋转的方法增强边界框。在预训练分辨率期间，我们随机裁剪 $224 \times 224$ 的图像，并在下游任务微调时将图像分辨率增加到 $384 \times 384$。

下游任务的超参数：表 III。编码器-解码器模型 BLIP 的最终解码器输出可用于多模态理解和生成。因此，我们评估其在流行的视觉语言基准上的性能，采用与 BLIP 论文 [27] 中介绍的相同设置。具体来说，我们对所有任务使用 AdamW 优化器，并仅训练检索任务 6 个 epoch 以提高效率。我们认为，增加 epoch 数量会产生更好的结果。

在 ViLT 基线的情况下，我们主要关注三个任务：视觉问答、图像-文本检索和自然语言视觉推理。ViLT 在这些下游任务中的超参数在表 IV 中报告。最后，对于 CLIP 基线，我们使用与 BLIP 中采用的相同超参数设置。

基线：我们评估了三种预训练框架的变体，包括单流 ViLT [22]、双编码器 CLIP [40] 和融合编码器 BLIP [27]，因为它们性能优越。为了公平比较，我们采用 ViT-B/16 [13] 作为基础视觉编码器并使用相同的数据集。

### 主要结果

在本节中，我们将我们的 PTP 集成到现有网络中，并与现有的 VLP 方法在广泛的视觉-语言下游任务中进行比较。包括五个图像-文本任务和两个视频-文本任务。

图像标注：此任务要求模型描述输入图像。我们考虑两个图像标注的数据集：No-Caps [1] 和 COCO 标注 [32]，[48]，均使用在 COCO 上微调的模型进行评估。与 BLIP 类似，我们以短语“a picture of”开头每个标注，这样可以产生略微更好的结果。由于图像标注需要一个解码头，双流 CLIP 和单流 ViLT 架构无法直接测试此任务，因为缺少解码头。为了避免信息泄漏，我们不在 COCO 上进行预训练。对于 No-Caps 数据集，按照 BLIP，我们采用零样本设置。


![](https://img-blog.csdnimg.cn/direct/a4bcaf096c3f4c38af223641a4980578.png)

![](https://img-blog.csdnimg.cn/direct/8898371c41bf4642a5461ae1917ea17d.png)





如表 II 所示，使用相当数量的预训练数据的相关工作表现明显不如 PTP-BLIP。我们的方法结果接近 VinVL [62]，使用更少的训练样本和更小的图像。最后，在 14 M 设置下，我们的方法与在数十亿数据上训练的 LEMON 的结果接近，并且需要两倍更高分辨率的图像。

图像-文本检索：我们在 COCO 和 Flickr30k 基准上评估 PTP 的图像到文本检索（TR）和文本到图像检索（IR）。对于 PTP-BLIP，根据原始实现，我们采用额外的重排序策略。具体来说，我们首先根据图像-文本特征相似度选择 $k$ 个候选，然后根据它们的成对 ITM 分数重新排序所选候选。我们设置 $k = 256$ 用于 COCO 和 $k = 128$ 用于 Flickr30k。

我们首先在表 V 中报告零样本检索结果，涵盖图像到文本和文本到图像设置。VLP 中的主流方法，例如 BUTD [3]、OSCAR [31] 和 UNITER [10]，经常使用目标检测器，例如也在 Visual Genome 上预训练的 Faster-RCNN。此外，表 1-4 中比较的方法大多使用目标检测器。例如，我们使用 VinVL 采用的 Faster-RCNN，但我们的模型（220 M）比 VinVL（347 M）表现更好。因此，比较是公平的。我们发现 PTP 在所有指标上显著改善了基线。例如，对于 ViLT [22] 基线，PTP 导致图像到文本检索在 MSCOCO 上 Recall@1 提高了 13.8%（从 41.3% 提高到 55.1%）。此外，我们的 PTP-BLIP 即使使用更少的数据，也在 MSCOCO 上大多数召回率上超越了 CoCa [58]。

表 VI 总结了不同模型在微调设置下的比较。可以看出：

1. PTP 在两个数据集中大幅超越了 BLIP 和 ViLT 基线。例如，PTP-ViLT 在 MSCOCO 上 TR 的 R@1 提高了 5.3%。
2. 使用强大的 BLIP 基线，PTP-BLIP 在相同规模上实现了最先进的性能。值得注意的是，PTP 的训练成本与 BLIP 基线相同，因为我们在不增加最大输入文本标记的情况下，以与基线相同的设置训练 PTP。此外，我们甚至可以缩小 4 M 设置与 ALBEF [28]（14 M 设置）之间的差距，使用类似的双流融合编码器架构。

从以上所有结果中，我们指出 UNITER [10]、OSCAR [31]、VinVL [62]、ImageBERT [39] 都使用了我们使用的 faster-rcnn。然而，我们的 PTP 比这些相关工作表现更好。此外，我们仅在预训练阶段使用目标检测器。这表明目标检测器不是成功的秘密，如何利用位置信息对 VLP 模型至关重要。

视觉问答：在视觉问答的背景下，VQA [4] 要求模型根据图像和相应的问题预测答案。对于 PTP-ViLT，我们将 VQA 视为多答案分类任务。另一方面，对于 PTP-BLIP，我们遵循 [27]，[28] 中使用的方法，并将 VQA 视为答案生成任务，以促进开放词汇 VQA 以提高性能。

表 VII 报告了性能结果。我们提出的模型 PTP 显示出显著的改进，相较于 ViLT 基线，两个分割均提高了 1.8%。此外，在 14 M 设置下，PTP-BLIP 超越了 SimVLM [54]，后者使用了 ViT-Large 视觉骨干和 18 亿个训练样本。

视觉推理：自然语言视觉推理（NLVR2）[46] 任务是一个二元分类任务，给定由两个图像和一个自然语言问题组成的三元组。此任务严重依赖于位置信息。如表 VII 所示，SimVLM [54] 被 PTP-BLIP 超越，后者具有合理的模型规模并在更少的实例上进行了预训练。同时，我们的方法也接近 VinVL_large 模型，后者采用更大的模型并使用来自强大目标检测器的目标特征，而不是原始像素图像作为输入。

视觉定位：我们按照 ViLBERT [35] 中使用的方法来评估我们模型在 Referring Expression 任务中的视觉定位能力，该任务涉及使用文本来定位图像区域。表 IX 展示了 4 M 设置下的结果，展示了我们提出的 PTP 模型的强大定位能力。值得注意的是，我们观察到 PTP 超越了依赖使用重型 Faster R-CNN 架构提取的目标特征的几个相关工作。此外，PTP 在 BLIP 上实现了显著改进。

视频-文本检索：在本实验中，我们测试了我们方法在视频-语言任务中的泛化能力。具体来说，我们在表 VIII 中对文本到视频检索进行零样本传输评估，在其中直接评估在 COCO 检索上训练的模型。为了处理视频输入，我们简单地从每个视频中均匀采样 8 帧，并将帧特征平均成一个序列。

我们的方法优于专注于检索的 OA-Trans [52]，展示了 PTP 的通用能力。此外，请注意这种简单的方法没有很好地探索时间信息。

视频问答：我们在表 XI 中报告了视频问答的结果。按照 All-in-one [49]，我们探索了三个广泛使用的基准：MSVD-QA [55]、TVQA [25] 和 TGIF [19]。TGIF FrameQA 和 MSVD-AQ 是开放式 VQA 任务，TVQA 是多选 VQA 任务。类似于视频-文本检索，我们为每个视频采样 8 帧。我们观察到 PTP-BLIP 在多选和开放式设置中表现良好。例如，我们的 PTP2R-BLIP 在 MSVD-QA 上优于 All-in-one [49] 4.3%，在 TGIF-FrameQA 上优于 8.3%。

![](https://img-blog.csdnimg.cn/direct/ab4b661d1bf7488b889bcf2adc54f2fc.png)




### 扩展实验

1. 数据规模扩展：在本节中，我们将研究方法扩展到 DataComp-1B 数据集 [14]，该数据集以其相对于 LAION 数据集 [43] 的卓越数据质量而闻名。DataComp-1B 数据集最初包括 14 亿个样本；然而，值得注意的是，由于某些 URL 已不可用，导致下载了 11.7 亿个数据样本。

鉴于 BLIP 和 VILT 架构所需的巨大计算资源，我们的主要评估集中在 CLIP（ViT B-16）模型上，选择它是因为其训练效率。利用 PTP 的内在简单性，它消除了复杂超参数选择的需要，并在数据层面无缝运行，我们能够在这一巨大规模上进行全面比较。此外，按照 Datacomp 评估框架，我们对模型进行了一系列图像分类基准的全面评估，采用零样本评估方法。

鉴于部署目标检测器模型所需的显著时间投资，我们通过使用 CLIP 模型生成的数据来加快训练过程，如方法部分所述。我们在表 XII 中总结了原始 CLIP 模型与 PTP-CLIP 的比较性能。值得注意的是，在仔细审视增强版时，我们发现即使在处理数十亿级数据时，实验结果仍然令人鼓舞。

2. 语言模型规模扩展：在本实验中，我们将语言模型规模从 BERT-base(200 M) [12] 变为 LLAMA(7B) 模型 [47]，分析大语言模型（LLM）在多模态学习中的影响。具体来说，我们在 open-flamingo 架构中实施我们的方法 [5]。保持与原始实现一致，我们将我们的方法与 OPT1.7B [63] 和 LLaMA-7B [47] 作为语言模型集成，同时使用 Open-CLIP [18] ViT-L 模型作为视觉编码器。总参数为 3B 和 9B。

值得注意的是，由于 LAION400M [43] 和 MMC4 [67] 数据集中的某些图像 URL 存在意外问题，导致无法重新评估，样本大小略小于 open-flamingo，我们报告了我们的实现结果。鉴于 MMC4 的文档性质，我们仅引入文本提示。

模型在 64 个 Nvidia V100 GPU 上进行了 6.3 天的严格训练，3B 模型和 9B 模型分别训练了 17 天。综合结果在表 X 中展示。我们的重点主要集中在少量样本任务上，通过上述表中的结果展示了模型表示能力的细致理解。必须强调的是，即使对于大型模型，数据提示样式的重要性，正如下游任务所显示的那样。

![](https://img-blog.csdnimg.cn/direct/76c708761952492d946d80dc68462a9d.png)



### 消融与设计选择

在本节中，我们首先在 4 M 设置下在三个知名基线上评估我们的方法，然后在 CC3M 上训练 BLIP 模型作为基线，并进行各种消融实验。

探索多样化架构：在本研究中，我们利用三个不同的基线模型，具体为 ViLT、CLIP 和 BLIP，来研究 PTP 对一系列性能指标的影响。这些实验的结果详见表 XIII，展示了 COCO 5 K 测试集和 Flickr30 K 1 K 测试集的性能。从基线模型获取的数据表明，我们提出的方法 PTP 在图像到文本（i2t）和文本到图像（t2i）性能上显著提升，突显了其在各种视觉语言任务中的适应性和适用性。

此外，我们评估了我们的模型相对于基线模型的执行时间。由于我们不在下游任务中使用目标检测器或提示，因此计算开销与基线模型一致。令人印象深刻的是，PTP 被确定为比依赖目标特征的 VinVL [62] 快 20 倍。重要的是，尽管执行时间显著减少，我们的模型仍然产生与 VinVL 相当的结果，突显了其效率和有效性。

比较文本提示和额外的预文本任务：本研究考察了在视觉语言模型预训练阶段引入 PTP 作为补充预文本任务的效果。通过实施这一策略，预文本任务不会与其他预训练目标（如 ITM 和 ITC）冲突，尽管它可能会增加计算费用。相反，提示设计会修改文本输入，影响所有预训练目标。

结果如表 XIV 所示。我们注意到，预文本和提示方法都增强了基线在所有四个任务上的性能。然而，提示方法明显优于预文本方法，特别是在 COCO 标注 CIDER 分数（127.2 对 123.5）上。因此，由于其优越的效率，我们在本研究中默认采用提示设计。

探索各种文本提示：在本实验中，我们研究了六种不同类型的提示：

1. The [O] is in block [P].
2. The block [P] resembles [O].
3. In which block is the [O]? In [P].
4. The [O] is situated in block [P].
5. $(X1, Y1, W, H)$ contains a [O]. $(X1, Y1)$ 表示左上角点，而 $W,H$ 表示边界框的宽度和高度。
6. The block [P] features a [O].
7. The block [NP] includes a [O]. $NP$ 指代使用名词描述块位置，例如，从左上到右下。



![](https://img-blog.csdnimg.cn/direct/ce96b70b67f74185abfe340ff38eeb7c.png)

![](https://img-blog.csdnimg.cn/direct/1479b1fb16304e70a6bc8fdd534b5416.png)




结果如表 XV 所示，我们观察到：

精确位置与块相比并不产生优越的结果，可能是因为精确定位难以学习。此外，我们发现使用块 ID（例如 0）或名词（例如左上）产生类似的结果。最终，我们发现混合版本并未生成最佳结果。我们还注意到，单词变化可以显著影响性能，这是提示学习中的常见问题，正如 GPT-3 [7] 所观察到的。我们的工作并不主要关注解决这个问题。此外，表 XV 表明，使用提示来预测目标边界框的精确位置（四个坐标）比预测块的性能更差。

研究二阶提示：表 XVII 深入研究了目标关系提示，其中 $R$ 包括“left”、“right”、“top”和“bottom”等术语。我们探索了 PTP2R 的三种不同变体，包括简单重复的一阶关系、目标的相对位置和所选块的相对位置。 $O2/P2$ 表示不同的目标/位置。此外，我们还探索了仅基于目标和位置关系的二阶文本提示。

结合关系提示显著提高了 COCO 文本到图像（t2i）检索性能。这种改进可能归因于学习关系提示的复杂性增加，如语言掩码损失从 1.29 增加到 1.47 所示。语言掩码损失的增加意味着模型在掌握目标关系的细微差别方面遇到了更大的困难，从而推动它发展出对视觉领域中目标关系的更精细理解。结果，模型在处理涉及目标关系的复杂任务方面变得更加熟练，最终在 COCO t2i 检索任务中表现得更好。我们注意到，当初始二阶提示中缺乏位置信息时，模型在确定第一个目标的精确位置时遇到挑战。这一问题主要由于在前 $K$ 大目标中存在重复的目标类别。


![](https://img-blog.csdnimg.cn/direct/4b5831a868e74179888d3bf0ebcd763c.png)

![](https://img-blog.csdnimg.cn/direct/6e575400633f429aa930e80f1a5c8bf7.png)





探索其他提示设计：在我们的方法中，一个目标可能跨多个块，每个类别可能包含多个目标。为了解决这个问题，我们还预测所有块或目标，并研究其他几种提示。模型在 CC3M 上训练，并在三个下游任务上进行评估。具体来说，我们探索以下方法：

1. 多个标签：我们注意到，在许多情况下，一个块可能包含多个目标。我们尝试将文本提示细化为 The block [P] has objects [O1], [O2], and [O3]。需要记住的是，每个块包含的目标数量不同。
2. 多个位置：考虑到一个目标可能出现在多个块中，我们创建了一个多位置设置。我们使用问答对细化提示。
3. 同义替换：我们将“block”替换为“region”，将“is”替换为“looks like”。
4. CPT [57]：根据这项工作，我们为每个标签着色检测区域，为每个区域分配一个唯一颜色。

结果如表 XVI 所示。我们观察到，结合多个目标或位置并未显著提高模型在下游任务上的性能，语言建模损失高于基线。这表明该分配对模型来说太难学。我们还发现，简单同义替换的结果与原始文本提示的结果一致。建模位置信息只需要一个简单的提示。CPT 旨在通过着色区域建议进行下游视觉定位，而 PTP 是用于预训练，仅为许多下游任务预训练 VLP 模型。由于大多数下游任务没有可用的区域建议（例如 VQA），CPT 无法生成颜色提示来提升定位，而 PTP 可以，因为它在预训练阶段提升了定位。事实上，我们尝试了预训练的 CPT，并观察到较差的性能，例如，在 NLVR 上，CPT 为 75.1，而 PTP 为 77.8。对于 CPT，下游任务如 NLVR 没有颜色提示（CP），而其预训练使用 CP，导致阶段不一致。

我们还发现，选择前 1 个预测目标并使用其相应的边界框提供了最佳性能。需要注意的是，边界框是矩形的，而实际目标可能具有各种形状。一种可能的解释是，其他区域或块可能包含过多的背景噪音，难以识别。


![](https://img-blog.csdnimg.cn/direct/9878223f7b8845229ede1f28361fdc34.png)


![](https://img-blog.csdnimg.cn/direct/5b0e4c53d41546fb8c60b4f3e10488f9.png)


![](https://img-blog.csdnimg.cn/direct/54aa285bd79f423f988968c13e0ecb81.png)




评估文本提示中位置的作用：在本实验中，我们研究了在不同粒度级别上提示我们 PTP 的有效性，例如没有位置提示。移除提示时，我们只使用 [P] has [O]。结果列在表 XVIII 中。从这些结果中我们观察到：

1. 有趣的是，每个组件都很重要。没有任何一个组件，下游性能逐步下降。
2. 虽然 OSCAR [31] 发现当使用区域特征作为输入时，使用目标标签作为补充输入提高了结果，我们证明了当使用原始像素图像时，目标标签是无效的。这突显了设计一个功能性提示以学习目标标签和图像区域对齐的重要性。

探索块数量：我们调查了更多精细位置信息是否有利于我们的 PTP。在图 5 中，我们将块数量从 $1 \times 1$（移除 PTP 中的位置信息）变化到 $4 \times 4$，并报告相对于 BLIP/ViLT 模型的相对性能。可以看出，当块数量超过 1 时，两个骨架的结果都有所改善。然而，当有 16 个块时，所有下游任务的性能相对下降。原因可能是预测的边界框偏离实际目标的定位，导致网格太小，可能不包含选定的目标。因此，我们选择使用 $3 \times 3$ 块，因为这种配置提供了更好的准确性。

确定最佳目标数量：生成目标标签的一种方法是使用 Faster-RCNN，并检测至少 10 个图像中的目标。我们将目标数量从 5 到 30 进行了变化，探索不同目标数量的效果。结果如图 4 所示。

我们观察到，BLIP 基线在开始时表现出轻微的性能提升，强调了数据多样性对后续任务的重要性。然而，由于大量低置信度分数的目标带来的高错误预测可能性，结果在目标数量较多时不那么理想。PTP 基于 Faster-RCNN 预测的置信度分数选择目标。选择的目标数量越多，置信度分数越低，标签中的噪声越大，因此需要在选择的目标数量和相关标签噪声之间进行权衡。在这项研究中，我们将选择的目标数量默认设置为 10。

部分边界框注释：由于 CC3M 数据集的一些 URL 不再有效，我们选择从 CC3M 数据集中提取 270 万个数据点，从 CC12 M 数据集中提取 700 万个数据点。因此，只有 970 万个预训练样本有可用的目标。我们还报告了 1400 万设置的结果，其中在没有目标可用的情况下，我们使用原始文本而不是文本提示。

不同采样子集的结果如表 XX 所示。我们的分析表明，68.6% 的目标可用性导致 COCO 标注的 CiDER 值为 134.6，NLVR 测试 P 的准确率为 83.2。这些发现表明，注释样本数量的增加有助于整体性能的提升。此外，这一观察支持了 PTP 方法非常适合大规模预训练的观点，进一步验证了其在开发更先进模型方面的适用性和潜力。

## V. 讨论

评估目标检测器的必要性：在这项工作中，部分预测的边界框信息来自 Faster-RCNN [42]。为了验证目标的表达能力，我们还考虑了两个变体：

1. 纯 CLIP 相似性：这一设计选择主要是出于效率考虑，因为使用目标检测器耗时且不易访问。
2. 除了强大的基于 ResNext152 的目标检测器外，我们还使用了较小的基于 ResNet101 的 Faster-RCNN。

结果如表 XIX 所示。我们还报告了在 8 个 NVIDIA V100 GPU 上的整体特征提取时间。VLP 中的主流工作使用目标检测器，慢速预处理是一个常见问题。社区容忍这一点，因为目标只需要提取一次并保存到磁盘（我们的特征从 VinVL 下载）。为了进一步降低成本，PTP 使用 CLIP 作为特征提取器，比 Faster-RCNN 快 42 倍。在 8 个 A100 GPU 上，4 M 设置下，BLIP 需要 19.4 小时，而 PTP 使用 CLIP 需要 8 + 19.4 = 27.4 小时从头训练，这是可以接受的。

从表中可以看出，我们发现使用更强的检测器导致更好的结果，但产生了显著的计算成本。此外，我们观察到 CLIP 嵌入的结果非常接近 Faster-RCNN（ResNeXt152）。此外，提取每个网格的伪标签只需要 Faster-RCNN 版本时间的 2.3%左右。我们得出结论，CLIP 模型是 PTP 中目标检测器的合适替代品。

位置信息探索：为了探索使用 PTP 框架训练的模型是否确实学习到了位置信息，我们在本节设计了一个填空评估实验。根据 ViLT [22]，我们屏蔽了一些关键词，并要求模型预测被屏蔽的词，并显示其相应的热图。我们设计了两个文本提示：一个给定名词来预测定位，另一个给定定位来预测缺失的名词。我们显示了前三个预测作为参考。


![](https://img-blog.csdnimg.cn/direct/f781db46006d4e7595a39348d91e1416.png)




结果如图 7 所示。我们的发现表明，PTP-ViLT 能够通过利用块位置信息及其对应的视觉概念进行准确预测。此外，当我们只屏蔽位置名词时，我们仍然观察到较高的正确块预测概率。例如，如图 7 底部所示，我们的模型准确识别出所有与“man”相似的图像补丁。基于这些实验和图 1 中展示的见解，我们得出结论，PTP 是一个有效的工具，通过一个简单而强大的文本提示，促进视觉语言模型中位置信息的学习。

此外，我们使用 K-Means 算法对 ViLT 和 PTP-ViLT 的标记级特征进行聚类。从直观上看，具有相似语义的标记应聚集在一起。我们在图 6 中展示了可视化结果。与 ViLT 基线相比，我们观察到我们的方法可以更准确地聚类相似补丁。这说明我们的 PTP 已经相当准确地学习了语义信息。

PTP 有助于哪种样本？我们对视觉问答任务进行了全面分析。我们的研究发现，VQA 数据集样本中有相当一部分包含与位置相关的词语，例如“top”和“sitting in”。为了进一步研究这一观察，我们构建了一个包含 30 个常见位置词汇的词汇表。随后，我们根据文本中是否包含上述词汇，将 VQA 数据集分为位置相关子集（约 27%）和位置无关子集（约 73%）。

表 XXI 展示了所提出分类方案的有效性及其在测试开发集上的对应表现。我们的分析表明，PTP 在位置相关子集上的准确率为 78.4%，比 BLIP 基线高出 5.9%。这一结果突显了 PTP 对于提高模型高效学习位置信息的重要贡献，并强调了其强大的视觉定位能力。因此，我们提出的模型可以作为解决视觉问答任务的有价值的资产。

与直接目标回归的比较：从目标检测器中学习位置信息确实具有挑战性。在本节中，我们将预测的目标边界框的坐标视为真实标签，并在预训练期间进行回归。具体来说，我们在 BLIP 图像编码器之后添加了一个轻量级检测头。表 XXII 显示了 PTP 在预训练损失（ITC、ITM、MLM）和下游任务性能（TR、IR、NLVR）方面比真实标签方法表现更好。


![](https://img-blog.csdnimg.cn/direct/cb00f6cd69cf4dfb91c67aa8c161a069.png)

![](https://img-blog.csdnimg.cn/direct/12645744621845eb9b14e51013441f84.png)


![](https://img-blog.csdnimg.cn/direct/8db3eeaa2a034cd0b3917ea0e39713c1.png)




我们发现 Faster-RCNN 提供的边界框并不十分精确，强制模型回归坐标可能会影响其定位能力。直接回归目标需要预测坐标，这并不十分精确。这种实现仅关注图像编码器，并不能改善文本解码器模型。此外，这种实现复杂，涉及许多探索技巧，难以轻松扩展到其他框架。相比之下，我们的块位置表示更准确，确保模型学习正确的位置信息。通过位置引导的文本提示（例如，给定位置/块来预测目标），模型学习哪些块包含目标以及每个块中有哪些目标。这样，模型隐式地学习了视觉定位，正如前几节实验中所展示的那样。

探索高阶 PTP 集成的可行性：将高阶关系引入图像描述的考虑提出了一个重要的讨论点。这一思考基于标题长度可能扩展的潜力，必须加以审视。值得注意的是，传统视觉-语言预训练（VLP）模型遵循特定的最大标题长度限制，通常设置为 32 个标记，这一限制在我们的研究中具有相关性。


![](https://img-blog.csdnimg.cn/direct/894be878e637402a85e5778f41ca18bb.png)




关键是要认识到，语料库中的大多数图像显示出相对有限的不同目标的多样性。为了阐明这一背景，我们进行了广泛分析，以量化每个图像中独特目标类别的数量，消除了频繁出现的非区分性标签，如“sky”、“road”和“tree”，以及超过图像宽度 1/10 的目标。这一严格分析的结果记录在表 XXIII 中以供参考。

我们的发现揭示了一个显著的统计数据；约 40% 的样本来自著名的预训练数据集，如 DataComp1B [14]，包含两个或更少的独特目标。鉴于这一经验见解，在这些以目标为中心的数据集中生成三阶关系的努力显得具有挑战性。因此，在我们研究范围内，审慎选择二阶关系显得更为实际。

通过 PTP 实现消除错位：现有大规模视觉语言预训练数据集中观察到的一个主要挑战是图像和文本对之间的显著错位。换句话说，提供的图像和相应文本描述之间通常存在显著的对齐缺乏，使模型难以有效学习。

在本实验中，我们通过评估 LAION400M [43] 数据和 Datacomp1B [14] 的点积相似度分数来解决这一问题。具体来说，我们随机选择了两个数据集的 10% 子集进行分析。利用预训练的 CLIP 模型，我们计算了图像-文本对的点积相似度分数。比较结果在图 10 中以可视化形式展示，展示了我们方法在消除噪音图像-文本对影响方面的有效性。

值得注意的是，在每个 epoch 中，我们的方法生成不同的标题，因为位置和目标的随机选择。这不仅消除了错位问题，还促进了数据方面整体表示的增强。


![](https://img-blog.csdnimg.cn/direct/f923df346dce44a2946ca6a45e2c2bf8.png)


## VI. 可视化

### 案例分析

在本实验中，我们展示了图 9 中涉及位置信息的几个案例。我们观察到位置信息对于各种下游任务至关重要，包括标注、VQA 和检索。为了理解这些任务，训练的模型需要学习位置信息。

由于 VQA 任务中的大量样本通常包括位置信息，我们在 VQA 任务上评估我们的模型，并选择一些具有代表性的样本。具体来说，我们在该图的底部显示了预测概率和预测名词。我们观察到 PTP 在大多数情况下提供了准确的预测，说明我们的 PTP 更有效地学习了位置信息。

### 边界框可视化

在本节中，我们展示了使用我们生成的文本提示获得的目标检测结果。具体来说，我们从集合 $V$ 中随机选择一个目标，然后可视化原始图像及其相应的边界框掩码。重要的是，我们对这些边界框应用与原始图像相同的仿射变换，以确保图像与相应边界框掩码之间的一致性。

我们从整个数据集中随机选择一些样本，结果如图 8 所示。我们还观察到，在一些示例中，边界框可能非常大，跨多个块（例如，第三行的第一个案例）。由于我们使用 RandAugment [11]，某些目标可能位于输入图像的边界之外。对于这种情况，我们简单地用 [X] 替换特定位置，最终 PTP 为 The block [X] has a [O]。我们还发现一些掩码可能不是正方形，如第三行所示。

## VII. 局限和结论

最初，我们尝试利用现有目标检测器或训练模型中的位置信息，通过简单的提示来增强视觉-语言预训练模型。为了帮助提示工程，我们开发了一种成功的跨模态提示设置实践。除了目标和位置之间的一阶关系外，我们还探索了更复杂的目标之间的二阶关系。通过严格的实验，我们证明 PTP 作为一个通用管道，在不产生显著额外计算成本的情况下，改进了位置信息的学习。

尽管当前版本的 PTP 在处理和解释各种输入数据方面取得了显著进展，但仍需承认一些限制。目前，PTP 尚未具备有效处理错误目标标签实例的能力。此外，本研究的当前范围尚未深入探讨更复杂提示相关的复杂性。对这些提示的深入研究将有助于更好地理解模型的优缺点，为未来的改进和增强铺平道路。展望未来，扩大研究视野以评估 PTP 在多样化视觉-语言任务中的性能至关重要。

# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。



