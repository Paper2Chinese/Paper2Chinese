# 题目：[Diffeomorphic Counterfactuals With Generative Models](https://ieeexplore.ieee.org/document/10345703)  
## 具有生成模型的变形反事实
**作者：Ann-Kathrin Dombrowski; Jan E. Gerken; Klaus-Robert Müller; Pan Kessel** 

** 源码链接： https://github.com/annahdo/counterfactuals
****

# 摘要

反事实（Counterfactuals）能够以人类可解释的方式解释神经网络的分类决策。我们提出了一种简单但有效的方法来生成此类反事实。更具体地说，我们执行适当的微分同胚坐标变换，然后在这些坐标中执行梯度上升，以找到被指定目标类别高度信任分类的反事实。我们提出了两种方法来利用生成模型构建这样的合适坐标系，这些坐标系要么完全微分同胚，要么近似微分同胚。我们使用黎曼微分几何对生成过程进行了理论分析，并使用各种定性和定量措施验证了生成的反事实的质量。

# 关键词

- 反事实解释，可解释人工智能，数据流形，生成模型

# I. 引言

深度神经网络模型被广泛用于解决计算机视觉（例如，[1]，[2]，[3]，[4]，[5]）、策略游戏和机器人技术（例如，[6]，[7]，[8]）以及医学（例如，[9]，[10]，[11]，[12]）和科学（例如，[13]，[14]，[15]，[16]，[17]）中的复杂问题。然而，它们传统上被视为黑盒模型，即给定网络模型后，对于用户甚至设计算法的工程师来说，什么对于达到特定的输出预测最为重要一直不清楚。这可能会为应用带来严重的障碍，因为例如，网络可能未被注意到地使用了仅在训练数据中出现的虚假图像特征。这种不良行为妨碍了网络的泛化能力，确实已经使用解释方法[18]，[19]被识别出来，特别是在安全关键领域中尤其成问题。

提供这种所需的透明度一直是可解释AI（XAI）[20]，[21]，[22]，[23]，[24]领域最近发展的课题，改善了上述挑战。这一领域的杰出技术[20]，[25]，[26]，[27]，[28]，[29]，[30]，[31]，[32]，[33]，[34]，[35]，[36]，[37]，[38]，[39]构建了例如分类器或回归器的显著性图[40]，突出显示了输入中对分类特别重要的区域。

解释神经网络的不同方法是通过提供原始输入的反事实[41]，[42]，[43]。这些是看起来真实的图像，与原始图像在语义上接近，但在明显特征上有所不同，以便它们的分类与期望的目标类别匹配，参见图1。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/18a8b1d6f3b34d4da47d6e42457a41d1.png#pic_ center" width="70%" />
</div>

反事实旨在回答像“为什么这个输入被分类为A而不是B？”或“输入需要改变什么，以便它不再被分类为A而是B？”这样的问题[41]，[42]，[43]，从而为分类器提供解释。与归因方法不同，反事实不提供相关性图，而是提供一个与原始输入相似的图像，作为原始预测的一种反例或假设性的替代品。对于某些应用，反事实可以提供归因方法无法提供的洞察力：例如，如果我们的目标是在数据集中对圆形和正方形进行分类，其中所有圆形的尺寸都是正方形的两倍，那么分类器可能会根据所描绘对象的大小而不是其形状来进行选择。在这种情况下，归因图将无法检测到大小相关性，因为它们只能突出显示现有特征。相比之下，反事实将展示更大的正方形和更小的圆形，揭示分类器利用了虚假的相关性。

强调反事实必须是数据分布中的真实样本，以便阐释网络对数据的行为，这一要求构成了计算反事实的最大实际挑战，因为通过梯度上升简单地优化网络的输出相对于输入会在原始输入中加入少量噪声，如图1中给出的例子所示。这种行为可以使用流形假设来理解：假设图像位于高维输入空间中的低维流形上，该流形可以使用生成模型学习。然后，梯度上升算法沿着与决策边界正交的方向行走，这也很可能与数据流形正交，导致小的扰动，这不是语义上的，如图2（b）所示。这对于对抗性示例是可以接受的，因为它们的目标是通过不察觉地微扰输入来翻转分类，这也可能使输入位于数据流形之外。相比之下，反事实应该通过人类操作者促进对分类器决策的解释。因此，无结构的、嘈杂的、因此不可解释的扰动对于此任务是无益的，即图像的变化必须采取语义上有意义的形式。

我们建议使用微分几何的数学学科的见解来缓解这个问题。微分几何可以被理解为对曲线（超-）表面的分析，因此提供了适当的工具来研究数据流形上的梯度。它对机器学习领域[45]，[46]，[47]以及特别是XAI[48]，[49]，[50]一直非常有价值。微分几何的一个基石是几何量可以在不同的坐标系中等价地描述的思想。然而，并非所有坐标系在实践中都同样有用。这种现象在物理学中无处不在：例如，控制行星运动的数学表达式在日心（以太阳为中心）坐标系中大大简化，而不是地心（以地球为中心）坐标系。在日心坐标中，行星运动可以用简单的开普勒椭圆来描述，因此相关的自由度很容易识别。相比之下，地心坐标中的轨道在夜空中观察到的非常复杂。因此，在日心坐标中更容易推导出物理直觉和解释。类似地，我们将通过优化神经网络分类器的输出相对于输入来构建反事实的困难归因于图2（b）中所示的输入空间X中给出的原始图像数据的坐标系X的选择不佳。相比之下，在适当选择的坐标系Z中，数据流形将更均匀地向所有方向扩展，允许优化保持在数据流形上，从而产生与原始图像相比在语义上发生改变的反事实。为了找到这样的坐标变换（在微分几何中称为微分同胚），我们使用在考虑中的图像数据集上训练的归一化流。由于流本质上是双射的并且具有可微逆，因此它满足了微分几何中微分同胚的技术条件。此外，流的基础分布在技术上被固定为单变量高斯分布，因此没有病态方向。此外，这种坐标系的变化将不会导致信息丢失，这与现有的生成反事实的方法形成鲜明对比。

在我们的方法中，反事实是通过在归一化流的基础空间中的表示中取梯度来进行梯度上升更新计算的，而不是分类器的输入（见图2（b））。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/18f6de6e591c4052adeb87ce1ac36c41.png#pic_ center" width="70%" />
</div>

这种方法具有严格的理论保证，我们称之为微分同胚反事实。特别是，我们展示了这在更新步骤中引入了一种度量，它在与数据流形正交的方向上收缩梯度。此外，我们提出了两种分别只近似导致微分同胚的方法。虽然这些近似方法在理论上保证程度较低，并且在实践中可能会导致一些信息丢失，但它们可以很容易地扩展到非常高维的数据集，正如我们在实验中展示的。我们称这些方法为近似微分同胚反事实。我们在理论上证明了这些方法在适当的假设下也保持在数据流形上。因此，我们的理论分析为生成模型在反事实背景下的应用提供了统一的数学框架。重要的是，我们不仅可以以这种方式优化分类器网络的输出，还可以优化回归器的输出。

这种分析得到了我们在各种应用领域，如计算机视觉和医学放射学以及多种分类器、回归器和生成模型架构的实验结果的支持。请注意，我们强调使用定量指标来评估提出的方法；一些定量结果在图2（d）和（e）中举例说明。

我们工作的主要贡献如下：1）我们提出了一个简单且在理论上有原则的框架，使用生成模型生成反事实解释。2）我们使用微分几何严格证明了对于训练良好的生成模型，得到的反事实位于数据流形上。3）我们展示了提出的框架可以直截了当地扩展到广泛的任务和生成模型架构类别，并在详细的数值实验中展示了这一点。

本文的结构如下：在第二节中，我们将详细介绍我们提出的方法。具体来说，我们将在第二节C中介绍微分同胚解释，以及在第二节D中介绍其近似版本。然后，我们将在第三节中使用黎曼微分几何对提出的方法进行理论分析。接下来是第四节，它提供了对我们提出方法的详细实验分析。在第六节中，我们将对相关工作进行广泛的讨论。一个玩具示例和我们的主要实验的代码是公开可访问的。

# II. 方法

在本节中，我们将详细介绍我们的新颖的微分同胚和近似微分同胚反事实解释。为此，我们将首先回顾反事实解释的基础知识，然后介绍我们提出的两种方法。

## A. 反事实解释

考虑一个分类器  $f: X \rightarrow \mathbb{R}^C$ ，它将输入  $x \in X$  分配到类别  $c \in \{1, ..., C\}$  的概率  $f(x)_ c$ 。分类器  $f$  的反事实解释提供了最小的变形  $x' = x + \delta x$ ，以便改变分类器的预测。

在许多实际相关的情况下，数据大致位于输入空间  $X$  中的一个显著低维的子流形  $D \subset X$  上。这在文献中被称为流形假设（例如，见 [51]）。对于反事实解释，与对抗性示例不同，我们对位于数据流形上的变形  $x'$  感兴趣。此外，我们要求对原始数据的变形是最小的，即，扰动  $\delta x$  应尽可能小。参见下文 (3)。然而，相关范数是沿数据流形测量的，而不是在输入空间中计算的。例如，MNIST图像中轻微旋转的数字可能具有大的像素距离，但应被视为原始图像的无穷小扰动。

我们通过假设数据集中在  $D$  周围的一个小区域  $\delta$  中来数学形式化流形假设，该区域的扩展  $\delta$  相对于输入空间  $X$  上的欧几里得距离非常小。我们将在第三节中展示，这意味着数据密度  $p$  的支撑  $S$  是一个乘积流形。

$$
S = D \times I_ \delta^1 \times \cdots \times I_ \delta^{N_ X - N_ D}
$$

其中  $I_ \delta = (-\frac{\delta}{2}, \frac{\delta}{2})$  是长度为  $\delta$  的开区间（相对于输入空间  $X$  上的欧几里得距离）。我们假设  $\delta$  很小，即数据大致位于低维流形  $D$  上，因此满足流形假设。我们可以认为  $I_ \delta$  来自于数据的固有噪声。

此外，我们定义在  $S$  中以置信度  $\Lambda \in (0, 1)$  被分类为类别  $t \in \{1, ..., C\}$  的点的集合为：

$$
S_ {t,\Lambda} = \{x \in S | t = \arg\max_ j f_ j(x) \text{ and } f_ t(x) > \Lambda\}
$$

原始样本  $x \in X$  的反事实  $x' \in X$  然后是在  $S_ {t,\Lambda}$  中距离  $x$  最近的点：

$$
x' \in S_ {t,\Lambda} \text{ 且 } \arg\min_ {y} d_ \gamma(x, y) = x'
$$

其中  $d_ \gamma(x', x)$  是通过由生成模型给出的微分同胚在  $S$  上的黎曼度量  $\gamma$  计算的距离。我们将在第三节中回顾黎曼几何的所需概念。

## B. 反事实的生成

通常，通过在输入空间  $X$  中执行梯度上升来生成反事实，参见 [42] 对反事实的最新综述。更准确地说，对于步长  $\eta$  和目标类别  $t$ ，执行梯度上升步骤：

$$
x^{(i+1)} = x^{(i)} + \eta \frac{\partial f_ t}{\partial x}(x^{(i)})
$$

直到分类器达到阈值置信度  $\Lambda$ ，即  $f(x^{(i+1)})_ t > \Lambda$ 。然而，得到的样本通常不会位于数据流形上，并且与原始图像  $x$  只在添加的无结构噪声上有所不同，而不是以可解释和语义有意义的方式。特别是当应用于高维图像数据时，这些样本通常被称为对抗性示例，而不是反事实。噪声梯度的原因在于分类器仅在数据流形上接受训练，因此梯度方向垂直于数据流形是不确定的（例如，见 [52]）。

因此，我们建议使用微分同胚  $g: Z \rightarrow S$  来估计原始数据点  $x$  的反事实  $x'$ 。然后我们在潜在空间  $Z$  中执行梯度上升，即：

$$
z^{(i+1)} = z^{(i)} + \lambda \frac{\partial (f \circ g)_ t}{\partial z}(z^{(i)})
$$

其中步长  $\lambda \in \mathbb{R}^+$ 。这样做的重要优点是得到的反事实将位于数据流形上。此外，由于我们考虑的是一个微分同胚  $g$ ，特别是一个双射映射，因此在考虑  $Z$  上的分类器  $f \circ g$  而不是数据流形  $S$  上的原始分类器  $f$  时，不会丢失信息，即对于任何  $x \in S$  都存在唯一的  $z = g^{-1}(x) \in Z$ 。我们在算法 1 中展示了我们方法的伪代码。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fdaf4cb59a404770925374fbe4d0ed0c.png#pic_ center" width="70%" />
</div>

正如图 3 所示，X 中的梯度上升和 Z 中的梯度上升分别非常适合生成对抗性示例和反事实示例。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ce10eafcef134f9095b3bf99e83c96e1.png#pic_ center" width="70%" />
</div>

对于回归任务，没有明确的决策边界，但我们仍然可以按照相同的算法直接最大化（或最小化）回归器  $f(x)$  的输出  $r$ ，直到达到所需的目标回归值。

## C. 新方法 1: 微分同胚反事实

我们建议通过归一化流来建模映射  $g$ ，并且我们将相应的修改数据  $x'$  称为微分同胚反事实。

具体来说，流  $g$  是一个可逆神经网络，它通过变量替换定理，为输入空间  $X$  提供了一个概率密度：

$$
q(x) = q_ Z(g^{-1}(x)) \left| \det \frac{\partial z}{\partial x} \right|
$$

其中  $q_ Z$  是潜在空间  $Z$  上的简单基础密度，例如单变量正态密度。流可以通过最大似然估计来训练，即通过最小化：

$$
KL(p|q) = -\mathbb{E}_ {x\sim p} \log q(x) + \text{const} \approx -\frac{1}{N} \sum_ {i=1}^{N} \log(q(x_ i)) + \text{const}
$$

其中  $x_ i \sim p$  是来自数据密度  $p$  的样本。由于流在整个输入空间  $X$  上是双射的，它特别地在数据流形  $S \subset X$  上也是双射的。此外，我们还将严格展示在第三节中，一个训练良好的流（非常近似地）仅映射到数据流形，即  $g(Z) \approx S$ 。

因此，流保证了在潜在空间  $Z$  中执行梯度上升时不会丢失信息，并且还确保得到的反事实位于数据流形  $S$  上。实际上，流可以被理解为特别适用于生成反事实的输入空间  $X$  的某种坐标变换。

## D. 新方法 2: 近似微分同胚反事实

虽然上一节中的方法非常有吸引力，因为它具有强大的保证，但可能很难扩展到非常高维的数据集。这是因为流在这些数据集上具有非常大的内存占用，因为每一层都具有与数据空间  $X$  相同的维度，以确保双射性。因此，我们提出了一种替代方法，称为近似微分同胚反事实，它具有不那么严格的理论保证，但可以更好地扩展到非常高维的数据。

具体来说，我们提出了两种近似微分同胚反事实的变体：

自编码器基础的：自编码器（AE）的重建损失，即：

$$
L = \mathbb{E}_ {x\sim p} \| g(e(x)) - x \|^2
$$

其中编码器  $e: X \rightarrow Z$  和生成器  $g: Z \rightarrow X$  被最小化，如果编码器是数据流形  $S$  上生成器的逆，则意味着：

$$
e|_ S = g^{-1}|_ S
$$

这特别意味着，如果潜在空间  $Z$  的维度与数据空间  $S$  相同，那么  $g(Z) = S$ 。与归一化流一样，如果模型经过完美训练，自编码器的图像就是数据流形。然而，自编码器只有在完美训练极限下，并且在潜在空间  $Z$  的维度与数据维度完全匹配的情况下，才会在数据流形上可逆。这与归一化流不同，因为它们通过构造在所有  $X$  上都是可逆的。因此，除非模型经过完美训练，并且潜在空间的维度完全匹配数据的维度，否则自编码器必然会导致信息丢失。

生成对抗网络基础的：生成对抗网络（GANs）由生成器  $g: Z \rightarrow X$  和鉴别器  $d: X \rightarrow \{0, 1\}$  组成。然后通过最小化某个特定的最小最大损失函数来进行训练，详见 [53]。可以证明，这个损失函数的全局最小值确保了最优生成器  $g$  的样本按照数据分布分布，即：

$$
g(z) \sim p \text{ 对于 } z \sim q_ Z
$$

我们参考 [53] 的第四节来获取证明。然而，最优生成器  $g$  并不一定在数据流形上是双射的。这意味着即使对于一个完美训练的 GAN，也可能不存在一个唯一的  $z \in Z$  给定数据样本  $x \in X$  使得  $x = g(z)$ 。此外，没有明确的方法来获得给定输入  $x \in X$  对应的潜在样本  $z \in Z$ 。这与归一化流和自编码器不同，因为对于这些生成模型，逆映射  $g^{-1}: X \rightarrow Z$  要么是显式的，要么是近似已知的。

然而，有大量文献关于 GAN 反演，见 [54] 的最新综述。对于给定的生成器  $g$  和数据样本  $x \in X$ ，这些方法旨在找到一个潜在向量  $z \in Z$  使得  $x \approx g(z)$ 。这通常是通过最小化某个辅助网络中间层的激活与输入  $x$  之间的差异来完成的，即：

$$
z = \arg\min_ {\hat{z}\in Z} \| h(g(\hat{z})) - h(x) \|
$$

例如，可以选择  $h$  作为在数据密度  $p$  的样本上训练的 Inception 网络的中间层 [55]。注意，这些反演方法并不具有严格的保证，因为优化目标是非凸的，并且不清楚中间层激活的值是否足以区分不同的输入。

# III. 理论分析

在本节中，我们使用微分几何的工具来展示对于训练良好的生成模型，潜在空间  $Z$  中的梯度上升更新（公式 5）确实保持在数据流形上，这一点通过我们在第四节中展示的实验结果得到了证实。直观地说，因为在（公式 5）中我们在  $Z$  中采取小步骤，其中概率分布例如是单位方差的正态分布，我们不会离开潜在空间中的高概率区域，因此在  $X$  中也是高概率区域内。

我们证明了这一点对于微分同胚反事实的情况，即对于归一化流，并且在更强的假设下也证明了近似微分同胚反事实的情况，即对于自编码器和生成对抗网络。

## A. 微分几何

在这一部分，我们简要介绍在下文中使用到的最基本的微分几何概念。有关全面的教科书，参见例如 [56]。

微分几何是研究平滑（超）曲面的学科。这个数学分支的核心概念是  $n$  维（可微）流形  $M$ ，它配备了坐标函数  $x^\mu : M \rightarrow \mathbb{R}^n$ （所谓的图表，它们被组装成一个图集）。这些坐标允许在  $\mathbb{R}^n$  中进行显式计算，但几何对象（张量）是独立于所选坐标的，并且在坐标变换下以明确定义的方式进行变换。这种坐标变换可以被解释为一个可微双射  $\phi : M \rightarrow M$ ，它的逆也是可微的，所谓的微分同胚。

在  $M$  的每一点  $p \in M$  上，我们附加了一个  $n$  维向量空间  $T_ pM$ ，即在  $p$  处的切空间。坐标  $x^\mu$  在  $T_ pM$  中引入了一个基，并且我们将  $v \in T_ pM$  的分量在这个基中表示为  $v^\mu$ 。在微分同胚  $\phi$  下， $v$  的分量变换为：

$$
v^\alpha = \sum_ {\mu=1}^n \frac{\partial \phi^\alpha}{\partial x^\mu} v^\mu = \frac{\partial \phi^\alpha}{\partial x^\mu} v^\mu
$$

其中在第二个等式中，我们引入了爱因斯坦求和约定，意味着对重复的上下标进行求和（分母中的微分算作下标）。我们将在下面使用这个求和约定来简化符号。

为了捕捉  $M$  上的距离（和曲率）概念，使用了一个度量张量  $\gamma(p) : T_ pM \times T_ pM \rightarrow \mathbb{R}$ 。度量定义了  $T_ pM$  与其对偶空间  $T^\*_ pM$  之间的规范同构，由  $v^\* = \gamma(p)(v, \cdot)$  和  $v = \gamma^{-1}(p)(v^\*, \cdot)$ 。按照广义相对论文献中的常规约定，我们使用下标  $v_ \mu$  来表示对偶向量  $v^\*$  的分量。这意味着在分量中同构读作：

$$
v_ \mu = \gamma_ {\mu\nu} v^\nu \quad \text{和} \quad v^\mu = \gamma^{\mu\nu} v_ \nu
$$

其中对  $\nu$  的求和是隐含的，并且我们引入了  $\gamma^{\mu\nu}$  作为  $\gamma_ {\mu\nu}$  的逆，即由坐标  $x^\mu$  诱导的基的度量分量。简而言之，与度量的收缩用于提升和降低指标。

给定一个度量，很自然地考虑  $M$  上点之间的最短路径。相应的曲线称为测地线。如果测地线的切向量的长度（由度量测量）沿测地线是恒定的，那么测地线就是仿射参数化的。重要的是，仿射参数化测地线的概念是与坐标无关的，因此本身可以用来构造  $M$  上的坐标，正如我们下面将看到的。

## B. 数学设置

为了分析潜在空间  $Z$  中的梯度上升（公式 5），我们在本节中定义必要的流形和坐标。

如上所述，设  $X$  是一个  $N_ X$  维流形，它是分类器  $f: X \rightarrow \mathbb{R}^C$  的输入空间，其中  $C$  个类别。分类器的一个实现对应于  $f$  在  $\mathbb{R}^{N_ X}$  上的一个函数，我们用  $x^\alpha$  表示  $X$  上的坐标，其中我们的分类器给出。这些坐标可以是适当归一化的像素值。我们另外使用一个  $N_ Z$  维流形  $Z$  作为我们的生成模型  $g: Z \rightarrow X$  的潜在空间。对于 GANs 和 AEs，我们通常有  $N_ Z < N_ X$  并且对于归一化流  $N_ Z = N_ X$ 。在后一种情况下，我们还有  $X = Z$  并且  $g$  是双射且具有可微逆，意味着  $g$  是一个微分同胚。与分类器类似，生成模型也在  $Z$  上的特定坐标上实现，我们用  $z^a$  表示  $Z$  上的坐标。

我们用一个平坦的欧几里得度量  $\delta_ {ab}$  来装备  $Z$ 。然后，生成模型  $g$  通过

$$
\gamma_ {\alpha\beta} = \delta_ {ab} \frac{\partial g^\alpha}{\partial z^a} \frac{\partial g^\beta}{\partial z^b}
$$

在  $g(Z)$  上诱导了一个逆度量。在  $N_ Z < N_ X$  的情况下， $\gamma$  是奇异的。这个度量是在潜在空间中执行梯度上升更新（公式 5）时的关键新成分，与在输入空间（公式 4）中不同，正如以下计算所示。

潜在空间  $Z$  中的梯度上升一步由  $g$  下的更新步骤（公式 5）给出。在  $x^\alpha$  坐标中，并且在线性阶  $\lambda$  的近似下，它由

$$
x^\alpha(z^{(i+1)}) = x^\alpha(z^{(i)}) + \lambda \frac{\partial x^\alpha}{\partial z^a} \frac{\partial (f \circ g)_ t}{\partial z^a} + O(\lambda^2)
$$
$$
= x^\alpha(z^{(i)}) + \lambda \frac{\partial x^\alpha}{\partial z^a} \frac{\partial g^\beta}{\partial z^a} \frac{\partial f_ t}{\partial x^\beta} + O(\lambda^2)
$$
$$
= x^\alpha(z^{(i)}) + \lambda \gamma_ {\alpha\beta} \frac{\partial f_ t}{\partial x^\beta} + O(\lambda^2)
$$

如果我们从相同的点开始， $x^{(i)} = g(z^{(i)})$ ，潜在空间（公式 5）和输入空间（公式 4）中的梯度上升之间的差异仅由  $f$  对  $x$  的梯度与逆诱导度量  $\gamma_ {\alpha\beta} = \frac{\partial g^\alpha}{\partial z^a} \frac{\partial g^\beta}{\partial z^a}$  的收缩给出。因此，为了理解为什么处方（公式 5）保持在数据流形上，我们将在接下来的内容中研究  $\gamma$  对于训练良好的生成模型的性质。

在回到  $\gamma$  之前，我们首先讨论数据的结构。数据在  $X$  上的概率密度由  $p : X \rightarrow \mathbb{R}$  表示，由  $g$  诱导的概率密度由  $q : X \rightarrow \mathbb{R}$  表示。对于  $q$  在  $x^\alpha$  坐标中，我们使用符号  $q_ x : \mathbb{R}^{N_ X} \rightarrow \mathbb{R}$ 。数据由  $S = \text{supp}(p) \subset X$  特征化，它在  $x^\alpha$  坐标中变为  $S_ x \subset \mathbb{R}^{N_ X}$ 。我们将假设数据大致位于  $X$  中  $D \subset S$  的子流形上，其维度  $N_ D \ll N_ X$ 。关于我们的生成模型的维度，我们假设  $N_ D \leq N_ Z \leq N_ X$ 。作为  $X$  的子集，并且在  $x^\alpha$  坐标中， $D$  将由  $D_ x \subset \mathbb{R}^{N_ X}$  表示。为了捕捉数据不会在  $D$  之外延伸得很远，我们假设  $S$  在  $x^\alpha$  坐标中具有  $\delta \ll 1$  的欧几里得扩展，垂直于  $D$ ，即：

$$
S_ x = \{ x \in \mathbb{R}^{N_ X} | x^\alpha = (x_ D, x_ \delta), x_ D \in D_ x, x^\alpha_ \delta \in (-\frac{\delta}{2}, \frac{\delta}{2}) \}
$$

接下来，我们将在  $D$  的邻域中定义坐标，这些坐标将  $D$  切线方向和法线方向分开，如图 4 所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/49b2076257e34f699b8c674feee8e3b9.png#pic_ center" width="70%" />
</div>

我们的构造类似于黎曼和高斯正规坐标的构造，适应于余维数大于一的子流形。首先，我们在  $D$  上选择坐标  $y^\parallel$ ，对于每个  $p \in D$  ，选择  $T_ pD^\perp$  的一组基  $\{n_ i\}$ ，它是  $p$  处  $D$  的法向量空间。按照黎曼正规坐标的通常构造，我们通过构造一个仿射参数化测地线  $\sigma : [0, 1] \rightarrow X$  来为  $D$  的某个邻域内的点  $q$  分配坐标，该测地线满足  $\sigma(0) = p$  和  $\sigma(1) = q$ ，并且其切向量  $\sigma'(0) \in T_ pD^\perp$ 。然后  $q$  的坐标为  $y(q) = (y^\parallel(p), y^\perp) \in \mathbb{R}^{N_ D} \oplus \mathbb{R}^{N_ X - N_ D}$ ，其中  $y^\perp$  的第  $i$  个分量由  $\sigma'(0)$  在基  $\{n_ i\}$  中的第  $i$  个分量给出。在  $D$  的足够小的邻域中，我们可以为每个  $q$  找到一个唯一的基点  $p \in D$  和测地线  $\sigma$ 。

这种构造的一个重要方面是，通过重新缩放基向量  $\{n_ i\}$  ，我们可以重新缩放  $\sigma'(0)$  的分量。这意味着我们可以任意重新缩放  $y^\perp$  坐标，因此我们可以使用在（公式 16）中出现的相同  $\delta$  来限制  $y$  坐标中的  $S$  ，

$$
S_ y = \{ (y^\parallel, y^\perp) \in \mathbb{R}^{N_ X} | y^\parallel \in Dy, y_ i^\perp \in (-\frac{\delta}{2}, \frac{\delta}{2}) \}
$$

此外，在  $g(Z)$  中，我们可以选择一个相对于（奇异的）度量  $\gamma$  正交的基  $\{n_ i\}$  ，并在  $D \cap g(Z)$  的某个邻域中获得：

$$
\gamma_ {\mu\nu}(y) = \begin{pmatrix}
\gamma^{-1}_ D(y) & 0 \\
0 & \gamma^{-1}_ \perp(y)
\end{pmatrix}
$$

这种度量形式以及（公式 17）特别意味着  $S$  采用了（公式 1）中提到的形式。接下来，我们将展示对于训练良好的生成网络和瘦数据分布（即，对于小  $\delta$  ）， $\gamma^{-1}_ \perp$  会消失。

为了理解梯度上升更新步骤的后果，考虑（公式 15）在  $y^\mu$  坐标中的表达：

$$
\gamma_ {\alpha\beta} \frac{\partial f_ t}{\partial x^\beta} = \frac{\partial x^\alpha}{\partial y^\mu} \gamma^{\mu\nu} \frac{\partial f_ t}{\partial y^\nu} = \frac{\partial x^\alpha}{\partial y^\mu_ \parallel} \gamma^{\mu\nu}_ D \frac{\partial f_ t}{\partial y^\nu_ \parallel} + \frac{\partial x^\alpha}{\partial y^\mu_ \perp} \gamma^{-1}_ \perp \frac{\partial f_ t}{\partial y^\mu_ \perp}
$$

对于  $\gamma^{-1}_ \perp \rightarrow 0$  并且  $\frac{\partial x}{\partial y^\perp}$  有界，第二项消失，我们得到：

$$
\gamma_ {\alpha\beta} \frac{\partial f_ t}{\partial x^\beta} \rightarrow \frac{\partial x^\alpha}{\partial y^\mu_ \parallel} \gamma^{\mu\nu}_ D \frac{\partial f_ t}{\partial y^\nu_ \parallel}
$$

因此，更新步骤（公式 15）中正交方向，导致远离数据流形  $D$  的方向被抑制。因此，（公式 15）产生反事实而不是对抗性示例。

## C. 微分同胚反事实

在本节中，我们展示对于训练良好的归一化流，逆度量的正交分量  $\gamma^{-1}_ \perp$  对于瘦数据流形消失，正如下面定理中正式化的。

**定理 1:** 对于  $\epsilon \in (0, 1)$  和归一化流  $g$ ，如果 Kullback–Leibler 散度  $KL(p, q) < \epsilon$  ，那么  $\gamma^{-1}_ \perp \rightarrow 0$  当  $\delta \rightarrow 0$  对于所有  $i \in \{1, ..., N_ X - N_ D\}$ 。

**正式证明的主要论点** 在附录 B.1 中给出，可在线获取，过程如下：首先，我们展示一个小的 Kullback–Leibler 散度意味着大部分诱导的概率质量位于数据分布的支持内，

$$
\int_ {S_ x} q_ x(x) \, dx > 1 - \epsilon
$$

接下来，我们将  $q_ x$  写成在流  $g$  下的潜在分布  $q_ z$  的拉回，使用归一化流的熟悉变量替换公式。在上述  $y^\mu$  坐标中，得到的积分就根据度量  $\gamma$  的分块对角结构（公式 18）和积分域  $[-\frac{\delta}{2}, \frac{\delta}{2}]$  对  $y^\perp$  方向分解。随着  $\delta$  接近 0，只有当相关度量分量  $\gamma_ \perp$  发散时，界限（公式 21）才能保持满足，这意味着  $\gamma^{-1}_ \perp \rightarrow 0$ 。

按照第三节末尾的步骤，我们看到这必然意味着梯度上升更新（公式 5）保持在数据流形上，因为  $\frac{\partial x}{\partial y^\perp}$  作为  $\delta$  接近 0 是常数（因此有界）。

## D. 近似微分同胚反事实

我们现在为近似微分同胚反事实的情况，即自编码器和生成对抗网络，提出一个类似于定理 1 的定理，表明这些模型也可以用来构建反事实。然而，这将需要更强的假设，因为生成模型在这种情况下不是双射的。特别是，我们将假设生成模型捕获了所有数据，即  $D \subset g(Z)$ ，这意味着在  $y$  坐标中，尽管  $\gamma$  对于  $N_ Z < N_ X$  是奇异的， $\gamma_ D$  组件是非奇异的。因此，我们将  $y^\perp_ i$  方向拆分为  $N_ X - N_ Z$  个奇异方向和  $N_ Z - N_ D$  个非奇异方向。由于逆度量按定义在奇异方向上消失，定理集中在非奇异方向上，然后可以陈述如下：

**定理 2:** 如果  $g : Z \rightarrow X$  是一个生成模型，满足  $D \subset g(Z)$  并且图像  $g(Z)$  在任何非奇异正交方向  $y^\perp_ i$  外扩展，

$$
\gamma^{-1}_ \perp \rightarrow 0
$$

对于  $\delta \rightarrow 0$  对于所有非奇异正交方向 \(y^\perp_ i$ 。

证明可以在附录 B.2 中找到，可在线获取，过程如下：首先，我们构造一条曲线  $\tau : [0, 1] \rightarrow Z$  ，它沿着  $y^\perp_ i$  坐标轴穿过  $S$  并在  $g(Z)$  中完全位于  $D$  之外，如图 5 所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/52c6e7e8a71f469cbb580ef45af8148a.png#pic_ center" width="70%" />
</div>

然后，这条曲线  $\tau$  的长度  $L(\tau)$  （相对于  $\gamma$  计算）在  $y^\mu$  坐标中，对于小  $\delta$  ，近似给出为

$$
L(\tau) \approx \sum_ {i=1}^{N_ X - N_ Z} \gamma_ \perp^{-1}(x_ D) (x_ {1,i}^\perp - x_ {0,i}^\perp)
$$

通过  $\delta$  限制差异，并使用  $L(\tau)$  是常数，得到所需的结果。如上文定理 1 的情况一样，这再次意味着梯度上升更新（公式 5）不会离开数据流形，如（公式 20）所示。

# IV. 实验

配备我们的理论结果，我们现在准备展示我们的实验发现。

我们首先通过三维空间中的玩具例子来说明微分同胚解释，这使我们能够直接可视化数据流形和梯度上升在 X 和 Z 中的轨迹。

然后，我们应用我们的微分同胚反事实方法，使用归一化流，到四个不同的图像数据集。我们使用 MNIST、CelebA 和 CheXpert 进行分类任务，以及 Mall 数据集进行回归任务。我们定性和定量地评估结果。此外，我们讨论了近似微分同胚反事实，使用 VAE 和 GAN，这使我们能够考虑高分辨率数据。

对于所有实验，我们使用相同的设置：我们需要一个预先训练好的生成器  $g$  和一个预先训练好的分类器  $f$  。我们从测试集中的某个数据点  $x$  开始，分类器  $f$  预测它属于源类别。我们定义目标类别  $t$  和目标置信度  $\Lambda$  。为了产生对抗性示例，我们随后根据 X 中的梯度更新原始数据点  $\frac{\partial f_ t(x)}{\partial x}$  ，直到达到所需的目标置信度。为了产生反事实，我们首先将原始数据点投影到生成模型  $g$  的潜在空间中，通过应用逆生成模型  $g^{-1}(x) = z$  ，或者对于 GANs 的适当近似。然后，我们根据 Z 中的梯度更新原始潜在表示  $z$  ， $\frac{\partial (f_ t \circ g)(z)}{\partial z}$  ，直到达到所需的目标置信度。

有关模型配置、训练和超参数的更多详细信息，请参阅附录 C，可在线获取。

## A. 玩具例子

我们考虑在三维空间中嵌入的螺旋线上均匀分布的数据，并训练一个简单的归一化流来近似数据分布。如图 6 所示，我们将数据分为两个类别，对应于螺旋线的上半部分和下半部分，并训练一个分类器。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/68608b8129f64f9ab00e6f47f9157435.png#pic_ center" width="70%" />
</div>

然后，我们通过在输入空间 X 和流的潜在空间 Z 中执行梯度上升优化来生成反事实，即使用（公式 4）和（公式 5）。

从原始数据点  $x$  开始，我们观察到在 X 中的梯度上升导致点显著地离开数据流形  $S$  。相比之下，在潜在空间 Z 中的梯度上升更新沿着数据流形的轨迹移动，从而产生具有期望目标分类的反事实，它们位于数据流形上。我们如图 6 所示。

由于我们对数据流形有解析描述，我们可以可靠地计算通过在 X 或 Z 中执行梯度上升找到的所有点到数据流形的距离。我们比较了在输入空间 X 和潜在空间 Z 中执行的 1000 次成功优化（所有优化都达到了所需的目标置信度）到数据流形的距离。在 X 或 Z 中执行梯度上升时，到数据流形的中位距离分别为 2.34 和 0.01（另见附录中的图 17，可在线获取）。这直观且清晰地展示了在潜在空间 Z 中执行梯度上升的好处。

我们理论洞见的一个非平凡后果是，我们可以从我们的流  $g$  推断出数据流形上每个点的切空间。具体来说，我们对雅可比矩阵  $\frac{\partial g}{\partial z} = U \Sigma V$  进行奇异值分解，并重写逆诱导度量为

$$
\gamma^{-1} = \frac{\partial g}{\partial z} \left( \frac{\partial g}{\partial z} \right)^T = U \Sigma^2 U^T
$$

正如我们在第三节中看到的，对于集中在  $D$  维数据流形  $D \subset \mathbb{R}^{N_ X}$  中的数据，在  $X$  维嵌入空间中，逆诱导度量  $\gamma^{-1}$  有  $N_ X - D$  个小的特征值。此外，对应于大特征值的特征向量将近似地跨越数据流形的切空间。对于我们在第四节 A 中的玩具例子，我们可以直接在三维空间中展示由三个特征向量跨越的平行六面体。图 7（左）确实显示平行六面体在两个维度上显著收缩，使它们看起来像一维线。对于在第四节 B 中讨论的高维图像数据集，我们展示了每组数据集上 100 个随机数据点的平均排序特征值，如图 7（右）所示。我们的实验证实了理论预期，即大的特征向量确实跨越了流形的切空间。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d9e78c1ae9974748843fe2519f7b471b.png#pic_ center" width="70%" />
</div>

## B. 图像分类和回归

现在我们展示微分同胚反事实在几个领域的图像分类应用。

1)设置：我们使用 MNIST [57]、CelebA [58]、CheXpert [59]（一个标记胸部 X 光片的数据集）和 Mall 数据集 [60]、[61]、[62]、[63]（一个购物中心视频帧中带有行人头部注释的拥挤计数数据集）。

分类器/回归器：我们在 MNIST 上训练了一个十类 CNN（测试准确率为 99%）。对于 CelebA，我们在金发属性上训练了一个二元 CNN（测试准确率为 94%）。对于 CheXpert，我们在心脏肥大属性上训练了一个二元 CNN（测试准确率为 86%）。

对于 Mall 数据集，我们训练了一个 U-Net [64]，它输出图像大小的概率图和一个标量回归值，对应于图片中行人的近似数量。根据 Ribera 等人 [65] 的定义，我们训练的 U-Net 在头部计数的 RMSE 为 0.63。当我们运行我们的梯度上升算法时，我们的目标仅仅是最大化/最小化回归值  $r$  ，即图片中的行人数量。

生成模型：对于微分同胚反事实，我们为 MNIST 选择了具有 RealNVP 型耦合的流 [66] ，并为 CelebA、CheXpert 和 Mall 数据集选择了 Glow 架构 [67]。对于近似微分同胚反事实，我们为 MNIST 使用了简单的卷积 GAN (dcGAN) 和卷积 VAE (cVAE) [68]，为 CelebA 使用了渐进式 GAN (pGAN)。

生成反事实：我们从 MNIST 的 "four" 类、CelebA 的 "not blond" 类和 CheXpert 的 "healthy" 类开始。我们选择 MNIST、CelebA 和 CheXpert 的 "nine"、"blond" 和 "cardiomegaly" 类作为目标  $t$  ，并将置信度阈值设为  $\Lambda = 0.99$  。对于 Mall 数据集，如果在原始图像  $x$  中检测到很少的行人，我们最大化回归值  $r$  （阈值设为  $r = 10$  ），如果检测到很多行人，则最小化回归值（阈值设为  $r = 0.01$  ）。我们使用 Adam 优化器在 X 或 Z 中进行优化，直到达到目标类别  $t$  的所需目标置信度  $\Lambda$ 。

如我们在第二节 D 中所讨论的，GANs，我们用于近似微分同胚反事实，通常在训练过程中不需要编码器。我们应用 GAN 反演方法来找到源图像的编码。具体来说，对于低维 MNIST 数据集，我们通过最小化解码潜在表示  $g(z)$  和原始图像  $x$  之间的欧几里得范数来找到潜在表示  $z$  。对于更高维度的数据集，我们使用 HyperStyle [69] GAN 反演技术找到初始潜在表示。

2)定性分析：由归一化流产生的我们的微分同胚反事实确实显示出语义上有意义的变形，特别是与通过在数据空间 X 中的梯度上升产生的对抗性示例相比。

我们在图 8 中展示了示例。反事实看起来像是数据集中具有目标类别作为真实标签的图像。同时，反事实在与源图像和目标类别之间的区别无关的特征方面与其相应的源图像保持（近似）恒定。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/4d682a90b0624dbdb5fa8703a9cfee84.png#pic_ center" width="70%" />
</div>

对于 MNIST，反事实中的笔画宽度和书写角度保持不变，而 "four" 的上部缺口变为 "nine" 的特征上环。

对于 CelebA，反事实的变化集中在头发区域，正如热图所示。面部特征和背景保持（近似）恒定。

对于 CheXpert 数据集，反事实主要使图片中央区域的像素变亮，导致出现扩大的心脏的外观。图像中的其他结构大部分保持不变。

对于 Mall 数据集中的图片，我们也观察到反事实与原始图像非常接近。当最大化回归值时，行人在图片的边缘生成或出现在原始图像中的较暗区域周围。当最小化行人数量时，我们观察到反事实再现了地板的较暗部分和瓷砖之间的线条。

近似微分同胚反事实的结果如图 9 中间和右块所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/31795d6e87794d28a3cf4b5ae327019c.png#pic_ center" width="70%" />
</div>

MNIST 上的 dcGAN 产生了一些随机像素伪影，但生成的图像比 cVAE 产生的图像更清晰。对于 CelebA 上的 pGAN 生成的图像，我们看到原始图像的解码优化潜在表示与原始图像略有偏差。这在构图不典型（第一行的手臂没有正确再现）或背景高度结构化（第二行）时尤为明显。对于近似微分同胚反事实，我们观察到背景中的变化甚至更大。这可能归因于不完美的反演过程和 pGAN 的质量，即微分同胚只是近似而不是精确的。为了证明近似微分同胚解释可以扩展到非常高维的数据集，我们使用了预训练的 StyleGAN [70]、[71] 来处理来自 CelebA-HQ 数据集 [68] 的 1024×1024 分辨率图像。请注意，即使对于更多样化的 ImageNet 数据集，StyleGAN 也可以在相同的分辨率下使用，见 [72]。为了使用与之前相同的分类器，我们在将其作为输入提供给分类器之前，将图像缩小到 64×64 分辨率。正如图 10 所示，近似微分同胚反事实即使在这些非常高维的数据集上也会产生语义上有意义和可解释的结果。更多关于微分同胚和近似微分同胚反事实的示例可以在附录 D 中找到，可在线获取。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fada7d3a1f8345c68b1832dcf165264b.png#pic_ center" width="70%" />
</div>

3)定量分析：为了定量评估我们的反事实的质量，我们使用几种度量方法，详见下文。

预言者：我们在 MNIST 上训练了一个 10 类 SVM（测试准确率为 92%），在 CelebA（测试准确率为 85%）和 CheXpert（测试准确率为 70%）上训练了二元 SVM。通过在流的基础空间中执行梯度上升找到的反事实，在这些简单模型上显著地更好地泛化，表明它们确实使用了比通过在 X 空间中执行梯度上升产生的传统对抗性示例更语义相关的变形。

对于 Mall 数据集，我们训练了一个稍大的 U-Net（头部计数的 RMSE 为 0.72），并计算了原始图像、通过在 X 空间中执行梯度上升修改的图像和通过在 Z 空间中执行梯度上升修改的图像的回归值。正如预期的那样，反事实的回归值显著更接近目标值（当最大化行人时为 0.01，最小化行人时为 10），而不是原始图像和对抗性示例的回归值。图 11 总结了这些发现。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ce75babf2fe74404a639bd42b1a42569.png#pic_ center" width="70%" />
</div>

在图 12 中，我们展示了图 8 中 Mall 数据集的反事实和对抗性示例的头部定位，使用原始和预言者 U-Net。为了找到头部位置，将回归值四舍五入到最近的整数，表示图像中的行人数量。然后对概率图拟合具有与行人数量相同数量的组件的高斯混合模型。最后，头部位置被定义为拟合的高斯的均值。原始 U-Net 被对抗性示例欺骗：当最大化行人时（第二行），原始 U-Net 产生误报，导致在没有行人的头部位置标记。当最小化行人时，对抗性示例（第四行）使原始 U-Net 犯下误判错误，即使行人清晰可见也没有检测到。预言者 U-Net 另一方面产生回归值和概率图，可以正确识别对抗性示例的头部位置（或缺少头部位置）。对于微分同胚反事实（图 12 的奇数行），两个 U-Net 的预测相似，表明这些反事实泛化到了独立训练的预言者 U-Net。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/85a736a016f14a708aa7c19e88dd5103.png#pic_ center" width="70%" />
</div>

最近邻：我们将原始图像和在 X 和 Z 中修改的图像与数据集中的数据进行比较。我们找到 k 最近邻（相对于欧几里得范数）及其各自的真实分类/回归值。对于 MNIST、CelebA 和 CheXpert，我们检查最近邻中有多少百分比被分类为目标类别。对于 Mall，我们检查平均存在的行人数量。图 13 显示，对于 MNIST、CelebA 和 CheXpert 的微分同胚反事实，十个最近邻的真实分类更频繁地与目标值匹配，而不是原始图像或对抗性示例。对于 Mall 数据集，每个反事实的三个最近邻平均具有更接近目标回归值的回归值（当最大化时为 r = 10，最小化时为 r = 0）。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fdcfd699c5a143de95e8aad9ba3fa600.png#pic_ center" width="70%" />
</div>

IM1 和 IM2：Van Looveren 和 Klaise [73] 提出了两个测试可解释性的度量标准：IM1 定义为

$$
\text{IM1} = \frac{\|x' - \text{AE}_ t(x')\|}{\|x - \text{AE}_ {c0}(x')\| + \epsilon}
$$

其中  $\text{AE}_ {c0}$  和  $\text{AE}_ t$  是分别在原始类别  $c_ 0$  和目标类别  $t$  的数据上训练的两个自编码器， $\epsilon$  是一个小的正值。第二个度量标准 IM2 定义为

$$
\text{IM2} = \frac{\|\text{AE}_ t(x') - \text{AE}(x')\|}{\|x'\|_ 1 + \epsilon}
$$

其中  $\text{AE}$  是在所有类别上训练的自编码器。

IM1 和尤其是 IM2 已经被反复批评 [74]、[75]、[76]。对于 IM2，我们用修改图像的一范数  $\|x'\|_ 1$  来除。如果图像有更多的明亮像素，这个值就会很大。因此，即使它们可能不是更可解释的，具有更明亮像素的图像往往会有更小的 IM2。因此，我们仅限于评估 IM1。

在表 I 中，我们展示了两个数据集，MNIST 和 CelebA 的可解释性度量 IM1 的平均值和标准差。我们为原始图像、通过在 X 空间中执行梯度上升产生的对抗性示例和通过在 Z 空间中执行梯度上升产生的微分同胚反事实计算了值。IM1 的低值意味着图像更好地被训练在仅目标类别上的自编码器表示。微分同胚反事实实现了比对抗性示例更低的 IM1 值，表明它们更具可解释性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/8e13b89aa1fd4992b875cc5de821c27d.png#pic_ center" width="70%" />
</div>

原始图像与反事实的相似性：通常要求反事实是最小的，即它们应该是在数据流形上且达到目标类别  $t$  的所需置信度  $\Lambda$  的最近点。与其他方法不同，我们不通过在 X 空间中显式地最小化反事实与原始图像之间的欧几里得范数来鼓励相似性，因为相关距离应该由数据流形  $S$  上的诱导度量或等效地潜在空间 Z 中的平坦度量来计算。然而，我们的反事实仍然与各自的源图像保持高相似性。我们通过计算反事实与源类别中所有图像之间的欧几里得距离来证实这一点（这种效应在 CelebA 数据集的图 14 中被说明）。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/4ec21f9d731a4881a43e78fea575cc25.png#pic_ center" width="70%" />
</div>

反事实与各自源图像之间的平均欧几里得范数显著低于反事实与源类别中所有图像之间的平均欧几里得范数。对于对抗性示例，我们期望 X 中的欧几里得距离到各自的源图像非常小，而 Z 中的欧几里得距离应该更大。图 14 显示了反事实/对抗性示例与各自源图像之间以及反事实/对抗性示例与源类别中所有图像之间的 X 和 Z 中的距离分布。

我们在附录 C.7 中提供了其他数据集的图表，可在线获取。

在表 II 和表 III 中，我们展示了反事实和对抗性示例在 X 和 Z 中的平均欧几里得范数  $L_ 2$  ，证实了我们的预期。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/23b87ee2caed4abbb7ba33a9f4ce45a9.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/8fd16e0376ba438793f72390a349855d.png#pic_ center" width="70%" />
</div>

# V. 与其他方法的比较

在第二节中，我们介绍了我们生成微分同胚和近似微分同胚反事实的方法。这些方法独立于所使用的特定分类器和生成模型。尽管有许多生成反事实的方法（见第六节），但很少有独立于分类器和生成模型的。由于我们比较方法的兴趣在于生成反事实而不是比较生成模型或分类器，因此我们将我们的方法的比较限制在可以应用于独立训练的生成模型和分类器的方法上。我们确定了三种符合这些标准的最近的方法：Joshi 等人 [77]、Zhao 等人 [78] 和 Looveren 等人 [73]。我们使用详细在第四节 B 中设置的 CelebA 数据集生成反事实。由于 Looveren 的算法不能保证找到具有高目标置信度的反事实，我们将所有算法的目标置信度设置为  $\Lambda = 0.5$  ，并重复我们的实验。

我们在图 15 中展示了生成的反事实的示例，并在表 IV 中总结了定量评估结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/19afaf5315ba4f549354b50ad1567725.png#pic_ center" width="70%" />
</div>

Joshi 等人 [77] 引入了一种算法，该算法在生成模型（特别是 VAE 和 GAN）的潜在空间中执行梯度上升，同时最小化原始和修改数据点之间的差异。这种方法与我们的非常相似，因为我们在 [77] 中取  $\lambda \rightarrow 0$  时得到了我们提出的方法。将  $\lambda$  设置为更大的值会导致原始图像和反事实之间的相似度更大，特别是当在图像空间中测量时。然而，生成的反事实泛化到独立训练的 SVM 的性能较差，并且 Joshi 的算法需要更长的运行时间才能找到具有所需目标置信度的反事实（见表 IV）。

Zhao 等人 [78] 提出使用穷尽搜索或连续松弛来扰动 Wasserstein GAN 的潜在表示，并使用分类器测试生成的图像。然后算法返回在最大迭代次数内达到所需目标置信度的图像。由于我们在归一化流上测试 Zhao 的方法，基空间维度和搜索空间比可比 GAN 大得多。这导致计算时间很长（中位数为 15 分钟）。此外，不太可能找到最小的反事实，导致原始图像和反事实之间的相似度降低（见表 IV）。我们还注意到，并非所有搜索都产生了反事实，因为只有 438 次运行中的 500 次成功。这个数字可以通过增加搜索半径或每次迭代采样的图像数量来提高，但代价是更长的计算时间。

Looveren 等人 [73] 提出使用原型损失来引导反事实远离不代表源类别的最近原型，同时最小化图像空间中扰动的 L1 和 L2 范数。原型在潜在空间中定义。Looveren 等人将他们的算法应用于在 MNIST 上训练的 VAE。由于我们将他们的算法应用于 CelebA，这是一个更加多样化的数据集，寻找原型需要很长时间（每张图像超过 6 分钟，因为我们在训练集中搜索以捕获更多的多样性），并且产生的原型（被分类为目标类别的 5 个最近邻的平均值）质量一般。因此，生成反事实时会丢失原始图像的高级信息，如图 15 所示。结果，生成的反事实与原始图像之间的 SSIM 显著低于其他方法。然而，反事实确实泛化到了独立训练的 SVM。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d08845fe0125490aaf777dafc329e80e.png#pic_ center" width="70%" />
</div>

总结，我们的方法产生令人信服的反事实（见图 15），并且在所有定量评估指标上表现良好，而且比 Joshi 等人 [77]、Zhao 等人 [78] 或 Looveren 等人 [73] 提出的算法快得多。

# VII. 结论

在这项工作中，我们提出了理论上严谨而实用的方法来生成分类和回归任务的反事实，即精确和近似微分同胚反事实。精确的微分同胚反事实是通过在归一化流的基础空间中进行梯度上升得到的。而近似微分同胚反事实则是借助生成对抗网络或变分自编码器得到的。我们使用黎曼微分几何进行了彻底的理论分析，表明对于训练良好的模型，我们的反事实在搜索过程中必然保持在数据流形上，因此展现出对应目标类别的语义特征。近似微分同胚反事实存在信息丢失的风险，但允许在更高维度数据上的优秀扩展性。我们的理论发现得到了实验的支持，实验定量和定性地展示了我们方法在不同分类和回归任务以及众多数据集上的性能。

我们的反事实解释方法的应用是直接的，不需要重新训练，因此可以立即应用于调查深度学习中的常见问题，例如识别分类器或训练数据的偏见，审查错误分类的例子——这些都是计算机视觉应用中的常见任务。

未来的工作，我们打算研究我们的反事实在科学领域，特别是在医学应用中的益处，如数字病理学[11]或脑机接口[99]。

此外，本文提出的方法不仅限于生成图像数据的反事实或在计算机视觉中。特别地，可以想象在化学和物理学中的应用，其中这里提出技术可以用来优化稳定分子的期望属性，这些属性被限制在相关势能面的最小值[15]、[100]、[101]、[102]。

在实际应用中，通常有益的是加入对称性作为归纳偏置，这是机器学习中一个久经考验的范例[103]、[104]。通过使用等变归一化流，可以很容易地将对称性纳入我们的方法，例如在[105]中构建的。

总之，我们的方法适用于广泛的计算机视觉问题及其之外，因为它提供了一种优化预测模型在数据流形上的输出的方法，而数据流形仅由训练好的生成模型间接给出。
# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
