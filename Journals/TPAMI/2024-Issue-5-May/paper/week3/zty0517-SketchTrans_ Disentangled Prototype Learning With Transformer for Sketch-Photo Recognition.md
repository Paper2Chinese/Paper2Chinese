# 题目：[SketchTrans: Disentangled Prototype Learning With Transformer for Sketch-Photo Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10328884)  
## SketchTrans：使用Transformer进行草图照片识别的纠缠原型学习
**作者：Cuiqun Chen; Mang Ye; Meibin Qi; Bo Du** 

**源码链接：**  https://github.com/ccq195/SketchTrans
****

# 摘要

在手绘制草图与照片匹配（即草图-照片识别或重新识别）中，由于草图模态的抽象性质，存在信息不对称的挑战。现有的工作倾向于通过丢弃照片图像的外观线索或引入生成对抗网络（GAN）进行草图-照片合成，来学习共享嵌入空间的CNN模型。前者不可避免地损失了可判别性，而后者则包含了无法抹去的生成噪声。在本文中，我们首次尝试设计一种通过跨模态解耦原型学习的信息对齐草图变换器（SketchTrans+），而变换器在判别性视觉建模方面展现出巨大潜力。具体来说，我们设计了一种不对称解耦方案，通过动态可更新的辅助草图（A-sketch）来对齐模态表示，而无需牺牲信息。不对称解耦将照片表示分解为与草图相关和与草图无关的线索，将与草图无关的知识转移到草图模态中，以补偿缺失的信息。此外，考虑到两种模态之间的特征差异，我们提出了一种模态感知原型对比学习方法，该方法使用模态感知原型而不是原始特征表示来挖掘代表性的模态共享信息。通过在类别和实例级别的基于草图的数据集上的广泛实验，验证了我们提出的方法在各种指标下的优越性。

# 关键词

- 不对称解耦（Asymmetric disentanglement）
- 动态合成（dynamic synthesis）
- 原型学习（prototype learning）
- 草图-照片（sketch-photo）
- 识别（recognition）

# I. 引言

草图-照片识别和重新识别允许使用艺术家或业余爱好者绘制的草图图像来搜索目标照片图像[2]。最近关于草图-照片识别的研究集中在基于自由手草图的图像检索[3]和基于专业手绘草图的草图重新识别(ReID)[4]。例如，随着触摸屏技术的普及，基于自由手草图的图像检索(SBIR)[3],[5],[6],[7]使人类-计算机交互更加方便和高效。在智能视频监控系统中，草图重新识别[4],[8],[9],[10],[11],[12],[13]使用目击者描述来绘制目标人物，当没有目标行人照片可用时。此外，草图到照片的搜索也在社交媒体应用中得到了广泛应用。

草图和照片的不同成像原理使得这两种模态异构，导致信息不对称。此外，草图图像通常是抽象和扭曲的，这降低了它们传达有关对象的详细信息（如其颜色和纹理）的能力。这些因素在模态之间产生跨模态差异，使草图-照片识别极具挑战性。图像转换方法[14],[15]可以通过生成具有一致风格的跨模态图像来替代原始图像，从而减少草图和照片之间的差异，例如草图到照片的合成。然而，这种方法在对齐跨模态表示时面临困境，因为由于大样本多样性和固有领域差异引入的不可避免的生成噪声。

另一种流行的方法是[3],[4],[16],[17]集成跨模态度量学习，将草图和照片表示嵌入共享嵌入空间中。通常，在CNN模型基于信息不对称的表示空间中直接执行跨模态度量学习，其中忽略了两种模态之间的信息不对称。这导致优化目标不一致，具有较弱的辨别能力。为了学习模态共享的表示空间，必须消除跨模态差异，并在输入图像和特征级别对齐模态表示。最近，Sain等人[18]引入了一个对称解耦模型，将草图和照片模态的特征分解为模态共享的内容和模态特定的风格部分。然后，他们使用内容组件进行跨模态匹配。如图1(a)所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/da91209a3ed64cca89fca887f4abcd2d.png#pic_ center" width="70%" />
</div>

这种对称解耦方案严重依赖于通过生成模型建模结构线索，导致不可避免的生成噪声和对模态差异的有限鲁棒性。

本文提出了一种新颖的变换器框架(SketchTrans+)，通过跨模态不对称解耦原型学习来处理草图-照片识别和重新识别中的模态差异。考虑到草图和照片模态之间的信息不对称性，设计了一种不对称解耦方案，利用动态合成的辅助A-sketch来开发结构和风格属性。这种策略能够在不丢失信息的情况下实现模态对齐。为了进一步挖掘代表性的模态共享特征，我们提出最小化模态感知原型之间的距离，以促进跨模态特征对齐。原型级别的学习具有比一般特征级别优化在两种模态之间的特征差异中更强的辨别能力[19],[20]。与使用CNN模型学习区域级模态特征不同，我们应用变换器提取全局级语义表示。变换器能够更准确地捕获细粒度的结构信息并描述长距离关系[21]。SketchTrans+的基本思想如图1(b)所示，它在解耦空间中实现了一致的跨模态特征优化。它由两个主要组成部分构成：不对称解耦和模态感知原型对比学习。

首先，所提出的不对称解耦方案执行照片解耦和信息转移，形成对齐的潜在模态空间。具体来说，照片特征表示被分解为与草图相关和与草图无关的线索。与草图相关的表示包括与草图模态相关的对象轮廓和结构。与此同时，与草图无关的表示传达了照片的重要外观细节，如颜色和风格。此外，为了生成模态特征的信息对齐表示，我们的方法通过将与草图无关的知识转移到草图模态中来补偿缺失的信息。最后，设计了一种模态感知原型对比学习方法，以挖掘基于对齐模态嵌入空间的判别特征表示。它从不同模态中聚类类级别模态特征，并对比不同模态感知原型的相似性。

另外，由于对象的结构线索无法直接获取，因此从照片图像中分离与草图相关的信息是具有挑战性的。为了缓解这个问题，SketchTrans+开发了一个可更新和可学习的辅助草图模态(A-sketch)，从照片模态派生以促进不对称解耦。特别是，A-sketch模态是通过自适应调整策略生成的，它与手绘草图模态具有相似的风格，但保留了照片模态相同的结构。在A-sketch模态的指导下，任务驱动的注意力机制[22]从照片模态中提取与草图相关的表示，使得照片表示的分解成为可能。此外，为了充分利用A-sketch模态，我们引入了一个多模态联合学习框架，通过权重共享网络探索三种模态之间的关系。在这个框架中，A-sketch作为草图和照片模态之间的桥梁，在多模态框架中减轻了两种模态之间视觉失真的影响。

我们的主要贡献总结如下：

- 我们提出了一种新颖的信息对齐变换器框架(SketchTrans+)，通过解耦原型学习来处理草图-照片识别和重新识别中的模态差异。我们的模型在推理阶段实现了显著的性能提升，而没有增加计算复杂性。
- 我们提出了一种不对称解耦方案来解决草图和照片模态之间的信息不对称问题。通过照片解耦和信息转移，该方案在不牺牲模态信息的情况下统一了跨模态表示。
- 我们提出了一种模态感知原型对比学习方法，通过对齐的模态嵌入空间挖掘代表性的模态共享特征表示。基于对齐的模态嵌入空间，它对比了不同模态感知原型的相似性。
- 我们设计了一个从照片模态生成的动态可更新辅助草图(A-sketch)模态。通过自适应调整策略，A-sketch模态有效地指导了照片模态表示的解耦，并在多模态框架中架起了草图和照片模态之间的桥梁。
- 我们引入了一个强大的变换器基线，用于学习草图和照片模态的全局级语义，以进行草图-照片识别和重新识别。在实例和类别级别的草图数据集上的广泛实验表明，所提出的方法通常以显著的优势超越了现有技术。

这项工作的初步会议版本已在[1]中发布。本文扩展了四个主要改进：首先，我们深入分析了用于草图照片识别和重新识别的不对称解纠缠方案背后的原理。该方案旨在使特征表示在不损害信息的情况下跨模态对齐。其次，我们引入了一种新的模态感知原型对比学习方法来充分挖掘具有代表性的模态特征表示。基于对齐的模态表示空间，原型学习对比了类级特征的相似性，并明确地拉近了不同模态感知原型的距离。第三，我们将我们的方法推广到基于图像检索的类别级草图，以进一步验证其有效性。在大规模基于草图的数据集上进行的实验验证了其优于最先进的方法。最后，我们进行了广泛的实验分析和可视化，以证明所提出的方法在各种环境下的有效性。

# III. 提出的方法

本文提出了一种新颖的解耦原型和动态合成学习方法，该方法基于变换器框架（SketchTrans+），用于处理素描-照片识别与再识别任务中的模态变化。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b571540c66534323a223348ab4a745d0.png#pic_ center" width="70%" />
</div>

图2展示了提出方法的总体概述。首先，我们在第III-A节介绍了动态合成方法以获得A-sketch模态。随后，在第III-B节中，我们引入了基于变换器的特征提取器，用于三种模态的特征学习。接下来，在第III-C节中，我们提出了用于构建对齐的跨模态嵌入空间的非对称解耦方案，并在第III-D节中，模态感知原型对比学习用于挖掘代表性的模态共享特征。最后，在第III-E节中，我们描述了多模态联合学习方法和整体损失函数。

## A. 动态合成：A-Sketch模态

为了学习跨模态共享的特征表示，我们提出通过将照片特征分解为与素描相关（例如，轮廓和结构信息）和与素描不相关（例如，颜色和风格信息）的线索来构建对齐的潜在模态空间。由于结构线索不是直接可访问的，从照片图像中分离出与素描相关的信息是困难的。为了解决这个问题，我们生成了一个辅助素描模态（A-sketch），它描述了对象的结构信息以指导照片模态的解耦。
素描合成是人们希望在机器中模仿的能力之一，并且已经得到了广泛的研究。目前，许多素描合成技术[11]，[12]都是基于特定对象领域，如面部素描和卡通素描的合成。借鉴这些方法，我们提出从照片图像合成辅助素描模态以促进跨模态特征学习。在本文中，我们使用了一个现有的素描合成网络[15]来生成辅助素描模态。基于照片图像p，辅助素描被定义为 $as = G(p; \theta_ G)$ ，其中 $\theta_ G$ 对应于生成器 $G(·)$ 的参数，并且使用在Scibble[59]数据集上预训练的参数进行初始化。
考虑到源域（素描合成数据集）和目标域（素描-照片识别/再识别数据集）之间的差异可能很大，使用预训练和固定的网络参数可能很难使用生成器 $G(·)$ 正确描述对象的全部结构。这可能对跨模态识别过程产生负面影响。为了合成具有与素描模态相似风格但保持与照片模态相同结构的细粒度辅助素描模态，我们提出了一种自适应调整策略来细化和更新生成器参数。这种策略涉及合成和跨模态识别任务之间的联合训练。以下是优化生成器所产生的目标损失：

$$
L_ G(\theta_ G, \theta_ F) = L_ e + \lambda_ 1 L_ {reg},
$$

其中 $\theta_ F$ 表示与其它模态共享的A-sketch特征提取器 $F(·)$ 的参数。 $L_ e$ 代表一个身份引导的损失，用于缩小同一类别样本之间的距离。在我们的方法中， $L_ e$ 涉及交叉熵损失[60]和对比三元组损失（参见第III-E节）。 $\lambda_ 1$ 表示一个超参数。生成器的监督训练使其能够更好地关注与身份相关的结构信息。然而，在训练过程中，素描合成网络逐渐失去了对素描风格的关注。为了保持与素描模态相关的风格信息，设计了一个风格绑定损失 $L_ {reg}$ ，表示为

$$
L_ {reg}(\theta_ G) = \frac{1}{R} ||as - \bar{a}s||^2_ F,
$$

其中 R 表示 A-sketch 图像中的像素数。as 和 $\bar{a}s$ 分别指的是使用预训练参数生成的初步A-sketch和精炼的A-sketch。同时， $||·||^2_ F$ 是弗罗贝尼乌斯范数，它约束了初步和精炼A-sketch图像之间的距离。因此，生成器适应于生成包含与身份相关的结构信息并保留特定于素描风格信息的辅助A-sketch，以指导跨模态特征的学习。例如，我们在图3中可视化了一些使用预训练和精炼（w LG）合成网络生成的PKU-Sketch数据集中的A-sketch模态图像。粗略和细粒度A-sketch（w LG）的比较表明，自适应调整策略为A-sketch模态提供了更丰富的服装细节和行人结构信息。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/816a93c99bc84529bfe66b0ad744c2f5.png#pic_ center" width="70%" />
</div>

## B. 跨模态特征提取器

在素描-照片识别和再识别任务中，对象的素描图像与照片集相匹配。对于一个素描-照片数据集，让 P = {p_ i}^Np_ i=1 和 S = {s_ i}^Ns_ i=1 分别代表照片和素描。结果，总共有 N = Ns + Np 图像代表 I 个身份，带有地面真实标签 Y = {y_ i}^I_ i。值得注意的是，每个身份在具有挑战性的 PKU-Sketch [4] 数据集中只有一张素描。

具体来说，我们通过生成网络 G(·) 从照片集中收集 A-sketch 集 As。如图2所示，我们的方法采用单流网络来提取三种模态特征。由于素描图像的抽象和象征性，照片和素描图像在区域级信息方面存在显著差异。相反，全局级模态特征表示描述了身体的形状和局部信息之间的相关性，这在素描-照片匹配过程中提供了更好的区分能力[22]，[61]。特别是，视觉变换器（ViT）[62]，作为计算机视觉任务的第一个纯变换器骨干网络，显示出挖掘全局级信息的巨大潜力，并在性能上取得了显著提升。受[62]，[63]，[64]，[65]的启发，我们的方法专注于分析局部特征之间的长距离关系，并使用ViT挖掘模态特征的全局级表示。具体来说，为了从照片/素描/A-sketch图像 Ii中提取特征，我们将其划分为 M 个不重叠的固定大小补丁 {I_ m^i | m = 1, 2, ..., M}。因此，ViT的输入序列可以定义为

$$
X^i = [I_ {class}; U(I_ 1^i); U(I_ 2^i); U(I_ 3^i); ...; U(I_ M^i)] + PE,
$$

其中 U(·) 表示一个线性投影层，将像素补丁转换为一维向量。输入序列附加了一个额外的学习嵌入标记 Iclass，以获得最终的图像表示。图像补丁的位置嵌入由可学习参数 PE 表示。我们的模型使用基于变换器的特征提取器来学习具有更大区分度的全局级跨模态表示。请注意，出于效率考虑，三种模态共享单个 ViT。

## C. 非对称解耦

在素描-照片识别和再识别中，照片图像描述了素描图像对应的其他外观、详细信息和结构信息。照片和素描模态之间的信息不对称性使得学习模态共享的特征表示变得困难。本文提出了一种非对称解耦方案，以制定一个对齐的模态嵌入空间并处理跨模态差异。对齐学习的基本思想是通过补充素描模态中缺失的信息，创建一个信息对齐的跨模态特征嵌入空间，使模型能够突出模态共享的特征表示。所提出的非对称解耦方案包括两个步骤：照片解耦和信息转移。基于可学习和可更新的 A-sketch 模态，第一步是将照片特征表示解耦为与素描相关和与素描不相关的部分。在第二步中，使用知识转移将素描特征表示转换为照片特征表示。通过优化对齐的模态嵌入空间，模型可以在模态表示中识别共享的素描-照片信息。

照片解耦。对于计算机视觉任务，解耦表示学习[18]、[50]被广泛用于学习对象的通用表示。根据方法[22]，我们提出一个任务驱动的注意力机制，包含两个编码器，将照片表示解耦为与素描相关和与素描不相关的部分。A-sketch 模态具有与照片模态相同的空间结构，但在颜色和强度信息上与照片不同。因此，使用 A-sketch 模态，从照片模态中解耦结构信息是可行的。首先，我们定义一个补丁级掩码 α 来描述照片表示 $f_ p \in \mathbb{R}^{M \times C}$ 和 A-sketch 表示 $f_ {as} \in \mathbb{R}^{M \times C}$ 之间的空间关系，如图 4 所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/bbfb1f3ec0eb402a911506412ab70ecc.png#pic_ center" width="70%" />
</div>

在这里，M 和 C 分别表示图像补丁的数量和特征通道的大小。掩码的元素 $\alpha_ {i,j} \in [0, 1]^{M \times M}$ 定义如下：

$$
\alpha_ {i,j} = \frac{e^{Z(K(f_ {ip}),Q(f_ {j}^{as}))}}{\sum_ {j} e^{Z(K(f_ {ip}),Q(f_ {j}^{as}))}},
$$

其中 K(.) 和 Q(.) 分别表示参数化为 $W_ k$ 和 $W_ q$ 的线性全连接层。Z(.) 指两种表示之间的相似性计算。掩码矩阵 $\alpha_ {i,j}$ 使照片模态的结构信息得到增强。因此，我们可以获得照片图像的解耦素描相关表示 $f_ {+}^{p}$ 和素描不相关表示 $f_ {-}^{p}$ ，公式化定义为：

$$
f_ {+}^{p} = E_ {+}(\alpha \times V_ {+}(fp)),
$$

$$
f_ {-}^{p} = E_ {-}((1 - \alpha) \times V_ {-}(fp)),
$$

其中 $E_ {+}(\cdot)$ 和 $E_ {-}(\cdot)$ 分别指素描相关特征表示编码器和素描不相关特征表示编码器，它们由线性全连接层组成。 $V_ {+}(\cdot)$ 和 $V_ {-}(\cdot)$ 的目的是将特征 $fp$ 映射到不同的表示空间。具体来说，特征 $f_ {+}^{p}$ 描述了与素描和照片模态相关的结构特征。 $f_ {-}^{p}$ 描述了照片模态的非结构化特征表示。

信息转移。对齐学习通过在两种模态之间补充素描模态中缺失的信息来创建信息对称性。最近，自适应实例归一化方法[66]已广泛用于图像到图像翻译任务中的两个特征的重新组合。由于该方法在特征图级别重新组织数据分布，并根据仿射参数操作信息，因此它不需要太多的计算和存储资源。受到 Huang 等人[66]的启发，我们采用自适应实例归一化根据分解出的素描不相关特征 $f_ {-}^{p}$ 来校正手绘素描 $fs$ ，如图 2 所示。具体来说，素描模态表示被转换为照片模态表示，从而产生更新的素描表示 $\bar{f}_ s$ ：

$$
\bar{f}_ s = (1 + \gamma) f_ s - \mu[fs] \odot \sigma[fs] - \epsilon + \beta,
$$

其中 $\gamma$ 和 $\beta$ 描述了与素描不相关特征表示 $f_ {-}^{p}$ 相关联的可学习仿射参数。这些参数来自多层感知器（MLP）。均值 $\mu[·]$ 和标准差 $\sigma[·]$ 用于归一化素描特征 $fs \in \mathbb{R}^{M \times C}$ 。为了学习模态共享的嵌入，训练期间优化对齐的跨模态表示之间的关系至关重要。在本文中，我们进一步提出了一个解耦损失函数 $L_ D$ ：

$$
L_ D(\theta_ G, \theta_ F) = D(fp, \bar{f}_ s) + \lambda_ 2 D(fs, f_ {+}^{p}),
$$

距离函数 $D(\cdot)$ 用于约束对齐的跨模态表示之间的差距。具体来说，我们采用平滑 L1 损失[67]来衡量我们实验中的距离。这个方程中的第一项探索了与照片域（ $S_ p$ ）相关联的特征空间如何被优化。方程中的另一个项指的是优化与素描域（ $S_ s$ ）相关联的特征表示。这里有一个平衡参数 $\lambda_ 2$ 。通过这种双向对齐嵌入学习，我们的模型可以考虑更多的模态共享特征表示，从而最小化模态差异的影响。

## D. 模态感知原型对比学习

素描-照片识别/再识别的目标是获取共享模态表示（即类原型），以实现跨模态检索。不同类别具有不同的特征，而同一类别的不同模态表示具有一致性。通常，跨模态再识别方法同时优化类间样本距离和模态间距离[67]，以确定代表性的类级特征。本文提出了一种基于对齐模态嵌入空间的模态感知原型对比学习方法，以挖掘最具代表性的跨模态原型特征。模态感知原型学习的理念是在不同对齐的表示空间中分别学习类级原型。其学习目标是同一类别的原型尽可能接近，而不同类别的原型尽可能远离。

由于两个对齐空间中的样本差异很大，使用锚点到全部约束（例如，三元组损失）进行比较可能过于严格[68]。如果存在异常值，这种约束可能会误导网络参数的更新。因此，基于获得的对齐模态空间，我们首先将同一身份的模态实例聚合起来构建相应的类级原型：

$$
c_ {i}^{Sp} = \frac{1}{2k} \sum_ {j=1}^{k} (f_ {p,j} + \bar{f}_ {s,j})
$$

$$
c_ {i}^{Ss} = \frac{1}{2k} \sum_ {j=1}^{k} (f_ {s,j} + f_ {+,i}^{p,j})
$$

其中 k 表示小批量中单个类别的照片/素描图像数量。 $c_ {i}^{Sp}$ 和 $c_ {i}^{Ss}$ 分别代表第 i 类样本（i ∈ [1, B]，B 代表小批量中的类别数量）在照片感知域（Sp）和素描感知域（Ss）中的原型。受[51]的启发，为了最佳地最小化同类样本之间的原型距离，同时最大化不同类样本之间的原型距离，我们提出了模态感知原型对比（MAPC）损失，定义为：

$$
L_ {MAPC}(i, j) = -\log \frac{\exp(sim(c_ {i}^{Sp}, c_ {j}^{Ss})/\tau)}{\sum_ {n=1}^{B} \exp(sim(c_ {i}^{Sp}, c_ {n}^{Ss})/\tau)}
$$

其中 $c_ {i}^{Sp}$ 和 $c_ {j}^{Ss}$ 是一对正样本对。sim(·) 表示样本对之间的相似性。τ 是温度系数。MAPC损失优化了不同分解空间中类内和类间样本的距离，与解耦损失 $L_ D$ 互为补充。通过引入模态感知对比学习方法，我们的模型能够展现出更大的跨模态变化的鲁棒性，并提取最具代表性的模态不变特征。

## E. 整体目标

在传统方法中，基于素描的方法使用素描和照片的配对，通过身份分类损失和三元组损失来执行跨模态特征学习。与成对优化不同，我们的方法采用了一种多模态联合训练策略，以进行素描-A-素描-照片三向优化。在这一策略中，A-素描模态得到了充分利用，结果，模态差异得到了缓解，而无需进一步改进网络。更具体地说，我们将素描、A-素描和照片图像组合成一个批次。在多模态联合学习框架中，分别评估身份分类损失 $L_ {ID}$ 和对比三元组损失 $L_ {CTL}$ 。身份分类损失将同一身份的三种模态图像拉近，定义如下：

$$
L_ {ID} = L_ {id}(f_ s) + L_ {id}(f_ {as}) + L_ {id}(f_ p),
$$

其中 $L_ {id}$ 代表交叉熵损失。此外，由于跨模态差异很大，使用批量硬三元组损失学习素描到照片识别的小类内距离可能很困难。为了解决这个问题，我们提出了对比三元组损失（CTL），以最大化所有正/负对在整个模态内部和跨模态之间的益处，如下所示：

$$
L_ {CTL} = \sum_ {i=1}^{T} (1 - \exp(h_ {i,t}^n - h_ {p})),
$$

$$
h_ {p} = \sum_ {i,j} \exp(h_ {i,j}^p) / \sum_ {i,j} \exp(h_ {i,j}^p),
$$

其中 T 表示锚点 i 的负样本数量。 $h_ {i,t}^n$ 和 $h_ {i,j}^p$ 分别是负样本和正样本对之间的相似性值。由于大的模态差异使得不同正样本对之间存在大的类内差距，我们使用所有正样本对之间相似性的加权和（ $h_ {p}$ ）来表示正样本对的相似性。然后，通过将所有负样本对相似性与 $h_ {p}$ 进行比较，CTL 通过寻找模态共享的特征嵌入空间进一步加强跨模态鲁棒性。

我们方法的整体损失函数 L 表示为：

$$
L = L_ {ID} + L_ {CTL} + L_ {D} + \lambda_ 1 L_ {reg} + L_ {MAPC}.
$$

需要注意的是，在推理过程中，我们只使用特征提取器，而不使用 A-素描生成器或解耦方案。因此，与基线相比，SketchTrans+ 的测试设置和推理时间保持不变。

# IV. 实验结果

为了验证我们提出方法的有效性，我们在第IV-A节中对实例级素描基础检索数据集进行了广泛的实验，并在第IV-B节中对类别级素描基础数据集进行了实验。具体来说，我们采用了更具挑战性的未见过测试类别，例如零样本素描基础图像检索，用于类别级设置。消融研究和进一步的探索分别在第IV-C节和第IV-D节中呈现。最后，可视化分析在第IV-E节中说明。为了与现有方法[3]，[4]，[69]，[70]进行公平比较，所有实验验证都是在相同设置下进行的。

## A. 实例级素描基础检索

数据集和评估指标。按照现有方法[4]，我们在PKUSketch[4]、QMUL-ShoeV2[3]、[44]和QMUL-ChairV2[3]上进行了实例级实验。如[3]、[4]、[44]中的实例级设置所述，我们选择Rank-k匹配准确率作为评估指标，该指标代表检索正确跨模态图像的概率。

PKU-Sketch[4]数据集包含200名行人，是监控系统中第一个素描基础人员重新识别数据集。每个人拥有两张拍照图像和一张专业素描图像。照片是使用两个交叉视角摄像机拍摄的，五位专业画家根据目击者描述绘制了素描。具体来说，为了消除绘画风格变化的影响，Pang等人[4]提出将每种绘画风格的四分之三图像分配为训练集，四分之一为测试集。如Pang等人[4]所述，随机选择150个身份用于训练集，50个身份用于测试集。为了性能评估，我们使用十次实验的平均值。

QMUL-ChairV2（ChairV2）和QMUL-ShoeV2（ShoeV2）[3]数据集是从不同网站收集的，分别包括400张椅子照片和2000张鞋子照片。对于商业应用，这两个数据集作为代表性的实例级自由手素描数据集。需要注意的是，每张照片至少对应三个素描，这些素描是由AMT收集的。最后，椅子的素描总数为1275，鞋子的素描总数为6730。Yu等人[3]提供了有关训练和测试集划分的详细信息。

实施细节。所提出的方法使用ViT-Base(L = 12)作为骨干网络，最初使用ImageNet数据集预训练的参数进行配置。通常，每个批次随机选择16个身份，每个身份包含一张拍照图像、一张A-素描图像和一张素描图像。数据增强包括训练阶段的随机裁剪、填充和水平翻转，如参考文献[3]、[4]、[44]、[79]、[80]所述。此外，随机梯度下降(SGD)优化器在80个周期内训练模型。初始学习率设置为0.008，并根据余弦学习率函数衰减。参数λ1对所有实例级素描数据集均设置为10.0。参数λ2在PKU-Sketch、ChairV2和ShoeV2数据集上分别取5.0、5.0和10.0。

1)在PKU-Sketch数据集上的比较。我们在PKU-Sketch数据集上评估我们方法的结果如表I所示。为了挖掘模态不变特征表示，Pang等人[4]首先为素描-照片识别设计了一个跨域对抗学习框架。Yang等人[44]提取多级模态特征，并采用空间注意力机制来解决跨域问题。CSIG[44]试图在双流网络中使用交叉频谱图像来弥合模态差距。与这些方法不同，我们的方法探索了一种模态对齐学习方案，显著提高了性能。此外，通过模态感知原型对比学习，模型性能提高到85.8%的Rank-1准确率。表I中的结果表明，扩展方法的Rank-1准确率比会议版本[1]高出1.2%。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9890809a74d943ddb00cbbf22e1d8904.png#pic_ center" width="70%" />
</div>

2)在ChairV2和ShoeV2数据集上的比较。如表II所示，所提出的方法在两个自由手素描数据集上表现出色。具体来说，Bhunia等人[6]引入了一个新的基于素描的图像检索(SBIR)框架，使用不完整的素描检索照片。2021年，他们提出通过照片到素描的生成模型增强成对的素描-照片数据，以执行跨模态特征学习[5]。Stylemeup[18]提出了一种非对称解耦方法来处理变化的素描风格问题。该方法将素描和照片特征分解为内容和风格线索，基于内容线索执行跨模态匹配。与这些方法不同，所提出的方法首先提出了一个解耦原型学习框架，以形成信息对齐的跨模态嵌入空间。在两个数据集上，我们的方法分别达到了83.3%和41.1%的Rank-1准确率，显著优于SketchTrans[1]，分别提高了1.6%和2.4%。具体来说，在ShoeV2数据集上，NT[41]实现了比我们方法更高的Rank-1准确率。这可能是由于不同鞋子之间的小结构差异，使得模型容易受到素描笔画噪声的影响。NT通过识别预处理阶段中有效的素描笔画子集来增强对噪声的鲁棒性。然而，我们的方法不是消除噪声笔画，而是旨在挖掘素描和照片图像之间的模态共享信息，无需素描的预处理，从而促进高效的模型部署。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a2a98de7430148678d2e81077e3eaf34.png#pic_ center" width="70%" />
</div>

## B. 类别级素描基础检索

数据集和评估指标。基于[81]中描述的零样本素描基础图像检索设置，我们使用Sketchy[69]和TU-Berlin[70]数据集进行实验。我们对类别级检索任务的评估标准是平均精度均值(mAP@all)和精度(P@100)。

Sketchy[69]数据集包含12,500张照片和75,479幅素描，涵盖125个不同类别。Liu等人[28]通过从ImageNet数据集[82]中选取60,502张额外图像扩展了这个数据集，以生成73,002张图像检索库(Sketchy Ext)。为了零样本素描基础检索，Shen等人[30]提出随机选择25个素描和图像类别作为未见过的测试集，并在剩余的100个见过的类别上进行训练。

TU-Berlin数据集包括250个对象类别，代表大多数日常物品。该数据集收集了每个类别由绘制者描述的80幅素描图像。特别是，Zhang等人[23]从ImageNet数据集或使用Google搜索收集了每个素描图像的真实照片。最后，这个数据集总共包含204,489张照片和20,000幅素描(TU-Berlin Ext)。按照[30]，选择30个相同类别作为未见过/测试集，其余220个类别在我们的实验中用作训练集。我们在Sketchy和TU-Berlin数据集上分别进行了五次实验，并将结果平均作为最终结果，从而避免了结果的随机性[81]。在测试阶段，素描图像作为查询，而照片图像作为检索库。

实施细节。与上述实例级实验相同，类别级实验采用相同的网络和数据增强。不同之处在于，每个批次中每个身份包含四张照片图像、四个A-素描图像和四个素描图像。参数λ1和λ2对所有类别级素描数据集分别设置为1.0和0.001。

1)在Sketchy和TU-Berlin数据集上的比较。为了验证所提出方法在素描基础类别级数据集上的泛化能力，我们在Sketchy和TU-Berlin数据集上进一步进行了实验。结果如表III所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/8d191c889f314a8b9cec696b794c2b71.png#pic_ center" width="70%" />
</div>

现有的零样本素描基础图像检索(ZS-SBIR)方法采用单一特征提取器或生成对抗机制来学习共享的语义信息。最近，Wang等人[16]和Zhang等人[17]通过度量学习探索类别间信息来处理跨模态差异。具体来说，Tian等人[21]提出使用视觉变换器挖掘全局上下文，以消除模态变化。他们设计了一种基于标记的策略和三路视觉变换器(TVT)网络。尽管在Sketchy数据集上，我们模型的P@100性能低于TVT，但在各种指标下，我们的方法显著提高了TU-Berlin数据集的性能，证明了其有效性。与Sketchy数据集相比，TU-Berlin是一个更复杂的数据集，包含更多类别和抽象素描。TVT在Sketchy数据集上具有较高的P@100，可能是因为其三个变换器使用知识蒸馏来保持每个实例的模态特定全局信息，从而提高检索精度。此外，SketchTrans+在两个数据集上分别比我们的会议版本[1]提高了1.5%和2.7%的mAP@all，表明它更能检索到所有正确的结果，在实践中更有意义。总之，我们方法的竞争性能表明，解耦原型学习方案适用于实例级素描检索和类别级素描基础图像检索。

## C. 消融研究

为了深入理解所提出方法的各个组成部分对整体性能的贡献，我们在PKU-Sketch、ChairV2和TU-Berlin数据集上进行了消融研究。具体的实验设置和结果如下：

1)ViT骨干网络的效果。如表IV所示，基于变换器块的ViT在与ResNet-50模型（'Res'）[92]进行比较时，无论是使用交叉熵损失还是三元组损失，都展现出了更好的性能。一个可能的解释是变换器块能够建立素描/照片图像的长距离局部依赖性，这有助于学习全局级线索。在后续实验中，我们选择ViT作为特征提取器。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/6648673950724b499e95a81290b79bcb.png#pic_ center" width="70%" />
</div>


2)对比三元组损失的效果。当优化单一正样本对时，由于素描和照片模态之间存在显著的类内变化，学习共享特征空间变得困难。因此，我们提出了对比三元组损失（CTL）来最小化大类内差异的影响。这种损失函数涵盖了批次中所有正负样本对，用于跨模态度量学习。通过比较索引3和索引2，我们发现CTL在PKU-Sketch数据集的Rank-1准确率上提高了7.2%，在TU-Berlin数据集的mAP@all上提高了7%。

3)多模态联合学习的效果。已有大量研究关注从照片到素描的生成任务。在本文中，我们提出使用从照片派生出的素描图像构成一个多模态联合框架，这可能有助于学习跨模态共享特征。如表IV索引4所示，在PKU-Sketch和ChairV2数据集上，Rank-1准确率从78.6%和77.7%分别提高到81.0%和78.4%。它还在TU-Berlin数据集上实现了0.3%的mAP@all提升。

4)非对称解耦方案的效果。考虑到两种模态之间的信息不对称性阻碍了共享表示的学习，我们设计了一种非对称解耦方案来对齐两种模态嵌入。如表IV索引5所示，解耦损失（LD）在PKU-Sketch和ChairV2数据集上的Rank-1准确率分别提高了2.4%和4.2%。这种显著的提升表明非对称解耦学习可能是细粒度跨模态检索的一个重要方向。然而，在TU-Berlin数据集上性能略有下降，原因可能是显式的样本级约束容易对类别级任务造成过拟合。

5)模态感知原型学习的效果。为了进一步挖掘代表性的模态共享特征，我们设计了一种模态感知原型对比损失，该损失在对齐的特征空间中对比不同模态感知原型的相似性。通过比较索引6和索引5，我们发现所提出的原型损失有效地提高了模型的性能。它在三个数据集上分别获得了85.8%和83.3%的Rank-1准确率以及64.4%的mAP@all。总之，所提出的组件相互加强，使我们的模型能够达到最佳性能。

## D. 进一步探索

A-Sketch模态的重要性。A-Sketch模态在帮助弥合跨模态差距和促进解耦学习过程中起着重要作用。然而，基于预训练参数生成的粗糙A-Sketch图像由于领域差异，质量不是很高，这对识别过程有负面影响。因此，我们提出了一种自适应调整策略来更新A-Sketch生成器。该策略以端到端框架中联合训练生成器和跨模态识别网络。如表V所示，与固定生成器相比，没有约束Lreg的简单生成器(`Re`)显示出相当的性能提升。此外，通过添加约束Lreg(`Re+Lreg`)，模型性能进一步提高，在PKU-Sketch上达到了84.6%的Rank-1准确率。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5b94734791bb49d7aa944147eab9b0c8.png#pic_ center" width="70%" />
</div>

对比三元组损失在其他任务中的有效性。我们进一步评估了所提出的对比三元组损失(CTL)在其他跨模态检索任务中的有效性，即可见光-红外人再识别(VI-ReID)。具体来说，我们在AGW[94]的基础上进行了实验，AGW是一种受欢迎的新颖VI-ReID方法。采用了公开代码提供的默认设置。我们通过用CTL替换AGW中的加权正则化三元组损失来测试CTL的有效性。结果显示在表VI中。这些结果表明，所提出的对比三元组损失显著提高了VI-ReID模型在不同指标下的性能。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c0e1b76717b64f26bc66b370d5e860b0.png#pic_ center" width="70%" />
</div>

超参数分析。图5显示了权重λ1（公式(1)中）和λ2（公式(8)中）的影响。我们将λ1的权重从0提高到15，以验证模型效果。在两个数据集中，当等于10时，模型表现更好。当λ2=5.0时，PKU-Sketch和ChairV2分别达到了83.4%和81.4%的Rank-1准确率。这些结果表明，双向对齐特征的平衡权重优化有助于学习区分性特征。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/812f18ca491f4c1b81faa9dba84ecb0e.png#pic_ center" width="70%" />
</div>

## E. 可视化分析

在这一节中，我们使用三种可视化方法对所提出方法的有效性进行了定性分析：注意力图、距离分布和检索结果。我们在PKU-Sketch、ChairV2和TU-Berlin三个数据集上对模型进行了测试。需要注意的是，我们的测试阶段只涉及使用特征提取器，不包括使用A-素描生成器和解耦模块。

注意力图的可视化。首先，我们采用Score CAM[95]生成注意力图。在PKU-Sketch数据集上训练的基线和SketchTrans+模型的结果如图6所示。我们注意到，对于素描和照片模态，SketchTrans+专注于相同位置的代表性显著区域，这验证了所提出的解耦原型学习为两种模态实现了一致的优化，例如第一个人身上的条纹短袖上衣。我们进一步注意到，SketchTrans+允许更多的全局级特征，增强了对跨模态变化的鲁棒性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b270f398aa7543548921119e5fbd54f0.png#pic_ center" width="70%" />
</div>

距离分布的可视化。我们可视化了测试集中正样本和负样本的成对欧几里得距离，如图7所示。为了消除随机性并保持公平性，我们选择了PKU-Sketch和TU-Berlin数据集十个和五个实验结果中正负样本的平均距离进行可视化。基线模型使用ViT骨干的结果如图7(a)所示，SketchTrans+的结果如图7(b)所示。例如，在实例级和类别级素描基础数据集中，我们的方法通过合成和解耦方案学习模态共享特征，使得素描和照片模态之间的差距缩小，同时保持了比基线更大的类间距离。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/467ccf6fbe8e43789bcb8924877f8651.png#pic_ center" width="70%" />
</div>

检索结果的可视化。如图8所示，我们在三个数据集的测试集上可视化了一些检索结果。基于这些结果，我们的方法似乎具有以下优点：(1)在自由手和专业精细素描数据集中，我们的方法减少了类内距离，表明通过解耦原型学习，可以更有效地学习跨模态表示。(2)通过探索全局级模态特征，我们的方法具有处理素描风格变化的能力，如图8中ChairV2数据集最后三个示例所示。(3)我们的方法能够通过动态合成和解耦原型学习的结合，有效地关注目标的整个和详细特征。例如，PKU-Sketch显示了一些局部区域内行人样本之间的高度相似性，这可能导致行人样本之间的匹配错误。与基线模型相比，我们的方法挖掘了代表性的模态共享特征，增强了对干扰样本和模态变化的鲁棒性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e5584d8a4680487f9d96cef3642245bf.png#pic_ center" width="70%" />
</div>

# V. 结论

本文专注于素描-照片识别和再识别任务。两种模态之间的异构性和信息不对称性使得学习模态共享特征变得复杂。为了学习代表性的模态不变嵌入，我们提出了一种新颖的解耦原型和动态合成学习方法，该方法在变换器框架内（SketchTrans+）。基于解耦和知识转移，我们的方法构建了信息对齐的跨模态嵌入空间来处理模态变化。

首先，我们设计了一种非对称解耦方法，仅将照片特征分解为与素描相关和与素描不相关的线索。然后，将素描不相关的特征转移到素描模态中，以补充素描中缺失的信息，从而产生更新的素描表示。最后，我们执行模态感知原型对比学习，基于对齐的解耦空间学习模态共享特征表示。此外，为了协助照片解耦，我们引入了一个动态辅助素描（A-sketch）模态，它具有与照片相同的结构信息，但采用了素描的风格。

在五个代表性的实例级和类别级素描基础数据集上进行的全面评估验证了所提出方法的优越性。未来，我们将探索所提出方法在无监督跨模态任务中的有效性，以在没有标记样本的情况下自适应地学习模态共享特征。



