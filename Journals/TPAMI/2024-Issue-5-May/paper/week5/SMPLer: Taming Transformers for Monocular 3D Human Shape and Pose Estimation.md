# 题目：[SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation] (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10354384)  
## SMPLer: 驾驭Transformer 用于单目 3D 人体形状和姿态估计
**作者：Xiangyu Xu; Lijuan Liu; Shuicheng Yan** 

****
# 摘要
神经辐射场（NeRF）在新视图合成方面表现出了巨大的成功。然而，由于潜在的校准信息不完善和场景表示不准确，现有的基于 NeRF 的方法在从现实世界场景中恢复高质量细节方面仍然面临挑战。即使使用高质量的训练帧，NeRF 模型生成的合成新视图仍然存在明显的渲染伪影，如噪声和模糊。为了解决这个问题，我们提出了 NeRFLiX，一种学习降解驱动的视点间混合器的通用 NeRF-无关恢复范式。特别地，我们设计了一种 NeRF 样式的降解建模方法并构建了大规模的训练数据，使得深度神经网络能够有效地去除 NeRF 原生的渲染伪影。此外，除了降解去除外，我们还提出了一种视点间聚合框架，该框架融合了高度相关的高质量训练图像，将最先进的 NeRF 模型的性能推向了新的水平，并生成高度逼真的合成视图。在此基础上，我们进一步提出了 NeRFLiX++，它具有更强的两阶段 NeRF 降解模拟器和更快的视点间混合器，实现了卓越的性能和显著提高的计算效率。值得注意的是，NeRFLiX++ 能够从噪声较大的低分辨率 NeRF 渲染视图中恢复逼真的超高分辨率输出。大量实验表明 NeRFLiX++ 在各种新视图合成基准上具有出色的恢复能力。


# 关键词
- 3D人体形状和姿态
- 注意力
- 关节感知
- 多尺度
- SMPL
- transformer



## I. 引言


单目 3D 人体形状和姿态估计是计算机视觉中的一个基础任务，旨在从单一输入图像中恢复未穿衣的人体形状及其 3D 姿态。它已广泛应用于许多领域，包括视觉跟踪，虚拟/增强现实，运动生成，图像处理和神经辐射场。与多视角 3D 重建不同的是，该任务由于单个 2D 图像固有的深度模糊性而特别具有挑战性，通常需要从大量数据中学习的强先验知识以生成合理的结果。因此，最先进的算法使用 Transformers 来完成这项任务，因为它们具有强大的抓取知识和从数据中学习表示的能力。

现有的单目 3D 人体形状和姿态估计的 Transformers 一般遵循 ViT 风格设计网络。如图 1(a) 所示，目标嵌入首先与输入特征连接，然后由一个全注意力层处理，该层建模了所有成对依赖关系，包括目标-目标、目标-特征、特征-目标和特征-特征。虽然这种设计取得了令人印象深刻的成果，但它导致了相对于图像特征长度的二次计算和内存复杂性，即 $O((l_ F + l_ T)^2)$，其中 $l_ F$ 和 $l_ T$ 分别是特征和目标标记的数量（或简单地说是图像特征 $F$ 和目标嵌入 $T$ 的长度）。这种复杂性对于较大的 $l_ F$ 来说是无法承受的，阻碍了现有方法在 Transformers 中使用高分辨率图像特征，这些特征具有丰富的细粒度特征，有助于准确的 3D 人体形状和姿态恢复。

在这项工作中，我们提出了两种策略来改进 Transformer 框架，以便更好地利用高分辨率图像特征进行高质量 3D 人体形状和姿态重建。其中之一是注意力解耦。我们注意到，与原始 ViT 不同的是，图像特征是通过注意力操作学习的，3D 人体 Transformers 通常依赖卷积神经网络 (CNNs) 来提取这些特征，注意力操作主要用于聚合图像特征以改进目标嵌入。因此，建模图 1(a) 中的特征-特征和特征-目标相关性并不重要，因为它们对目标没有直接影响。因此，我们提出将全注意力操作解耦为目标-特征注意力和目标-目标注意力，这些注意力串联在一起以避免二次复杂性。如图 1(b) 所示，解耦注意力的计算和内存复杂性仅为 $O(l_ F l_ T + l_ T^2)$，它相对于特征长度 $l_ F$ 呈线性关系。

我们提出的另一种改进先前 3D 人体 Transformer 的策略是基于 SMPL 的目标表示。现有的 3D 人体形状和姿态估计 Transformers 大多采用基于顶点的目标表示，其中嵌入长度 $l_ T$ 通常相当大，等于人体网格上的顶点数量。即使采用了提出的注意力解耦策略，较大的 $l_ T$ 仍会带来相当大的计算和内存成本，这阻碍了高分辨率特征的使用。为了解决这个问题，我们引入了一种基于参数人体模型 SMPL 的新目标表示，使用这种表示，我们只需要学习少量目标嵌入来表示人体形状以及 3D 身体部件旋转。

结合上述两种策略，形成了一种更简洁的注意力学习过程，这不仅允许在 Transformer 中利用高分辨率特征，还激励我们探索更多新设计以改进 3D 人体形状和姿态估计。特别是，由于我们模型的效率提高，我们引入了一种多尺度注意力操作，该操作在一个简单和统一的框架中有效地利用了多尺度信息。此外，由于提出的目标表示明确描述了身体部件之间的 3D 相对旋转，这些旋转大多是局部的，我们进一步提出了一种关节感知注意力模块，该模块强调身体关节周围的局部区域，以更好地推断 3D 人体的关节状态。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/de427bc420ce41f1bfb23f1461bfdcf6.png)



总之，我们做出了以下贡献：
- 通过引入注意力解耦和基于 SMPL 的目标表示，我们提出了一种新的 Transformer 框架 SMPLer，该框架可以利用大量特征标记以准确和高效地进行 3D 人体形状和姿态估计。
- 基于上述两个关键设计，我们进一步开发了多尺度注意力和关节感知注意力模块，这显著提高了重建结果。
- 大量实验证明，SMPLer 在效率更高的情况下，相比基线方法表现更好。特别是，与最先进的方法相比，提出的算法在 Human3.6M 上将 MPJPE 误差降低了 10% 以上，而其参数不到前者的三分之一。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/7103f5df7dd949428102c6525477be5a.png)




## III. 方法

在这项工作中，我们提出了一种新的基于 SMPL 的 Transformer 框架 (SMPLer) 用于 3D 人体形状和姿态估计，该框架可以基于高效的解耦注意力和紧凑的目标表示来利用大量图像特征标记。图 2 展示了概述。

### A. 高效注意力公式

注意力操作是 Transformers 的核心组件，可表示为：

$$
h(Q,K, V) = f_{\text{soft}} \left( \frac{(QW_ q)(KW_ k)^\top }{\sqrt{d}} \right) (VW_ v),
$$

其中 $f_{\text{soft}}$ 是沿行维度的 softmax 函数。 $Q \in \mathbb{R}^{l_ Q \times d}$ 和 $K,V \in \mathbb{R}^{l_ K \times d}$ 是此操作的输入，分别表示查询、键和值。 $l_ Q$ 是查询 $Q$ 的长度， $l_ K$ 是 $K$ 和 $V$ 的长度。 $d$ 对应通道维度。 $W_ q,W_ k,W_ v \in \mathbb{R}^{d \times d}$ 代表可学习的线性投影权重。我们在实现中使用多头注意力，每个头遵循公式 (1)，不同头使用不同的线性投影权重。类似于先前的工作，我们在注意力操作中也使用层归一化和 MLP，在 (1) 中为了简洁起见被省略。

本质上，(1) 用点积建模了 $Q$ 和 $K$ 中标记对之间的依赖关系。输出 $h(Q,K, V) \in \mathbb{R}^{l_ Q \times d}$ 可以看作是通过从 $V$ 聚合信息增强的新查询嵌入，聚合权重由 $Q$ 和 $K$ 之间的依赖关系决定。值得注意的是，当查询、键和值相同时，(1) 被称为自注意力，我们表示为 $h_{\text{self}}(Q) = h(Q,Q,Q)$。当仅键和值相同时，而查询不同，操作变为交叉注意力，表示为 $h_{\rm cross}(Q,K) = h(Q,K,K)$。

#### 1) 全注意力：现有的 3D 人体重建 Transformers 基本上遵循 ViT 风格结构，采用图 1(a) 所示的全注意力公式。从数学上讲，全注意力可以写为：

$$
h_{\text{self}}(T \| F),
$$

其中使用了自注意力 $h_{\text{self}}(Q)$， $Q$ 是目标嵌入 $T$ 和图像特征 $F$ 沿标记维度的连接，表示为 $T \| F \in \mathbb{R}^{(l_ T+l_ F) \times d}$。目标嵌入表示感兴趣的变量，在这项工作中对应于 ViT 中的类标记和 3D 人体表示。

由于 (2) 中的图像特征 $F$ 涉及到查询和键，全注意力导致了相对于图像特征长度 $l_ F$ 的二次计算和内存成本，即 $O((l_ F + l_ T)^2)$。因此，先前的工作仅在 Transformer 中使用低分辨率特征，其中 $l_ F$ 较小。特别是，假设来自 CNN 主干网的不同分辨率特征表示为 $F = \{F_ 1, \ldots, F_ S\}$，其中 $S$ 是尺度数量，Mesh Graphormer 仅在注意力操作中使用 $F_ S$，并且直接包括 (2) 中的高分辨率特征，例如 $F_ 1$，在计算上是无法承受的。

#### 2) 解耦注意力：我们注意到注意力操作在 ViT 和 3D 人体形状和姿态估计的 Transformers 中起着不同的作用：原始 ViT 完全依赖注意力来学习有表现力的图像特征，而 3D 人体 Transformer 使用深度 CNN 进行特征提取，注意力主要用于聚合图像特征以改进目标嵌入。因此，在这里建模特征-特征依赖关系不那么重要，因为它对目标没有直接影响，这意味着可以通过修剪全注意力来避免二次复杂性。

受此启发，我们提出了一种解耦注意力，绕过特征-特征计算，如图 1(b) 所示。它由目标-特征交叉注意力和目标-目标自注意力组成，可以写为：

$$
h_{\text{self}}(h_{\text{cross}}(T, F)).
$$

值得注意的是，(3) 的计算和内存成本为 $O(l_ F l_ T + l_ T^2)$，相对于特征长度 $l_ F$ 呈线性关系。

### B. 紧凑目标表示

虽然注意力解耦策略有效地降低了计算负担，但大的 $l_ T$ 可能仍会阻碍 (3) 中高分辨率特征的利用。现有的 3D 人体 Transformers 通常通过回归顶点位置 $Y \in \mathbb{R}^{N \times 3}$ 来恢复 3D 人体网格，这导致了冗余的目标表示 $T \in \mathbb{R}^{N \times d}$，其中 $T$ 的第 $i$ 行是第 $i$ 个顶点的嵌入。此目标嵌入的长度通常相当大（对于 SMPL 网格，默认 $N = 6890$），即使经过网格下采样，仍会导致注意力操作中计算和内存成本很大。为了解决这个问题，我们设计了一种基于参数化人体模型 SMPL 的更紧凑目标表示。

#### 1) 参数化人体模型：SMPL 是一个灵活且表现力丰富的人体模型，已广泛用于 3D 人体形状和姿态建模。它通过一组姿态参数 $\theta \in \mathbb{R}^{H \times 3}$ 和紧凑形状向量 $\beta \in \mathbb{R}^{1 \times 10}$ 进行参数化。身体姿态 $\theta$ 由一个包含 $H = 24$ 个关节（包括身体根部）的骨骼装配定义。 $\theta$ 的第 $i$ 行（表示为 $\theta_ i$）是第 $i$ 个身体部件在李代数空间 $so(3)$ 中旋转的轴角形式的斜对称矩阵。身体形状通过主成分分析从 3D 人体扫描训练集学习的低维空间表示， $\beta$ 是主基向量的系数。

通过 $\theta$ 和 $\beta$，我们可以获得 3D 人体网格：

$$
Y \in \mathbb{R}^{N \times 3} = f_{\text{SMPL}}(\theta, \beta),
$$

其中 $f_{\text{SMPL}}$ 是生成预定义三角网格上的顶点 $Y$ 的 SMPL 函数。3D 人体关节可以通过使用预训练矩阵 $M \in \mathbb{R}^{H \times N}$ 对顶点进行线性映射获得：

$$
J \in \mathbb{R}^{H \times 3} = MY.
$$

通过 3D 人体关节，我们可以使用弱透视投影进一步获得 2D 关节。将相机参数表示为 $C \in \mathbb{R}^3$，表示缩放因子和投影过程中 2D 主点平移，可以获得 2D 关节：

$$
J \in \mathbb{R}^{H \times 2} = \Pi_ C(J),
$$

其中 $\Pi_ C$ 是弱透视投影函数。

#### 2) 基于 SMPL 的目标表示：受到 SMPL 紧凑性和表现力的启发，我们用基于 SMPL 的表示替换了原始的基于顶点的表示。由于感兴趣的变量是 $\{\theta_ i\}_{i=1}^H, \beta, C$，如 (4) 和 (6) 所述，我们将新目标表示设计为 $T \in \mathbb{R}^{(H+2) \times d}$，其中 $T$ 的前 $H$ 行对应 $\theta$ 的 $H$ 个身体部件旋转，剩余的两行描述身体形状 $\beta$ 和相机参数 $C$。

这种新目标表示具有几个优点。首先，这种表示的长度远小于基于顶点的表示 $(H + 2 \ll N)$，从而促进了我们模型的高效设计。其次，我们的目标表示能够将解决方案限制在 SMPL 身体空间，自然确保了平滑的网格，而基于顶点的表示容易出现异常值，可能导致尖刺的身体表面。第三，基于 SMPL 的表示显式地建模了身体部件的 3D 旋转，因此在许多应用中更容易使用，例如驱动虚拟化身。相比之下，顶点不能直接使用，必须先转换为旋转（通常是以迭代优化方式），这在效率和准确性方面都不是最佳的。

### C. 多尺度注意力

上述策略，即注意力解耦和基于 SMPL 的目标表示，使我们能够在 Transformer 框架中探索不同分辨率特征，这激发了多尺度注意力设计，用于高质量的 3D 人体形状和姿态估计。

#### 1) 结合多尺度特征：我们的见解是不同分辨率特征相互补充，应协同用于 3D 人体形状和姿态估计。结合这些特征的一个简单方法是将每个尺度视为标记子集，并将所有标记连接成单个特征嵌入。然后，连接的特征可以用作 (3) 中交叉注意力 $h_{\text{cross}}(Q,K)$ 的 $K$。生成的多尺度注意力可以写为：

$$
\tilde{h}_ {\text{ms}}(T, F) = h_{\text{cross}}(T, F_ 1 \| F_ 2 \| \cdots \| F_ S).
$$

然而，这种策略对不同尺度特征使用相同的投影权重，从而在同一子空间中建模目标-特征依赖关系，忽略了不同尺度的特征，灵活性较差，不能充分利用丰富的多尺度信息。

为了解决这个问题，我们引入了一种改进的多尺度策略，可以写为：

$$
h_{\text{ms}}(T, F) = \frac{1}{S} \sum_{i=1}^S h_{\text{cross}}(T, F_ i),
$$

我们为每个尺度使用不同的投影权重，输出是所有尺度的平均值。尽管概念上简单，(8) 有效地将多尺度图像特征纳入注意力计算，与仅依赖单尺度低分辨率特征的现有工作有显著区别。

#### 2) 多尺度特征位置编码：由于 (1) 中的注意力操作本身对位置不敏感，位置编码通常用于 Transformers 向输入中注入位置信息。类似于 ViT，我们对目标和特征使用可学习的位置编码，通常采用 $x + \phi$ 的形式，其中 $\phi$ 是一组可学习的参数，表示标记 $x$ 的位置信息。

特别地，图像特征的位置编码可以写为 $\phi_ i \in \mathbb{R}^{l_{F_ i} \times d}$ $(i = 1, 2, \ldots, S)$，直接加到特征 $F_ i$ 上。直接地，我们可以为所有尺度学习位置编码 $\phi_ i$，这不仅会导致过多的参数，还会忽略不同尺度之间的位置关系，例如，不同尺度上相似的空间位置应具有相似的位置嵌入。为了解决这个问题，我们调整策略，仅为最高尺度学习位置嵌入，即 $\phi_ 1$，其他尺度的嵌入通过聚合 $\phi_ 1$ 产生：

$$
\phi_ i = f_{\text{pool}}^{(2i-1)} (\phi_ 1),
$$

其中 $f_{\text{pool}}^{(2i-1)}$ 是步幅和窗口大小为 $2^{i-1}$ 的平均池化。在实际实现中，我们对 $\phi_ 1$ 迭代应用步幅为 2 的池化层：

$$
\phi_ i = f_{\text{pool}}^{(2)} (\phi_{i-1}), \quad i = 2, \ldots, S,
$$

这等价于 (9) 但需要稍少的计算。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/5421a430f7df4c96b299dadadb108714.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/07102b152d4a416ca027a01d44d01c9c.png)


### D. 关节感知注意力

除了多尺度方法，基于 SMPL 的目标表示 $T$ 还激发了关节感知注意力的设计。回想一下， $T$ 的前 $H$ 行描述了 $H$ 个身体部件的相对旋转。如图 6 所示，人体关节周围的局部关节状态强烈暗示了相邻身体部件之间的相对旋转。

因此，在注意力操作中充分关注关节周围的局部图像特征，有助于改进 $T$ 的前 $H$ 行，以更好地估计人体姿态。

为此，我们设计了一种关节感知注意力操作，通过将 $K$ 限制在人体关节的局部邻域来修改交叉注意力 $h_{\text{cross}}(Q,K)$。这种操作可以写为：

$$
h_{\text{ja}}(T_ i, F) = f_{\text{soft}} \left( \frac{(T_ i W_ q) (F_ 1^{N(J_ i)} W_ k)^\top }{\sqrt{d}} + \eta \right) (F_ 1^{N(J_ i)} W_ v),
$$

其中 $T_ i$ 是 $T$ 的第 $i$ 行， $i = 1, \ldots, H$。 $N(J_ i)$ 表示 $i$ 个关节 $J_ i$ 周围大小为 $r \times r$ 的图像块， $F_ 1^{N(J_ i)} \in \mathbb{R}^{r^2 \times d}$ 代表从 $F_ 1$ 中 $N(J_ i)$ 采样的局部特征。(10) 的形式与 (1) 类似， $T_ i$ 和 $F_ 1^{N(J_ i)}$ 分别作为 $Q$ 和 $K$。值得注意的是，我们这里只使用最高分辨率特征 $F_ 1$ 进行局部注意力，因为 $N(J_ i)$ 可以覆盖较小特征图上的大面积，使得低分辨率特征上的关节感知注意力变得类似于 (8) 中的全局注意力。类似于 Swin Transformer，我们在 softmax 函数中加入相对位置编码 $\eta \in \mathbb{R}^{1 \times r^2}$，根据 $J_ i$ 和 $N(J_ i)$ 中像素之间的距离，从可学习张量 $\tilde{\eta} \in \mathbb{R}^{(r+1) \times (r+1)}$ 进行双线性采样。局部注意力模块的计算和内存成本为 $O(Hr^2)$，相对于全局关注图像特征的普通注意力几乎可以忽略不计。

最终，我们通过简单取平均值结合了多尺度注意力 (8) 和关节感知注意力 (10)。将组合注意力表示为 $h_{\text{co}}(T, F) \in \mathbb{R}^{(H+2) \times d}$，输出的第 $i$ 行可以写为：

$$
h_{\text{co}}(T_ i, F) = \begin{cases}
\frac{1}{2} \left( h_{\text{ja}}(T_ i, F) + h_{\text{ms}}(T_ i, F) \right), & i \le H \\
h_{\text{ms}}(T_ i, F), & i > H
\end{cases}
$$

请注意， $h_{\text{ja}}(T_ i, F)$ 仅定义于 $H$ 个身体部件，即 $i = 1, \ldots, H$，因此我们直接对身体形状 $\beta$ 和相机 $C$ 使用多尺度注意力而不取平均值，这对应于 (11) 中的 $i = H + 1, H + 2$。

根据 (3) 中的注意力解耦，我们的注意力模块的最终公式可以写为

$$
h_{\text{final}}(T, F) = h_{\text{self}}(h_{\text{co}}(T, F)),
$$

如图 7 简要说明。

### E. 分层架构

如 (10) 所示，我们当前设计的一个重要问题是关节感知注意力依赖于 2D 关节 $J$，而这是我们算法的输出。换句话说，我们需要 $J$ 来重建 3D 人体，同时需要 3D 人体来回归 $J$，这本质上导致了鸡和蛋的问题。为解决这一问题，我们提出了一种分层架构，用于迭代地细化 2D 关节估计和 3D 重建结果。

我们将图 3(a) 中第 $b$ 阶段的输出表示为 $P^b = \{R_{\theta_ 1}^b, \ldots, R_{\theta_ H}^b, \beta^b, C^b\}$，其中 $R_{\theta_ i}^b \in \text{SO}(3)$ 是第 $i$ 个身体部件的旋转矩阵，对应于 $\theta_ i$（ $\theta$ 的第 $i$ 行）。然后，细化过程可以写为：

$$
T^b = f_{\text{TB}}^b (T^{b-1}, P^{b-1}, F), \quad P^b = f_{\text{fusion}} (T^b, P^{b-1}), \quad b = 1, 2, \ldots, B,
$$

其中 $f_{\text{TB}}^b$ 是第 $b$ 个 Transformer 块，用图像特征 $F$ 和当前 3D 估计 $P^{b-1}$ 改进目标嵌入 $T^{b-1}$。 $f_{\text{fusion}}$ 是一个融合层，根据改进的目标嵌入 $T^b$ 生成细化估计 $P^b$。如图 3 最右侧所示，在融合层中，我们首先使用线性层将 $T^b$ 映射为一组残差：

$$
\Delta P^b = \{ \Delta R_{\theta_ 1}^b, \ldots, \Delta R_{\theta_ H}^b, \Delta \beta^b, \Delta C^b \},
$$

然后将残差添加到当前估计 $P^{b-1}$ 中。类似于 Gram-Schmidt 正交化，我们将 Gram-Schmidt 正交化应用于旋转残差，使得 $\Delta R_{\theta_ i}^b \in \text{SO}(3)$。我们使用矩阵乘法（图 3 中的 MatMul）而不是加法来调整旋转参数。

如图 3(a) 所示，分层架构由 $B$ 个 Transformer 块组成，每个 Transformer 块由图 3(b) 中的 $U$ 个 Transformer 单元组成。Transformer 单元表示上述解耦注意力 $h_{\text{final}}$，如图 7 所示。为引导这一细化过程，我们对 CNN 特征应用全局池化层和 MLP，以获得类似于 HMR 的初始粗略估计 $P^0$。对于初始目标嵌入 $T^0$，直接选择使用类似于 DETR 的学习特征嵌入。然而，我们通过实验发现，这一选择使训练不太稳定。相反，我们采用启发式策略，结合全局池化图像特征和线性变换 $P^0$，即 $T^0 = f_{\text{global}}(F_ S) + f_{\text{linear}}(P^0)$，其中 $f_{\text{global}}$ 是全局平均池化函数。

### F. 损失函数

为了训练所提出的 Transformer，我们采用 Mesh Graphormer 的损失函数，在顶点和身体关节方面限制重建的人体：

$$
\mathcal{L}_ {\text{basic}} = w_ Y \cdot \|Y - \hat{Y}\|_ 1 + w_ J \cdot \|J - \hat{J}\|_ 2^2 + w_ J \cdot \|J - \hat{J}\|_ 2^2,
$$

其中 $Y, J, J$ 是使用 (4)-(6) 通过我们 Transformer 的最终输出，即图 3(a) 中的 $P^B$ 计算的预测顶点、3D 关节和 2D 关节。 $\hat{Y}, \hat{J}, \hat{J}$ 表示相应的地面真实值。 $w_ Y, w_ J, w_ J$ 是平衡不同项的超参数。接下来，我们对顶点使用 $L_ 1$ 损失，对人体关节使用 MSE 损失。

此外，我们包含一个旋转正则化项：

$$
\mathcal{L}_ {\text{rotation}} = w_ R \cdot \frac{1}{H} \sum_{i=1}^H \|R_{\theta_ i} - \hat{R}_ {\theta_ i}\|_ 1,
$$

我们鼓励预测旋转 $R_{\theta_ i}$ 接近地面真实旋转矩阵 $\hat{R}_ {\theta_ i}$。 $w_ R$ 是损失的权重。最终，我们的训练损失是 $\mathcal{L}_ {\text{basic}}$ 和 $\mathcal{L}_ {\text{rotation}}$ 的组合。我们不限制身体形状 $\beta$，因为通过实验发现没有收益。注意 $\mathcal{L}_ {\text{rotation}}$ 只能在我们的 Transformer 中使用，它使用基于 SMPL 的目标表示；与现有的 3D 人体重建 Transformers 不同，它们不直接考虑旋转。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/72f23de2bef743e89f8d6d32b7c29402.png)




### G. 讨论

解耦注意力：虽然视频分类中的 [89] 触及了与我们注意力解耦相似的想法，但我们的工作标志着首次在单目 3D 人体形状和姿态估计中探索它，解决了现有工作的全注意力形式的固有局限性。虽然我们以简化方式介绍了这一概念，但我们解耦注意力的创新不仅仅是一个基本概念的介绍。不同于 [89] 中仅依赖单尺度低分辨率特征的方法，我们提出了一个多尺度解耦注意力框架。这种独特设计允许模型利用粗粒和细粒视觉信息，显著提高 3D 人体形状和姿态估计的性能。值得注意的是，这种多尺度方法得益于我们注意力解耦策略的效率提升，允许结合高分辨率特征而不会导致计算成本过高。

此外，我们强调，有效结合多尺度特征并不是一件简单的事情，需要专门的算法设计和精心的工程努力。特别地，我们没有直接将所有尺度特征连接成单个特征向量，而是对每个尺度分配不同的投影权重，以尺度感知方式建模目标-特征依赖关系。此外，我们设计了基于池化的多尺度位置编码，以更好地表示不同尺度的空间信息。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/62517f96a0d54cb8924a6fa81adabb5b.png)



基于 SMPL 的目标表示：虽然 SMPL 作为输出已在先前的方法中使用，但我们的工作标志着首次在 Transformer 框架中使用基于 SMPL 的目标表示。

我们强调，基于 SMPL 的目标表示的贡献源于其独特优势。首先，如图所示，它显著降低了计算成本。其次，它确保了一致、平滑的 SMPL 网格，避免了基于顶点的表示产生的人体表面尖刺异常值。第三，它显式地建模了身体部件的旋转，促进了其在驱动虚拟化身中的应用。此外，基于 SMPL 的目标表示允许开发关节感知注意力，激发了我们 Transformer 的分层架构，这两个都是以前工作和并行工作中没有的新设计，显著提高了重建结果。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/94dc84d6d3304663abe7a5fe75d4d794.png)





## IV. 实验

在本节中，我们提供了对所提出算法的定量和定性评估。代码和模型将向公众开放。

### A. 实现细节

对于网络结构，我们默认设置 Transformer 块的数量为 $B = 3$，每个块中的 Transformer 单元数量为 $U = 2$。我们为 Transformer 使用四个注意力头，并将关节感知注意力的特征采样范围设置为 $r = 8$。它对应于输入图像中的 $32 \times 32$ 区域，足够大以涵盖关节附近。我们将特征尺度的数量设置为 $S = 4$。我们使用 HRNet 作为 CNN 主干网。我们提出了两个变体：基础模型 SMPLer 和较大的模型 SMPLer-L。这两个模型使用相同的架构，唯一的区别是 SMPLer-L 的通道维度增加了一半。我们使用批量大小 200，训练模型 160 个周期。我们在 $\mathcal{L}_ {\text{basic}}$ 和 $\mathcal{L}_ {\text{rotation}}$ 中的损失权重经验上设定为 $w_ Y = 100$， $w_ J = 1000$， $w_ J = 100$， $w_ R = 50$。

数据集：类似于，我们通过结合多个人体数据集（包括 Human3.6M、MuCo-3DHP、UP-3D、COCO、MPII）广泛训练我们的模型。类似于，我们使用了 Human3.6M 的伪 3D 网格训练数据。我们遵循通常的设置，使用受试者 S1、S5、S6、S7 和 S8 进行训练，受试者 S9 和 S11 进行测试。我们使用 P2 协议呈现所有结果。我们使用 3DPW 训练数据微调模型以进行 3DPW 实验。

度量标准：我们主要使用平均关节位置误差 (MPJPE) 和 Procrustes 对齐平均关节位置误差 (PA-MPJPE) 进行评估。MPJPE 定义为：

$$
\frac{1}{H} \sum_{i=1}^H \|J_ i - \hat{J}_ i\|_ 2,
$$

其中 $J_ i$ 是 $J$ 的第 $i$ 行，即第 $i$ 个关节的位置， $\hat{J}$ 代表地面真实值。(15) 直接测量关节到关节的误差，可能受全局因素影响，包括缩放、全局旋转和平移。PA-MPJPE 定义为：

$$
\min_{s, R, t} \frac{1}{H} \sum_{i=1}^H \|s J_ i R + t - \hat{J}_ i\|_ 2,
$$

$$
\text{s.t. } s \in \mathbb{R}, R \in \text{SO}(3), t \in \mathbb{R}^{1 \times 3},
$$

其中 $s, R, t$ 表示缩放、全局旋转和平移。与直接计算关节到关节的误差不同，(16) 首先通过缩放刚体变换将预测 $J$ 对齐到地面真实值 $\hat{J}$（对齐的闭形式解决方案由 Procrustes 分析给出）。因此，PA-MPJPE 能够排除全局因素，专注于人体形状和姿态的内在属性，强调相邻身体部件之间相对位置的测量。

### B. 与最先进方法的比较

定量评估：我们与最先进的 3D 人体形状和姿态估计方法进行比较，包括基于 CNN、基于 GNN 和基于 Transformer 的方法。如表 I 所示，提出的 Transformer 在 Human3.6M 和 3DPW 数据集上表现优于基线方法。值得注意的是，与 Mesh Graphormer 相比，提出的 SMPLer 和 SMPLer-L 将 Human3.6M 上的 MPJPE 误差分别降低了 8.2% 和 11.7%，而其参数仅为 Mesh Graphormer 的 16.5% 和 32.5%，清楚地展示了我们算法的有效性。

定性评估：为了更直观地理解结果，我们还在图 8 中提供了视觉比较。例如，在图 8 的第一行，现有方法不能很好地定位人腿，因为它们仅在 Transformer 中使用低分辨率特征，因此在自遮挡和具有挑战性的姿态下容易出错。相比之下，提出的 SMPLer 能够更好地利用丰富的图像特征，以多尺度（粗和细）和多范围（全局和局部）方式，有效提高复杂场景中的估计性能。另一方面，由于现有 Transformers 主要依赖基于顶点的目标表示，其结果并不总是位于 SMPL 网格流形上，如图 10 所示。这导致了人体表面尖刺异常值，如图 8 第二行所示，甚至导致了网格变形，如图 8 第四行所示。相比之下，我们的方法引入了基于 SMPL 的目标表示，自然保证了解决方案位于平滑的人体网格空间，从而如图 8 所示，获得更高质量的结果。

此外，我们在图 9 中展示了更多具有替代视角的 SMPLer 定性结果，提出的网络在各种姿态和复杂背景下表现出色。特别地，结果显示，我们的模型能够准确恢复相对于相机坐标系的全局旋转，这也通过 MPJPE 和 MPVE 指标的改进得到验证。

控制虚拟化身：3D 人体形状和姿态估计的一个重要应用是控制虚拟化身，例如在元宇宙中。由于我们基于 SMPL 的目标表示显式地建模了身体部件的 3D 旋转，提出的 SMPLer 可以轻松地用于驱动虚拟人类。图 11 中显示了一个示例。相比之下，之前的 Transformers 以顶点为输出，在此任务中无法直接使用，必须首先转换为旋转。更具体地说，预测的顶点需要以迭代优化方式拟合到 SMPL 模型，这相当于将结果投影到 SMPL 流形上。与提出的本质上一步解决方案相比，两步方法（顶点 → 旋转）不仅由于耗时的拟合过程导致效率问题，还因为累积误差导致低准确性。

为定量评估旋转精度，我们引入了一个新指标，称为平均每身体部件旋转误差 (MPRE)，定义为：

$$
\text{MPRE} = \frac{180}{\pi H} \sum_{i=1}^H \arccos \left( \frac{\text{trace}(R_{\theta_ i} \hat{R}_ {\theta_ i}^\top ) - 1}{2} \right),
$$

其中 $R \hat{R}^\top$ 表示预测旋转 $R$ 和地面真实值 $\hat{R}$ 之间的旋转矩阵。本质上，(17) 描述了预测与地面真实值之间的旋转角度 $\omega$（以度为单位），因为旋转矩阵的三个特征值为 1 和 $e^{\pm i \omega}$，因此 $\text{trace}(R \hat{R}^\top ) = 1 + e^{i \omega} + e^{-i \omega} = 1 + 2 \cos(\omega)$。

由于 Human3.6M 的地面真实旋转不可用，我们在 3DPW 上评估旋转精度。提出的 SMPLer 实现了 9.9 的 MPRE，显著优于 Mesh Graphormer 的 57.0，展示了提出方法在控制虚拟化身方面的优势。图 11 中的视觉比较进一步验证了改进。

![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/138298b25b88487b940314f42ee46727.png)



### C. 分析与讨论

我们在 Human3.6M 上进行了全面的消融研究，以研究我们方法的能力。我们还提供了更多关于模型效率和注意力机制的分析和讨论。以下部分中，我们使用 SMPLer 作为默认模型。

解耦注意力和基于 SMPL 的表示：我们提出了一种新的 Transformer 框架，能够利用高分辨率特征进行准确的 3D 人体重建。在这个框架的核心是解耦注意力模块和基于 SMPL 的目标表示。为了分析这些设计的有效性，我们研究了不同注意力操作（全局与解耦）和目标表示（基于顶点与基于 SMPL）的选择。

如表 II(b) 所示，使用全注意力和基于顶点的表示的简单方法导致利用多分辨率图像特征 $F$ 的内存和计算成本高昂。由于计算资源有限，我们无法以合适的批量大小训练这个模型。尽管解耦注意力可以很好地减少模型复杂性（表 II(c)），由于基于顶点的目标表示的高维性，内存占用仍然很大。相比之下，提出的 Transformer 结合了解耦注意力和基于 SMPL 的表示（表 II(d)），显著减少了内存和计算负担，从而实现了多尺度特征的更有效利用。注意，特征维度不再是我们网络的计算瓶颈，因为使用高分辨率特征只会导致边际计算开销，如表 II(d) 和 (e) 所示。

多尺度注意力的有效性：如介绍的，我们提出了一种注意力操作 $h_{\text{ms}}$，用于更好地利用多尺度图像信息。如表 III 所示，使用单一尺度特征进行 3D 人体形状和姿态估计，无论是低分辨率（仅尺度 $S$）还是高分辨率（仅尺度 1），其结果均逊色于我们的全模型。

另一方面，在 Transformer 中统一多尺度特征并非易事。如介绍的，我们没有简单地将不同分辨率特征连接成单一特征向量（ $\tilde{h}_ {\text{ms}}$），而是分别用不同的投影权重处理每个尺度，即 $h_{\text{ms}}$。如表 III 所示，由于使用相同投影权重处理不同尺度的限制，简单的连接方法不能充分利用多尺度信息，导致 3D 人体估计的准确性低于提出的方法。

此外，我们应用多尺度特征位置编码来补充注意力操作的位置信息。如表 IV 所示，模型在没有特征位置编码的情况下（无 FPE）表现出显著的性能下降，特别是在 MPJPE 上。此外，我们提出了一种基于池化的策略，如图 5 所示，以更好地处理不同尺度的空间关系，而不是直接为所有尺度学习特征位置嵌入（全尺度 FPE）。相比表 IV 中的“全尺度 FPE”，这种策略进一步改善了 3D 人体形状和姿态估计结果。

关节感知注意力的有效性：受到基于 SMPL 的目标表示的启发，我们提出了关节感知注意力 $h_{\text{ja}}$，以更好地利用关节周围的局部特征。如表 V 所示，没有 $h_{\text{ja}}$ 的模型表现出显著的性能下降，显示了该模块在推断相对身体部件旋转方面的重要性。此外，我们在 (10) 中包括了一个相对位置编码，以建模目标嵌入和图像特征之间的空间关系，如表 V 所示（无 $\eta$），这稍微提高了性能。

分层架构的分析：我们在图 3 中应用了一个分层架构，由多个 Transformer 块和多个 Transformer 单元组成，以逐步细化 3D 人体估计结果。我们在图 12 中可视化了细化过程，显示了更多块后的重建变得更加准确。

此外，我们研究了分层架构的不同设置的效果。如表 VI 所示，添加更多块和单元的性能增益在开始时显著，但在 $B > 1$ 和 $U > 1$ 后迅速收敛。因此，我们不再添加更多块，并将 $B = 3, U = 2$ 作为默认设置，以更好地平衡性能和计算成本。

注意力可视化：为了更全面地研究所提出的注意力模块，我们在图 13 中可视化了学习到的注意力图。首先，我们在图 13(e)–(h) 中展示了多尺度注意力 $h_{\text{ms}}$，其中不同尺度对应于图 4 中的交叉注意力模块。高分辨率注意力模块的注意力图（如图 13(e) 所示）更集中在特定身体部件的细粒特征上。特别地，所提出的 Transformer 依赖于脚和头部姿态，以帮助推断膝盖周围的身体部件旋转。相比之下，对于低分辨率（如图 13(h) 所示），注意力在更广泛的空间中更分散，表明多尺度注意力可以利用全局信息，包括背景，以决定身体的全局旋转和尺度。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/09601a6039234cad9e791263cbb506f7.png)




为了进一步分析不同尺度的注意力，我们还在图 14 中提供了定量结果。我们使用注意力图和均匀分布之间的 KL 散度来测量注意力分布在空间中的扩散程度。如图 14 所示，高分辨率注意力模块与均匀分布的散度更高，表明其注意力扩散较少，更集中在细粒特征上。这验证了低分辨率和高分辨率特征的互补性。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/ce6f31e58e0e4686bd32a94f0f7aa118.png)



此外，我们在图 13(d) 中提供了学习到的关节感知注意力的可视化，显示了有趣的局部模式，其中强调目标关节周围的信息，以更好地进行 3D 人体重建。具体地，它在膝盖的上下两侧具有更高的权重，捕捉了关节周围的运动状态，有助于推断大腿和小腿之间的相对旋转。


![](https://img-blog.csdnimg.cn/direct/cf99703fd2fc489da74e3c0960ebf98d.png)




最后，我们采用了一个自注意力层 $h_{\text{self}}$ 进行注意力解耦（图 7），建模图 1(c) 中的目标-目标依赖关系。我们通过展示一个查询关节与所有其他关节之间的交互来可视化这一层，其中 $h_{\text{self}}$ 专注于运动树上的相邻关节，以产生更合理的预测。

运行速度：如表 VII 所示，SMPLer 和 SMPLer-L 的推理速度分别为 96.0 和 73.9 帧每秒 (fps)，实际上是实时的。虽然利用了高维多分辨率特征，SMPLer 的运行速度几乎是基线 Transformers 的三倍，其 GFlops 仅为后者的五分之一，展示了所提出算法的效率。




![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/326f2848d429441ba14df79104b8d91d.png)




与先前工作的关系：所提出的 SMPLer 基于两个基本设计：注意力解耦和基于 SMPL 的目标表示。这些设计使得我们的算法与现有 Transformers 显著不同，后者基于全注意力和基于顶点的表示，因此只能在注意力操作中使用低分辨率特征。此外，提出的框架也明显不同于现有的基于 SMPL 的方法，例如 HMR、SPIN 和 RSC-Net，这些方法仅基于 CNNs，不能利用 Transformers 的强大学习能力进行 3D 人体形状和姿态估计。结果，它们与最先进方法相比存在很大的性能差距，如表 I 所示。

这两个基本设计自然导致了几个新模块的发展，包括多尺度注意力和关节感知注意力。虽然这些模块在概念上很简单，但要使它们在我们的框架中正常工作绝非易事，需要精心的算法设计，例如多尺度特征融合方法、基于池化的位置编码、关节感知注意力中的相对位置编码和分层架构。最终，通过上述设计，SMPLer 在性能和效率上显著超过了最先进的方法。

## V. 结论

我们开发了一种新的 Transformer 框架，用于从单个图像中进行高质量 3D 人体形状和姿态估计。在这项工作的核心是解耦注意力和基于 SMPL 的目标表示，允许高效利用高维特征。这两种策略还激发了多尺度注意力和关节感知注意力模块的设计，这些模块可以潜在地扩展到其他需要多尺度和多范围信息的领域，例如运动估计和图像恢复。同时，所提出的算法也可以应用于其他 3D 重建问题，例如通过替换 SMPL 为其他参数模型（如 SMAL 和 MANO）进行 3D 动物重建和 3D 手部重建。然而，这超出了本工作的范围，将是未来研究的一个有趣方向。

类似于现有的 Transformers，所提出的 SMPLer 仍然是一个混合结构，包含 CNN 层作为主干。在未来的工作中，将注意力基于的主干集成到 SMPLer 中，例如，值得探索。
声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
