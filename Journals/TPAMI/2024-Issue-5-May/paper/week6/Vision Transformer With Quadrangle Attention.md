# 题目：[Vision Transformer With Quadrangle Attention](https://ieeexplore.ieee.org/document/10384565/)  
## 四边形注意力的视觉Transformer
**作者：Qiming Zhang; Jing Zhang; Yufei Xu; Dacheng Tao** 

****
# 摘要
基于窗口的注意力由于其优异的性能、较低的计算复杂度和较小的内存占用，已成为视觉转换器中的热门选择。然而，手工设计的窗口是数据无关的，限制了转换器适应不同大小、形状和方向的对象的灵活性。为了解决这个问题，我们提出了一种新的四边形注意力（QA）方法，该方法将基于窗口的注意力扩展到通用的四边形公式。我们的方法采用端到端的可学习四边形回归模块，该模块预测变换矩阵，将默认窗口变换为目标四边形，以便对令牌进行采样和注意力计算，从而使网络能够模拟具有不同形状和方向的各种目标，并捕捉丰富的上下文信息。我们将QA集成到普通和分层的视觉转换器中，创建了一种名为QFormer的新架构，该架构只需要少量代码修改，几乎没有额外的计算成本。在公共基准上的广泛实验表明，QFormer在各种视觉任务（包括分类、目标检测、语义分割和姿态估计）中都优于现有的代表性视觉转换器。代码将在QFormer上公开。


# 关键词
- 目标检测
- 姿态估计
- 四边形注意力
- 语义分割
- 视觉转换器

# 引言
视觉转换器（ViT）已成为各种视觉任务中的一种有前途的方法。ViT通过将2D图像划分为补丁并将其嵌入为令牌来将输入图像视为1D序列，然后使用堆叠的包含自注意力和前馈网络的转换器块处理这些令牌。尽管其结构简单，但这种架构表现出了优越的性能。然而，原始自注意力在输入令牌长度上的二次复杂性对处理高分辨率图像构成了挑战。为了解决这个问题，提出了局部窗口注意力，它将图像划分为几个不重叠的方块（即窗口），并在每个窗口内分别执行注意力。这种设计在性能、计算复杂度和内存占用之间取得了平衡，从而显著扩展了普通和分层转换器在各种视觉任务中的应用。然而，它也对窗口的设计形式施加了约束，即方形，从而限制了转换器模拟长程依赖以及处理不同大小、形状和方向的对象的能力，这对视觉任务至关重要。

先前的研究集中在通过高级设计使基于窗口的注意力能够模拟长程依赖。一种简单的方法是，如Swin转换器中探索的那样，将窗口大小从7×7扩大到32×32，以包含更多的令牌进行计算，尽管计算成本更高。其他研究尝试手动设计各种形式的窗口，例如焦点注意力，它包含粗粒度的令牌以捕捉长程上下文，十字形窗口注意力，它使用两个十字矩形窗口从垂直和水平方向模拟长程依赖，以及Pale，它在扩展的垂直/水平方向上关注令牌以从对角方向模拟长程依赖。这些方法通过扩大注意力距离提高了图像分类性能。然而，所有这些方法都采用固定的矩形窗口进行注意力计算，尽管图像中目标的大小、形状和方向是任意的。这种数据无关的手工设计窗口可能对视觉转换器来说不是最优的。在这项研究中，我们通过将默认窗口从矩形扩展到四边形来提出一种数据驱动的解决方案，其中的最佳参数（例如形状、大小和方向）可以自动学习。这使得转换器能够学习更好的特征表示以处理各种对象，例如通过自适应四边形包含的长程令牌捕捉丰富的上下文。

具体来说，我们提出了一种名为四边形注意力（QA）的新型注意力方法，该方法旨在从数据中学习自适应的四边形配置以进行局部注意力计算。它使用默认窗口划分输入图像，并使用端到端的可学习四边形回归模块为每个窗口预测参数化的变换矩阵。变换包括平移、缩放、旋转、剪切和投影，用于将默认窗口变换为目标四边形。为了增强训练的稳定性并提供良好的可解释性，变换矩阵被公式化为几个基本变换的组合。与窗口注意力不同的是，在多头自注意力（MHSA）层中，不同头之间共享窗口定义，所提出的四边形变换是对每个头独立执行的。这种设计使得注意力层能够模拟不同的长程依赖，并促进重叠窗口之间的信息交换，而无需窗口移动或令牌置换。我们将QA集成到普通和分层的视觉转换器中，创建了一种名为QFormer的新架构，该架构只需要少量代码修改，几乎没有额外的计算成本。我们在各种视觉任务（包括分类、目标检测、语义分割和姿态估计）上进行了广泛的实验和消融研究。结果表明，QA的有效性和QFormer相对于现有代表性视觉转换器的优越性。


![](https://img-blog.csdnimg.cn/direct/1641da2d4d854894aac3bac11d8dd599.png)



总的来说，本研究的贡献有三点：

1. 我们提出了一种新型的四边形注意力方法，可以直接从数据中学习自适应的四边形配置。它打破了现有架构中固定大小窗口的限制，使得转换器能够更容易地适应各种大小、形状和方向的对象。
2. 我们在普通和分层的视觉转换器中采用了QA，创建了一种名为QFormer的新架构，该架构只需要少量代码修改且几乎没有额外的计算成本。
3. 在公共基准上的广泛实验表明，QA的有效性以及QFormer在各种视觉任务（包括图像分类、目标检测和语义分割）中优于代表性的视觉转换器。


![](https://img-blog.csdnimg.cn/direct/72501afcd34a4762a11f9da7181bcc2f.png)



## III. 方法

在本节中，我们介绍了视觉转换器的预备知识、QA的技术细节、通过在普通ViTs和分层转换器（如Swin Transformer）中采用QA实现的QFormer，以及计算复杂度分析。

### A. 预备知识

我们将首先简要回顾视觉转换器中的典型基于窗口的注意力。如图2（a）所示，给定输入特征图 $X \in \mathbb{R}^{H \times W \times C}$ 作为输入，它被划分为不重叠的窗口，即 $\{ X_i \in \mathbb{R}^{w \times w \times C} | i \in [1, \dots, \frac{H \times W}{w^2}] \}$，其中 $w$ 是预定义的窗口大小，窗口通常是正方形的。在空间维度上展平分割的令牌，并按照原始自注意力的相同投影过程将它们投影为查询、键和值令牌，即

$$
\{ Q, K, V \} = \{ Q_i, K_i, V_i \in \mathbb{R}^{w^2 \times C} | i \in [1, \dots, \frac{H \times W}{w^2}] \}
$$

其中 $Q, K, V$ 代表查询、键和值令牌。 $C$ 是通道维度。为了允许模型从不同的表示子空间中捕捉上下文信息，将令牌在通道维度上等量分块为 $N$ 个头，结果令牌为

$$
\{ Q_h, K_h, V_h \} = \{ Q_i^h, K_i^h, V_i^h \in \mathbb{R}^{w^2 \times C'} | i \in [1, \dots, \frac{H \times W}{w^2}] \}
$$

其中 $h \in [1, \dots, N]$， $C'$ 是每个头的通道维度，即 $C' = \frac{C}{N}$。在以下内容中，我们省略下标 $h$ 以简化表示。给定来自第 $i$ 个窗口的展平查询 $Q_i$，键 $K_i$ 和值 $V_i$ 令牌，窗口注意力层执行自注意力操作，即

$$
F_i = SA(Q_i, K_i, V_i)
$$

$F_i \in \mathbb{R}^{w^2 \times N \times C'}$ 是自注意力操作 $SA(\cdot)$ 后的输出特征。然后分别沿空间和通道维度连接特征以恢复特征图。请注意，每个窗口内的令牌以相同的方式处理，我们在以下内容中省略查询、键和值令牌的窗口索引符号 $i$。注意力操作计算为值的加权和，其中权重（即注意力矩阵）由查询和相应键之间的相似度通过点积和softmax函数确定，即

$$
F = (\text{softmax}(Q K^T) + r) V
$$

其中 $r$ 是相对位置嵌入，用于编码空间信息。训练过程中始终是可学习的。

与原始自注意力相比，基于窗口的注意力的一个关键优势是将计算复杂度减少到线性与输入大小有关，即每个窗口注意力的复杂度为 $O(w^4 C)$，每个图像的窗口注意力计算复杂度为 $O(w^2 H W C)$。为了促进不同窗口之间的信息交换，在Swin中，在两个相邻的转换器层之间使用了偏移窗口策略，并且在ViTDet中间隔采用了几层原始自注意力层。结果，通过顺序堆叠转换器层，模型的感受野被扩大。然而，当前基于窗口的注意力限制了每个转换器层内的令牌在手工设计的固定大小窗口内的注意力区域。这限制了模型捕捉远距离上下文并学习更好的特征以表示不同大小、方向和形状的对象的能力。因此，他们需要仔细调整窗口的大小以适应不同的任务，例如在输入分辨率变大时扩大Swin Transformers中的窗口大小。

![](https://img-blog.csdnimg.cn/direct/d7a2d6e7407b412893fdc5798f280121.png)

![](https://img-blog.csdnimg.cn/direct/550349c4fe0b4583a37c9e5a55c0e4fa.png)


### B. 四边形注意力

基本窗口生成：为了减轻使用手工设计窗口处理各种对象的困难，我们提出了QA，允许模型以数据驱动的方式动态确定每个窗口的适当位置、大小、方向和形状。QA易于实现，只需对基于窗口注意力的视觉转换器的基本结构进行少量修改，通过简单地替换注意力模块，如图3所示。由于提出的QA对每个头进行相同且独立的操作，我们以下以一个头为例。技术上，给定输入特征图，首先将其划分为几个窗口 $X$，窗口大小为预定义的 $w$，与基于窗口的注意力相同，如图2（b）所示。我们将这些窗口称为基本窗口，并分别从每个窗口的特征中获取查询、键和值令牌，即

$$
Q, K, V = \text{Linear}(X)
$$

我们直接使用查询令牌进行QA计算，即 $Q_w = Q$，同时将键和值令牌重塑为特征图，以便在QA计算中进行后续采样步骤。

四边形生成：我们将基本窗口视为参考，并通过投影变换将每个基本窗口转换为目标四边形。由于投影变换不保留平行性、长度和方向，获得的四边形在位置、大小、方向和形状方面非常灵活，使其非常适合覆盖不同大小、方向和形状的对象。我们将简要介绍投影变换的定义。它由一个八参数的变换矩阵表示：

$$
T = \begin{pmatrix}
a_1 & a_2 & b_1 \\
a_3 & a_4 & b_2 \\
c_1 & c_2 & 1 \\
\end{pmatrix}
$$

其中 $a = [a_1, a_2; a_3, a_4]$ 定义了缩放、旋转和剪切的变换， $b = [b_1; b_2]$ 定义了平移， $c = [c_1, c_2]$ 是投影向量，定义了观察者视点在深度维度变化时感知对象的变化。

如图2（b）所示，给定基本窗口中的令牌 $X_w$，QA使用四边形预测模块预测关于基本窗口的投影变换。然而，直接回归投影矩阵的八个参数并不容易。相反，我们将投影变换解耦为几个基本变换，并分别预测每个基本变换的参数。具体来说，四边形预测模块首先预测替代参数 $t \in \mathbb{R}^9$，该模块依次由平均池化层、LeakyReLU激活层和1×1卷积层组成：

$$
t = \text{Conv} \circ \text{LeakyReLU} \circ \text{AveragePool}(X_w)
$$

然后，根据输出 $t$ 获得基本变换，包括缩放 $T_s$、剪切 $T_h$、旋转 $T_r$、平移 $T_t$ 和投影 $T_p$：

$$
T_s = \begin{pmatrix}
t_1 + 1 & 0 & 0 \\
0 & t_2 + 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}, 
T_h = \begin{pmatrix}
1 & t_3 & 0 \\
t_4 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
$$

$$
T_r = \begin{pmatrix}
\cos t_5 & -\sin t_5 & 0 \\
\sin t_5 & \cos t_5 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}, 
T_t = \begin{pmatrix}
1 & 0 & \beta_1 t_6 \\
0 & 1 & \beta_2 t_7 \\
0 & 0 & 1 \\
\end{pmatrix}, 
T_p = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
t_8 & t_9 & 1 \\
\end{pmatrix}
$$

其中 $\beta_1 = \frac{W}{w}$ 和 $\beta_2 = \frac{H}{w}$ 缩放关于图像大小的平移，以帮助模型适应不同的输入大小。最后，通过顺序乘以所有变换得到变换矩阵 $T$：

$$
T = T_s \times T_h \times T_r \times T_t \times T_p
$$

给定估计的投影矩阵，通过标准投影过程获取投影点的位置，即给定点的坐标 $(x_1, y_1)$，变换可以通过简单的乘法完成：

$$
[x_\text{tmp}, y_\text{tmp}, z_\text{tmp}]^T = T \times [x_1, y_1, 1]^T
$$

最终坐标为：

$$
(x_2, y_2) = (x_\text{tmp} / z_\text{tmp}, y_\text{tmp} / z_\text{tmp})
$$

并行对基本窗口中的每个点进行计算，以获取投影四边形中的目标位置。需要注意的是，该公式还包括传统窗口注意力作为QA的一种特殊情况，其中 $t = 0$，因此 $T$ 是单位矩阵。默认情况下，我们初始化四边形预测模块的权重以产生 $t = 0$。

然而，仅使用相同坐标系下的令牌坐标在生成四边形时会导致歧义，如图5所示。例如，给定两个位置不同的窗口，离原点较远的窗口（b）与靠近原点的窗口（a）相比，即使它们具有相同的投影变换矩阵（即旋转），也会有显著不同的平移。这将导致训练过程中四边形回归模块优化困难。为了解决这个问题，投影变换通过使用每个窗口的相对坐标而不是绝对坐标进行，如图4所示。具体来说，给定窗口内令牌的坐标 $\{ (x_i, y_i) | i = 0, \dots, M \}$，其中 $i$ 索引令牌， $M = w \times w$，我们将其坐标转换为相对坐标：

$$
(x_i^r, y_i^r) = (x_i - x_c, y_i - y_c)
$$

其中 $x_c, y_c$ 表示窗口中心的中心，如图2（b）所示， $(x_i^r, y_i^r)$ 是相对于中心的相对坐标。获得变换矩阵后，我们可以根据（12）和（13）从 $(x_i^r, y_i^r)$ 获得目标四边形中每个令牌的相对坐标 $(x_q^r, y_q^r)$。然后，变换后的绝对坐标如下：

$$
(x_q, y_q) = (x_q^r + x_c, y_q^r + y_c)
$$

获得每个目标四边形中令牌的坐标后，我们使用网格采样函数从 $K, V$ 中采样键和值令牌 $K_w, V_w$。由于QA中窗口配置的数据驱动学习机制，可能会有许多多样且重叠的四边形，从而促进跨窗口的信息交换。然而，这样的设计可能会生成覆盖特征图以外区域的四边形。为了解决这个问题，提出了一种简单的采样策略：（1）对于特征图内的采样坐标，采用双线性插值进行采样；（2）对于特征图外的坐标，使用零向量作为其采样值。最后，使用采样的 $K_w, V_w$ 和原始 $Q_w$ 进行自注意力计算，如（4）所述。

### C. 正则化

如上所述，学习的四边形可能覆盖特征图以外的区域，在这种情况下采样到的值为零。这种现象阻碍了四边形回归模块的学习，因为这些区域的梯度总是为零。为了解决这个问题，我们设计了一种正则化项，以鼓励投影四边形覆盖更多特征图内的有效区域。具体来说，给定四边形内的令牌坐标 $(x_q, y_q)$，我们定义一个惩罚函数，以惩罚那些特征图外的坐标：

$$
R(x) = \begin{cases}
-\lambda, & x < -1 \\
0, & -1 \leq x \leq 1 \\
\lambda, & x > 1 \\
\end{cases}
$$

所有令牌坐标的正则化损失为：

$$
L_\text{reg} = \sum_i R(x_q, y_q) \cdot x_q + R(y_q) \cdot y_q
$$

其中 $\lambda$ 是超参数。我们将正则化损失与常见训练损失相加，例如图像分类的交叉熵损失。


![](https://img-blog.csdnimg.cn/direct/2b522dfed2f04100bfb839cec7d6918f.png)


### D. 模型规格

我们将QA集成到普通和分层的视觉转换器中，创建了一种名为QFormer的新架构，其模型规格如表I所示。两种架构都由一系列转换器层组成，如图6所示。分层架构QFormerh在几个阶段中逐步下采样特征图，即分别按4×、2×、2×、2×的比例下采样，如图6（b）所示。我们使用Swin Transformer作为参考架构，并在注意力层之前采用CPVT中的条件位置嵌入（CPE）将空间信息编码到模型中，如图3（c）所示，即

$$
X = Z^{l-1} + \text{CPE}(Z^{l-1})
$$

每个阶段中的转换器层数和通道维度与Swin Transformer相同，但我们为了简化移除了窗口移动操作。对于大模型，我们使用普通的ViT作为参考架构，并采用MAE预训练权重进行初始化。如图6（a）所示，普通架构QFormerp在所有转换器层中具有相同的特征大小，遵循ViT中的设置。

### E. 计算复杂度分析

QA带来的额外计算来自CPE和四边形预测模块，而其他部分，包括基于窗口的MHSA和FFN，与参考架构完全相同。给定输入特征 $X \in \mathbb{R}^{H \times W \times C}$，分层架构中的QA首先使用一个7×7内核的深度卷积层生成CPE，这带来了额外的 $O(49 \cdot HWC)$ 计算。为了学习投影矩阵，我们首先使用内核大小和步幅等于窗口大小的平均池化层，从基本窗口中聚合特征，其计算复杂度为 $O(HWC)$。随后的激活函数不引入额外计算，最后一个内核大小为1×1的卷积层以 $X_\text{pool} \in \mathbb{R}^{\frac{H}{w} \times \frac{W}{w} \times C}$ 作为输入，并为每个头预测变换参数 $\{ t_i | i = 1, \dots, 9 \}$。因此，其计算复杂度为 $O(\frac{H W}{w^2} \cdot 9NC)$。在获得变换矩阵后，我们将默认窗口变换为均匀采样 $w \times w$ 令牌的四边形。这里我们忽略计算新坐标的计算复杂度，因为它非常小（即27HW）。每个四边形中采样的计算复杂度为 $w^2 \times 4 \times C$，总采样计算复杂度为 $O(4 \cdot HWC)$。因此，QA带来的总额外计算为 $O{(54 + \frac{4 N}{w^2})HWC}$，远低于（≤ 5%）参考架构的总计算。请注意，FFN的计算复杂度为 $O(2\alpha HWC^2)$，其中 $\alpha$ 是扩展比， $C$ 对于普通和分层的视觉转换器始终大于96。当模型大小随着较大的令牌维度（ $C$）增加时，QA带来的额外FLOPs可以忽略不计，即QFormerp-B为0.1%， $C = 768$。



## IV. 实验

### A. 实验设置

1. **数据集**：为了全面评估QA的有效性，我们在各种视觉任务上进行了实验，包括分类、语义分割、实例分割、检测和姿态估计，使用了ImageNet-1k、ADE20k和MS COCO等知名公共数据集。
![](https://img-blog.csdnimg.cn/direct/985e6e7d4f214a0eb41172231242b7aa.png)



2. **评估指标**：对于分类任务，我们报告广泛使用的Top-1和Top-5准确率作为评估指标，而对于目标检测、实例分割和人体姿态估计任务，我们采用广泛使用的平均精度（mAP）作为主要评估指标。mAP值通过在0.5到0.95之间每隔0.05计算的阈值平均计算得到。对于姿态估计，我们使用对象关键点相似性（OKS）来确定阈值。对于语义分割任务，我们使用交并比（IoU）作为评估指标。

3. **实现细节**：在本研究中，我们在普通和分层的视觉转换器中采用QA。我们采用与ViT和Swin Transformer相同的模型规格用于QFormerp和QFormerh。如果没有特别说明，图像分类的输入图像分辨率设置为224×224。对于分层架构，我们遵循Swin Transformer的超参数设置，以监督方式训练QFormerh。对于普通架构，我们采用MAE预训练策略，为QFormerp提供良好的初始化，这已被证明在缓解普通ViT的过拟合方面非常有效。对于不同的下游任务，我们使用ImageNet-1k预训练权重进行初始化。值得注意的是，我们仅在QFormerp的微调阶段引入了所提出的QA，以便重复使用普通ViTs的预训练权重并降低训练成本，尽管在预训练中引入四边形注意力可能会带来更好的性能。

### B. 普通模型

1. **图像分类**。 

**设置**：对于普通模型，我们采用了[31]中提出的微调实践，包括在ImageNet-1k数据集上预训练模型1,600个周期，然后进行监督微调。我们对骨干网络进行了几处修改，包括在每个注意力层引入相对位置编码以结合位置信息，使用所有视觉令牌的平均池化而不是类别令牌进行最终预测，并采用窗口大小为7、补丁大小为16的窗口注意力，遵循[3]中的常见做法。所提出的正则化项的超参数设置为1，所有其他参数与[31]中的设置相同。我们使用AdamW优化器，权重衰减为0.05，层次学习率衰减为0.75。模型训练了100个周期，前5个周期用于预热。

![](https://img-blog.csdnimg.cn/direct/f12aa0c84dee455fb523327c62fce8b8.png)



**消融研究**：为了评估QA中灵活窗口设计的有效性，我们使用QFormerp-B进行了消融研究。我们逐渐在QA中加入基本变换，包括缩放和平移、剪切、旋转和投影变换。结果如表II所示，其中✓表示使用了相应的变换。第一行表示默认设置，即使用固定大小的窗口。我们观察到，随着使用更多变换，模型的表示能力不断提高，Top-1准确率从81.2增加到82.9（+1.7）。这种改进归因于生成的灵活四边形能够捕捉更丰富的上下文并更好地处理不同大小、方向和形状的对象。此外，这种灵活的窗口配置使网络能够更好地捕捉长程依赖，从而释放自注意力的潜力。值得注意的是，在表II中， $T_s \times T_t$（第二行）是QA的特定情况，即VSA [27]。由于更灵活的配置，QA可以学习更多样的窗口进行注意力计算，从而促进更好的特征表示学习。因此，QA通过0.6的Top-1准确率提高了VSA的结果，即从82.3提高到82.9。此外，QA和VSA共享相同的公式、相似的计算流程和相似的复杂度，如表IV所示。只需很少的额外资源（小于0.01%）即可将VSA升级到更强的QA。

解决窗口注意力中有限注意力距离问题的另一种方法是将其与原始全注意力结合使用，这允许每个令牌关注所有其他令牌，从而实现长程依赖建模[3]。ViTDet在网络中交替使用窗口注意力和全注意力层，以增强模型的能力。我们在这种设置中研究了QA的有效性。具体来说，我们在网络中均匀分布不同数量的全注意力层，并评估在其他转换器层中使用固定大小窗口注意力或四边形注意力的性能。实验使用了12层的QFormerp-B骨干网络。结果如表III所示，第三列表示全注意力层的数量。令人惊讶的是，即使使用全注意力，QA在所有设置中持续优于窗口注意力。例如，在没有全注意力的情况下，QA带来了最显著的性能提升，即从81.2提高到82.9的Top-1准确率。这是由于QA更好地建模长程依赖并促进跨窗口信息交换的能力。这也解释了全注意力对QA带来的较少收益，相比于窗口注意力。值得注意的是，使用六个全注意力层和窗口注意力的模型（即最后一组中的第一行）仅获得了与仅使用QA的模型（即第一组中的第二行）相当的性能。由于QA的计算量远少于全注意力，结果验证了所提出的QA的优越性。

为了进一步验证QA的有效性，我们将其与窗口注意力以及其偏移窗口版本进行了比较，使用了ViT-B/16作为骨干网络。表IV中的结果显示，所提出的QA在Top-1准确率方面达到了最佳性能，为82.9，甚至比偏移窗口注意力高出0.9，表明QA中的灵活四边形不仅促进了跨窗口信息交换，还帮助提取丰富的上下文并学习更好的特征表示。此外，所提出的分解公式，如（11）所示，相比于直接预测变换矩阵提高了0.2的性能。这突显了基本变换组合的有效性。

根据表IV，QA引入的额外计算成本可以忽略不计，即与原始窗口注意力和偏移窗口注意力相比不到0.06%（0.01 GFlops）。尽管成本极低，QA带来了显著的性能提升。为了全面评估各种注意力方法，我们进一步分析了推理成本，包括吞吐量和内存利用率。例如，跨窗口信息交换在偏移窗口框架内的使用将基线性能从81.2提升到82.0。然而，这种提升以13.5%的推理速度降低为代价。相比之下，我们的QA模型不仅通过显著提升0.9的准确率超越了偏移窗口注意力，还仅产生了微小的7%的额外推理成本。在训练过程中，QA表现出略微降低的速度和更大的内存占用。这是由于QA中的几个独立操作优化不足导致的，例如令牌坐标的计算、四边形内令牌采样过程和注意力中的矩阵乘法。通过CUDA加速技术将这些操作合并为一个统一的操作，可以实现更快的速度和更低的内存使用，这是我们未来的工作。

我们对所提出的正则化项中的超参数 $\lambda$ 进行了分析，结果如表V所示， $\lambda$ 范围从0到10。所有结果均使用QFormerp-B获得。当 $\lambda = 0$ 时，训练过程中不应用正则化项。在这种情况下，四边形覆盖了太多无效区域（即特征图外的区域），这些区域的梯度总是为零，导致模型发散。通过引入所提出的正则化项以惩罚不合理的注意力区域，我们稳定了训练过程。我们对 $\lambda$ 的值进行了扫频，最佳性能在0.1至1的范围内。

![](https://img-blog.csdnimg.cn/direct/f7dcde0aad42445dac09dd5c0322bbf8.png)


![](https://img-blog.csdnimg.cn/direct/b5e2bf39695f48f5a6ecc63fccdaa48a.png)


2. **目标检测和实例分割**。

**设置**：目标检测模型按照ViTDet的设置进行构建和训练。具体来说，从骨干网络的最后一层提取特征，并通过下采样和上采样构建特征金字塔。为了更好地促进长程依赖建模，在骨干网络中均匀放置了四个全注意力层，而其他注意力层采用了所提出的QA。在表VI中的‘完全微调’中，使用官方的MAE预训练权重对骨干网络进行初始化，并在MS COCO数据集上对整个模型进行100个周期的微调。训练过程中，drop path rate设置为0.2，采用ViTDet的超参数，例如AdamW优化器，初始学习率为0.0001，权重衰减为0.1，层次学习率衰减为0.7。输入图像分辨率设置为1024×1024。评估并报告边界框目标检测（mAPbb）和实例分割（mAPmk）的性能。对于‘部分微调’，我们以ViTDet为初始化，只微调模型的不同组件，即变换预测模块T和FFN层。

**结果**：表VI展示了MS COCO验证集上的结果。QFormerp-B和ViTDet-B的区别仅在于使用了所提出的QA或窗口注意力。QFormerp-B模型在保持模型大小和计算效率的情况下，比基线ViTDet-B模型的mAPbb和mAPmk分别提高了0.7。这一显著改进表明QA在帮助检测器学习更好的对象表示方面的有效性。此外，QFormerp-B在APs和APl方面表现出色，分别提高了0.6 APbb s、0.9 APbb l和1.6 APmk l。这一结果表明，QA由于其灵活的窗口配置设计，能够处理不同尺度的对象。从表中还可以看出，仅调优QA中的变换预测模块T（即具有0.67 M参数），比ViTDet-B基线模型的mAPbb和mAPmk分别提高了0.4。值得注意的是，它在mAPbb和mAPmk方面甚至与完全训练的ViT-B-VSA模型相当。这表明我们提出的四边形注意力可以通过即插即用的方式有效提升预训练模型的性能，同时只涉及0.6%的额外训练参数。当同时调优变换T和FFN模块时，结果达到了52.2 mAPbb和46.5 mAPmk，分别超过了ViTDet-B 0.6 mAPbb和0.6 mAPmk，几乎匹敌完全训练的QFormerp-B的性能，同时具有更少的可训练参数。

3. **语义分割**。

**设置**：我们在ADE20k数据集上评估我们的QFormer用于语义分割任务。与目标检测任务中仅使用最后一层提取特征不同，语义分割任务中从ViT-B骨干网络的每1/4层提取特征。通过简单的反卷积和下采样层构建特征金字塔。使用MAE预训练权重初始化骨干网络，并对整个模型进行160 k次迭代微调。采用UPerNet作为分割头，遵循常见做法，采用MMSegmentation中的默认训练设置。例如，使用AdamW优化器，初始学习率为6e-5。采用多项式调度器调整学习率，权重衰减和层次学习率衰减分别设置为0.01和0.9。使用两种不同输入大小的训练设置，即512×512和640×640，比较模型性能。所有模型使用8个A100 GPU，总批量大小为16进行训练。结果以mIoU和准确率报告，包括单尺度和多尺度测试设置。

**结果**：表VII展示了结果。前两行表示在ViT-B中使用固定窗口注意力或偏移窗口注意力的结果。最后一行报告了在ViT-B中使用所提出的QA的结果，即QFormerp-B。可以观察到，QA相比其他两种变体表现出更优越的性能，特别是在这个具有多样对象的密集预测任务中。具体来说，QFormerp-B在512×512图像中的mIoU为43.6，在640×640图像中的mIoU为44.9，分别比偏移窗口注意力高出2.0和2.6 mIoU。这种改进是由于QA的灵活设计能够学习自适应窗口配置，以提取丰富的上下文并获得更好的特征表示。此外，当比较使用不同输入分辨率训练的模型时，可以观察到，默认或偏移窗口注意力的模型从更大图像中获得的好处较少或甚至没有。默认窗口注意力在这种设置下表现较差，因为它缺乏建模全局上下文的能力。偏移窗口注意力有助于促进跨窗口信息交换并略微提高性能。相比之下，QFormerp-B能够适应不同大小的对象，并有效利用更大图像中的上下文信息，导致更好的分割性能。例如，QFormerp-B在将输入分辨率从512增加到640时，分别获得了1.4 mIoU和1.1 mIoU*的提升。

4. **人体姿态估计**。

**设置**：我们遵循ViTPose中的做法评估QA在人体姿态估计任务中的有效性。采用自上而下的流水线，以人体实例图像作为输入，根据SimpleBaseline的检测结果提取。使用Udp进行后处理。我们使用官方的MAE预训练模型初始化ViT-B骨干网络，并采用MMPose中的默认训练设置，即输入图像大小为256×192，学习率为5e-4。模型由AdamW优化器优化，训练210个周期。学习率在第170和200个周期分别降低10倍，层次衰减设置为0.75。通过调整补丁嵌入层中的卷积步幅，使用较大的特征尺寸，将下采样比例从1/16改为1/8。报告在MS COCO数据集上不同注意力方法（包括原始全注意力‘Full’、固定窗口注意力‘Window’、偏移窗口注意力‘Shift’和所提出的QA）的性能。

**结果**：如表VIII所示，仅使用窗口注意力的模型（第一行）由于其提取全局上下文信息的能力较差，表现较差。偏移窗口注意力和我们的QA都促进了跨窗口信息交换，并显示出更好的性能，其中QA在大多数指标上取得了更大的提升。结果表明，学习到的各种形状的四边形可以很好地适应多样的人体姿势并学习更好的特征表示。此外，比较第3行和第4行时，我们发现仅使用QA已经超过了同时使用全注意力和窗口注意力的模型，但GPU内存占用和FLOPs更少。当同时使用全注意力和QA时，性能达到了77.2 AP（倒数第二行），与全注意力设置相当，同时需要的内存占用和FLOPs更少。

### C. 分层模型

1. **图像分类**。

**设置**：在训练过程中，我们使用AdamW优化器和余弦学习率调度器。训练过程包括在ImageNet-1k训练集上进行300个周期的训练，并在前20个周期使用线性预热。初始学习率设置为0.001，批量大小为1024。我们应用了各种数据增强，包括随机裁剪、自动增强、CutMix、MixUp和随机擦除。此外，使用权重为0.1的标签平滑。对于输入尺寸大于224×224的训练，我们对预训练的输入尺寸为224×224的模型进行微调，使用相同的训练设置，但没有预热阶段。

**结果**：我们在ImageNet-1k验证集上评估了不同模型。如表IX所示，所提出的QFormerh-T比其对应的Swin-T（使用偏移窗口注意力）提高了1.3%的Top-1准确率，即从81.2%提高到82.5%。这一结果表明，通过学习适当的窗口配置，QA可以更有效地捕捉有用的上下文信息，从而关注默认窗口之外的远处但相关的令牌。此外，QFormerh-T在MSG-T和Focal-T上取得了优越的结果，后者利用了额外的信使令牌进行跨窗口信息交换，表明我们的QA可以在没有手工设计的情况下实现充分的信息交换，使其简单且高效。当增加模型大小时，我们的QFormerh-S比Swin-S提高了1.0%的准确率，甚至超过了更大的Swin-B和Focal-B，分别提高了0.6%和0.2%的准确率，同时参数和FLOPs显著更少。当进一步扩大QFormer的规模时，性能提升并未减少，例如QFormerh-B达到了更好的84.1%准确率，尽管与较小版本QFormerh-S相比，增益较小。我们怀疑大模型的性能可能受到较小输入尺寸的限制。

为了研究图像尺寸的影响，我们比较了在不同设置下Swin和我们的QFormerh的性能，如图1（c）所示。由于GPU内存的限制，我们只对Swin-T和我们的QFormerh-T进行了实验。当将输入图像尺寸从224×224增加到320×320和384×384时，我们观察到所提出的QA在所有设置下持续帮助提供更好的分类性能，显著优于（偏移）窗口注意力。值得注意的是，QFormerh-T在224×224图像尺寸下达到了82.5%的准确率，甚至比384×384的Swin-T高出0.5。这些结果表明，将注意力区域限制在固定大小的正方形窗口内的窗口注意力在处理不同大小的对象时效果不佳，而所提出的四边形注意力非常灵活，可以直接从数据中学习自适应窗口。


![](https://img-blog.csdnimg.cn/direct/3d60fb784e5e49d7ba48f6aeb5f95046.png)


2. **目标检测和实例分割**。

**设置**：我们在MS COCO数据集上评估了分层模型中的QA用于目标检测和实例分割任务。我们使用在ImageNet上预训练的骨干网络，输入尺寸为224×224，并采用两个流行的目标检测框架，即Mask RCNN和Cascade RCNN。我们按照mmdetection中的常见做法对所有模型进行训练和评估。具体来说，我们使用AdamW优化器，批量大小为16进行多尺度训练。初始学习率和权重衰减分别设置为0.0001和0.05。我们在1×（12个周期）和3×（36个周期）时间表下训练模型。

**结果**：表X和XII分别展示了QFormerh-T与Mask RCNN和Cascade RCNN的结果。更多较大骨干网络的结果，例如QFormerh-S和QFormerh-B，分别展示在表XI和XIII中。如表X所示，QFormerh-T与Mask RCNN检测器中的QA相比基线方法Swin-T在目标检测和实例分割任务中表现出显著优越的性能。具体来说，与1×训练时间表相比，QFormerh-T的性能分别提高了2.2 mAPbb和1.7 mAPmk。这些结果证实了所提出的QA在处理不同尺度对象方面的有效性，这在MS COCO这样的目标检测数据集中非常常见。此外，QFormerh-T的性能随着训练时间表的延长（3×）而提高，分别比Swin-T提高了1.5 mAPbb和1.1 mAPmk，这表明所提出的可学习QA通过更好地适应具有多样对象的数据，从更长的训练时间表中持续受益。此外，QA相对于其他高级窗口注意力方法（例如多窗口方法，即DW-T）表现出优越的性能，在3×训练时间表下获得了0.8 mAPbb的增益。这是因为QA可以学习任意大小和形状的四边形以适应不同的对象，而DW-T中使用的多个窗口仍然是手工设计的，需要为每个数据集和任务仔细调整。QA相对于焦点注意力（即Focal-T）的优越性能也很明显，在1×训练时间表下获得了1.1 mAPbb的性能提升。值得注意的是，堆叠更多的转换器层并不能减轻手工设计的窗口注意力的固有缺点，我们的QA仍然保持了优势。例如，QFormerh-S在所有其他骨干网络中表现最佳，即分别达到了49.5 mAPbb和44.2 mAPmk，如表XI所示。

在Cascade RCNN检测器中，不同骨干网络的检测结果如表XII所示。在1×时间表下，QFormerh-T与Swin-T相比具有显著的1.7 mAPbb和1.3 mAPmk的提升。此外，随着训练周期的增加，所提出的QA分别带来了1.2 mAPbb和1.0 mAPmk的性能提升。对于较大的骨干网络，如QFormerh-S和QFormerh-B，我们进行了更详细的比较，关于不同尺度对象的检测性能，如表XIII所示。结果表明，QFormerh-S和QFormerh-B在检测精度方面比Swin Transformer表现出显著更好的性能，最小增益为0.9 mAPbb和0.7 mAPmk。这些结果表明，QA在增强具有更强表示能力的大模型方面具有潜力。此外，结果还表明，在不同尺度对象的检测精度方面，QFormerh始终优于Swin变体。例如，通过采用所提出的QA，Swin-B在小、中、大尺寸对象的性能，即APbb s、APbb m、APbb l，分别提高了0.9 AP、1.3 AP和1.1 AP。这样的观察进一步验证了QA在处理不同大小对象方面的有效性。

3. **语义分割**。

**设置**：在本节中，我们使用ADE20k数据集评估不同骨干网络在语义分割任务中的性能。具体来说，我们采用UPerNet作为分割框架，遵循Swin Transformer。所有模型的训练和评估遵循常见做法。我们使用具有多项式学习率调度器的Adam优化器训练模型160 k步。初始学习率初始化为6e-5，权重衰减设置为1e-2。所有实验均在8个NVIDIA A100 GPU上进行，总批量大小为16。我们提供单尺度和多尺度测试结果。

**结果**：我们在表XIV中报告了QFormerh和其他模型的结果。可以看出，我们的QFormerh模型在所有手工设计窗口的骨干网络中表现最佳。例如，QFormerh-T分别比Swin-T提高了2.4 mIoU、2.7 mAcc和2.3 mIoU*。尽管Focal-T和DW-T通过关注更多令牌允许提取长程上下文和跨窗口信息交换，但其性能仍然落后于所提出的QFormerh-T，这表明我们的QA的可学习设计优于手工设计的窗口。此外，QA的好处延伸到扩大骨干网络。例如，QFormerh-S分别达到了48.9 mIoU和50.3 mIoU*，优于其他骨干网络。值得注意的是，QFormerh-S甚至优于更大的Swin-B和DW-B模型，表明我们的QA可以显著提高视觉转换器的表示能力。

4. **VSA和QA之间的比较**：在表XV中，我们在利用分层视觉转换器的各种视觉任务中进行了VSA和QA之间的比较。具体来说，我们采用Mask RCNN进行目标检测，UPerNet进行语义分割。由于更灵活的公式和更强的表示能力，QA在所有视觉任务中具有一致且明显的性能提升。值得注意的是，与Swin-T-VSA相比，它在ADE20k上表现出了显著的1.2 mIoU提升，与Swin-S-VSA相比提高了0.7 mIoU，突显了QA方法的显著效用。此外，由于QA和VSA的相似公式，通过QA增强的能力类似于‘免费的午餐’，只需最少的额外成本。例如，QA在推理期间仅引入了2.5%的额外速度和内存成本，如表IV所示，同时在ImageNet-1k的图像分类中提供了显著的0.6%的准确率提升，表明QA的优越公式和有效性。


![](https://img-blog.csdnimg.cn/direct/aaf95d14b46d47eeb93f3408aa568bfd.png)

![](https://img-blog.csdnimg.cn/direct/77a781ae6df94eada411fe412e675c62.png)


![](https://img-blog.csdnimg.cn/direct/4e8dbf3a7d4241698fe595c9ca33fc00.png)


![](https://img-blog.csdnimg.cn/direct/0e412dd852674ef388f2e48ee06c9dc8.png)
![](https://img-blog.csdnimg.cn/direct/e2f9ff3b1cac457387f584b596c87c72.png)


### D. 推理速度

我们评估了普通和分层架构的推理速度，结果如表XVI所示。除了所提出的QFormer，基线模型即Swin和ViT也进行了评估。对于语义分割和目标检测任务，我们采用UPerNet和Cascade Mask RCNN作为任务头。输入图像大小在最后一列中提供。我们运行每个模型30次并记录平均速度。所有实验均在NVIDIA A100 GPU上进行。

如表中所示，QFormerh比Swin Transformer在各种视觉任务上的速度略慢约13%，但性能显著更好。对于普通模型，QFormerp将速度差距减少到比ViT带窗口注意力少于9%，而获得超过4 mIoU的增益，显示了QA在实现速度和准确性更好帕累托前沿的巨大潜力。我们模型略微慢的速度归因于在PyTorch框架中采样操作与矩阵乘法操作相比优化不足，后者通过cuBLAS进行了更好的优化。通过使用CUDA加速技术将采样操作与后续注意力和线性投影操作整合，可以实现更快的速度。

### E. 可视化和分析

**学习到的四边形的可视化**：为了检查QA如何关注各种图像，我们可视化了Swin-T中使用的默认窗口和QA生成的四边形，如图7和8所示。所使用的图像来自ImageNet和MS COCO数据集。图7中的结果来自在ImageNet上训练的用于图像分类的QFormerh-T，不同注意力头中的四边形显示在不同列中。用于获得图8中的结果的模型在MS COCO上训练。

图7中的结果表明，与仅能覆盖目标有限部分的默认固定大小窗口相比，QA生成的四边形可以覆盖目标对象图像中的更多多样区域。例如，默认窗口将感受野限制在狗和马头部区域。相比之下，通过学习适当的注意力区域，QA扩展了窗口大小并捕捉到了动物头部的更全面的上下文。此外，不同头生成的四边形在位置、大小和形状上有所不同，以关注目标的不同部分，促进了捕捉丰富的上下文信息和改进目标特征表示。此外，如图8所示的多样四边形促进了跨窗口信息交换，使得可以删除伴随窗口注意力的偏移窗口操作。

我们在图9中展示了不同层的四边形。四边形大小的变化源于分层模型中不同层的步幅变化。不同层模型中的四边形具有不同的形状和尺度，以建模长程依赖，捕捉丰富的上下文，并促进重叠窗口之间的信息交换，从而使模型能够学习到判别性的特征表示。此外，四边形大小遵循本文图9中的结论，在第6层达到最小值，在第4层和第9层之间波动。

![](https://img-blog.csdnimg.cn/direct/fbfacefd30434814bb3d9a4bc5014c89.png)



**QA中学习到的变换分析**：为了探索不同层的注意力区域变化，我们对投影变换 $T$ 中学习到的参数进行了分析。为此，我们在ImageNet验证集上使用了三个不同深度和大小的QFormerh模型进行了实验。这些实验的结果如图11所示，表明变换参数在不同层中表现出不同的模式。例如，缩放变换中的相对参数，即 $t_1$ 和 $t_2$，在较低层中趋于增加，而在较高层中趋于减少。这表明较低层通过扩展注意力区域来关注提取低级特征，而较高层则更关注从相对较小的注意力区域聚合高级信息。此外，上层中的投影变换表现出较小的变化，低层中的变化较小，如参数 $t_8$ 和 $t_9$ 所示。这一观察与我们的理解一致，即网络的上层更专注于提取任务特定的高级特征，而下层则负责从更广泛的区域捕捉一般低级特征。

![](https://img-blog.csdnimg.cn/direct/0d646a9caea045b4bc25d64c94292a0c.png)

## V. 讨论

除了窗口注意力层之外，令牌化层也基于窗口处理特征图，例如使用卷积在局部窗口中合并特征。因此，QA背后的概念也有可能为这些令牌化层带来优势。从技术角度来看，可以采用类似于QA的方法，在卷积计算过程中为每个局部窗口回归一个四边形。这将使得能够直接从这些四边形中合并令牌。一方面，这种不规则令牌化方法可能会学习到更合适的策略来合并令牌特征，从而有潜力以一种补充QA优势的方式提升模型性能。另一方面，由于令牌化层总是需要将图像嵌入为令牌，有两个因素需要仔细处理。首先，浅层中的局部结构特征在允许视觉转换器提取语义特征方面起着至关重要的作用。引入不规则令牌化方法可能会破坏这些局部结构，可能会抵消通过增强学习能力获得的优势。其次，涉及密集预测的任务通常需要位置感知输出，例如对象位置或每像素分类。然而，不规则令牌化方法可能会导致空间混叠问题。尽管在实现这一想法上存在障碍，但我们认为这是一个有前途的方向，值得进一步研究。

## VI. 结论

在本文中，我们提出了一种四边形注意力（QA），这是一种将基于窗口的注意力扩展到通用四边形公式的新型注意力方法。QA学习变换参数以自适应调整窗口配置，并在不同大小、形状和方向的四边形中采样令牌特征。这使得视觉转换器能够更好地处理多样的对象并捕捉丰富的上下文。广泛的实验表明，QA在提高普通和分层视觉转换器在各种视觉任务中的性能方面非常有效。我们提出的QFormer在计算成本极低的情况下，实现了优于现有最先进视觉转换器的优异结果。未来的工作包括通过CUDA加速优化QA的计算效率，并进一步探索其在其他领域的潜力。

声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。






















