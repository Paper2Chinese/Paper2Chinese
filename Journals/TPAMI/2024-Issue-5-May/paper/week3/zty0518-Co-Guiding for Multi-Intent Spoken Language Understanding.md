# 题目：[Co-Guiding for Multi-Intent Spoken Language Understanding](https://ieeexplore.ieee.org/document/10333988)  
## 多意图口语理解的协同引导
**作者：Bowen Xing; Ivor W. Tsang** 


****

# 摘要

最近基于图的多意图口语理解（SLU）模型通过模拟从意图预测到槽填充解码的指导获得了有希望的结果。然而，现有方法（1）只模拟了从意图到槽的单向指导，而意图和槽之间存在双向相互关系；（2）采用同构图来模拟槽语义节点和意图标签节点之间的交互，这限制了性能。在本文中，我们提出了一个名为Co-guiding Net的新颖模型，它实现了一个两阶段框架，实现了两个任务之间的相互指导。在第一阶段，生成了两个任务的初始估计标签，然后在第二阶段利用这些估计标签来模拟相互指导。具体来说，我们提出了两个异构图注意力网络，它们在提出的两个异构图语义-标签图上工作，有效地表示了语义节点和标签节点之间的关系。此外，我们还进一步提出了Co-guiding-SCL Net，它利用单任务和双任务语义对比关系。对于第一阶段，我们提出了单任务监督对比学习；对于第二阶段，我们提出了共同指导监督对比学习，它在对比学习过程中考虑了两个任务的相互指导。实验结果表明，我们的模型在多意图SLU上大幅超越了现有模型，在MixATIS数据集的总体准确率上比之前最好的模型提高了21.3%。我们还在零样本跨语言场景中评估了我们的模型，结果表明，我们的模型可以在9种语言的总体准确率上平均提高现有最先进模型的33.5%。

# 关键词

- 对话系统
- 图神经网络
- 多任务学习
- 口语理解

# I. 引言

口语语言理解（SLU）[1]是对话系统中的一个基础任务。其目标是捕获用户话语的全面语义，它通常包括两个子任务：意图检测和槽填充[2]。意图检测的目标是预测用户话语的意图，而槽填充的目标是提取话语中表达的额外信息或约束。

最近，研究人员发现这两个任务是紧密相关的，并且提出了一些模型[3][4][5][6][7]，这些模型采用多任务框架结合单一意图检测和槽填充，以利用它们之间的相关性。然而，在现实世界场景中，用户通常在单个话语中表达多个意图。为此，[8]开始解决多意图检测任务，[9]首次尝试在多任务框架中联合建模多意图检测和槽填充。[10]提出了一个AGIF模型，通过图注意力网络（GAT）[11]自适应地将细粒度的多意图预测信息集成到槽填充的自回归解码过程中。[12]进一步提出了一个基于非自回归GAT的模型，增强了预测的多个意图和槽隐藏状态之间的交互，取得了最先进的结果和显著的加速。

尽管现有的多意图SLU联合模型取得了有希望的进展，但我们发现它们存在两个主要问题：

(1)忽略了从槽到意图的指导：由于先前的研究人员意识到“槽标签可能依赖于意图”[9]，现有模型利用预测的意图信息来指导槽填充，如图1(a)所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b48c2bf511a64ac9b9a2c7c6245dd963.png#pic_ center" width="70%" />
</div>

然而，它们忽略了槽标签也可以指导多意图检测任务。基于我们的观察，多意图检测和槽填充是双向相互关联的，并且可以相互指导。例如，在图2中，不仅意图可以指示槽，槽也可以推断意图。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ed5ee7cb4c07404fa8d1518a651c72fd.png#pic_ center" width="70%" />
</div>

然而，在以前的工作中，联合模型只能从多个意图检测任务中获得来自槽填充任务的基础语义的单向指导。因此，缺乏从槽到意图的指导限制了多个意图检测，以及联合任务。

(2)语义-标签图中的节点和边的歧义：[10][12]应用GATs在构建的图上模拟槽语义节点和意图标签节点之间的交互。然而，他们的图是同构的，在这种图中所有节点和边被视为相同类型。对于一个槽语义节点来说，来自意图标签节点和其他槽语义节点的信息扮演着不同的角色，而同构图无法区分它们具体的贡献，导致歧义。因此，应该设计异构图来表示语义节点和标签节点之间的关系，以促进更好的交互。

本文提出了一个名为Co-guiding Net的新颖模型，以解决上述两个问题。对于第一个问题，Co-guiding Net实现了一个两阶段框架，如图1(b)所示。第一阶段生成两个任务的初始估计标签，第二阶段利用估计标签作为先验标签信息，允许两个任务相互指导。对于第二个问题，我们提出了两个异构图语义-标签图（HSLGs）：(1) 一个从槽到意图的语义-标签图（S2I-SLG），有效地表示了意图语义节点和槽标签节点之间的关系；(2) 一个从意图到槽的语义-标签图（I2S-SLG），有效地表示了槽语义节点和意图标签节点之间的关系。此外，我们提出了两个异构图注意力网络（HGATs），分别在两个提出的图上工作，用于模拟从槽到意图和从意图到槽的指导。

为了进一步利用两个任务实例之间的微妙语义差异，即以前研究中忽略的语义对比关系，我们提出了Co-guiding-SCL Net，它基于Co-guiding Net并引入了监督对比学习，以拉近具有相同/相似标签的语义，并将具有不同标签的语义推开。在第一阶段，由于两个任务是单独执行的，我们为多个意图检测和槽填充分别提出了两个特定的单任务监督对比学习机制。由于多个意图检测是一个多标签分类任务，实例之间的关系不是简单的正/负样本。为了处理多意图实例之间的细粒度相关性，我们提出了多意图监督对比学习，可以动态地为每个实例分配细粒度权重，考虑到其意图与锚点意图的相似性。对于槽填充，我们采用了传统的单标签多类监督对比学习。在第二阶段，由于两个任务之间实现了相互指导，存在双任务语义对比关系。如图3所示，话语A和话语D与话语B表达了类似的意图（它们共享意图标签atis_ ground_ fare）。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b158b079763d4748943dc50f801201f1.png#pic_ center" width="70%" />
</div>

然而，我们可以观察到话语A和话语B在句子层面上的语义更相似，而话语D的语义则更为不同。话语A和B提到了一些城市的交通，而话语D包含了关于从哪里到哪里，方式和日期的信息。这可以从话语D与话语A和B具有相当不同的槽标签这一事实上反映出来。同样，尽管话语A中的‘san’，话语B中的‘dallas’和话语C中的‘washington’对应相同的槽标签B-city_ name，‘san’和‘dallas’应该具有更相似的语义，而‘washington’应该具有更不同的语义。原因是‘san’和‘dallas’在上下文语义上比‘washington’的上下文语义更相似，这可以从话语A，话语B和话语C的意图标签中反映出来。因此，意图和槽标签在基于句子层面和单词层面的语义上微妙地影响彼此的任务，这种微妙的指示信息可以作为监督信号，通过利用上述双任务对比关系，通过双任务相互指导来受益。基于此，我们提出了共同指导监督对比学习，以在对比学习过程中整合双任务相关性。在训练过程中，一个任务的表示之间的距离会根据另一个任务的对比标签的指导进行调整。

本文的最初版本[13]在2022年的EMNLP会议上以口头报告的形式发表。它的贡献可以概括为三个方面：

1)我们提出了Co-guiding Net，实现了一个两阶段框架，允许多个意图检测和槽填充相互指导。据我们所知，这是第一次尝试在两个任务之间实现相互指导。

2)我们提出了两个异构图语义-标签图作为适当的平台，用于语义节点和标签节点之间的双任务交互，并且我们提出了两个异构图注意力网络来模拟两个任务之间的相互指导。

3)在两个公共多意图SLU数据集上的实验结果表明，我们的Co-guiding Net显著优于以前的模型，并且模型分析进一步验证了我们模型的优势。

在本文中，我们在以下方面显著扩展了我们之前的工作：

1)我们提出了Co-guiding-SCL Net，它通过监督对比学习机制增强了Co-guiding Net，以进一步捕获样本中的单任务和双任务语义对比关系。

2)对于第一阶段，我们为两个任务提出了单任务监督对比学习机制。对于多个意图检测，我们提出了一种新颖的多意图监督对比学习机制，以捕获多意图实例之间的动态和细粒度的相关性。

3)对于第二阶段，我们提出了共同指导监督对比学习，它可以通过联合考虑两个任务的标签作为监督信号，执行每个任务的监督对比学习，从而捕获经过微调的双任务语义对比相关性。

4)我们在公共多意图SLU数据集上进行了广泛的实验。除了LSTM之外，我们还评估了我们的模型在几种预训练语言模型（PTLM）编码器上的性能。实验结果表明，我们的模型可以在各个阶段显著且一致地超越现有最先进模型。而且，模型分析进一步验证了我们提出的双任务监督对比学习机制的优势。

5)我们还评估了我们的模型在零样本跨语言多意图SLU任务上的性能，这是以前从未探索过的。实验结果表明，我们的模型可以显著提高现有最佳性能模型在9种语言的平均总体准确率上的表现。

本文的其余部分组织如下。在第二节中，我们总结了口语语言理解、自然语言处理中的图神经网络和自然语言处理中的对比学习的相关工作，并强调了我们的方法与先前研究之间的差异。第三节详细阐述了Co-guiding Net的细节。第四节描述了Co-guiding-SCL Net中提出的监督对比学习机制。实验结果在第五节中报告和分析。注意，零样本跨语言多意图检测和槽填充的任务定义以及此任务的实验在第五节-I中介绍。最后，在第六节中提供了这项工作的结论和一些前瞻性的未来方向。

# III. CO-GUIDING

Problem Definition：给定一个输入话语表示为 $U = \{u_ i\}^n_ {i=1}$ ，多意图检测可以被表述为一个多标签分类任务，输出与输入话语相对应的多个意图标签。而槽填充是一个序列标注任务，将每个 $u_ i$ 映射到一个槽标签。

接下来，在深入Co-guiding Net架构的细节之前，我们首先介绍两个异构图的构建。

## A. Graph Construction

1)Slot-to-Intent Semantics-Label Graph: 为了提供一个适当的平台来模拟从估计的槽标签到多意图检测的指导，我们设计了一个从槽到意图的语义-标签图（S2I-SLG），它表示多意图检测的语义和估计的槽标签之间的关系。S2I-SLG是一个异构图，一个例子如图4(a)所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d86cf2da3de44c57807ba2c8316243ae.png#pic_ center" width="70%" />
</div>

它包含两种类型的节点：意图语义节点（例如， $I_ 1,...,I_ 5$）和槽标签（SL）节点（例如， $SL_ 1,...,SL_ 5$）。图4(b)显示了四种边的类型。每种边类型对应于图中的一种特定类型的信息聚合。

数学上，S2I-SLG可以表示为 $G_ {s2i} = (V_ {s2i}, E_ {s2i}, A_ {s2i}, R_ {s2i})$ ，其中 $V_ {s2i}$ 是所有节点的集合， $E_ {s2i}$ 是所有边的集合， $A_ {s2i}$ 是两种节点类型的集合， $R_ {s2i}$ 是四种边类型的集合。每个节点 $v_ {s2i}$ 和每条边 $e_ {s2i}$ 都与它们的类型映射函数 $\tau(v_ {s2i}) : V_ {s2i} \rightarrow A_ {s2i}$ 和 $\phi(e_ {s2i}) : E_ {s2i} \rightarrow R_ {s2i}$ 相关联。例如，在图4中，节点 $SL_ 2$ 属于 $V_ {s2i}$ ，而它的节点类型 SL 属于 $A_ {s2i}$ ；从 $SL_ 2$ 到 $I_ 3$ 的边属于 $E_ {s2i}$ ，而它的边类型 slot_ to_ intent_ guidance 属于 $R_ {s2i}$ 。此外，S2I-SLG中的边基于局部连接。例如，节点 $I_ i$ 连接到 $\{I_ {i-w}, ..., I_ {i+w}\}$ 和 $\{SL_ {i-w}, ..., SL_ {i+w}\}$ ，其中 w 是局部窗口大小的超参数。

2)Intent-to-Slot Semantics-Label Graph: 为了提供一个平台，以适应从估计的意图标签到槽填充的指导，我们设计了一个从意图到槽的语义-标签图（I2S-SLG），它表示槽语义节点和意图标签节点之间的关系。I2S-SLG也是一个异构图，一个例子如图5(a)所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d0aba27cc4a44e898dbb0feb92875c10.png#pic_ center" width="70%" />
</div>

它包含两种类型的节点：槽语义节点（例如， $S_ 1,...,S_ 5$ ）和意图标签（IL）节点（例如， $IL_ 1,...,IL_ 5$）。图5(b)显示了四种边的类型。每种边类型对应于图中的一种特定类型的信息聚合。

数学上，I2S-SLG可以表示为 $G_ {i2s} = (V_ {i2s}, E_ {i2s}, A_ {i2s}, R_ {i2s})$ 。每个节点 $v_ {i2s}$ 和每条边 $e_ {i2s}$ 都与它们的类型映射函数 $\tau(v_ {i2s})$ 和 $\phi(e_ {i2s})$ 相关联。I2S-SLG中的连接与S2I-SLG略有不同。由于意图是句子级别的，每个IL节点与所有节点全局连接。对于Si节点，它连接到 $\{S_ {i-w}, ..., S_ {i+w}\}$ 和 $\{IL_ 1, ..., IL_ m\}$ ，其中w是局部窗口大小，m是估计意图的数量。

## B. Model Architecture

在本节中，我们介绍Co-guiding Net的详细信息，其架构如图6所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5db3da2bbc2a45e499c67e83ee5a8711.png#pic_ center" width="70%" />
</div>

1)Shared Self-Attentive Encoder: 遵循[10]、[12]，我们采用共享的自注意力编码器来产生包含基础语义的初始隐藏状态。它包括一个双向长短期记忆网络（BiLSTM）和一个自注意力模块。BiLSTM捕获时间依赖性：

$$ 
h_ i = \text{BiLSTM}(x_ i, h_ {i-1}, h_ {i+1}) 
$$

其中 $x_ i$ 是 $u_ i$ 的词向量。现在我们获得了上下文敏感的隐藏状态 $\hat{H} = \{\hat{h}_ i\}^n_ {i=1}$ 。

自注意力捕获全局依赖性：

$$ 
H' = \text{softmax}\left(\frac{QK^T}{\sqrt{d_ k}}V\right) 
$$

其中 $H'$ 是自注意力输出的全局上下文隐藏状态；Q、K和V是通过在线性投影上应用不同的变换得到的矩阵。

然后我们连接BiLSTM和自注意力的输出，形成共享自注意力编码器的输出： $H = \hat{H} \| H'$ ，其中 H = $\{h_ i\}^n_ {i=1}$ ， $\|$ 表示连接操作。

2)Initial Estimation. Multiple Intent Detection: 为了获得多意图检测的任务特定特征，我们在 H 上应用一个 BiLSTM 层：

$$ 
h^{[I,0]}_ i = \text{BiLSTM}_ I(h_ i, h^{[I,0]}_ {i-1}, h^{[I,0]}_ {i+1}) 
$$

按照[10]、[12]，我们进行词级多意图检测。每个 $h^{[I,0]}_ i$ 被送入意图解码器。具体来说，第 i 个词的意图标签分布通过以下方式获得：

$$ 
y^{[I,0]}_ i = \sigma(W^1_ I \cdot \sigma(W^2_ I h^{[I,0]}_ i + b^2_ I) + b^1_ I) 
$$

其中 $\sigma$ 表示非线性激活函数； $W^\*$ 和 $b^\*$ 是模型参数。

然后通过词级意图投票[12]获得估计的句子级意图标签 $\{IL_ 1, ..., IL_ m\}$ 。

Slot Filling: [12]提出了一个非自回归范式用于槽填充解码，这实现了显著的速度提升。在本文中，我们也进行并行槽填充解码。

我们首先在 H 上应用一个 BiLSTM 来获得槽填充的任务特定特征：

$$ 
h^{[S,0]}_ i = \text{BiLSTM}_ S(h_ i, h^{[S,0]}_ {i-1}, h^{[S,0]}_ {i+1}) 
$$

然后使用 softmax 分类器为每个词生成槽标签分布：

$$
y^{[S,0]}_ i = \text{softmax}(W^1_ S \cdot \sigma(W^2_ S h^{[S,0]}_ i + b^2_ S) + b^1_ S)
$$

并且通过在 $y^{[S,0]}_ i$ 上应用 arg max 获得每个词的估计槽标签。

3)Heterogeneous Graph Attention Network：现有的最先进模型[10]、[12]使用同构图连接槽填充的语义节点和意图标签节点，并采用图注意力网络（GAT）[11]来实现信息聚合。在第一节中，我们提出这种方式无法有效地学习一个任务的语义和另一个任务的估计标签之间的交互。为了解决这个问题，我们提出了两个异构图（S2I-SLG 和 I2S-SLG）来有效地表示语义节点和标签节点之间的关系。为了在所提出的图上模拟语义和标签之间的交互，我们提出了异构图注意力网络（HGAT）。当在节点上聚合信息时，HGAT 能够区分来自不同类型节点沿着不同关系传递过来的具体信息。我们分别为 S2I-SLG 和 I2S-SLG 应用了两个异构图注意力网络（HGATs），即 S2I-HGAT 和 I2S-HGAT。具体来说，S2I-HGAT 的公式可以表示如下：

$$
h_ {i}^{l+1} = \sum_ {k=1}^{K} \sigma\left(\sum_ {j \in N_ {s2i}^i} W_ {s2i}^{[r,k,1]} \alpha_ {[r,k]}^{nij} h_ j^l \right), \quad r = \phi(e_ {ji}^{s2i})
$$

$$ 
\alpha_ {[r,k]}^{nij} = \frac{\exp\left(W_ {s2i}^{[r,k,2]} h_ i^l \cdot W_ {s2i}^{[r,k,3]} h_ j^l \right)}{\sqrt{d} \sum_ {u \in N_ {r,i}^{s2i}} \exp\left(W_ {s2i}^{[r,k,2]} h_ i^l \cdot W_ {s2i}^{[r,k,3]} h_ u^l \right)}
$$

其中 K 表示头的总数； $N_ {s2i}^i$ 表示在 S2I-SLG 中第 i 个节点的传入邻居节点集合； $W_ {s2i}^{[r,k,\*]}$ 是第 k 个头上的边类型 r 的权重矩阵； $e_ {ji}^{s2i}$ 表示在 S2I-SLG 中从节点 j 到节点 i 的边； $N_ {r,i}^{s2i}$ 表示在 S2I-SLG 中与节点 i 通过 r 类型边连接的节点集合；d 是节点隐藏状态的维度。I2S-HGAT 的公式可以类似地推导出来。

4)Intent Decoding With Slot Guidance：在第一阶段，我们获得了初始意图特征  $H^{[I,0]} = \{h^{[I,0]}_ i\}^n_ {i=1}$  和初始估计的槽标签序列  $\{SL_ 1, ..., SL_ n\}$ 。现在我们将槽标签投影到向量形式，使用槽标签嵌入矩阵，获得  $E_ {sl} = \{e_ {1sl}, ..., e_ {nsl}\}$ 。

然后我们将  $H^{[I,0]}$  和  $E_ {sl}$  输入到 S2I-HGAT 中以模拟它们的交互，允许估计的槽标签信息指导意图解码：

$$
H^{[I,L]} = S2I\text{-HGAT}\left([H^{[I,0]}, E_ {sl}], G_ {s2i}, \theta_ I\right)
$$

其中  $[H^{[I,0]}, E_ {sl}]$  表示输入节点表示； $\theta_ I$  表示 S2I-HGAT 的参数。L 表示总层数。

最后，将  $H^{[I,L]}$  送入意图解码器，为话语词产生意图标签分布： $Y^{[I,1]} = \{y^{[I,1]}_ i, ..., y^{[I,1]}_ n\}$ 。通过在  $Y^{[I,1]}$  上应用词级意图投票，获得最终输出的句子级意图。

5)Slot Decoding With Intent Guidance：由于槽标签的B-I-O标签具有时间依赖性，我们使用意图感知的双向长短期记忆网络（Intent-aware BiLSTM）来模拟带有估计意图指导的槽隐藏状态之间的时间依赖性：

$$ 
\tilde{h}^{[S,0]}_ i = \text{BiLSTM}\left(y^{[I,0]}_ i \oplus h^{[S,0]}_ i, \tilde{h}^{[S,0]}_ {i-1}, \tilde{h}^{[S,0]}_ {i+1}\right)
$$

其中  $y^{[I,0]}_ i$  是第一阶段估计的意图标签向量， $h^{[S,0]}_ i$  是槽填充的初始隐藏状态， $\tilde{h}^{[S,0]}_ i$  是意图感知的隐藏状态，而  $\oplus$  表示特征的拼接操作。

接下来，我们将意图感知的隐藏状态  $\tilde{H}_ S$  和意图标签嵌入  $E_ {il}$  输入到 I2S-HGAT 中以模拟它们的交互，允许估计的意图标签信息指导槽解码：

$$ 
H^{[S,L]} = I2S\text{-HGAT}\left([\tilde{H}_ S, E_ {il}], G_ {i2s}, \theta_ S\right) 
$$

其中  $[\tilde{H}_ S, E_ {il}]$  表示输入节点表示， $G_ {i2s}$  是意图到槽的语义-标签图（I2S-SLG）， $\theta_ S$  表示 I2S-HGAT 的参数。

最后，将  $H^{[S,L]}$  送入槽解码器，为每个词生成槽标签分布  $Y^{[S,1]} = \{y^{[S,1]}_ i, ..., y^{[S,1]}_ n\}$ 。最终输出的槽标签通过在  $Y^{[S,1]}$  上应用  $\arg\max$  函数获得。

## C. Training Objective

1)Loss Function。多意图检测的损失函数定义为交叉熵损失（Cross-Entropy Loss）：

$$ 
LI = -\sum_ {t=0}^{n_ {\text{intents}}} \sum_ {i=1}^{N_ {\text{utterances}}} \sum_ {j=1}^{n_ {\text{labels}}} \hat{y}_ {i[j]}^{\text{Intent}} \log(y_ {i[j]}^{[I,t]}) 
$$

槽填充的损失函数同样定义为交叉熵损失：

$$ 
LS = -\sum_ {t=0}^{n_ {\text{slots}}} \sum_ {i=1}^{N_ {\text{utterances}}} \sum_ {j=1}^{n_ {\text{labels}}} \hat{s}_ {i[j]} \log(y_ {i[j]}^{[S,t]}) 
$$

其中， $n_ {\text{intents}}$  和  $n_ {\text{slots}}$  分别代表意图标签和槽标签的总数； $\hat{y}_ {i[j]}^{\text{Intent}}$  和  $\hat{s}_ {i[j]}$  是对应的真实意图标签和槽标签。

2)Margin Penalty。为了确保模型在第二阶段的性能优于第一阶段，我们设计了一个边际惩罚项（Margin Penalty）来增强正确标签的概率。具体地，多意图检测和槽填充的边际惩罚分别定义为：

$$ 
L_ {mp}^{\text{Intent}} = \sum_ {i=1}^{N_ {\text{intents}}} \max(0, y_ {i[j]}^{[I,0]} - y_ {i[j]}^{[I,1]}) 
$$

$$ 
L_ {mp}^{\text{Slot}} = \sum_ {i=1}^{N_ {\text{slots}}} \max(0, y_ {i[j]}^{[S,0]} - y_ {i[j]}^{[S,1]}) $$

3)Model Training。模型的总训练目标  $L$  是两个任务的损失函数和边际正则化的加权和：

$$ 
L = \gamma (LI + \beta_ I L_ {mp}^{\text{Intent}}) + (1 - \gamma) (LS + \beta_ S L_ {mp}^{\text{Slot}}) 
$$

其中， $\gamma$  是平衡两个任务的权重系数； $\beta_ I$  和  $\beta_ S$  分别是针对意图检测和槽填充任务的边际正则化系数。

# IV. CO-GUIDING-SCL NET

现有的方法存在三个问题：1) 忽略了从槽到意图的指导；2) 语义-标签图中的节点和边歧义；3) 忽略了两个任务实例之间的微妙语义差异。第III节中描述的Co-guiding Net旨在解决前两个问题。在本节中，我们专注于解决第三个问题。如第一节所述，单任务和双任务语义对比关系可以促进单任务推理和双任务相互指导。基于Co-guiding Net，我们提出了Co-guidingSCL Net，它通过我们提出的单任务监督对比学习和共同指导监督对比学习进行了增强，这些内容在图7中进行了说明。本节中使用的符号描述可以在表I中找到。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/81d0872d99c94021b6047f4954d46631.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b0973acbd43e4634b96b01d343862884.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c5f93cd5ba5b4eceabb74622d6f1c305.png#pic_ center" width="70%" />
</div>

由于我们的模型执行监督对比学习，受[36]的启发，我们维护了一系列样本队列，这些队列不仅存储了之前编码的特征，还存储了它们的标签：

$$
Q^0_ u, Q^0_ s, Q^1_ u, Q^1_ s, Q^I_ l, Q^S_ l, \text{和} Q^{ss}_ l
$$

队列的描述可以在表I中找到。注意，句子级槽标签用于在提出的槽引导意图监督对比学习中提供句子级槽指导（第IV-B1节）。由于数据集中没有提供，我们自己构建了它，详细信息可以在第IV-B1节中找到。在当前批次之后，我们使用当前批次的特征和标签更新样本队列，同时出队最旧的样本。

接下来，我们将详细描述我们提出的监督对比学习机制。

## A. Single-Task Supervised Contrastive Learning

在第一阶段，执行初始估计以预测为另一个任务提供指导的初始标签。初始估计仅基于当前任务的语义。如第一节所述，每个任务的表示中存在固有的语义对比关系。直观地说，对应于相同/相似意图或相同槽的语义表示应在表示空间中彼此靠近。对比地，对应于相同/相似意图或相同槽的语义表示应在表示空间中彼此靠近。为了实现这一点，我们分别为多个意图检测和槽填充提出了两种监督对比学习机制。

1)Multi-Intent Supervised Contrastive Learning。这个对比学习机制的目的是将具有相同/相似意图标签的话语表示拉近，同时推开具有不同意图标签的表示。锚点是  $h^{[I,0]}_ u$  ，对比实例来自  $Q^0_ u$  。与常规的单标签多类监督对比学习不同，我们提出的多意图监督对比学习可以处理多意图实例之间的细粒度和动态关系。具体来说，这个对比学习机制可以表述为：

$$
L_ {\text{MI-SCL}} = -\sum_ {k} \mu_ k \log \frac{e^{s(h^{[I,0]}_ u, h^{[k,0]}_ {uq})/\tau}}{\sum_ {j} e^{s(h^{[I,0]}_ u, h^{[j,0]}_ {uq})/\tau}}
$$

$$
\mu_ k = (l^{I} \odot l^{k,I}) / \sum_ {j} (l^{I} \odot l^{j,I})
$$

其中  $\odot$  表示哈达玛积（Hadamard product）， $s(a, b) = \frac{a^T \cdot b}{\|a\| \cdot \|b\| \cdot \tau}$  是余弦相似度函数， $\tau$  表示对比学习温度参数。 $l^{I} \odot l^{k,I}$  表示锚点和第 k 个多意图实例之间的金色相似度。较大的  $l^{I} \odot l^{k,I}$  表示第 k 个实例与锚点非常相似，导致损失函数中较大的  $\mu_ k$  被分配用来拉近它们。相反，如果它们具有完全不同的标签， $l^{I} \odot l^{k,I} = 0$  然后  $\mu_ k = 0$  。在这种情况下， $s(h^{[I,0]}_ u, h^{[j,0]}_ {uq})$  ，表示它们之间的距离，只出现在分母中。因此，锚点和第 k 个多意图实例将通过负梯度被推开。

2)Slot Supervised Contrastive Learning。由于每个词对应于只有一个槽标签，这种对比学习机制是常规的单标签多类监督对比学习。它的目的是将对应于相同槽的词表示拉近，同时推开对应于不同槽的词表示。锚点是  $h^{[S,0]}_ i$  ，对比实例来自  $Q^0_ s$  。具体来说，这种对比学习机制可以表述为：

$$
L_ {\text{S-SCL}} = -\sum_ {i} \sum_ {j} \sum_ {k} \frac{l_ {i}^S \odot l_ {[k,j]}^S}{E_ {i}^S} \log \frac{e^{s(h^{[S,0]}_ i, h^{[k,j]}_ {sq,0})}}{E_ {i}^S}
$$

$$
E_ {i}^S = \sum_ {j} \sum_ {k} \frac{l_ {i}^S \odot l_ {[k,j]}^S}{M_ {i}}
$$

其中  $l_ {i}^S \odot l_ {[k,j]}^S$  等于 1 或 0，表示第 k 个实例在  $Q^0_ s$  中的第 j 个词表示是当前话语中第 i 个词表示的正样本或负样本。

## B. Co-Guiding Supervised Contrastive Learning

在第二阶段，两个任务之间实现了相互指导。第二阶段的表示（例如， $h^{[I,1]}_ u, h^{[S,1]}_ i$）包含两种信息：(1) 表示自身任务语义的信息，可以指示自身任务的标签；(2) 另一个任务的初始标签信息，提供双任务指导。在两个任务的语义表示中，存在双任务语义对比关系，这在第一节中已经陈述。因此，我们提出了共同指导监督对比学习，以将双任务相关性整合到对比学习过程中，它联合考虑两个任务的标签作为监督信号来执行每个任务的监督对比学习。接下来，我们介绍槽引导多意图监督对比学习的细节。

1)Slot-Guided Multi-Intent Supervised Contrastive Learning。多意图检测是一个句子级别的分类任务。尽管槽填充是词级别的，但一个话语中所有槽标签的汇总可以提供句子级别的槽语义。对于具有相同意图的话语，它们中的一些可能具有不同的句子级别槽语义，这可以用来区分这些话语的表示。对于具有不同意图的话语，它们中的一些可能具有相似的句子级别槽语义，这可以用来调整它们表示之间的距离，从而更好地学习。槽引导多意图监督对比学习被提出以实现上述两个方面。

首先，我们必须自己构建句子级别的槽标签，因为数据集中没有提供。当前话语的句子级别槽标签向量是通过以下方式获得的：

$$ 
l^{ss} = \sum_ {i, l_ i^S = O} l_ i^S
$$

$l^{ss}$ 中每个维度的值范围从 0 到 1，可以被视为反映对应槽对句子级别槽语义程度的分数。然后我们通过连接 $l^I$ 和加权 $l^{ss}$ 来构建联合任务标签：

$$ 
l^J = \text{concat}(l^I, \lambda_ I \cdot l^{ss}) 
$$

其中 $\lambda_ I$ 是一个超参数。

然后，槽引导多意图监督对比学习可以表述为：

$$ 
L_ {\text{SGMI-SCL}} = -\sum_ {k} \mu_ k \log \frac{e^{s(h^{[I,1]}_ u, h^{[k,1]}_ {uq})}}{\sum_ {j} e^{s(h^{[I,1]}_ u, h^{[j,1]}_ {uq})}} 
$$

$$ 
\mu_ k = (l^J \odot l^{J}_ k) / \sum_ {j} (l^J \odot l^{J}_ j) 
$$

注意 $l^J \odot l^{J}_ k = l^I \odot l^{I}_ k + \lambda_ I \cdot \lambda_ I \cdot l^{ss} \odot l^{ss}_ k$ 。通过这种方式， $\lambda_ I$ 可以控制槽标签在槽引导多意图监督对比学习中的整合程度。

2)Intent-Guided Slot Supervised Contrastive Learning。通常，话语中表达的意图的语义包含在每个词的表示中。对于来自不同话语的一些词表示，它们对应的话语可能具有不同的意图。即使它们对应于相同的槽，根据它们包含的不同意图语义，它们的语义也有些不同。还有一些其他词的话语可能具有相同/相似的意图。即使它们对应于不同的槽，根据它们包含的相同/相似的意图语义，它们的语义可能并不完全不同。上述两个方面可以用来进一步区分对应于相同槽的词表示，并调整对应于不同槽的表示之间的距离。为此，我们提出了意图引导槽监督对比学习。

首先，我们构建联合任务标签 $l^J_ i = \text{concat}(l^S_ i, \lambda_ S \cdot l^I)$ ，其中 $\lambda_ S$ 是一个超参数。然后意图引导槽监督对比学习可以表述为：

$$ 
L_ {\text{IGSL-SCL}} = -\sum_ {i} \sum_ {j} \sum_ {k} \frac{l^J_ i \odot l^J_ {k,j}}{E^S_ i} \log \frac{e^{s(h^{[S,1]}_ i, h^{[k,j]}_ {sq,1})}}{E^S_ i} 
$$

$$ 
E^S_ i = \sum_ {j} \sum_ {k} l^J_ i \odot l^J_ {k,j} 
$$

注意 $l^J_ i \odot l^J_ {k,j} = l^S_ i \odot l^{S}_ {k,j} + \lambda_ S \cdot \lambda_ S \cdot l^I \odot l^I_ k$ 。通过这种方式， $\lambda_ S$ 可以控制意图标签在意图引导槽监督对比学习中的整合程度。


## C. Training Objective

Co-guiding-SCL Net 的最终损失是 Co-guiding Net 的损失和所有对比损失项的总和：

$$ 
L = \gamma (L_ I + \beta_ I L_ {mp}^I) + (1 - \gamma) (L_ S + \beta_ S L_ {mp}^S) + \eta_ I \left( L_ {MI-SCL} + L_ {SGMI-SCL} \right) + \eta_ S \left( L_ {S-SCL} + L_ {IGSL-SCL} \right) 
$$

其中  $\eta_ I$  和  $\eta_ S$  是平衡对比损失项的超参数。注意，所有的对比学习机制仅参与训练过程。Co-guiding Net 和 Co-guiding-SCL Net 具有相同的推理过程。

# V. EXPERIMENTS

## A. Datasets and Metrics

遵循先前的工作，我们采用 MixATIS 和 MixSNIPS [10]、[37]、[38] 作为测试平台。MixATIS 包括 13,162 条话语用于训练，756 条用于验证，828 条用于测试。MixSNIPS 包括 39,776 条话语用于训练，2,198 条用于验证，2,199 条用于测试。

至于评估指标，遵循先前的工作，我们采用意图检测的准确率（Acc）、槽填充的 F1 分数，以及句子级语义框架解析的整体准确率（Acc）。整体准确率表示其意图和槽都被正确预测的句子的比例。

## B. Implementation Details

我们构建了基于 LSTM 编码器和预训练语言模型（PTLM）编码器（例如 BERT [41]、RoBERTa [42]、XLNet [43]）的几组模型。

LSTM: 按照先前的工作，单词和标签嵌入从头开始训练。在 MixATIS 上，单词嵌入、标签嵌入和隐藏状态的维度为 256，而在 MixSNIPS 上分别为 256、128 和 256。所有 GNNs 的层数为 2。使用 Adam [44] 以 1e-3 的学习率和 1e-6 的权重衰减训练我们的模型。至于系数（14），在 MixATIS 上 γ 是 0.9，在 MixSNIPS 上是 0.8；在两个数据集上，βI 是 1e-6 而 βS 是 1e0。上述超参数设置适用于 Co-guiding Net 和 Co-guiding-SCL Net。Co-guiding-SCL Net 中对比学习机制的超参数设置如下：τ 是 0.07。ηI 和 ηS 分别是 0.1 和 0.01。

PTLM: 基于 PTLM 编码器的模型用 PTLM 编码器替换自注意力编码器。我们采用每个 PTLM 编码器的基础版本。学习率设置为 1e-5（从 [5e-6, 1e-5, 3e-5, 5e-5] 中调整），并采用默认配置的 AdamW 优化器。隐藏状态维度为 768。所有其他超参数设置与基于 LSTM 的模型相同。

在 dev 集上表现最佳的模型被选中，然后我们在 test 集上报告其结果。所有实验都在 RTX 6000 和 DGX-A100 服务器上进行。

## C. Baselines

我们将基于 LSTM 的 Co-guiding Net 和 Co-guiding-SCL Net 与 Attention BiRNN [39]、Slot-Gated [3]、SF-ID [6]、Stack-Propagation [7]、Joint Multiple ID-SF [9]、AGIF [10] 和 GL-GIN [12] 进行比较。我们使用 GLGIN 的官方源代码和默认超参数设置来复现结果。我们将基于 PTLM 的 Co-guiding Net 和 Co-guiding-SCL Net 与 PTLM 版本的 GL-GIN 进行比较，该版本由我们自己实现。为了公平比较，我们采用与我们基于 PTLM 的模型相同的学习率和优化器设置。其他超参数设置与 GL-GIN 相同。

## D. Main Results

我们的模型和基线的性能结果如表III所示，

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d956678d828a4abca9a2d7c9eb2f43e0.png#pic_ center" width="70%" />
</div>

从中我们可以观察到以下几点：

1) 我们模型与基线的比较：(1) Co-guiding Net 和 Co-guiding-SCL Net 在所有任务和数据集上都取得了显著且一致的改进。具体来说，在 MixATIS 数据集上，与 GL-GIN 相比，Co-guiding-SCL Net 在句子级语义框架解析、槽填充和多意图检测上分别取得了 21.3%、2.4% 和 4.1% 的显著改进；在 MixSNIPS 数据集上，它在句子级语义框架解析、槽填充和多意图检测上分别超过了 GL-GIN 5.3%、1.2% 和 1.8%。我们模型的有希望的结果可以归因于多意图检测和槽填充之间的相互指导，允许两个任务为彼此提供关键线索。此外，我们设计的 HSLGs 和 HGATs 可以有效地模拟语义节点和标签节点之间的交互，从初始预测中提取指示性线索。我们提出的单任务监督对比学习和共同指导监督对比学习可以进一步捕获单任务和双任务语义对比关系。

(2) 我们的模型在多意图检测上取得的改进比槽填充更大。原因是除了从多意图检测到槽填充的指导外，我们的模型还实现了从槽填充到多意图检测的指导，而以前的模型都忽略了这一点。此外，以前的方法通过同构图和 GAT 对语义-标签交互进行建模，限制了性能。不同地，我们的模型使用异构图语义-标签图来表示语义节点和标签节点之间的不同关系，然后在图上应用所提出的 HGATs 来实现交互。因此，它们的表现（特别是在多意图检测上）明显不如我们的模型。

(3) 整体准确率的改进更为显著。我们认为原因是实现的相互指导使两个任务深度耦合，并允许它们使用初始预测相互激发。对于每个任务，其最终输出由其自身和另一个任务的初始预测指导。通过这种方式，两个任务的正确预测可以更好地对齐。结果，更多的测试样本获得了正确的句子级语义框架解析结果，然后整体准确率得到提升。

(4) 基于 PTLM 编码器，我们的模型比 GL-GIN 带来了更显著的改进。这是因为 GL-GIN 执行语义交互，而 PTLM 在语义方面有很强的能力。不同地，我们的模型首次尝试实现语义-标签交互，这不能由 PTLM 实现。因此，我们模型的优势与 PTLM 的不重叠，由 PTLM 生成的高质量语义表示可以与我们模型的共同指导机制很好地配合。

2) Co-guiding Net 和 Co-guiding-SCL Net 的比较：Co-guiding-SCL Net 相对于 Co-guiding Net 的性能改进来自于我们提出的单任务监督对比学习和共同指导监督对比学习。我们可以观察到，Co-guiding-SCL Net 在基于 PTLM 编码器的情况下比 LSTM 编码器获得了更大的改进。我们怀疑的原因是 PTLM 可以生成比 LSTM 更高质量的语义表示，然后对比学习机制可以在表示上得到改进。

## E. Model Analysis of Co-Guiding Net

我们进行了一系列的消融实验，以从不同角度验证我们工作的优势，结果如表IV所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/3d33015e3ec04da58d7f403c18538b57.png#pic_ center" width="70%" />
</div>


1)Effect of Slot-to-Intent Guidance。我们工作的核心贡献之一是在多意图检测和槽填充之间实现相互指导，而以前的工作只利用了从意图到槽的单向信息。因此，与以前的工作相比，我们工作的一个优势是模拟了从槽到意图的指导。为了验证这一点，我们设计了一个变体，称为 w/o S2I-guidance，其结果如表IV所示。我们可以观察到，意图准确率在 MixATIS 上下降了 2.0%，在 MixSNIPS 上下降了 0.8%。此外，整体准确率显著下降：在 MixATIS 上下降了 3.6%，在 MixSNIPS 上下降了 0.9%。这证明了从槽到意图的指导可以有效促进多意图检测，并实现两个任务之间的相互指导，可以显著提高整体准确率。

此外，尽管 w/o S2I-guidance 和 GL-GIN 都只利用了从意图到槽的单向信息，但 w/o S2I-guidance 在大幅度上优于 GL-GIN。我们认为这是因为我们提出的异构图语义-标签图和异构图注意力网络的优势，这些优势在第 V-E3 节中得到了验证。

2)Effect of Intent-to-Slot Guidance。为了验证意图到槽指导的有效性，我们设计了一个变体，称为 w/o I2S-guidance，其结果如表IV所示。我们可以发现，意图到槽指导对性能有显著影响。具体来说，w/o I2S-guidance 导致整体准确率几乎出现了同样程度的性能下降，证明了意图到槽指导和槽到意图指导都是不可或缺的，实现两个任务之间的相互指导可以显著提高性能。

3)Effect of HSLGs and HGATs。在本文中，我们设计了两个异构图语义-标签图（即 S2I-SLG、I2S-SLG）和两个异构图注意力网络（即 S2I-HGAT、I2S-HGAT）。为了验证它们的效果，我们设计了一个变体，称为 w/o relations，通过删除两个 HSLGs 上的关系。在这种情况下，S2I-SLG/I2S-SLG 崩溃为同构图，S2I-HGAT/I2S-HGAT 崩溃为基于多头注意力的通用 GAT。从表IV中，我们可以观察到，w/o relations 在两个数据集的所有指标上都出现了显著下降。w/o relations 和 Co-guiding Net 之间的明显性能差距证明了 (1) 我们提出的 HSLGs 可以有效地表示语义节点和标签节点之间的不同关系，为模拟两个任务之间的相互指导提供适当的平台；(2) 我们提出的 HGATs 可以充分且有效地通过在 HSLGs 上实现特定关系的注意力信息聚合，来模拟语义和指示性标签信息之间的交互。

此外，尽管 w/o relations 明显表现不如 Co-guiding Net，但它仍然显著优于所有基线。我们认为这是因为我们的模型实现了两个任务之间的相互指导，允许它们通过跨任务相关性相互促进。

4)Effect of I2S-HGAT for Capturing Local Slot Dependencies。[12] 提出了一个局部槽感知 GAT 模块，以缓解由槽填充的非自回归特性引起的不协调槽问题（例如，B-歌手后面跟着 I-歌曲）[17]。[12] 中的消融研究证明了这个模块通过模拟槽隐藏状态之间的局部依赖性有效地提高了槽填充性能。在他们的模型（GL-GIN）中，局部依赖性在局部槽感知 GAT 和随后的全局意图-槽 GAT 中都被建模。我们认为 GL-GIN 需要局部槽感知 GAT 的原因是 GL-GIN 中的全局意图-槽 GAT 无法有效地捕获局部槽依赖性。GL-GIN 的全局槽-意图图是同构的，在其上工作的 GAT 将槽语义节点和意图标签节点同等对待，不加区分。因此，每个槽隐藏状态从其局部槽隐藏状态和所有意图标签中接收不区分的信息，使得捕获局部槽依赖性变得混乱。相反，我们相信我们的 I2S-HLG 和 I2S-HGAT 可以有效地通过模拟与其他关系一起建模的特定槽语义依赖性关系来捕获槽的局部依赖性。因此，我们的 Co-guiding Net 没有包括另一个模块来捕获槽的局部依赖性。

为了验证这一点，我们设计了一个变体，称为 +Local Slot-aware GAT，通过在意图感知 BiLSTMs 之后增加 Local Slot-aware GAT [12]（与 GL-GIN 中的相同位置）来实现。其结果如表IV所示。我们可以观察到，Local Slot-aware GAT 不仅没有带来改进，反而导致了性能下降。这证明了我们的 I2S-HGAT 可以有效地捕获局部槽依赖性。

## F. Analysis of Supervised Contrastive Learning Mechanisms in Co-Guiding-SCL Net

我们进行了一系列的消融实验，以验证 Co-guiding-SCL Net 中我们提出的监督对比学习机制的优势，结果在 MixATIS 数据集上如图8和图9所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/677b635b908a48568619d92df296e362.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1ee9f85ef79c4a079e38cfbe2f2998ee.png#pic_ center" width="70%" />
</div>


1) Effect of Single-Task/Co-Guiding Supervised Contrastive Learning。从图8中我们可以观察到，移除单任务监督对比学习（ST-SCL）会导致性能下降。这是因为 ST-SCL 可以通过利用单任务语义对比关系，在第一阶段改善初始估计的标签分布，通过拉近对应相同/相似标签的表示，同时推开对应不同标签的表示。更好的标签分布可以为第二阶段的双任务共同指导机制提供更可靠的指示信息。

我们可以发现，没有共同指导监督对比学习（CG-SCL）的变体比完整模型表现得明显更差。这证明了 CG-SCL 的优势，它可以通过将两个任务的标签作为监督信号，整合到对比学习过程中，在第二阶段进一步捕获双任务语义对比关系。在第二阶段，CG-SCL 与 HGATs 合作，全面有效地模拟双任务相互指导，显著提高了最终预测的性能。

2)Effect of Intent/Slot Supervised Contrastive Learning。从图9中我们可以观察到，移除意图监督对比学习（IntentSCL）会导致意图准确率下降，同时也会使模型在槽填充和句子级语义解析上表现更差。移除槽监督对比学习（SlotSCL）不仅会导致槽 F1 下降，而且还会导致意图准确率和整体准确率下降。有两个原因。首先，IntentSCL 和 SlotSCL 可以有效地提高模型在多意图检测和槽填充上的性能。其次，共同指导监督对比学习进一步使两个任务在彼此的性能上深度耦合和相互关联。因此，移除 IntentSCL 和 SlotSCL 中的任何一个都会导致整体准确率、槽 F1 和意图准确率的下降。

## G. Case Study

为了展示我们的模型如何允许两个任务相互指导，我们以图10中的两个案例为例进行说明。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9dd71fe322c3468daff9e759178f636e.png#pic_ center" width="70%" />
</div>


1)Slot-to-Intent Guidance。从图10(a)中我们可以观察到，在第一阶段，所有槽都被正确预测，而多意图检测错误地多检测到了意图 `atis_ airport`。在第二阶段，我们提出的 S2I-HGAT 在 S2I-HLG 上操作。它聚合并分析了第一阶段槽预测中的槽标签信息，提取出大部分槽标签与 `city_ name` 相关，而没有提及机场的信息。然后，这些有益的指导信息被传递到意图语义节点，其表示随后被送入意图解码器进行预测。通过这种方式，槽填充的指导帮助多意图检测正确预测。

2)Intent-to-Slot Guidance。在图10(b)所示的例子中，在第一阶段，正确的意图被预测出来，而在预测的槽上存在错误。在第二阶段，我们提出的 I2S-HGAT 在 I2S-HLG 上操作。它综合分析了来自槽语义节点 `aircraft` 和意图标签节点 `atis_ aircraft` 的指示信息。然后，这些有益的指导信息被传递到槽语义 `m80`，因此其槽被正确推断出来。

## H. Computation Efficiency

我们的模型和现有最先进模型的训练时间和延迟如图5所示。我们可以发现，由于对比学习操作，Co-guiding-SCL Net 的训练时间更长一些。至于延迟，我们的 Co-guiding Net 和 Co-guiding-SCL Net 与 GL-GIN 相当，但它们可以显著优于它。我们提出的 Co-guiding-SCL 中的对比学习机制只在训练过程中工作，不影响延迟。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5963d8b6bf204e44ae20889c7a3f1c75.png#pic_ center" width="70%" />
</div>

## I. Zero-Shot Cross-Lingual Multi-Intent SLU

1)Experiment Setup: Dataset and Metrics。我们在 MultiATIS++ [45] 多语言基准数据集上评估我们的模型。该数据集包括英语的多意图训练样本和 9 种语言的测试样本：英语（en）、西班牙语（es）、葡萄牙语（pt）、德语（de）、法语（fr）、中文（zh）、日语（ja）、印地语（hi）和土耳其语（tr）。我们采用意图准确率（Acc）、槽填充和整体准确率（Acc）作为评估指标。

基线和实现：目前，零样本跨语言 SLU 的最先进模型是 GL-CLEF [35]，它利用无监督对比学习来对齐源语言语义和目标语言语义。然而，它被设计用于单意图 SLU。因此，我们修改了它的官方代码，使其适用于多意图 SLU。我们使用 Sigmoid 函数和线性层，类似于 (4)，替换了它的原始意图分类模块。我们用在我们模型中使用的损失函数 (11) 替换了它的原始损失函数。

除了评估 Co-guiding Net 和 Co-guiding-SCL Net 的性能外，我们还将它们与 GL-CLEF 结合，形成了 GL-CLEF+Co-guiding Net 和 Co-guiding-SCL Net。为了公平比较，GL-CLEF 在 GL-CLEF、GL-CLEF+Co-guiding Net 和 Co-guiding-SCL Net 中使用的超参数直接从其原始论文和官方代码中检索。至于 Co-guiding Net 和 Co-guiding-SCL Net 的超参数，我们只使用了第 V-B 节中设定的那些。

我们进行了两组实验，分别基于两种多语言预训练语言模型（例如 mBERT [41] 和 XLM-R [46]）。我们报告了使用不同随机种子的三次运行的平均结果。

2)Results Analysis。基于 mBERT 和 XLM-R 的模型结果分别显示在表 VI 和表 VII 中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0c2dc3e62fc84d8299785f750359e923.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5ccacb88c5a842c3a551101e6eff8a7e.png#pic_ center" width="70%" />
</div>

首先，我们可以观察到，尽管 Co-guiding Net 和 Co-guiding-SCL Net 在英语上取得了显著的改进，但很难说 Co-guiding Net 和 Co-guiding-SCL Net 在 9 种语言的总平均整体准确率上可以超越 GL-CLEF。我们怀疑的原因是我们的模型有很强的能力来模拟两个任务之间的相互指导和语义-标签交互，而我们的模型中没有多语言模块，这使得从源语言（英语）到目标语言的有益知识转移变得困难。然而，如果我们将我们的模型与 GL-CLEF 结合，我们可以观察到明显的进步。具体来说，基于 XLM-R，GL-CLEF+Co-guiding-SCL Net 在 9 种语言的总平均整体准确率上比 GL-CLEF 提高了 33.5% 的相对改进。有两个原因。首先，我们模型的优势在于通过模拟双任务相互指导和捕获细粒度的双任务相关性，从源语言中捕获有益的知识。其次，GL-CLEF 的语义对齐作为桥梁，可以将从源语言学到的知识转移到目标语言。因此，我们的模型与 GL-CLEF 配合得很好，为零样本跨语言多意图 SLU 取得了有希望的结果。

# VI. 结论

在本文中，我们提出了一个新颖的两阶段框架，该框架允许两个任务在第二阶段使用第一阶段预测的标签相互指导。基于此框架，我们提出了两种新模型：Co-guiding Net 和 Co-guiding-SCL Net。为了表示语义节点和标签节点之间的关系，我们提出了两个异构图语义-标签图和两个异构图注意力网络，以模拟意图和槽之间的相互指导。此外，我们提出了单任务监督对比学习以及共同指导监督对比学习，分别在第一阶段和第二阶段执行。我们在多意图 SLU 和零样本跨语言多意图 SLU 上对我们的模型进行了广泛的实验评估。基准数据集上的实验结果表明，我们的模型以显著的优势超越了先前的模型。在多意图 SLU 任务上，我们的模型在 MixATIS 数据集上比之前最好的模型相对提高了 21.3%。在零样本跨语言多意图 SLU 任务上，我们的模型平均在 9 种语言的总体准确率上比现有最先进模型提高了 33.5%。

此外，这项工作提供了一些通用的见解，利用词级和句子级标签之间的双任务对比关系来开发词级和句子级语义相关性。这个想法可以应用于其他场景，这些场景联合解决句子级和词级任务。
