# 题目：[ X^2-VLM: All-in-One Pre-Trained Model for Vision-Language Tasks](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10356651)  
## $\rm X^2$-VLM：视觉语言任务的全能预训练模型
**作者：Yan Zeng； Xinsong Zhang；Hang Li；Jiawei Wang；Jipeng Zhang；Wangchunshu Zhou** 

****
# 摘要
视觉语言预训练旨在从大量数据中学习视觉和语言之间的对齐。大多数现有方法仅学习图像-文本对齐。其他一些方法利用预训练的对象检测器在对象级别上利用视觉语言对齐。在本文中，我们提出了一种通过统一预训练框架同时学习多粒度对齐和多粒度定位的方法。基于此，我们提出了X2-VLM，一种具有灵活模块化架构的全能模型，我们进一步在一个模型中统一了图像-文本预训练和视频-文本预训练。X2-VLM能够学习与多样化文本描述相关的无限视觉概念。实验结果表明，X2-VLM在图像-文本和视频-文本任务上都表现最佳，在性能和模型规模之间取得了良好平衡。此外，我们展示了X2-VLM的模块化设计具有很高的可迁移性，可以用于任何语言或领域。例如，通过简单地将文本编码器替换为XLM-R，X2-VLM在没有任何多语言预训练的情况下，超越了最先进的多语言多模态预训练模型。


# 关键词
- 视觉语言基础模型
- 视觉语言预训练


## 引言
视觉语言预训练旨在从大量图像-文本或视频-文本对中学习视觉语言对齐。一个预训练的视觉语言模型（VLM）在用少量标记数据进行微调后，在许多视觉语言（V+L）任务中，如图像-文本检索和视觉问答（VQA），展示了最先进（SoTA）的性能。

现有的学习视觉语言对齐的工作通常分为两类：粗粒度和细粒度。粗粒度方法使用卷积神经网络 [1] 或视觉Transformer [2] 来编码整体图像特征 [3]，[4]，[5]，然而，这些方法难以从通常弱相关的噪声图像-文本对中学习细粒度的视觉语言对齐，例如在对象级别。为了学习细粒度的视觉语言对齐，许多方法采用预训练的对象检测器作为图像编码器 [7]，[8]，[9]，[10]，[11]，[12]，[13]。然而，对象检测器输出的对象中心特征无法编码多个对象之间的关系。此外，对象检测器只能识别有限数量的对象类别。

理想情况下，一个VLM应该在预训练中同时学习视觉和语言之间的多粒度对齐，而不局限于对象-文本对齐或图像-文本对齐。然而，学习多粒度对齐具有挑战性，以前的工作未能解决这一问题。挑战来自四个方面：1）使用哪种类型的数据来学习多粒度的视觉语言对齐；2）如何以统一的方式聚合不同类型的数据进行视觉语言预训练；3）如何通过单个视觉编码器表示多粒度的视觉概念，包括对象、区域和图像；4）如何高效地从数据中学习多粒度的视觉语言对齐。

在本文中，我们提出了一种统一框架来学习多粒度视觉语言对齐，即X2-VLM。我们利用三种类型的数据进行视觉语言预训练，包括图像上的对象标签 [14]，[15]，[16]，如“man”或“backpack”，图像上的区域注释 [16]，[17]，如“boy wearing backpack”，以及图像的文本描述，如“开学的第一天给学生和家长带来了复杂的感觉”。我们假设学习多粒度的视觉语言对齐可以帮助VLM更好地理解弱相关的图像-文本对，因为模型已经学会将图像中的组件（例如对象或区域）与文本描述（例如单词或短语）对齐。我们将所有视觉概念与文本描述关联，而不是类标签，包括对象、区域和图像。通过将所有视觉概念与语言关联，模型可以以统一的方式学习由多样化文本描述的无限视觉概念。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/db23340f0e1b4b79a2d3b206ef3a9573.png)



X2-VLM具有灵活的模块化架构，分别有三个模块用于视觉、文本和融合。所有模块都基于Transformer [18]。我们用视觉Transformer [2] 编码图像，并利用某些patch特征来表示图像中的多粒度视觉概念，这些概念可以是对象、区域或图像本身。通过这样做，X2-VLM以统一的形式输出对象、区域和图像的视觉特征。此外，我们提出直接将多粒度视觉特征与配对的文本特征对齐，并在视觉语言预训练中同时根据不同的文本描述定位同一图像中的多粒度视觉概念。在微调和推理中，X2-VLM可以利用学习到的多粒度对齐来执行下游V+L任务，而无需输入图像中的对象或区域注释。

X2-VLM可以轻松扩展到视频-文本预训练。对于视频编码，我们对视频帧进行采样，并分别用视觉Transformer编码这些帧。然后，我们在时间维度上对帧的patch特征求平均值以编码视频。编码器参数在视频-文本预训练和图像-文本预训练之间共享。通过这样做，我们利用视频-文本对，使模型能够理解时间维度中的视觉概念，并学习一个更通用的VLM。

此外，我们展示了X2-VLM模块化架构的灵活性。我们研究了预训练后的跨模态能力是否可以转移到其他语言或领域。这在实际应用中是一个重要问题，因为许多多模态任务存在于非英语语言中。然而，由于在某些语言或领域中收集图像-文本对或视频-文本对可能成本高昂，最近的SoTA VLM通常用英语数据训练，并且仅适用于英语文本，限制了它们的应用范围。我们发现，令人惊讶的是，X2-VLM可以通过简单地将文本模块替换为特定语言或领域的模块，在不进一步预训练的情况下有效适应不同语言或领域的V+L任务。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/ea7f0085e1af4b9da9e50ed398b9fc55.png)



我们进行了广泛的实验来验证X2-VLM的有效性。首先，我们在基础和大规模上比较了X2-VLM与SoTA图像-文本预训练方法，发现X2-VLM在图像-文本任务（包括检索、VQA、推理和定位）上大大超越了所有这些方法。此外，X2-VLM在图像字幕生成中超越了为生成任务设计的SimVLM [19]和BLIP [20]。X2-VLM还在跨模态理解任务中超越了也利用对象和区域图像注释的MDETR [21]和OFA [22]。具有约590M参数的X2-VLMlarge在图像-文本检索和视觉推理方面表现与具有约2B参数的CoCa [23]和BEiT-3 [24]相当。总之，如图1（a）所示，X2-VLM在性能和模型规模之间取得了良好的平衡。此外，我们发现通过大规模图像-文本对训练，X2-VLM学会了在开放域图像中定位多样化的细粒度视觉概念，如不同的苏打水、汽车、角色和名人。其次，X2-VLM也是新SoTA预训练模型，在视频-文本任务（包括视频-文本检索和视频VQA）上表现最佳，如图1（b）所示。大多数现有VLM仅处理图像-文本任务，但X2-VLM通过统一框架在两种任务上都取得了SoTA性能。第三，为了验证模块化设计的灵活性，我们在用英语数据进行视觉语言预训练后，将X2-VLM的文本编码器替换为多语言文本编码器XLM-R [25]。如图1（c）所示，X2-VLM在不需要多语言图像-文本对[26]，[27]和多语言句子对[28]的多语言多模态任务上超越了SoTA方法，这些数据收集成本高昂。

本文的贡献如下：
- 我们提出通过统一预训练框架同时学习多粒度对齐和多粒度定位。基于此，我们提出了X2-VLM，一种可以处理图像-文本和视频-文本任务的全能预训练VLM。
- 实验结果表明，X2-VLM在图像-文本和视频-文本基准测试中是最好的模型。此外，结果证实了所提出的多粒度视觉语言预训练框架可扩展到大规模数据和更大模型规模。
- 我们揭示了X2-VLM模块化设计的潜力，展示了它可以在其他语言或领域中使用。在用英语数据进行预训练后，通过将文本编码器替换为XLM-R，X2-VLM在多语言多模态任务上超越了SoTA方法。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/ba1a90fd1f3b4eab8bf8a73e48b3739c.png)


## III. 方法

### A. 概述

**架构**：X2-VLM由视觉、文本和多模态融合模块组成。所有模块都基于Transformer [18]。融合模块以文本特征为输入，通过每一层的交叉注意力将视觉特征与文本特征融合，其中文本特征作为查询，视觉特征作为键和值。在预训练中，这三个模块作为编码器，而文本和融合模块也可以在应用从左到右的自注意力时适应生成任务，如我们的实验所示。

**数据**：X2-VLM是一种统一的方法，将所有视觉概念与文本描述关联，包括图文对、视频文本对以及对象和区域的图像注释。也就是说，一张图像可能包含多个视觉概念，每个概念都与一个文本描述相关联，表示为 $(I, T, \{(V_1, T_1), ...\}_ N)$。 $\{(V_1, T_1), (V_2, T_2), ...\}_ N$ 是对象或区域的图像注释。这里， $V_ i$ 是在边界框 $b_ i = (cx, cy, w, h)$ 中的一个对象或区域，表示为归一化的中心坐标、宽度和高度。当图像本身表示一个视觉概念时， $b = (0.5, 0.5, 1, 1)$。对象的 $T_ i$ 原本是对象标签。如果对象注释包含对象属性，例如颜色，我们将属性与对象标签连接作为文本描述。区域的 $T_ i$ 是描述区域的短语。注意，如表I所示，有些图像没有关联的文本，即 $T$ 是NaN，有些图像没有注释，即 $N = 0$。尽管如此，我们在一个训练批次中混合所有类型的数据，因此在每次训练迭代中，我们通过多粒度对齐和多粒度定位同时优化模型。

### B. 统一视觉编码

X2-VLM统一了图像和视频编码，如图2所示。无论输入是什么，X2-VLM的视觉模块都会在视觉Transformer的潜在特征空间中生成隐藏状态。因此，图文预训练和细粒度预训练相互加强。此外，图像理解的能力可以更好地转移到视频理解上。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/992d6f288ea641a6adc5d197c3d0a0cf.png)



**视觉概念表示**：X2-VLM提出了一种高效的方法，通过视觉Transformer的一次前向传递来获取图像中的所有多粒度视觉概念。首先，我们将图像处理成补丁特征。然后，X2-VLM表示一个对象或区域，例如 $V_ i$，它对应于边界框中的一组补丁，例如 $b_ i$，通过聚合补丁之间的信息来表示，如图2所示。具体来说，我们在保持原始位置的同时展开相应的补丁特征。然后，我们计算补丁特征的平均值作为[CLS]补丁并预置它。因此，通过聚合所有补丁中的信息，得到整个图像 $I$ 的表示。

**视频表示**：由于视频由多张图像组成，为了利用大规模图文预训练更好地理解视频，我们以一种简单高效的方式统一了视频编码和图像编码。首先，我们为视频每秒采样一帧。视觉编码器将分别对帧进行编码，生成补丁特征。最后，我们向每帧的补丁特征添加时间信息，并在时间维度上计算平均值以表示视频。通过这种方式，视频通过一系列补丁特征进行编码，与对象/区域/图像相同，因此我们可以为视频文本对和对象/区域/图像文本对应用统一的预训练框架。


### C. 多粒度视觉语言预训练

我们在一个训练批次中混合所有类型的数据，因此对于每次训练迭代，如图 3 所示，我们通过两个目标同时优化 X2-VLM：1）学习视觉概念与文本之间的多粒度对齐；2）在给定不同文本描述的情况下定位图像中的多粒度视觉概念。

**1. 多粒度对齐**: 由于我们已经将所有视觉概念与文本描述关联起来，我们提出将多粒度视觉概念与相应的文本对齐。具体来说，在通过上述方法编码视觉概念后，我们以相同的方式将多粒度的视觉特征与相应的文本特征对齐。我们简单选择三种损失进行优化，包括对比损失、匹配损失和 MLM 损失。这些损失已被之前的工作 [5], [11], [50] 充分研究过，但我们建议在视觉概念到文本的层面上使用它们。请注意，本节中的 $V$ 表示一个视觉概念，包括对象、区域、图像或视频。

我们应用对比损失来预测 (视觉概念，文本) 对，使用批内负样本。给定一对 $(V, T)$， $T$ 是 $V$ 的正例，我们将小批量中的其他 $(N - 1)$ 文本视为负样本。首先，我们定义相似度为：

$$
s(V, T) = g_ v(v_ {cls}) \cdot g_ w(w_ {cls}),
$$

其中 $v_ {cls}$ 和 $w_ {cls}$ 分别是视觉编码器和文本编码器的输出 [CLS] 嵌入。 $g_ v$ 和 $g_ w$ 是将 [CLS] 嵌入映射到归一化低维表示的变换。基于此，我们计算批内的视觉到文本相似度为：

$$
p_ {v2t}(V) = \frac{\exp(s(V, T) / \tau)}{\sum_{i=1}^N \exp(s(V, T^i) / \tau)},
$$

类似地，文本到视觉的相似度为：

$$
p_ {t2v}(T) = \frac{\exp(s(V, T) / \tau)}{\sum_{i=1}^N \exp(s(V^i, T) / \tau)},
$$

其中 $\tau$ 是一个可学习的温度参数。让 $y_ {v2t}(V)$ 和 $y_ {t2v}(T)$ 表示真实的一热相似度，其中只有正对具有为 1 的概率。最后，对比损失定义为 $p$ 和 $y$ 之间的交叉熵 $H$：

$$
L_ {cl} = \frac{1}{2} \mathbb{E}_ {V, T \sim D} \left[ H \left( y_ {v2t}(V), p_ {v2t}(V) \right) + H \left( y_ {t2v}(T), p_ {t2v}(T) \right) \right]
$$

我们还利用匹配损失来确定视觉概念和文本对是否匹配。对于小批量中的每个视觉概念，我们根据 (2) 中的 $p_ {v2t}(V)$ 采样一个批内困难负样本。与概念更相关的文本更有可能被采样。我们还为每个文本采样一个困难负视觉概念。然后我们将这些对作为输入放入融合模块，然后我们使用 $x_ {cls}$，融合模块的输出 [CLS] 嵌入，来预测匹配概率 $p_ {match}$，损失为：

$$
L_ {match} = \mathbb{E}_ {V, T \sim D} H \left( y_ {match}, p_ {match}(V, T) \right),
$$

其中 $y_ {match}$ 是一个二维的一热向量，表示真实标签。

此外，我们应用掩蔽语言建模损失，基于视觉概念预测文本中的掩蔽词。我们以 40% 的概率随机掩蔽输入标记。我们使用融合编码器的输出进行预测。让 $\hat{T}$ 表示一个掩蔽文本， $p_ j(V, \hat{T})$ 表示融合模块预测的掩蔽标记 $t_ j$ 的概率。我们最小化交叉熵损失：

$$
L_ {mlm} = \mathbb{E}_ {t_ j \sim T; (V, T) \sim D} H \left( y_ j, p_ j \left( V, \hat{T} \right) \right),
$$

其中 $y_ j$ 是一个一热分布，其中真实标记 $t_ j$ 的概率为 1。

**2. 多粒度定位**: 我们已经在不同粒度上对齐了视觉概念和文本。我们进一步通过训练 X2-VLM 来定位在给定不同文本描述的情况下在同一图像中的不同视觉概念来优化它。具体来说，我们在视觉语言预训练中引入边界框预测任务，模型被要求预测视觉概念 $V_ i$ 的边界框 $b_ i = (cx, cy, w, h)$：

$$
\hat{b_ i}(I, T^i) = \text{Sigmoid} \left( \text{MLP} \left( x_ {cls}^i \right) \right),
$$

其中 Sigmoid 用于归一化，MLP 表示多层感知器， $x_ {cls}^i$ 是融合模块给定 $I$（整个图像）和 $T^i$（视觉概念描述）特征的输出 [CLS] 嵌入。

对于边界框预测， $L_ 1$ 是最常用的损失。然而，即使它们的相对误差相似，它对小框和大框有不同的尺度。为了解决这个问题，我们使用 $L_ 1$ 损失和广义交并比 (IoU) 损失 [51] 的线性组合，它是尺度不变的。总体损失定义为：

$$
L_ {bbox} = \mathbb{E}_ {(V^i, T^i) \sim I; I \sim D} \left[ L_ {iou} \left( b_ i, \hat{b_ i} \right) + \| b_ i - \hat{b_ i} \|_ 1 \right]
$$

最后，X2-VLM 的预训练目标定义为：

$$
L = L_ {bbox} + L_ {cl} + L_ {match} + L_ {mlm}
$$





## 4. 实验

### 4.1 预训练数据集

我们用两组数据预训练X2-VLM。4 M预训练数据集由两个领域内数据集COCO [14] 和Visual Genome (VG) [17] 以及两个领域外数据集SBU Captions [52] 和Conceptual Captions (CC) [53] 组成。这个预训练数据集被先前的工作广泛使用，因此我们选择这个设置与其他方法进行公平比较。我们还包括来自RefCOCO [54]，GQA [55] 和Flickr entities [56] 的COCO和VG图像的注释，遵循OFA [22] 和MDETR [21]。

然后，我们通过包括来自Conceptual 12 M数据集（CC-12 M）[57] 和LAION [58] 的领域外和噪声更大的图像-文本对，以及来自Objects365 [15] 和OpenImages [16] 的对象注释，来扩展预训练数据集。此外，为了支持视频-文本下游任务，我们包括来自WebVid2.5M [40]，Howto100M [59] 和YT-Temporal 180M [60] 的视频-文本对进行预训练。注意，我们使用的所有数据集都是公开可用的，并且在之前的工作中已被利用 [5]，[13]，[20]，[22]，[43]。此外，由于大多数下游任务都是基于COCO和VG构建的，我们排除了所有也出现在下游任务测试集中的图像以避免信息泄漏。我们在附录中提供了数据过滤的详细信息，可在线查看。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/3ba1b1e686d342f19d2c6412d34ade4d.png)



### 4.2 实现细节

表II列出了X2-VLM的参数。考虑到性能和模型规模之间的权衡 [61]，X2-VLMlarge 也使用一个12 L文本编码器。视觉编码器用BEiT-2 [33] 初始化。文本编码器用BERT [62] 初始化。X2-VLM在224×224的图像分辨率下进行预训练，使用16×16的patch大小。我们在一个训练批次中混合所有类型的数据，因此在每次训练迭代中，我们同时通过多粒度对齐和多粒度定位优化模型。使用4 M数据，我们在8 A100上预训练X2-VLMbase 500 K步，批量大小为1024，并在16 A100上预训练X2-VLMlarge 250 K步，耗时约1周。X2-VLMbase 的学习率在前2500步中升至1e-4，然后按线性计划递减。X2-VLMlarge 的学习率为5e-5。使用大规模数据，训练X2-VLM在32 A100上耗时2-3周，以获得基础模型，在64 A100上耗时更大模型。我们在附录中描述了更多实现细节，可在线查看。

### 4.3 图像-文本下游任务

我们在广泛使用的图像-文本下游任务上比较X2-VLM与最著名的最先进方法。一般来说，我们遵循先前工作的设置进行微调。我们描述了如何实现微调，如下所示。

#### 1. 图像-文本检索

我们在MSCOCO和Flickr30K [56] 数据集上评估X2-VLM。我们采用广泛使用的Karpathy拆分 [63] 用于两个数据集。我们优化Lcl 和Lmatch 进行微调。我们将批量大小设置为1024。输入图像的分辨率设置为384×384。根据先前的工作 [5]，X2-VLM首先分别编码图像和文本，并计算批内文本到图像和图像到文本的相似度以获得前k个候选，然后使用融合编码器对候选进行重新排序。k在MSCOCO数据集中设置为80，在Flickr30 K中设置为32。

表III显示，X2-VLM在图像-文本检索任务上实现了SoTA结果，即使现有方法要么具有更多的模型参数，要么具有更多的训练数据。具体来说，X2-VLMbase 超越了FLIP [64]，BLIPbase 和BLIPlarge 也利用了来自LAION的大规模图像-文本对，X2-VLMlarge 进一步提高了性能。与也支持图像-文本任务和视频-文本任务的OmniVL相比，X2-VLMbase 在用4 M数据或更多数据预训练时，大大超越了它。我们还在表IV中比较了X2-VLMlarge 和巨型基础模型BEiT-3。实验结果表明，尽管规模更小，但X2-VLMlarge 在与BEiT-3相比具有可比甚至更好的性能。此外，如表III所示，X2-VLMbase 在4 M设置中大大超越了VL-BEiT，这是BEiT-3的基础版本。

#### 2. 视觉问答

该任务要求模型在给定图像和问题的情况下预测答案。我们在VQA v2.0数据集 [65] 上评估X2-VLM。根据现有方法 [5]，[7]，[11]，我们使用训练和验证集进行训练，并包括来自Visual Genome的额外问题-答案对。根据ALBEF，我们使用六层Transformer解码器根据融合模块的输出生成答案。然后，通过优化自回归损失微调模型。在推理过程中，我们限制解码器仅从3129个候选答案中生成，以便与现有方法进行公平比较。根据先前的工作 [22]，[23]，[24]，输入图像的分辨率设置为768×768。

我们在表V中报告了VQA的实验结果。我们可以看到，X2-VLMbase 和X2-VLMlarge 超越了具有相似模型规模的其他方法。具体来说，X2-VLMbase 在4 M设置中大大超越了ALBEF，VLMo，METER和VL-BEiT。此外，随着更多预训练数据，X2-VLMbase 超越了也利用来自LAION的大规模图像-文本对的BLIP。与也支持图像-文本任务和视频-文本任务的OmniVL相比，X2-VLMbase 大大超越了它，取得了2%的绝对改进。X2-VLM在基础和大规模上也大大超越了SimVLM和OFA。SimVLM利用一个内部1.8B图像-文本数据集。OFA也利用了与X2-VLM相同的对象和区域图像注释。这些结果证实了所提出的多粒度视觉语言预训练框架的有效性。此外，当比较表III和V中的不同设置中的X2-VLM性能时，我们可以看到所提出的框架具有良好的可扩展性，可以受益于更大的模型规模。当用更多数据预训练更大模型时，性能提升甚至更加显著。

#### 3. 视觉推理

我们在广泛使用的基准NLVR2 [66] 上评估X2-VLM。该任务让模型确定文本是否描述了两张图像之间的关系。根据先前的工作 [35]，[67]，我们将三重输入公式化为两个图像-文本对，每个对包含文本描述和一个图像。然后，我们连接融合模块的最终输出[CLS]特征以预测标签。输入图像的分辨率设置为384x384。根据表V的结果，我们可以观察到视觉推理任务比预训练数据规模更受模型规模的影响。与其他基础规模模型相比，例如ALBEF，VLMo，VL-BEiT，SimVLM和BLIP，X2-VLMbase 具有更好的性能，取得了约3-4%的绝对改进，无论是在用4 M数据预训练还是用更多噪声数据预训练时。X2-VLMlarge 也大大超越了其他大规模模型，包括VLMolarge 和SimVLMlarge。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/36a8b4a5e47242029186d1d4def64c10.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/d29f6debb9184d5785e28122b886074a.png)



#### 4. 视觉定位

我们在RefCOCO+ [54] 上评估X2-VLM。给定图像作为输入和文本描述作为查询，融合模块的最终输出[CLS]特征用于预测视觉概念的边界框。输入图像的分辨率设置为384x384。如表V所示，X2-VLM超越了也利用对象和区域图像注释进行预训练的OFA [22]。不同的是，OFA采用编码器-解码器架构，以序列到序列的形式组织所有数据。此外，用于一般V+L目的的X2-VLM超越了专门用于视觉定位任务的MDETR [21]，在指标上取得了约7%的绝对改进。这些结果证实了所提出的多粒度视觉语言预训练相比于其他也利用对象和区域图像注释的方法的有效性。

#### 5. 图像字幕生成

该任务要求模型生成输入图像的文本描述。尽管X2-VLM更适用于跨模态理解，我们也在COCO Captioning数据集 [68] 上评估其生成性能。根据UniLM [69] 和BEiT-3，我们使用左到右MLM进行生成。具体来说，我们在左到右自注意力中使用文本模块和融合模块作为解码器，并采用该方法 [70] 以减少MLM生成中的微调生成差异。输入图像的分辨率设置为480x480。我们在Karparthy测试拆分上报告BLEU-4和CIDEr分数。如表V所示，X2-VLMbase 超越了为生成任务设计的BLIP [20] 和SimVLM [19]。BLIP利用了与X2-VLM相同的来自LAION的大规模图像-文本对。SimVLM利用一个内部1.8B图像-文本数据集。X2-VLM在BLEU-4方面也在图像字幕生成上超越了OFA [22]。总体来说，尽管X2-VLM更适用于跨模态理解，但其性能与SoTA生成方法相比具有竞争力，有时甚至更好。

#### 6. Winoground

Winoground [71] 提出了一项挑战性任务：给定两张图像和两个标题，目标是正确匹配它们，其中标题包含相同的词集，但顺序不同。几种竞争性VLM已被证明其性能接近或甚至低于随机机会。使用三个指标，即文本（模型是否能为给定图像匹配正确的标题）、图像（反之亦然）和组（模型是否能匹配每对），来评估性能。表VI中的实验结果表明，即使在用4 M数据训练时，X2-VLM也大大超越了其他模型，例如基于大型预训练对象检测器的UNITERlarge，以及由巨型ViT和FlanT5大型语言模型 [72] 组成的BLIP-2，并在一个更大数据集上预训练了129 M图像。值得注意的是，通过增加模型规模或预训练数据集，X2-VLM的性能可以进一步提高。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/be5ac344cdb447ea843113d52e696bc9.png)


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/b2d3f8d0c0274177a6b1e1ab54cd7fec.png)



#### 7. 开放词汇属性检测

开放词汇属性检测（OVAD）[73] 旨在识别图像中的开放集合对象以及每个对象的开放集合属性。我们遵循基准测试并在box-oracle设置中评估视觉语言模型在属性上的零样本性能。实验结果如表VI所示。在用4 M数据预训练时，X2-VLMbase 已经与用129 M数据预训练的BLIP相当。X2-VLMbase 还超越了由CLIP文本编码器和基于Faster-RCNN的对象检测器组成的OVADetector。此外，在其他任务中，随着更大预训练数据集或更大模型规模的一致扩展，X2-VLM的性能也得到了提高。

### 4.4 视频-文本下游任务

X2-VLM统一了图像-文本和视频-文本预训练。在本节中，我们在广泛使用的视频-文本任务上评估X2-VLM，包括视频-文本检索（MSRVTT [74]）和视频问答（MSRVTT-QA [75]、MSVD-QA [75] 和NExT-QA [76]）。我们实现了一个文本到视频检索模型，与图像-文本检索相同，首先计算前k个候选，然后使用融合模块重新排序候选。k设置为32。在训练和推理过程中，我们为每个视频采样五个帧。图像分辨率设置为384。对于视频问答，根据先前的工作，我们将其公式化为给定候选答案的分类任务。在训练和推理过程中，我们在MSRVTT数据集中为每个视频采样五个帧，在MSVD和NExT-QA数据集中为每个视频采样八个帧。MSRVTT的图像分辨率设置为320，MSVD和NExT-QA的图像分辨率设置为224。我们与几种视频语言基础模型进行比较：ALPRO [41]、All-in-one [43] 和OmniVL，也支持图像-文本任务和视频-文本任务。还有其他方法专门针对视频-文本检索 [44]，[45] 或视频问答 [46] 进行优化，这些方法未包含在我们的比较中。

结果如表VII所示。我们可以看到，X2-VLMbase 在视频问答和文本到视频检索上超越了以前的视频语言基础模型，X2-VLMlarge 进一步提高了性能，取得了视频-文本预训练的新SoTA结果。此外，我们在图像-文本（表III和V）和视频-文本基准测试中比较了X2-VLM和OmniVL。总体而言，X2-VLMbase 在所有图像-文本下游任务上大大超越了OmniVL，包括图像-文本检索、视觉问答、图像字幕生成和视频问答。




![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/014f73e4e0c64a7cbd56a60f7e42af6f.png)



### 4.5 多语言多模态任务

在X2-VLM架构中，文本编码、视觉编码和融合是分开的。因此，当替换文本编码器时，视觉编码和融合的能力将保留下来，从而实现新文本编码器的高效适应。我们的研究表明，在用英语数据进行预训练后，我们可以将文本编码器替换为特定语言或领域的编码器，以支持不同语言或领域中的更多应用。这样的特性很难通过像OFA和BEiT-3这样的统一模型实现。例如，BEiT-3在单个Transformer中共享图像、文本和融合，因此替换文本编码器可能导致图像编码和融合的能力也丧失。

在本节中，我们将X2-VLM的英语文本编码器替换为多语言文本编码器XLM-R [25]。然后，在没有第二步多语言多模态预训练的情况下，我们简单地在多语言多模态下游任务上微调X2-VLM。我们选择Multi30K [77] 和多语言MSCOCO [68]，[78]，[79] 进行评估，因为其他多语言多模态基准（如IGLUE [80]）没有训练集。根据先前的工作，我们计算图像到文本检索和文本到图像检索的平均Recall@K，K=1,5,10，作为评估指标。

我们将X2-VLM与SoTA多语言多模态预训练方法进行比较。M3P [49] 利用101 G文本覆盖100种语言进行预训练。UC2 [26] 将英语中的图像-文本对翻译成五种不同的语言。MURAL [27] 收集了110种语言的大规模图像-文本对。CCLM [28] 利用平行多语言文本对。所有这些方法都依赖于代价高昂的数据来执行多语言多模态预训练，而X2-VLM通过释放多语言多模态预训练过程，缓解了这个问题。如表VIII所示，X2-VLM在所有六种语言中都超越了所有这些方法。结果表明，X2-VLM有潜力通过使用不同的文本编码器在不进一步预训练的情况下适用于其他领域或语言。

此外，我们对两种模型变体进行了消融研究。与具有三个相对独立模块的X2-VLM不同，X-VLM [81] 将交叉注意力层引入一个12层BERT-base模型的最后六层。因此，对于第一个变体，我们将BERT模型替换为XLM-R，并且仅保留交叉注意力层。对于第二个模型变体，我们将X2-VLM的标记器和词嵌入层替换为XLM-R，这通常是没有模块化设计的模型在针对其他语言时使用的方法。表VIII中的结果表明，这两种变体都导致性能低于X2-VLM。值得注意的是，当仅更改标记器和词嵌入层时，模型在适应多语言任务方面的能力较差。




![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/b905176ad8314a43bac0c1d87bcd93d0.png)




### 4.6 消融研究

我们进行了深入的消融研究，结果如表IX所示。我们在附录中描述了实验设置，可在线查看。首先，我们研究了所提出框架中不同组件的作用，并分别进行了多粒度对齐和边框预测损失的消融。需要注意的是，这两种变体都利用了对象和区域数据。实验结果表明，多粒度对齐对模型性能比边框预测损失更重要，除了视觉定位任务。边框预测损失对视觉定位任务的性能至关重要，结合边框预测和多粒度对齐进一步提高了模型性能（我们的与w/o bbox loss相比）。

其次，我们探讨了X2-VLM中使用的不同类型注释数据的影响，并分别对对象数据和区域数据进行了消融。在这两种变体中同时应用多粒度对齐和边框预测损失。结果表明，这两种类型的注释对性能都很重要。对象数据提高了图像-文本检索，而区域数据对视觉定位和开放词汇属性检测至关重要。结合对象和区域数据可以产生最佳性能（我们的与w/o object和w/o region相比）。w/o X2-VLM变体，即消融多粒度对齐和边框预测损失或对象和区域数据，在所有任务中性能最差。我们还在附录中提供了对时间建模方法的消融研究，可在线查看。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/c48d1562030a44b7b50174f7af57a794.png)



### 4.7 多粒度对齐的定性研究

在本节中，我们对X2-VLM学习到的视觉语言对齐进行了定性研究。为此，我们要求X2-VLM生成图像字幕，以查看它是否可以适当地描述图像。我们还要求X2-VLM在给定手动输入描述的情况下定位图像中的视觉概念，以查看它是否可以理解图像中的细粒度对象或区域。我们使用在COCO Caption和RefCOCO+数据集上微调的X2-VLMlarge 进行此评估。我们在图4中可视化结果，其中我们选择了一些来自科学海报、视频游戏、卡通等的开放域图像。

可视化示例表明，X2-VLM可以适当地描述所有这些图像，准确理解主要角色或对象及其关系。当要求X2-VLM根据我们提供的描述定位图像中的视觉概念时，我们发现它可以捕捉背景中的小对象或部分遮挡的对象。此外，X2-VLM可以识别不同品牌的苏打水或汽车，或区分“Luffy”和“Zoro”与其他卡通角色。我们在附录中提供了更多示例，可在线查看，其中X2-VLM还可以识别“Albert Einstein”、“Edison”、“Ultraman”和“Doraemon”。这是令人惊讶的，因为我们在预训练中利用的对象或区域注释仅涉及常见对象，如“soda”、“car”或“man”。结果表明，通过大规模噪声图像-文本对训练，X2-VLM学会了在开放域图像中定位多样化的细粒度视觉概念。

## 5. 结论和讨论

在本文中，我们提出在预训练中学习视觉和语言之间的多粒度对齐。为此，我们提出了一个统一框架，用于多粒度视觉语言预训练，直接将多粒度视觉特征与配对的文本特征对齐，并同时根据不同的文本描述定位同一图像中的多粒度视觉概念。基于此，我们提出了X2-VLM，一种具有灵活模块化架构的全能预训练VLM，我们进一步统一了图像编码和视频编码，使其能够处理图像-文本任务和视频-文本任务。

我们进行了广泛的实验来验证X2-VLM的有效性。结果表明，X2-VLM在基础和大规模上大大超越了最先进的图像-文本预训练方法，在许多下游图像-文本任务中取得了良好的性能和模型规模平衡。X2-VLM也是新SoTA预训练模型，在视频-文本任务（包括视频-文本检索和视频VQA）上表现最佳。实验结果还表明，所提出的多粒度视觉语言预训练框架可扩展到大规模数据和更大模型规模。此外，我们揭示了X2-VLM模块化设计的潜力，表明它可以用于其他语言或领域。在用英语数据进行预训练后，通过将文本编码器替换为XLM-R，X2-VLM在多语言多模态任务上超越了SoTA方法。

我们还进行了深入的消融研究，以研究所提出框架中不同组件的作用。实验结果表明，多粒度定位和多粒度对齐都是所提出方法的关键组件。此外，我们还进行了定性研究，探讨了X2-VLM学习到的视觉语言对齐。我们发现，通过大规模图像-文本对训练，X2-VLM学会了在开放域图像中定位多样化的细粒度视觉概念，例如不同品牌的苏打水、汽车、角色或名人。
# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
