# [Variational Adversarial Defense: A Bayes Perspective for Adversarial Training](https://ieeexplore.ieee.org/document/10354457/)
## 题目：变分对抗性防御：对抗性训练的贝叶斯视角
**作者：Chenglong Zhao; Shibin Mei; Bingbing Ni; Shengchao Yuan; Zhenbo Yu; Jun Wang**  

****  
# 摘要

近年来，针对对抗性攻击的防御方法层出不穷。然而，这些方法普遍缺乏足够的理论保证，导致两个问题：首先，必要的对抗性训练样本的缺乏可能会削弱正常梯度的反向传播，从而导致过拟合并潜在的梯度掩盖问题。其次，逐点的对抗性采样为对抗性数据提供了不充分的支持区域，因此无法形成强大的决策边界。为了解决这些问题，我们提供了理论分析，揭示了对抗性训练中鲁棒准确性与训练集复杂性之间的关系。结果，我们提出了一种名为变分对抗性防御（Variational Adversarial Defense）的新的训练方案。基于对抗样本的分布，这种新构造从局部逐点升级到分布层面的防御方案，从而扩大了支持区域，为安全稳健的训练提供了保障，因此在防御攻击方面更有希望。所提出的方法具有以下优点：1）我们不是逐点（顺序方式）寻找对抗性示例，而是从推断出的分布中提取多样化的对抗性示例；2）通过更大的支持区域扩充训练集，巩固了决策边界的平滑性。最后，通过泰勒展开技术对所提出的方法进行了分析，使我们的解决方案具有自然的可解释性。

# 关键词

- 变分推断
- 对抗性防御
- 模型鲁棒性

# I. 引言

自从过去几年深度神经网络（DNNs）快速发展以来，在计算机视觉领域取得了一系列突破性进展[1]、[2]、[3]、[4]、[5]、[6]。然而，研究人员[7]、[8]表明，DNNs容易受到对抗性示例的攻击，其中合法输入被难以察觉的噪声扰动，导致不良的误分类甚至更严重的问题。即使没有模型知识的要求，针对一个模型制造的对抗性示例仍然可以成功攻击其他模型[9]、[10]。除了数字图像，[11]、[12]、[13]、[14] 发现在物理世界中存在对抗性示例，通过相机和其他传感器攻击机器学习系统。因此，对抗性示例对一些安全AI应用构成了高风险，例如自动驾驶、金融、监控、医学诊断等。

为了防御这些对抗性示例，许多工作[15]、[16]、[17]、[18]、[19] 提出提高模型的对抗鲁棒性，如防御性蒸馏[20]、混淆梯度[21]、对抗性训练[16]等。对抗性训练[16]是其中最有效的方法之一，它将每个批次中的所有良性示例替换为对抗性对应物。然而，在机器学习中，没有免费的午餐[22]。尽管取得了有利的防御性能，对抗性训练有时可能会损害决策边界，这严重降低了模型在干净示例上的准确性。TRADES[22]证明了在对抗性训练中，干净准确性和对抗准确性之间存在权衡。为了消除这个问题，许多对抗性训练的变体[15]、[22]、[23] 被提出，以在一定程度上补救这个缺陷。

然而，最近的对抗性防御方法存在两个问题。首先，这些对抗性防御方法很少提供关于对抗性训练密度的防御效果的理论保证。一些工作[15]、[22]、[24]、[25] 尝试分析对抗性训练的算法行为并探索对抗性训练的机制[26]、[27]、[28]。然而，缺乏揭示鲁棒准确性与对抗性训练集复杂性之间关系的理论分析，即缺乏对抗性训练规模的设计。大多数最近的对抗性防御方法都是启发式设计的。尽管这些方法取得了有利的性能，但仍存在一些问题，包括梯度掩盖[29]和过拟合[15]。

其次，先前的逐点对抗性训练方法[16]、[17]、[22]、[23] 在支持区域上是不完整的。这些方法的关键步骤是在每次参数更新时为对抗性训练制作对抗性示例，这需要在每次迭代中计算梯度。即便如此，这些方法还是以序列模型（即样本明智）生成对抗性样本，这些样本缺乏多样性，未能捕获潜在的对抗分布。换句话说，这些样本无法覆盖鲁棒训练所需的数据分布部分，或提供足够大的支持区域以形成强大且平滑的决策边界来防御各种攻击。

为了明确解决上述问题，我们首先给出了一个理论分析，建立了鲁棒准确性与对抗性示例复杂性之间关系的理论分析，从理论上表明对抗性攻击预算受到训练集大小的限制，即对抗性示例的增强保证对对抗鲁棒性有益。在这一理论的指导下，我们进一步提出了一种新的基于分布的对抗性训练方案，称为变分对抗性防御。基于良好参数化的对抗样本分布，这种新构造将防御方案从局部逐点升级到分布层面，从而获得了更大的支持区域，为安全稳健的训练提供了保障，并改善了决策边界的平滑性。动机如图1所示。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/1a9c69e244174db2b88929c28bf7fbce.png" width="70%" /> </div>


具体来说，我们首先重新审视了传统的对抗性鲁棒训练[24]，并提出了一种理论分析，以评估训练集所需的大小。我们还证明了鲁棒分类误差是有界于对抗性示例的复杂性，即训练数据的复杂性越高，鲁棒分类的性能就越好。因此，增加训练数据的复杂性可以提高模型的对抗鲁棒性。基于此，我们从概率的角度提出了一种新的对抗性训练方案。特别是，我们通过贝叶斯规则推断对抗性示例的适当分布，并从这个参数化分布中抽取对抗性示例，作为鲁棒训练的增强训练数据。由于这种基于分布的数据增强方式，训练集的支持区域可以尽可能扩大。使用增强样本进行训练可以使决策边界平滑且稳健，因此小的扰动不太可能导致样本越过决策边界，成功防御各种类型和强度的攻击。更重要的是，与每次仅制作一个示例（即顺序采样模式）不同，我们提出的方案每次可以抽取大量的对抗性示例（即批量采样模式），这得益于对抗性示例的估计分布。

此外，由于贝叶斯公式中的证据项是一个难以计算的积分，因此无法进行精确推断，我们进一步提出通过变分推断[30] [31]来估计对抗噪声的后验分布。与MCMC[32]相比，变分推断展示了更好的可扩展性，并且计算需求更少。为此，我们通过拉格朗日乘数法重新制定变分推断中的香草ELBO（优化函数），以获得强大且难以察觉的对抗噪声，从而得到一个具有良好近似和采样灵活性的变分参数化分布。更重要的是，我们通过一阶泰勒展开研究了变分对抗性防御方案，并且在理论上发现我们的形式可以被视为两个项的组合，即交叉熵损失函数和梯度正则化。这一结果使我们的解决方案具有自然的可解释性。也就是说，交叉熵损失使模型具有分类能力，而梯度正则化使模型对小扰动更加鲁棒，这从理论上进一步证明了我们方法的有效性。与使用GAN[35]生成对抗性示例的相关方法[33] [34]相比，我们提出的方法得益于变分推断，可以轻松训练并且无需复杂实现。与[36]不同，我们通过贝叶斯规则推断对抗性示例的分布，而不是解决复杂的最小最大优化问题。

值得注意的是，这项工作的主要贡献如下：
- 我们确定了鲁棒准确性与对抗示例复杂性之间的关系。我们从理论上证明了鲁棒分类误差与对抗样本的复杂性呈正相关。
- 我们提出了一种新的对抗性训练方案，称为变分对抗性防御，从概率角度出发。我们引入变分推断来估计对抗示例的分布，并进一步发展了一种通过拉格朗日方法增强的变分目标函数，以促进对抗性训练。
- 我们从推断出的变分参数化分布中采样多样化的对抗性示例，以提供支持区域，增强鲁棒训练并平滑决策边界。推断出的分布涵盖了样本分布的大部分，这大大保证了训练数据的多样性。
- 在几个基准测试上的广泛实验结果，如CIFAR、SVHN、Tiny-ImageNet和ImageNet，充分证明了与先前技术相比，我们提出的方法获得了优越的性能。

# III. 理论分析

在这一部分，我们对构建对抗性训练中鲁棒准确性与训练数据复杂性之间的关系进行了理论分析。我们证明了对抗性攻击预算受到训练集大小的限制，即对抗性示例的增强保证对提高模型的对抗鲁棒性是有益的。在理论结果的指导下，我们提出通过推断对抗性示例的分布来增强对抗性训练，这将防御方案从局部逐点方法升级为分布方法。

## A. 背景

Schmidt等人[24]已经证明了干净样本复杂性对鲁棒学习的重要性，我们重新审视了[24]中的定义，并从中推导出一些与[24]中定理相关的推论，以进一步揭示对抗性鲁棒性与样本复杂性之间的关系。按照Schmidt等人[24]的方法，我们讨论了高斯数据在线性分类器模型上的应用。

**定义 1.** （Schmidt等人）：设 $x \in X \subseteq \mathbb{R}^d$ 。我们从高斯分布 $N(\theta^ * , \sigma)$ 中抽取输入 $x$ ，其中 $\theta^ * \in \mathbb {R}^ d$ 表示每个类别的均值向量， $\sigma > 0$ 是方差参数。我们从 $Y = \{1, -1\}$ 中均匀随机抽取标签 $y$ ，然后( $\theta^*, \sigma$ )-高斯模型由 $(x, y) \in \mathbb{R}^d \times \{±1\}$ 上的分布定义如下。

按照Schmidt等人[24]的方法，标准分类误差和鲁棒分类误差的定义简要如下：

**定义 2.** （Schmidt等人）：设 $D : \mathbb{R}^d \times \{±1\} \rightarrow \mathbb{R}$ 是一个分布。分类器 $f : \mathbb{R}^d \rightarrow \{±1\}$ 的标准分类误差定义为：

$$
\text{err}_ {\text{std}} = \mathbb{E}_{(x,y) \sim D}[\lnot f(x) = y] \quad (1)
$$

**定义 3.** （Schmidt等人）：设 $D : \mathbb{R}^d \times \{±1\} \rightarrow \mathbb{R}$ 是一个分布，设 $B(0)_ \epsilon^p \in \mathbb{R}^d$ 是对抗性扰动集合，我们有 $B(0)_ \epsilon^p = \{\delta \in \mathbb{R}^d | \|\delta\|_ p \leq \epsilon\}$ 。然后分类器 $f : \mathbb{R}^d \rightarrow \{±1\}$ 的 $B(0)_\epsilon^p$ -鲁棒分类误差定义为：

$$
\text{err}_ {\text{rob}} = \mathbb{E}_ {(x,y) \in D}[\exists \Delta \in B(0)_\epsilon^p : f(x + \Delta) \neq y] \quad (2)
$$

假设一个线性分类器模型 $f_w : \mathbb{R}^d \rightarrow \{±1\}$ ，其中 $f_w(x) = \text{sgn}(\langle w, x \rangle)$ ，且 $w$ 表示可学习参数。我们可以从[24]中推导出以下推论。

**推论 3.1：** 设 $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n) \in \mathbb{R}^d \times \{±1\}$ 是从具有 $\|\theta^*\|_ 2 =  \sqrt{d}$ 和 $n = (1+4k\sigma)^2 / 4(1-k)^2$ 的( $\theta^ *, \sigma$ )-高斯模型中独立同分布地抽取的，其中 $0 < k < 1$ 。设线性分类器参数 $w \in \mathbb{R}^d$ 是 $\bar{z} = \frac{1}{n} \sum_{i=1}^{n} x_i y_i$ 方向上的单位向量，即 $w = \bar{z} / \|\bar{z}\|_2$ 。那么，以至少 $1 - 2\exp(-d / 8(\sigma^2+1))$ 的概率，线性分类器 $f_w$ 的鲁棒分类误差满足：

$$
\text{err}_{\text{rob}} \geq \exp\left(-\frac{(k - \epsilon)^2 d}{2\sigma^2}\right) \quad (3)
$$

**推论 3.2：** 如果线性分类器 $f_w$ 的鲁棒分类误差最多为 $\text{err}_{\text{rob}}$ ，且概率至少为 $[1 - 2\exp(-d / 8(\sigma^2+1))]^2$ ，那么我们有：

$$
n \geq \frac{(1 + 4\epsilon\sigma)^2}{4(1 - \epsilon)^2} \quad (4)
$$

在攻击设置下，对抗性训练所需的样本复杂性随着攻击预算 $\epsilon$ 的增长而多项式增加。我们认为，增强样本复杂性的多样性对于提高模型的鲁棒性是必要的，这与训练算法或模型族无关。

## B. 上界分析

Schmidt等人[24]讨论了对抗性泛化所需的样本复杂性。在本节中，我们扩展了[24]的上下文，揭示了模型可以容忍的对抗性攻击弹性的上限与对抗性样本的复杂性之间存在正相关关系。因此，随着对抗性样本数量的增加，模型可以处理的最大对抗性攻击强度也在增加。换句话说，我们证明了增加对抗性样本的复杂性可以提高模型的对抗鲁棒性。

**定理 1：** 设 $(x_1, y_1), ..., (x_n, y_n) \in \mathbb{R}^d \times \{±1\}$ 是从具有 $||\mu||_ 2 = \sqrt{d}$ 的( $\mu, \sigma$ )-高斯模型中独立同分布地抽取的，设 $(x_1 + \delta_1, y_1), ..., (x_m + \delta_m, y_m)$ 是对抗性示例。对抗性噪声 $\delta_i \in B(0)$ ，其中 $\delta_i$ 的期望值为零，方差为 $\sigma_r$ ，且 $f(x_i) = y_i \neq f(x_i + \delta_i)$ 。设均值向量 $\bar{z} = \frac{1}{m} \sum_{i=1}^{m}(x_i + \delta_i)y_i$ 和单位向量 $\hat{w} = \bar{z} / \|\bar{z}\|_ 2$ 。那么，在高概率下，线性分类器 $f_{\hat{w}}$ 的 $\epsilon$ -鲁棒分类误差最多为 $\text{err}_{\text{rob}}$ ，如果

$$
\epsilon \leq \frac{\sqrt{m} - (\sigma + \sigma_r)}{\sqrt{d}} \cdot \frac{\sqrt{m} + (\sigma + \sigma_r)}{\sqrt{d}} - \frac{\sigma}{\sqrt{2 \log(\text{err}^{-1}_{\text{rob}})}}
$$

随着对抗性样本数量的增加，模型可以承受的对抗性攻击的上限也随之增加。这意味着，通过增加对抗性样本的复杂性，可以提高模型的对抗鲁棒性。基于此，有两种可行的方法来促进模型的鲁棒性。第一种是减少样本的方差。一些工作[17]、[76]提出了一种新的设计损失，扩大类间距离并减少类内距离，以获得具有区分性的表示。这样，可以有效地扩大不同类别之间的边界距离，使得受到对抗性扰动的输入图像仍然停留在原始类别区域，而不会越过决策边界。第二种是增加训练数据集的大小。流行的对抗性训练方法[16]将对抗性示例纳入训练集，并在几个基准测试上取得了良好的性能。此外，Gowal等人[97]引入了未标记数据来改进鲁棒训练。直观地说，通过对抗性示例来增加训练集可以促进决策边界曲线的平滑，其中在输入上的微小扰动不会导致误分类。

在这项工作中，我们不是逐个制作示例，而是提出了一种简单有效的方法，通过估计对抗性示例的分布来增加对抗性示例。更多细节将在下一节中讨论。

# IV. 变分对抗性防御

在本节中，我们提出了一种新的方法来提高模型的对抗鲁棒性，称为变分对抗性防御。具体来说，我们首先提出通过变分贝叶斯推断来估计对抗扰动的分布。之后，我们可以从这个估计的分布中抽取大量对抗性示例，作为支持区域来增强对抗性训练，而不是逐个制作示例，如图3所示。直观地说，推断出的分布覆盖了数据流形的更大区域，因此从中抽取的样本非常多样化，这在很大程度上弥补了原始样本的不足。此外，我们还提供了理论证据，表明增加对抗性示例的数量对提高对抗鲁棒性是有益的，如上一节所述。最后，我们通过泰勒展开分析变分对抗性防御，并发现所提出的防御方法等同于标准训练方案，加上一阶梯度的正则化。这一结果使我们的方法具有自然的可解释性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/b37f54388f23467ba9768cf589d86edc.png" width="70%" /> </div>


## A. 重新审视对抗性训练

对抗性训练[16]是提高模型对攻击鲁棒性的最流行方法之一，它采用最小-最大优化策略来寻找最坏情况的示例，并在这些示例下最小化经验风险，如下所示：

$$
\min_{w} \mathbb{E}_ {(x,y) \sim D} \left[ \max_{||\delta||_p \leq \epsilon} l(f_w(x + \delta), y) \right]
$$

其中D表示训练集， $f_w$ 是带有可学习参数w的训练模型，l(·)表示损失函数（例如交叉熵损失）。 $\delta$ 表示对抗性噪声，其幅度被限制在一个小的范围内。传统的对抗性训练使用强大的PGD[16]攻击来生成对抗性示例作为最坏情况的样本。PGD攻击是FGSM[66]的迭代变体，它通过较小的步长更新，并在多个迭代中将更新后的对抗性示例剪切到一个有效范围内，如下所示：

$$
\delta_{t+1} = \text{Proj}_{||\delta||_p \leq \epsilon} (\delta_t + \alpha \cdot \text{sign}(\nabla_x l(f_w(x + \delta_t), y)))
$$

这样的迭代方法在生成对抗性示例时是不完整的，尤其是当对抗性训练需要更多示例时。因此，有必要增加对抗性示例的多样性[15]。

为了解决这个问题，增加训练示例的数量是避免潜在过拟合的一个可行方法，这也在机器学习社区中得到了广泛应用。如第III节所讨论的，我们还证明了增加对抗性示例的数量可以增加对抗性攻击预算 $\epsilon$ 的上限（即对抗性噪声的范数）。为了提高对抗性训练的效率，我们提出通过对抗性示例的分布来优化对抗性训练，而不是仅仅针对最坏情况的点：

$$
\min_{w} \mathbb{E}_ {(x,y) \sim D} \left[ \mathbb{E}_{\delta \sim Q} l(f_w(x + \delta), y) \right]
$$

其中Q表示对抗扰动的分布。现有方法[16]在单点最坏情况下优化模型，这可能会导致潜在的过拟合[15]、[98]。所提出的方法试图在对抗性分布下优化模型，将问题从局部逐点方法转变为分布方法。为此，我们提出估计对抗性示例Q的分布，然后从这个估计的分布中抽取样本以增强对抗性训练。因此，可以轻松获得大量对抗性示例，以作为支持区域，弥补原始样本的不足，并增加多样性。因此，我们认为所提出的方法可以在一定程度上避免对抗性训练中的过拟合问题，并提高模型的鲁棒性。我们注意到，新提出的优化过程(8)类似于分布鲁棒优化(DRO[99])，在鲁棒学习中非常流行。如图4所示，所提出的方法只需要推断一次分布，就可以从中抽取多个样本，而其他方法需要多次推断过程。我们从参数化的分布中抽取对抗性示例，并将其用作增强训练数据。这种数据增强策略旨在扩大训练集的支持区域，以便将防御方案从局部逐点方法升级为分布方法。也就是说，所提出的方法试图在底层对抗性分布下优化模型，而不是单一的单点情况，从而可能最大程度地避免过拟合问题[15]、[98]。总的来说，所提出的方法具有以下优点：

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/fa72d1a0efd341f4b9e4761b46636c85.png" width="70%" /> </div>


1. 我们提出估计对抗性扰动的分布，以替代对抗性训练中的最坏情况示例。估计的分布可以被视为保护对抗性训练的支持区域。
2. 我们从估计的分布中抽取对抗性示例，这可以增加训练数据的大小并增加对抗性示例的多样性。


## B. 通过变分贝叶斯估计分布

在本节中，我们提出通过贝叶斯规则来推断对抗性示例的分布。假设输入数据 $(x, y)$ 和参数化模型 $f_w(\cdot)$ ，其中 $y$ 是输入 $x$ 的真实标签，即 $y = f_w(x)$ 。设 $\delta$ 表示对抗性噪声，我们有 $y \neq \hat{y} = f_w(x + \delta)$ 。如上所述，我们的目标是模拟对抗性噪声 $\delta$ 的后验分布。根据贝叶斯规则， $\delta$ 的概率分布函数可以表述为：

$$
p(\delta|\hat{y}, x) = \frac{p(\hat{y}, x|\delta)p(\delta)}{p(\hat{y}, x)}
$$

其中 $p(\delta)$ 是关于对抗性噪声的先验分布。注意，在贝叶斯规则中的边缘似然 $p(\hat{y}, x)$ 是证据项，这是一个高维积分项，即 $p(\hat{y}, x) = \int p(\hat{y}, x, \delta) d\delta$ 。然而，这样的积分项是难以计算的。因此，直接通过贝叶斯规则来精确推断对抗性噪声的分布是不可行的。

另一种方法是变分推断，即对后验分布的近似估计。由于其可扩展性和与MCMC[100]相比更好的收敛性，变分推断[30]被引入来估计对抗性噪声的后验分布。特别是，变分推断的主要原理是基于最小化近似分布和真实后验分布之间的KL散度。设参数化近似分布为 $q_\theta(\delta|x)$ ， $\theta$ 表示可学习参数。KL散度用于衡量 $q_\theta(\delta|x)$ 和 $p(\delta|\hat{y}, x)$ 之间的分布距离，如下所示：

$$
q_{\theta^ * }(\delta|x) : \theta^* = \arg\min_{\theta \in \Omega} \text{KL}(q_\theta(\delta|x)||p(\delta|\hat{y}, x)), 
$$

其中 $\Omega$ 表示参数空间。我们的目标是通过最小化KL散度来找到最优值 $\theta^*$ ，并得到近似的后验分布。

## C. 证据下界

最小化KL散度等同于最大化证据下界(ELBO)，如下所示：

$$
L(\theta) = \mathcal{L}_ D(\theta) - \text{KL} (q_ \theta(\delta)||p(\delta)), 
$$

其中 $\mathcal{L}_ D(\theta) = \mathbb{E}_ {\delta \sim q_ \theta(\delta|x)} [\log p( \hat{y}, x| \delta)]$ 。

目标函数通过调整参数 $\theta$ 来达到最优值，其中(11)由两部分组成，即期望对数项和KL散度。期望对数项旨在使采样的示例被错误标签分类，即增强采样示例的攻击性能，这些示例可以成功欺骗给定的神经网络。KL散度项是一个正则化项，它确保对抗性噪声示例在小幅度内。具体来说，引入了一个先验分布来限制对抗性噪声是难以察觉的。我们的观点来自于在期望对数和KL散度之间权衡问题。通过这种方式，我们可以推断出适当的分布，以获得足够强大且难以察觉的对抗性示例。

此外，为了进一步控制正则化项的有效机制，我们将原始证据下界作为一个受约束的优化问题重新表述如下：

$$
\max_{\theta} \mathbb{E}_ {\delta \sim q_\theta(\delta|x)}[\log p(\hat{y}|x, \delta)] \quad \text{s.t.} \quad \text{KL}(q_\theta(\delta|x)||p(\delta)) \leq \epsilon
$$

KL散度的不等式约束将近似分布与先验分布之间的距离限制在某个约束程度 $\epsilon$ 以内，这也表示为扰动预算。目标函数最大化边缘期望对数似然，这保证了采样的对抗性示例可以成功欺骗DNN模型。为了解决受约束优化问题(13)，我们在KKT条件下结合拉格朗日乘数法重新表述它，如下所示：

$$
\mathcal{L}_ \alpha(\theta) = \mathbb{E}_ {\delta \sim q_\theta(\delta|x)}[\log p(\hat{y}|x, \delta)] - \alpha(\text{KL}(q_\theta(\delta|x)||p(\delta)) - \epsilon)
$$

其中 $\alpha$ 作为一个可调参数，用于在期望对数和正则化项之间实现平衡。此参数还有利于加速优化过程的收敛。

对于(12)中的期望对数项，期望的计算不是可微的，这将阻碍梯度反向传播。因此，我们引入了重参数化技巧[30]，以获得用于计算期望对数似然的无偏可微的基于小批量的蒙特卡洛估计器，如下所示：

$$
\mathbb{E}_ {\delta \sim q_\theta(\delta|x)}[\log p(\hat{y}|x, \delta)] \approx \frac{1}{M} \sum_{m=1}^{M} \log p (\hat{y}, x|\delta_m = f(\theta, \epsilon)), 
$$

其中， $\epsilon \sim \mathcal{N}(0, 1)$ 。

其中M是小批量大小，即样本数量。分布 $q_\theta(\delta|x)$ 被表述为一个可微函数 $\delta = f(\theta, \epsilon)$ 。参数 $\theta$ 表示估计分布的均值和方差，即 $\theta = (\mu, \sigma)$ 。因此，采样的对抗性示例 $x$ 可以表示为 $x = \mu + \sigma \cdot \epsilon$ ，其中 $\epsilon$ 是高斯噪声。

(13)中的KL散度度量了后验分布和先验分布之间的距离。按照均值场模型，我们将两个分布都设为完全分解的高斯分布。得益于这种解耦方式，计算KL散度变得简单，并且可以轻松扩展到大型数据集。我们的目标是通过更新可学习参数 $\theta = (\mu, \sigma)$ 来优化目标函数，其中 $\mu$ 和 $\sigma$ 分别是估计分布的均值和方差。在这项工作中，我们将先验初始化为零均值的高斯分布。我们将先验分布的方差设置为一个可以忽略的值，这使得高斯噪声更加难以察觉。为了简化计算，我们固定了两个分布的方差，以便可以轻松计算KL散度。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/af6792b591c5444e9243be70971a0904.png" width="70%" /> </div>


## D. 变分对抗性防御和梯度正则化

在估计了对抗性噪声的分布 $q_\theta(\delta|x)$ 之后，我们可以重新制定标准的对抗性训练（6）如下：

$$
\min_{w} \mathbb{E}_ {(x,y) \sim D} \left[ \mathbb{E}_ {\delta \sim q_\theta(\delta|x)} \left[ l(f_w(x + \delta), y) \right] \right] \quad (17)
$$

我们在对抗性噪声的分布期望下计算目标函数 $l(\cdot)$ ，并在整个数据集上优化模型的参数 $w$ 。训练流程如算法1所示。如（16）所示，我们还使用蒙特卡洛方法来计算（17）中的期望项。因此，对抗性扰动是从估计的分布中抽取的，以制作相应的对抗性示例。与通过PGD[16]逐个制作示例不同，我们可以从估计的分布中获得大量的对抗性示例作为支持区域，以促进鲁棒训练。我们称这种方法为变分对抗性防御。注意，所提出的方法可能会因为模型参数是迭代更新的而错过一部分可能的对抗性示例。然而，随着训练的进行，模型逐渐收敛到参数空间中的一个稳定点，学习率也会减小到一个小值，导致参数的更新非常有限。也就是说，模型参数的更新对我们的方法影响非常有限。

为了进一步研究所提出的变分对抗性防御的效果，我们使用泰勒展开来分析（17）。具体来说，我们令 $l(f_w(x + \delta), y)$ 表示为 $l(x + \delta, y, w)$ 。然后， $l(\cdot)$ 的一阶泰勒近似可以表示为：

$$
l(x + \delta, y, w) \approx l(x, y, w) + \delta^T \nabla_x l(x, y, w) + o(\delta^n) \quad (18)
$$

其中 $l(\cdot)$ 是关于 $x$ 的可微函数。将（18）代入（17），我们得到：

$$
\min_{w} \mathbb{E}_ {(x,y) \sim D} \left[ \mathbb{E}_ {\delta \sim q_\theta(\delta|x)} \left[ l(f_w(x + \delta), y) \right] \right] \approx \min_{w} \mathbb{E}_ {(x,y) \sim D} \left[ \mathbb{E}_ {\delta \sim q_\theta(\delta|x)} \left[ l(x) + \delta^T \nabla_x l(x) \right] \right] = \min_{w} \mathbb{E}_ {(x,y) \sim D} \left[ l(x) + \mu(q_\theta)^T \nabla_x l(x) \right] \quad (19)
$$

其中 $\mu(\cdot)$ 表示分布的均值。因此，有两个有趣的属性。首先，变分对抗性防御等同于标准训练（交叉熵损失）加上一阶梯度正则化。显然，标准训练可以最小化模型的标准误差，而梯度正则化项使模型满足Lipschitz连续性，这保证了对抗性鲁棒性。其次，由于估计的分布 $q_\theta(\delta|x)$ 是一个参数化的高斯分布，其均值可以直接获得。因此，采样操作可以简化为一阶梯度正则化， $\mathbb{E}_ {\delta \sim q_\theta(\delta|x)}[\delta^T \nabla_x l(x)] = \mu(q_\theta)^T \nabla_x l(x)$ 。一阶梯度展开将所提出的变分对抗性防御简化为简化形式，即交叉熵损失和梯度正则化项。简化形式与先前使用梯度正则化增强对抗性训练的工作[26]、[27]、[28]一致。也就是说，我们可以用正则化来解释我们方法的有效性。

# V. 实验

在本节中，我们进行了广泛的实验，以评估我们提出的方法在几个图像分类任务上的性能。实验充分证明了我们的方法有效地提高了模型对对抗性攻击的鲁棒性，并与其他防御方法相比取得了更好的性能。我们的实现基于PyTorch。

### A. 实施细节
数据集：我们在包括CIFAR-10、CIFAR-100、SVHN、Tiny-ImageNet和ImageNet的三个数据集上进行实验。对于所有数据集，我们使用整个测试集进行鲁棒性评估。对于CIFAR-10和CIFAR-100数据集，我们使用ResNet-50 [3]，对于SVHN数据集，我们使用ResNet-34 [3]。对于所有数据集，我们运行40 k训练步骤，固定批量大小为128。我们将初始学习率设置为0.01，并在达到总训练步骤的50%、75%、87.5%时，将学习率乘以衰减因子0.1。应用SGD优化器，动量为0.9，权重衰减为2e-4。对于变分贝叶斯分布推断，我们经验性地将参数α和ϵ分别设置为0.00001和10。

防御方法：为了验证我们方法的有效性，我们将性能与几种训练方法进行了比较，包括1)Vanilla：在标准设置下，从头开始在干净图像上训练的模型，2)PCL[76]：在表示学习上强制执行边界约束以防御攻击，3) GCE [17]：一种指导补充熵，以提高模型的鲁棒性，4) AT [16]：基于PGD的对抗性训练，这是最经典的防御方法，5) AVmixup [15]：通过线性插值扩展训练数据的对抗性训练。6) WP [77]设计了一种特殊的权重惩罚机制来防御对抗性扰动。7) LS [75]增加了类间距离以获得鲁棒模型。8) MMC [29]诱导同一类别的样本聚集在一起，以提高模型的鲁棒性。9) MART [23]重新利用错误分类样本，使模型意识到对抗性示例。10) TRADES [22]在鲁棒性和清洁准确性之间取得了很好的平衡。11) HE [78]在球面上优化模型。12) LL [26]引入梯度识别以增强鲁棒学习。13) AWP [73]简化了PGD的梯度计算，以降低对抗性训练的训练成本。

对抗性攻击：我们全面评估了所提出方法在白盒和黑盒攻击下的鲁棒性。对于白盒攻击，引入了最流行的攻击方法，包括FGSM(快速梯度符号方法)[8]、PGDT(T代表攻击步骤)[16]、CW [40]攻击、APGD [41]和Auto-Attack(AA) [41]。对于FGSM和PGDT攻击，扰动的大小设置为ϵ = 0.03，PGDT的步长设置为0.007。通过简单地增加攻击迭代次数，可以加强PGD攻击，因此我们设置了一系列的PGD攻击，以在不同强度下评估防御方法。对于黑盒攻击，引入了流行的黑盒攻击方法，包括自然进化策略(NES) [101]、简单黑盒攻击(SimBA) [102]、Nattack [51]和子空间攻击[59]。NES通过蒙特卡洛采样估计梯度，然后与现成的白盒方法(即FGSM和PGD)结合，形成黑盒攻击。Nattack是NES的改进版，专注于估计对抗性示例的分布。SimBA在L∞约束下对每个输入执行正交空间中的对抗性示例搜索。子空间攻击引入了替代模型的梯度来增强攻击能力。将查询次数限制为2000。在实验中，所有设置都是固定的。

## B. CIFAR和SVHN上的结果
在这一部分中，我们评估了我们的防御方法在CIFAR-10和CIFAR-100数据集下的鲁棒性。引入了几种流行的对抗性防御方法，包括PCL [76]、WP[77]、LS[75]、AT[16]、MMC[29]、HE[78]、AdvMixup[15]、TRADES [22]和MART [23]，与我们的方法进行比较。我们还应用了FGSM [8]、PGD10 [16]、PGD20、PGD50、PGD100、CW [40]和自动攻击(AA) [41]作为攻击者，以评估所有这些防御方法。

如表I所示，我们的方法在CIFAR-10和CIFAR-100数据集上都取得了有利的性能，特别是，与其他先前的方法相比，我们的方法在强大的对抗性攻击下获得了更好的性能。我们方法在干净图像上的分类准确率下降得不像基于PGD的对抗性训练那么戏剧性，这意味着我们提出的方法在清洁和鲁棒准确性之间取得了很好的平衡。尽管我们方法在FGSM攻击下的防御能力不如AVmixup，但在基于PGD的攻击下，我们方法的鲁棒性明显优于AVmixup。此外，随着PGD攻击的迭代次数增加，我们方法的性能比AVmixup更稳定。我们认为，AVmixup利用了一种线性增强，这在线性攻击者(例如FGSM)上允许良好的性能。然而，我们的方法在更强大的攻击者(例如PGD、CW、AA)下显示出更好的结果，我们相信这是由于估计的对抗性示例分布提供了更多样的样本，有助于对抗性鲁棒性。此外，我们的方法在CW和AA攻击中取得了最佳性能。因此，我们假设AVmixup可能在FGSM攻击上过拟合了。与TRADES和MART(AT的高级变体)相比，我们的方法在干净和对抗性示例上也表现出更好的性能。我们还比较了我们的方法与基于判别表示的防御方法，即PCL、WP、HE和MMC。我们在干净图像上取得了类似的性能，而在对抗性攻击下，我们拥有更好的防御能力。如表II所示，我们还对包括ADA [84]、ANT [85]和MAT [86]在内的方法进行了实验，这些方法通过生成噪声来增强鲁棒训练。显然，我们的方法在各种对抗性示例上都取得了最佳的准确性。这是因为推断出的分布提供了多样性和足够的对抗性示例，从而形成了更大的支持区域，有助于平滑决策边界。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/55ba38c5e99742cba9011693bacc5076.png" width="70%" /> </div>
<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/2b9d091873ec4275a20794c41c5b66b2.png" width="70%" /> </div>


为了进一步验证和评估我们方法的鲁棒性，我们在公共的SVHN(The Street View House Numbers)数据集上进行了类似的实验。我们使用几种攻击，如FGSM、PGD10、PGD20、PGD50等，来评估模型的对抗性鲁棒性。如表I所示，我们提出的方法在干净和鲁棒准确性上都取得了最佳性能。与以前的工作相比，我们提出的方法在这些基于迭代的攻击下取得了更好的对抗性防御性能。注意，我们的方法在迭代次数增加时性能稳定。对于PGD攻击，我们在不同迭代下获得了超过60%的分类准确率。即我们提出的方法在不同程度上的对抗性攻击下表现出稳定的性能。因此，我们可以推断出，出色的性能得益于从推断出的分布中采样更多样的对抗性示例。也就是说，样本复杂性作为支持区域扩大，有助于保护鲁棒训练，决策边界平滑地细化，以增强对抗性。

## C. Tiny-ImageNet和ImageNet上的结果
我们在Tiny-ImageNet上进行实验，以验证提出方法在大规模数据集上的性能。我们引入了几种对抗性攻击，如FGSM、PGD、CW和AA，以验证鲁棒模型的防御。如表III所示，提出的方法在与其他方法相比较时，获得了最佳清洁准确率，包括PCL、LS、AT、TRADES和MART。与普通模型相比，我们的方法只降低了约4%的准确率，但取得了明显的防御性能。对于Tiny-ImageNet，我们的方法在各种攻击方法下也获得了稳定的结果。我们认为所有这些有利的性能得益于我们的方法可以从估计的分布中抽取许多对抗性示例作为支持区域，以弥补训练集的不足，并可能避免过拟合问题。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e52de34b0e7248bcb565162b723c6fdb.png" width="70%" /> </div>


为了进一步验证我们方法在大规模数据集上的性能，我们在ImageNet数据集上进行实验，并与以前的工作进行比较，包括AT [16]、TRADES [22]和MART [23]。我们使用ResNet-50作为所有这些对抗性防御方法的骨干进行训练。如表IV所示，与这些对抗性防御方法相比，提出的方法在干净和鲁棒准确性上都取得了最佳性能。这是因为我们的分布式训练方案产生了更大的支持区域，有助于保护鲁棒训练，最有可能防止过拟合。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/70efa6b885734b5ab26cfa8b9771fdaa.png" width="70%" /> </div>


## D. 训练过程分析

如Fig. 5所示，我们展示了在CIFAR-10和CIFAR-100数据集上，训练过程中PGD20和PGD50攻击下的模型训练精度和测试精度的变化曲线。我们还比较了与AT [16]和AdvMixup [15]的精度曲线。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/1445eac0205341b0a98b809cba9c7b65.png" width="70%" /> </div>


在训练精度和测试精度的准确性曲线中，与对抗性训练[16]相比，所提出的方法显著减少了训练集和测试集之间的泛化差距。此外，所提出的方法显示出在不同迭代次数的PGD攻击下稳定防御的能力，这极大地验证了通过更大的支持区域估计对抗性分布可以安全地进行鲁棒训练。与Mixup [15]相比，后者对对抗性示例执行线性插值，所提出的方法在强大的PGD攻击下获得了更好的性能。

## E. 对黑盒攻击的评估

Athalye等人[103]确定了混淆梯度，这在对抗性示例的防御中导致了对安全性的虚假安全感。我们执行黑盒攻击实验以验证我们的方法中没有出现混淆梯度。（注意，在下一节的压力测试实验中，随着攻击预算增加，鲁棒性的下降也可以证明没有混淆梯度。）我们使用的黑盒攻击算法包括1) NES [101]，一种基于自然进化策略的梯度估计黑盒攻击方法，2) Nattack [51]，NES的变体，显著提高了攻击能力，专注于估计对抗性分布，3) SimBA [102]，在L∞约束下对样本施加对抗性噪声的贪婪估计，4) SubSpace Attack [59]，结合了基于转移的黑盒攻击和基于梯度估计的黑盒攻击，考虑了攻击成功率和查询次数。5) Square Attack [42]，通过执行随机搜索方案来改进查询效率的基于正方形的黑盒攻击。所有攻击方法的查询次数限制为2000次，CIFAR-10上的所有实验都使用相同的攻击参数进行公平比较。L∞约束ϵ设置为0.03。如Table V所示，与先前技术相比，我们的方法在黑盒攻击下表现出色，这表明所提出的变分训练方案有效提高了模型的对抗鲁棒性，并证明了我们的方法中没有混淆梯度。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/cbf72e96acc2439399bdd67eca1c3309.png" width="70%" /> </div>


如Tables VI和VII所示，我们在CIFAR100和SVHN数据集上补充了额外的实验，以评估我们的方法对黑盒攻击的性能。如Tables VI和VII所示，我们展示了在各种流行的黑盒攻击下，我们的方法和其他防御策略的准确性，例如NES、Nattack、SimBA、SubAttack和SAttack。我们的方法在CIFAR100和SVHN数据集上在准确性方面优于先前的防御方法，这极大地展示了我们的方法对黑盒攻击的优越防御能力。这可以归因于所提出的变分方法，它推断出一个适当的分布，提供了多样化且充足的对抗性示例来增强鲁棒训练，从而形成了一个平滑且鲁棒的决策边界。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/090576a0b91a4d13b83ae5302996121a.png" width="70%" /> </div>


## F. 时间成本分析

如Table VIII所示，我们评估了我们的方法与先前技术的时间成本，包括对抗性训练(AT) [16]、TRADES [22]和MART [23]在CIFAR-10和CIFAR-100上的时间成本。所提出的方法首先估计对抗性示例的分布，然后从分布中抽取示例进行鲁棒训练。也就是说，一旦推断出分布，我们可以以很小的计算消耗抽取对抗性示例。因此，我们方法的主要时间成本包括分布推断和对抗性训练。在这项工作中，我们经验性地将采样数量m设为5。因为少量的示例无法为保护对抗性训练提供足够的多样性，而过多的示例将耗费大量的计算。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/fdff7e8033fa46218f9a3b49d6d8d256.png" width="70%" /> </div>


当m设为1时，我们的方法达到了与对抗性训练相似的性能。随着采样数量m的增加，我们的方法在干净和鲁棒准确性方面都取得了越来越好的性能，同时时间消耗也有所妥协。当m设为5时，我们的方法在AT、TRADES和MART中，在干净样本和基于PGD的对抗性示例上都取得了最佳性能。我们认为这是由于从推断出的分布中抽取的增强型对抗性示例（AEs）提供了更多样化的样本，形成了鲁棒且平滑的决策边界，以防御各种攻击。AT和TRADES包括两个阶段：样本生成和模型训练。我们的方法包括三个阶段：分布推断、样本抽取和模型训练。样本生成（AT和TRADES）和分布推断（我们的方法）都通过多次迭代梯度反向传播来执行。在这项工作中，我们为这两个步骤设置了相同数量的迭代次数，因此它们的时间消耗是相等的。样本抽取的时间复杂度为O(n)，我们在论文中将采样数量限制为5。因此，采样过程所消耗的时间可以认为是微不足道的。此外，两种方法的模型训练（AT和我们的方法）是相同的。因此，所提出的方法与AT和TRADES具有相似的时间消耗。实际上，所提出的方法可以被视为一个模块，它生成足够的对抗性噪声以增强对抗性训练。与生成网络不同，我们通过从估计的分布中采样，以可忽略的计算成本抽取多样化的对抗性示例。具体来说，我们的方法通过从估计的分布中采样生成对抗性噪声，计算复杂度为O(n)。此外，我们的方法不会增加任何推理时间成本。

## G. 攻击预算ϵ下的鲁棒性

在本节中，我们在CIFAR-10和CIFAR-100数据集上测试了我们方法的鲁棒性，通过逐步增加对抗攻击者的攻击预算ϵ。所有实验都在ResNet-50模型上进行。在压力测试中，对抗性示例的扰动预算ϵ受到ϵ限制的Lp球的约束，界限从0.02上升到0.08。攻击预算ϵ越高，对抗攻击就越强。对于PGD攻击，我们将攻击步长设置为ϵ/5。如图6所示，我们增加了ϵ/5来评估我们的方法在不同压力下的表现。我们使用PGD10和PGD20评估模型的鲁棒性，因此，实验结果可以从两个方面反映攻击预算的增加：L∞约束和迭代次数。如图6所示，随着攻击强度的增加，其他方法的性能越来越差。我们观察到，我们的方法在广泛的攻击预算范围内更加稳健，与其它防御方法相比，只有轻微的性能下降。也就是说，所提出的方法在不同的攻击预算下表现稳定。原因是对抗性示例的估计分布为促进鲁棒训练提供了多样化的训练数据。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/e9d38b9af4b440ce89b9c95bd9d34c1d.png" width="70%" /> </div>


## H. 参数分析和结果可视化

在本节中，我们讨论了样本方差σ参数的敏感性，即应用不同的采样方差来获得对抗性训练的对抗性示例。防御模型的结果在CIFAR-10和CIFAR-100上报告。结果如Table IX所示。当σ在CIFAR-10数据集上从0.4变化到0.75，在CIFAR-100数据集上从0.4变化到0.6时，我们方法的鲁棒性保持稳定。在这项工作中，为了保持良好的性能，我们经验性地将采样分布方差分别设为0.6、0.4和0.5。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/9df06480911c4210885b9de6899bdeac.png" width="70%" /> </div>


如图7所示，我们在CIFAR-10和SVHN上展示了我们方法生成的对抗性示例的可视化。为了使结果更具代表性，我们从数据集中随机选取了12张图像。如图7所示，第一行和第三行显示原始图像，第二行和第四行显示我们方法生成的对抗性示例。如图中所示，我们方法生成的对抗性示例与原始图像难以区分。我们发展了通过拉格朗日方法增强的变分目标函数，以获得更可控的对抗示例约束，从而生成稳定且准确的梯度来制作对抗性示例。

如图8所示，我们通过T-SNE技术将对抗性示例投影到二维空间。我们从CIFAR-10中随机选取一张图像，并从估计的分布中抽取1000个对抗性示例。如图中所示，我们方法生成的对抗性示例比PGD用随机重启生成的示例具有更高的多样性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/45d6cf93ea0b4137a1ff6a4c5624739b.png" width="70%" /> </div>


如Table X所示，我们选择了AT，这是一种最广泛使用的基线方法，来研究其在不同攻击参数下的性能。具体来说，我们使用PGD攻击方法为训练AT创建对抗性示例，同时在训练过程中随机化PGD攻击参数。如Table X所示，AT-rand指的是带有随机化PGD攻击的AT方法。很明显，我们的方法在各种对抗性攻击下，包括FGSM、PGD10、PGD20和PGD50，都优于AT-rand。如Table X所示，我们观察到AT-rand与固定PGD参数的AT相比没有显著改进。因此，这种随机参数化方法并没有增强样本多样性和模型性能。这一结果也得到了我们的TSNE可视化的支持，我们方法生成的对抗性示例显示出比用随机参数的PGD生成的示例更大的多样性。

<div align=center>   <img src="https://img-blog.csdnimg.cn/direct/0184bddfd0b6433ba44a3e309a7d7424.png" width="70%" /> </div>

# VI. 结论

在本文中，我们提供了对对抗性训练中鲁棒准确性与训练集复杂性之间关系的理论分析。为此，我们提出了一种新的方法，通过变分推断来估计对抗性示例的分布。与迭代地制作对抗性示例不同，我们从估计的分布中便宜地抽取它们以促进鲁棒训练。此外，我们通过拉格朗日方法开发变分目标函数，以获得更可控的估计分布。我们还通过一阶泰勒展开分析了所提出的变分对抗性防御，并发现我们的方法可以分解为交叉熵损失函数和梯度正则化，这明确地自然解释了我们方法的有效性。最后，我们在几个流行的数据集上进行了广泛的实验，充分证明了所提出的方法有效地提高了模型的对抗性鲁棒性，并与其他方法相比取得了优越的性能。

# 声明

本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。
