# 题目：[DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition With Limited Annotations (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10373051)  
## DualCoOp++：一种快速高效的有限注释多标签识别方法
**作者：Ping Hu；Ximeng Sun；Stan Sclaroff；Kate Saenko** 

****
# 摘要
在低标签状态下的多标签图像识别是一项极具挑战性和实际意义的任务。之前的工作主要集中在学习文本和视觉空间之间的对齐，以弥补图像标签的不足，但由于高质量多标签注释的缺乏，可能会导致精度下降。在这项研究中，我们利用了通过数百万辅助图像-文本对预训练的强大文本和视觉特征对齐能力。我们引入了一种高效的框架，称为证据引导的双上下文优化 (DualCoOp++)，作为解决部分标签和零样本多标签识别的统一方法。在 DualCoOp++ 中，我们分别为目标类别编码证据、正面和负面上下文，作为语言输入的参数化组件（即提示）。证据上下文旨在发现目标类别的所有相关视觉内容，并作为指导，从图像的空间域中聚合正面和负面上下文，从而更好地区分相似类别。此外，我们引入了一个赢家通吃模块，在训练期间促进类别间的互动，同时避免了额外的参数和成本。由于 DualCoOp++ 对预训练的视觉语言框架施加了最小的额外学习开销，它能够快速适应有限注释甚至是未见类别的多标签识别任务。在两个具有挑战性的低标签设置下，对标准多标签识别基准的实验表明，我们的方法相比于最先进的方法具有更高的性能。

# 关键词
- 多标签图像识别
- 部分标签识别
- 视觉语言模型
- 零样本识别



## I. 引言

近年来，由于大规模数据集的开发 [1]，[2] 和先进模型架构的发展 [3]，[4]，[5]，[6]，图像识别已经成为一个非常流行和成功的研究领域。然而，大多数图像识别方法都集中在单标签预测上，这忽略了图像的内在多标签性质。与单标签识别 [3]，[4]，[5]，[6] 不同，多标签图像识别旨在识别图像中存在的所有语义标签 [7]，[8]，[9]，[10]，[11]，[12]，[13]，提供更全面的理解，并有助于图像检索、视频分析和推荐系统等应用。

多标签识别 (MLR) 通常处理复杂场景和多样对象的图像。收集多标签注释变得难以扩展，有两个原因：(i) 对图像进行完整的语义标签集注释是费力的，(ii) 特定类别的样本可能难以找到。第一个挑战可以通过部分标签的多标签识别来解决，每个训练图像仅注释部分类别。最近的工作提出了基于半监督学习 [14]，[15]，规范化训练目标 [16] 或标签相关性 [17]，[18]，[19] 的部分标签 MLR 解决方案。第二个设置涉及零样本 MLR，通过从已见类别转移知识来识别新的未见类别，解决方案包括主要图像特征 [20]，[21]，知识图 [22] 和注意机制 [23]，[24]。尽管在这两个设置中取得了显著进展，但现有方法并未设计成同时处理这两者。我们提出将这些设置统一为有限注释 MLR，并设计一种能够处理部分或缺失标签实际场景的解决方案。

解决上述问题的成功解决方案通过学习图像和类别名称之间的对齐，将知识从完全注释的类别转移到部分注释和新的类别 [17]，[19]，[21]。最近，视觉语言预训练模型通过大规模预训练弥合了视觉和文本之间的差距，例如，CLIP [27] 训练了 4 亿图像-文本对。在这项工作中，我们从这些模型的提示学习最近成功中获得启发 [25]，[28]，[29]，[30]，[31]，[32]，[33]，[34]，[35]，[36]。提示学习提供了一种将预训练的视觉语言模型转移到其他任务的方便方法。它设计了额外的模板化或可学习的提示令牌作为文本输入，以“告知”模型下游任务，并避免对整个模型进行微调，这可能效率低下且数据需求大。通过这样做，最近的工作如 CoOp [25] 展示了 CLIP 在各种零样本图像任务中的显著泛化能力 [25]，[27]，[28]。然而，这些方法主要集中在将每个图像与单个标签匹配，因此无法处理多标签设置。

为了将 CLIP 中学到的知识适应多标签图像识别，我们在这项工作的会议版本中提出了 DualCoOp。如图 2(d) 所示，DualCoOp 学习了一对可微提示，为目标类别提供正面和负面上下文。与仅关注正面预测 [25] 相比，双提示自然产生了两个独立且互补的正面和负面预测分类器。通过这种方式，该方法促进了多标签图像识别中不同注释类型的更平衡学习，并且还消除了为分类手动设置阈值的需要 [37]。与图 2(a) 和 (b) 中的先前模型相比，我们提出的方法避免了对整个视觉语言模型进行微调。相反，它仅在提示上进行训练，提示的规模明显小于整个模型。因此，当适应不同数据集时，这种简单的框架达到了显著更高的效率。

在 DualCoOp 中，我们提出了特定类别的区域特征聚合，其中正相关性直接归一化，作为空间注意力来聚合最终的正面和负面 logits。对于具有真实标签的样本（即图像包含目标类别），这种设计通过突出正面 logits 并抑制负面 logits，有效地促进了真正的正面预测。当处理错误标签（即图像缺少目标类别）时，目标转变为通过最小化正面 logits 和最大化负面 logits 来优化真正的负面预测。然而，在这种情况下最小化正面 logits 可能会阻碍正相关性图的响应性，这对于识别混淆区域至关重要。这反过来导致分散的聚合权重，使得负面 logits 的学习变得复杂，可能导致准确度的损失。为了应对这一限制，本文的扩展版本引入了 DualCoOp++，结合了一个证据引导的区域特征聚合模块，以从正面 logits 中解开相关性图。

如图 2(d) 所示，除了正面和负面上下文外，我们进一步引入了证据上下文，以引导正面和负面上下文的空间聚合。与直接表明对象类别存在与否的正面和负面 logits 不同，证据 logits 旨在提取显示相似表示的所有相关视觉内容。因此，优化正面分支不会影响负面分支的学习，模型能够更好地表示和区分目标类别和相似类别，如图 1 所示。此外，由于 DualCoOp 是针对每个类别单独优化的，类别之间缺乏互动也可能阻碍性能。特别是，图像区域可以对多个相似类别产生积极响应，从而导致错误的正面预测。为了解决这一挑战，我们进一步提出了一个赢家通吃 (WTA) 模块，规范每个空间位置仅对最多一个类别产生积极响应，从而进一步增强模型区分相似类别的能力。由于 WTA 是一个非参数模块，所提出的框架在不引入额外计算开销的情况下保持高效。通过这些设计选择，我们实现了一个统一的框架来应对有限注释多标签识别的一般挑战。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/382bafa3fdfb4361b25078c30fb18d64.png)




### 我们的贡献总结如下：
- 我们提出了 DualCoOp++，以高效和有效地适应强大的视觉语言模型，以解决使用有限注释的多标签识别任务。
- 我们提出了证据引导的区域特征聚合模块，以改善从有限注释中学习的上下文信息的空间聚合。
- 我们提出了赢家通吃 (WTA) 模块，以促进 MLR 中的类别间互动，从而更好地区分相似类别。
- 我们在多个基准数据集上进行了广泛的实验和分析，并证明 DualCoOp++ 在部分标签 MLR 和零样本 MLR 任务中均达到了最先进的性能。值得注意的是，在不引入额外计算开销的情况下，DualCoOp++ 在 MS-COCO 和 NUS-WIDE 等基准上，均比我们之前的 DualCoOp 提升了超过 2%。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/5f7bb99a26f44635ad3517a323aa5ebc.png)



## III. 方法

### 问题定义

我们正式定义了具有有限注释的多标签识别：考虑 $M$ 为描述图像中对象或属性的类别集合。给定训练图像 $I$，类别 $m \in M$ 的存在可以是正面的、负面的或未知的，分别对应标签 $y_m = 1,-1$ 或 $0$。在推理期间，我们预测输入图像的每个感兴趣标签。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/2da24bafeff74f95b492843243902288.png)



### 方法概述

为了弥补图像标签不足或缺失，有必要学习类别名称之间的含义是如何相互关联的，这样我们就可以在相关类别之间转移知识。这通常通过学习视觉和文本空间之间的对齐来完成。然而，我们的数据集太有限，无法学习广泛和通用的映射。相反，我们建议利用大规模视觉语言预训练（CLIP [27]）学习到的视觉和文本特征空间的强大对齐，结合一个轻量级的可学习开销，快速适应有限语义注释的 MLR 任务。图 3 提供了我们提出的方法的概述。DualCoOp++ 以三种可学习的词向量序列的形式学习“提示”上下文三元组，以提供给定类别名称的证据、正面和负面上下文。这生成了输入到预训练文本编码器中的证据、正面和负面文本特征。为了更好地区分和识别目标对象，这些对象可以位于图像的不同位置并与其他类别相似，我们引入了一个证据引导的空间聚合步骤。我们首先计算投影视觉特征图与三个上下文编码的相似度得分，以获得每个区域的三个预测 logits。对于每个类别，我们对所有正面/负面 logits 进行空间聚合，其中每个 logit 的权重由证据 logits 的相对大小决定，我们称之为证据引导区域特征聚合。通过这样做，所提出的框架可以更灵活地表示和区分目标类别和相似类别（例如，“狗”区域在证据 logits 图中对标签“猫”显示高相似性，而在正面 logits 图中避免响应）。我们进一步应用一个非参数赢家通吃模块，以促进类别间互动，并通过 ASL 损失 [37] 优化可学习的提示，同时保持所有其他网络组件冻结。在推理过程中，我们直接比较最终的正面和负面 logits，以对类别 $y$ 的标签进行预测。

### 可学习三重提示

与为一个类别学习一个 [72] 或两个 [26] 提示不同，我们提出了证据引导上下文优化 (DualCoOp++)，为每个类别学习三个对比提示上下文。三重提示中的可学习部分分别携带证据、正面和负面的上下文环境，并且可以通过数据端到端优化，采用二分类损失。具体来说，我们将给文本编码器的提示三元组定义如下：

$$P^e = [{V^e}_ 1 , {V^e_ 2} , \ldots , {V^e_ {N^e}} , \text{CLS}]$$

$$P^+ = [{V^+}_ 1 , {V^+}_ 2 , \ldots , {V^+_ {N^+}} , \text{CLS}]$$

$$P^- = [{V^-}_ 1 , {V^-}_ 2 , \ldots , {V^-_ {N^-}} , \text{CLS}]$$

其中每个 $V$ 是一个可学习的词嵌入向量（例如在 CLIP [27] 中的维度为 512），CLS 是给定的类别名称。 $N^e, N^+ 和 N^- $分别是证据、正面和负面提示中的词元数量。为简单起见，我们在实验中使用相同大小的提示。当使用部分标签解决 MLR 问题时，我们为每个类别学习提示三元组（即类别特定提示三元组），并在零样本 MLR 中为所有类别学习共享的提示三元组。使用提示三元组时，我们通过比较正面和负面上下文计算二分类输出 $p$：

$$
p = \frac{e^{\delta(E^I_v ,E^e_t ,E^+_t)/\tau}}{e^{\delta(E^I_v ,E^e_t ,E^+_t)/\tau} + e^{\delta(E^I_v ,E^e_t ,E^-_t)/\tau}}
$$

其中 $p$ 是给定（图像，标签）对作为正面样本的预测概率， $\tau$ 是温度参数。 $E^I_v$ 是视觉编码特征图。 $E^e_t, E^+_t, E^-_t$ 分别是证据、正面和负面提示的文本编码。 $\delta(\cdot, \cdot, \cdot)$ 是我们提出的证据引导的空间聚合函数，用于自适应地减少每个类别的视觉特征的空间维度，下一部分将对此进行讨论。

### 证据引导区域特征聚合

在多标签图像识别中，多个对象出现在图像的不同区域是很常见的。将所有类别生成单个图像级特征向量的池化会给出次优的性能，因为空间信息被减少并且不同的对象被混合。在这项工作中，我们重新表述了 CLIP [27] 中视觉编码器的最后一个多头注意池化层，并应用证据引导的类别特定池化，以在多标签设置中自适应地聚合区域特征。CLIP 中的原始注意池化层首先对视觉特征图进行池化，然后将全局特征向量投影到文本空间，如下所示：

$$
\text{AttnPool}(x) = \text{Proj}_ {v \rightarrow t}\left(\sum_i \text{softmax}\left(\frac{q(\bar{x})k(x_i)^T}{C}\right) \cdot v(x_i)\right)
= \sum_i \text{softmax}\left(\frac{q(\bar{x})k(x_i)^T}{C}\right) \cdot \text{Proj}_ {v \rightarrow t}(v(x_i))
= \text{Pool}(\text{Proj}_{v \rightarrow t}(v(x_i)))
$$

其中 $q, v 和 k$ 是独立的线性嵌入层， $x = E^I_v$ 是视觉编码器的输出特征图。通过删除池化操作，我们可以将每个区域 $i$ 的视觉特征 $F^i_v$ 投影到文本空间 [32]：

$$
F^i_v = \text{Proj}_{v \rightarrow t}(v(E^I_i))
$$

对于每个区域 $i$ 和一个目标类别，我们通过 $F^i_v$ 和类别的证据、正面和负面上下文之间的余弦相似度计算 logits，

$$
S^e_i = \frac{F^i_v \cdot E^e_t}{||F^i_v|| \cdot ||E^e_t||}
$$

$$
S^+_i = \frac{F^i_v \cdot E^+_t}{||F^i_v|| \cdot ||E^+_t||}
$$

$$
S^-_i = \frac{F^i_v \cdot E^-_t}{||F^i_v|| \cdot ||E^-_t||}
$$


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/09fd2ae337fd4abdb7d6875d94a7035c.png)


为了对整个图像进行单一预测，我们根据 $S^e_i$ 的大小聚合 $S^+_i$ 和 $S^-_i$ logits 图，并实现证据引导的空间聚合：

$$
\delta(E^I , E^e_t , E^+_t) = \sum_i \left(\text{softmax}(S^e_i) \cdot S^+_i\right)
$$

$$
\delta(E^I , E^e_t , E^-_t) = \sum_i \left(\text{softmax}(S^e_i) \cdot S^-_i\right)
$$

然后，通过应用 (4) 对目标类别进行预测。值得注意的是，我们在空间聚合函数的重新表述中没有引入任何新参数。用于将视觉特征投影到文本空间的所有参数都继承自 CLIP 中的原始多头注意池化层。

### 赢家通吃正则化

到目前为止，所提出的框架学习对不同类别进行独立预测。然而，如图 4(a) 所示，独立处理类别可能会导致错误的正面预测，因为相同的视觉特征可能对多个相似类别产生积极响应，特别是在注释有限的情况下，缺乏足够的监督。我们通过赢家通吃 (WTA) 模块解决这个问题，该模块对每个空间区域进行正则化，使其仅对最多一个类别产生积极响应。给定图像区域 $i$ 和包含 $M$ 个目标类别的标签集，我们根据 (8) 计算了 $M$ 个正面 logits 分数 ${S^+}_ i = [({S^+}_ i)_ 0, ({S^+}_ i)_ 1, \ldots, ({S^+}_ i)_ {M-1}]$。正则化权重 $w_i \in \mathbb{R}^M$ 在所有标签上计算，

$$
w_i = \text{softmax} \left(\gamma \cdot S^+_i \cdot \max_m(S^+_i)\right)
$$

其中 $\max_m(S^+_i)$ 表示区域 $i$ 在 $M$ 类别中的最大 logit 分数， $\gamma$ 是一个超参数。
然后，我们逐元素更新正面 logits，

$$
(S^+_i)' = w_i \odot S^+_i
$$

其中 $\odot$ 是 Hadamard 乘积。如图 4(b) 和 (c) 所示，与总是标示最大元素的 Gumbel softmax 相反，我们的 WTA 仅在多个 logit 具有较大值时突出较大的元素，这确保图像区域可以对给定类别中的零或一个类别产生积极响应。

### 优化

我们应用不对称损失 (ASL) [37] 处理多标签识别优化中的固有正负不平衡。具体来说，我们计算正面（图像，标签）对 $L^+$ 和负面（图像，标签）对 $L^-$ 的损失如下：

$$
L^+ = (1- p)^{\gamma^+} \log(p)
$$

$$
L^- = (p_c)^{\gamma^-} \log(1- p_c)
$$

其中 $p$ 是公式 (4) 中的概率， $p_c = \max(p- c, 0)$ 是通过边距 $c$ 硬阈值化的负样本的概率。我们设置超参数 $\gamma^- \geq \gamma^+$，以便 ASL 降低和硬阈值化简单的负样本。通过反向传播 ASL 来更新可学习的提示，同时保持文本编码器冻结。

### 方法论依据

在这一部分中，我们深入分析，以阐明负面提示和证据提示带来的优势。在最小化不对称损失时（见公式 (14) 和 (15)），优化正面提示需要优化真实类别（即图像中存在的类别）的真正正面预测和错误类别（即图像中不存在的类别）的真正负面预测。通过引入负面提示，模型解开了优化过程，显式进一步优化真实类别的错误负面预测和错误类别的错误正面预测。这种方法引入了独立和互补的上下文，从而增强了模型的辨别和泛化能力。此外，这种优化策略通过比较正面和负面 logits 促进分类，消除了在仅正面模型中通常需要手动选择阈值的需要。

基于上述分析，我们在 DualCoOp [26] 中引入了正面和负面提示，并通过第四节 C 部分的实验证明了它们的有效性。按照公式 (10) 和 (11) 的公式，DualCoOp 中的最终正面/负面 logits 表达如下：

$$
\delta^+ = \sum_i \left(\text{softmax}(S^+_i) \cdot S^+_i\right)
$$

$$
\delta^- = \sum_i \left(\text{softmax}(S^+_i) \cdot S^-_i\right)
$$

其中 $S^+_i 和 S^-_i$ 分别表示公式 (8) 和 (9) 计算的正面和负面 logit 图。基本概念是实现正面和负面提示的平衡整合，利用各自的 logit 图进行全面和有效的表示。

尽管 DualCoOp 已经证明了其有效性，但仍然存在一个持续的问题限制了其表示能力。在训练期间遇到负面（图像，标签）对的情况下，目标是最小化 $\delta^+$ 同时最大化 $\delta^- $。如公式 (16) 所述，最小化 $\delta^+$ 导致目标类别和混淆背景类别区域之间的相关性减少在 $\text{softmax}(S^+_i)$ 中。然而，这些混淆区域对于学习具有强大辨别能力的负面上下文至关重要。因此，在努力最小化 $\delta^+$ 的过程中产生的分散聚合图 $\text{softmax}(S^+_i)$ 可能会忽略最有信息的空间区域。这种忽视最终限制了模型有效学习负面提示的能力，限制了其整体性能。

提出证据提示是为了缓解这种限制，通过将聚合图与正面 logits 解开。通过允许证据 logits 图捕获所有与目标类别相关的视觉内容，正面和负面提示可以专注于识别目标类别和区分混淆类别，而不会产生冲突。结果，这种方法允许更有效和无冲突的学习，并且在各种设置和数据集上始终如一地带来改进。

## IV. 实验

在本节中，我们首先报告部分标签和零样本多标签识别基准上的性能，然后进行实验分析提出的方法。

### A. 部分标签的多标签识别

#### 数据集

我们在 MS-COCO [47]、VOC2007 [78] 和 BigEarth [79] 上进行实验，以评估部分标签的多标签识别。MS-COCO [47] 包含 80 个常见对象类别，我们使用官方的 train2014（82 K 图像）和 val2014（40 K 图像）分割进行训练和推理。VOC2007 [78] 包含 20 个对象类别，我们使用官方的 trainval（5 K 图像）和测试（5 K 图像）分割进行训练和测试。此外，由于 CLIP 预训练数据不可公开获取，且 CLIP 预训练数据覆盖许多粗粒和细粒视觉域，在许多下游任务的零样本评估中表现良好，我们还在一个遥感图像数据集 BigEarth [79] 上进行了实验，该数据集的域与主流论文中的数据集（即 PASCAL VOC、MS-COCO 和 NUS-WIDE）相去甚远。

为了创建部分标签的训练集，我们遵循标准做法 [16]，[17]，[19]，从完全注释的训练集中屏蔽标签，并使用剩余标签进行训练。保留标签的比例从 10% 到 90% 不等，与之前的工作 [17]，[19] 一致。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/8ec60d040db24444a8d835bd8d50b6bd.png)



#### 评估

我们报告每个优化标签比例（从 10% 到 90%）的平均准确率（mAP）及其所有比例的平均值。我们计算每个基线和 DualCoOp++ 的可学习参数数目（#P）以衡量优化的复杂性。

#### 实现

为了公平比较，我们采用 ResNet-101 [4] 作为所有基线和 DualCoOp++ 的视觉编码器，输入分辨率为 448×448，并使用 CLIP [27] 中相同的 Transformer [31]，[80] 作为文本编码器。视觉和文本编码器从 CLIP 预训练模型初始化，并在优化期间保持冻结。对于每个类别/标签，我们学习三个独立的上下文向量，每个上下文向量具有 12 个上下文标记（ $N = 12$），以保持与 DualCoOp 相似的参数大小。注意，这些上下文标记是 DualCoOp++ 唯一可学习的部分。我们使用初始速率为 0.002 的 SGD 优化器，并按照余弦退火规则进行衰减。我们以批次大小 32/32/8 训练上下文向量 50 轮，分别用于 MS-COCO/BigEarth/VOC2007。对于 ASL 损失，我们选择 $\gamma^+ = 1, \gamma^- = 2 和 c = 0.05$ 通过验证。训练在一个 RTX A6000 上完成。

#### 基线

为了评估 DualCoOp++ 的有效性，我们与以下基线进行比较：
- 采用图神经网络进行标签依赖的 SSGRL [77]、GCN-ML [7] 和 KGGR [53]。
- 生成未知标签伪标签的课程标签 [16] 和 SST [17]。
- 使用归一化 BCE 损失更好地利用部分标签的 Partial BCE [16]。
- 在不同图像之间混合类别特定表示以传递信息的 SARB [19]。
- 采用大规模预训练视觉语言模型 CLIP 的 SCPNet [74]、TaI-DPT [75] 和我们的 DualCoop [26]。

#### 结果

表 I 显示了在 10% 到 90% 标签优化时，DualCoOp++ 与所有基线的 mAP 比较。对于最近的两项工作（SST [17] 和 SARB [19]），我们进一步用 CLIP 预训练权重 [27] 替换 ImageNet 预训练权重 [4] 来初始化它们的视觉编码器，这在表 I 中导致 SST∗ 和 SARB∗。由于我们学习了类别特定的提示，DualCoOp++ 在 MS-COCO 上采用了比 VOC2007 更多的可学习参数。所提出的 DualCoOp++ 在训练期间可用的所有标签比例上均实现了最佳性能。值得注意的是，DualCoOp++ 在具有类似学习开销的情况下始终提高了 DualCoOp 的性能，并且以不到一半的可学习参数在 MS-COCO 和 VOC2007 上均优于第二好的方法 SCPNet [74]。特别是，当训练期间仅提供 10% 的标签时，DualCoOp++ 提高了 DualCoOp 超过 2%，并在两个数据集上均优于 SPCNet 超过 1%。我们还采用了 TaI-DPT [75] 中的训练协议，使用更大的批次大小和更多的可调超参数，并在表 II 中比较了结果。在没有额外训练数据的情况下，DualCoOp++ 可以进一步提升性能，并且始终优于 TaI-DPT，后者利用丰富的字幕数据进行训练，并使用两个 CLIP 模型进行推理。这表明 DualCoOp++ 能够快速适应有限标签的多标签识别任务。在 BigEarth 上，我们比较了 DualCoOp++ 与 DualCoOp 和一个强基线 SARB 的性能。表 III 显示，DualCoOp++ 在不同标签比例下始终优于 DualCoOp 和 SARB，显著提高了性能，证明了 DualCoOp++ 通过利用强大的视觉语言预训练提升了不同视觉域的性能。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/625baf9da1d44e939717fcb0ddd3c62d.png)



#### 完全标签训练

在 MS-COCO 上，我们还使用 100% 的训练标签进行训练。在不对视觉编码器进行微调的情况下，DualCoOp++ 实现了 85.3% mAP，比 DualCoOp 提高了 2%，并且在具有相同 ResNet-101 主干的情况下优于之前的 SOTA 方法，如 ASL [37]（85.0% mAP）和 CSRA [84]（83.5% mAP）。这表明 DualCoOp++ 能够利用预训练的 CLIP 模型来应对具有挑战性的 MLR 任务。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/5fffbad0427f488389be76d14e0e26e3.png)


#### 计算成本

我们在相同设备（一个 Nvidia A100 GPU）上比较了 DualCoOp++ 与先前方法的训练/测试延迟和内存的计算成本（见表 IV）。对于当前的多标签识别任务，类别在推理之前是预设的（即，我们已经知道在推理过程中要考虑哪个类别）。在这种情况下，我们在推理之前从学习到的提示和类别名称中计算每个类别的文本特征。然后，我们使用预计算的文本特征在测试期间对每个图像进行预测。由于文本特征是预计算的（非常轻量级的计算开销），因此在推理过程中不会执行文本编码器。在推理时，当使用相同的图像编码器主干时，DualCoOp++ 的延迟时间和内存消耗与 DualCoOp 相同。在训练期间，由于在前向传递过程中同时执行图像和文本编码器，并且仅更新提示，因此基于 CLIP 的方法略微提高了延迟时间和内存消耗。

### B. 零样本多标签识别

#### 数据集

根据 [20]，[23]，我们在 MS-COCO [47] 和 NUS-WIDE [8] 上进行实验，以执行零样本多标签识别。在 MS-COCO 上，我们根据 [20]，[85] 将数据集拆分为 48 个已见类别和 17 个未见类别。NUS-WIDE [8] 数据集包括 270 K 图像。根据 [20]，[23] 我们使用 81 个人工注释类别作为未见类别，并将 Flickr 标签获得的额外 925 个标签作为已见类别。

#### 评估

我们根据 [20] 的方法报告每张图像前 3 个预测的精度、召回率和 F1 得分。在 NUS-WIDE 上，我们根据 [20]，[23] 报告所有类别的 mAP 以及每张图像前 3 和前 5 个预测的精度、召回率和 F1 得分。我们在零样本设置（仅测试未见类别）和广义零样本设置（测试已见和未见类别）中评估所有方法。

#### 实现

我们采用 ResNet-50 [4]，类似于 [20] 作为 DualCoOp++ 的视觉编码器，输入分辨率为 224。我们学习具有 42 个上下文标记（ $N = 42$）的类别无关上下文向量，以取代类别特定的提示，这也是 DualCoOp++ 唯一可学习的部分。我们优化上下文向量 50 轮，MS-COCO/NUS-WIDE 的批次大小分别为 32/192。在推理过程中，我们将学习到的上下文向量与每个类别（无论是基础类别还是新类别）的类别名称结合，并计算文本特征。其他实现细节与 IV-A 节相同。

#### 基线

为了评估 DualCoOp++ 在零样本设置中的有效性，我们与以下基线进行比较：
- 采用分类器集成来处理未见类别的 CONSE [81]。
- 学习联合图像标签嵌入的 LabelEM [82]。
- 估计输入图像的一个或多个多样主方向的 Fast0Tag [21] 和 SDL [20]。
- 估计相关区域的 Deep0Tag [76] 和 LESA [23]，分别通过区域提议和注意技术。
- 增强基于区域特征以最小化类别间特征纠缠的 BiAM [24]。
- 基于预训练 CLIP 模型的 DualCoOp [26]。

#### 结果

表 V 和表 VI 显示了 DualCoOp++ 与所有零样本学习和广义零样本学习最先进方法在 NUS-WIDE 和 MS-COCO 数据集上的比较。DualCoOp++ 在所有情况下都达到了最佳精度，学习开销非常轻（0.07 M），显著提高了零样本学习的性能。与之前的最先进 SDL [20] 相比，DualCoOp++ 在 MS-COCO 上的 Top-3 零样本学习性能提高了 15.0，在 NUS-WIDE 上的 Top-3 提高了 14.1，在 Top-5 提高了 14.3。这显示了通过 DualCoOp++ 利用 CLIP 预训练的文本和视觉空间对齐解决多标签识别的能力。DualCoOp++ 还始终提高了 DualCoOp 在所有设置中的精度和召回率，证明了我们提出的方法在抑制错误预测方面的有效性。

### C. 方法分析

在本小节中，我们首先展示本文的总体概念，然后通过广泛的实验验证详细的模型设计。

#### 为什么提出的方法对 MLR 有用？

尽管 CLIP [27] 模型在不同概念上展示了强大的泛化能力，但处理下游任务如 MLR 仍然非同寻常。正如表 VI 和表 VII 所示，直接将 CLIP 应用于 MLR 的性能甚至比非 CLIP 方法（例如表 VI 中的 CLIP vs. SDL 和表 VII 中的 CLIP vs. SARB）差。观察到的性能差距主要源于 CLIP 与下游任务之间的领域差距和目标不一致。为了解决领域差距问题，我们首先通过提示调整来解决。如表 VI 和表 VII 所示，CoOp 方法通过在 CLIP 中引入可学习提示显著提升了性能。然而，它仍然无法超过非 CLIP 方法如 SDL 和 SARB 在低标签 MLR 中的表现。这可以归因于仅调整正面提示导致的不平衡表示和优化，在推广到测试数据时表现有限。为了解决这一问题，我们引入负面提示和证据提示，以显式调节调整过程并引入互补上下文，从而增强模型的泛化能力，如表 VIII、表 IX 和表 X 所示。限制 CLIP 的 MLR 性能的第二个挑战源于其设计用于一对一视觉文本对比目标，使得提取 MLR 所需的精细空间信息变得具有挑战性。通过设计区域特征聚合和赢家通吃正则化来区分空间细节，解决了这一限制，从而提高了准确性，如表 VIII、表 IX 和表 XI 所示。通过这些设计，所提出的方法缓解了在利用 CLIP 丰富语义解决 MLR 时的限制和挑战，在不同设置和数据集上实现了最先进的性能。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/d8c75c8c27f64146bffac7d189169e73.png)


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/9f7baf3776e74e2e93c8f5cf70a7d1d1.png)



#### 文本监督的有效性

为了展示标签空间文本监督的有效性，我们比较了五种方法（SST [17]、SARB [19]、CoOp [25]、DualCoOp 和 DualCoOp++）使用离散标签空间（“离散标签”）学习的模型，这些方法引入文本空间以利用表 VII 中的标签上下文相关性。我们发现，使用文本监督的方法通常比仅使用离散标签的方法表现更好。然而，当语义注释有限时，文本监督有时会导致性能更差（例如，在仅 10% 标签时，SST 的 mAP 比离散标签低 1.5%）。CoOp [25] 利用视觉文本对齐。然而，使用原始多头注意和单个正面提示，它的表现比离散标签更差。为了更好地利用 MLR 任务的良好对齐，DualCoOp++ 学习上下文三元组并采用证据引导的区域特征聚合，从而带来了极大的性能提升（例如在仅 10% 标签时比离散标签高 10.8%），并在有限标签下快速适应数据集。


![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/206a309b78fa4e96884b94a03735e521.png)


#### CoOp 与 DualCoOp++ 与 DualCoOp

在表 VI 和表 VII 中，我们进行了 CoOp 与所提出方法的性能比较。在零样本 MLR 的背景下，DualCoOp 和 DualCoOp++ 均在 F1 指标上表现出显著提升。具体而言，DualCoOp 提升超过 2%，而 DualCoOp++ 在零样本识别和广义零样本识别任务中均提高了超过 4%。在部分标签设置中，我们的方法始终将 CoOp 的性能提高超过 8%，跨越了可用标签的不同比例。这种一致的改进突显了我们提出的方法在从图像中识别多个标签方面的有效性。我们还分析了 DualCoOp++ 中新引入组件的影响，如表 VIII 和表 IX 所示。对于部分标签识别，我们报告了每个类别和整体平均的精度（C_P 和 O_P）、召回率（C_R 和 O_R）和 F1 指标（C_F 和 O_F）。如表 VIII 所示，引入证据引导的空间聚合（表示为“Evi.”）显著提高了精度和 F 指标，进一步应用赢家通吃模块可以进一步提升性能。这表明我们的方法可以有效缓解 DualCoOp 的限制并抑制错误的正面预测。对于零样本识别，如表 IX 所示，我们提出的组件在所有指标和数据集上均显示出一致的改进，证明了其在推广到未见类别方面的能力。

#### 提示设计的消融实验

我们在 MS-COCO 数据集上零样本设置下比较了我们提出的三重可学习提示与手工设计的提示和单一提示学习方法（见表 X）。手工设计的提示可以使用无上下文的类别名称 [86] 或手工设计的提示模板。在我们的实验中，我们仔细选择了正面和负面提示模板，如“a photo of a [classname]” 和 “a photo without a [classname]”。与使用多个可学习提示作为输入执行每个类别的二分类不同，我们还进行了学习一个单一的正面或负面上下文提示的实验，并使用一个选定的阈值（在我们的实验中为 0.5）对每个类别进行预测。正如我们所见，单一正面提示学习方法（M3）比不可学习方法（M0 和 M1）表现更好，单一负面可学习提示（M2）比其正面对应物（M3）的准确性差得多。包含正面和负面提示（M4）的表现比单一提示更好。当加入证据提示时，三重提示（M5）表现更高，这表明 DualCoOp++ 能够更好地处理来自正面和负面方面的互补和有益信息。在图 5(a)、(b) 和 (c) 中，我们分析了不同实验场景中提示上下文长度的影响。在具有部分标签的 MLR 中，我们学习类别特定的提示，因此在 DualCoOp++ 中较小的 $N$（例如 12）可以表现良好。对于 MLR 中的零样本学习，我们学习所有类别共享的统一提示，需要更大的 $N$（例如 36）以获得良好性能。在主要论文中，我们在所有具有部分标签的 MLR 实验中使用 $N = 12$，在零样本学习实验中使用 $N = 42$，以保持与 DualCoOp 相似的大小。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/08f03d7089d542409b0568abf1066968.png)



#### 聚合函数的影响

在表 XI 中，我们比较了不同空间聚合方法的自适应能力。通常，训练/测试时使用更大的分辨率是有益的，因为空间细节很重要。如所示，多头注意与预训练图像分辨率（CLIP 中为 224）绑定，而我们的证据引导区域聚合在训练或推理期间受益于更高的输入分辨率。我们的方法使用原始权重，但实际上表现比微调原始多头注意层更好。我们还比较了不同函数来聚合每个类别的区域 logits，如图 5(d) 所示。我们以四种方式计算最终 logits：（1）取所有空间位置的 logits 的平均值（“Ave”），（2）取具有最大正面 logit 的区域（“Max”），（3）通过对正面 logits 的 softmax 函数生成所有空间位置的聚合权重（“DualCoOp”），（4）通过对证据 logits 的 softmax 函数生成所有空间位置的聚合权重（“DualCoOp++”）。“Max” 的表现优于 “Ave”，这表明区域特征比全局特征在多标签识别中更有信息。考虑到区域和全局特征，“DualCoOp” 改善了性能。当进一步引入证据时，我们的 “DualCoOp++” 提供了最佳性能，这表明通过证据 logits 生成聚合权重优于通过正面 logits 进行聚合。DualCoOp 和 DualCoOp++ 中不同 logits 的可视化如图 6 所示。



![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/78c2f578ca2f44f0b72d20c3cc5a970d.png)



#### 微调 DualCoOp 的消融实验

为了更好地检查 DualCoOp++ 微调的有效性，我们还进行了微调证据引导区域聚合权重的实验。如表 XI 所示，微调聚合函数在不同标签数量下稳定提高了性能。我们还尝试了微调 CLIP 图像编码器中的所有权重，但发现性能显著下降，特别是在训练标签较少时，这表明在监督不足的情况下进行微调可能会破坏 CLIP 模型中预训练的视觉语言对齐。

## V. 结论

在本文中，我们扩展了之前的 DualCoOp 提出了一种新框架 DualCoOp++，统一了两种类型的有限注释多标签识别：部分标签和零样本。DualCoOp++ 利用从网络规模数据集中获得的强大视觉语言预训练模型。通过引入轻量级的可学习开销，它可以在收到少量标签后快速适应解决多标签识别问题。在 DualCoOp++ 中，我们学习了一个证据、正面和负面提示三元组，并以目标类别名称作为语言输入。此外，为了更好地聚合每个类别的视觉区域特征，我们重新表述了预训练模型中的原始视觉注意力为证据引导的区域特征聚合。此外，引入了一个赢家通吃模块，促进跨标签互动，并规范每个区域仅对最多一个类别产生积极响应。我们在 MS-COCO、VOC2007 和 NUS-WIDE 数据集上进行了广泛的实验，展示了 DualCoOp++ 对 DualCoOp 的改进以及我们提出的方法在最先进方法上的有效性。






