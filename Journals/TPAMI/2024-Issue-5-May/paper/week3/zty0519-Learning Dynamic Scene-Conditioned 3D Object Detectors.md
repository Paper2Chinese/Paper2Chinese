# 题目：[Learning Dynamic Scene-Conditioned 3D Object Detectors](https://ieeexplore.ieee.org/document/10330125)  
## 学习动态场景条件三维物体检测器
**作者：Yu Zheng; Yueqi Duan; Zongtai Li; Jie Zhou; Jiwen Lu** 

****

# 摘要

在本文中，我们提出了一个名为 HyperDet3D 的动态 3D 对象检测器，它基于超场景级知识实时自适应调整。现有方法努力为局部元素及其关系提供对象级表示，而不依赖于场景级先验，这在仅基于对个别点和对象候选的理解时，会导致结构相似的对象之间的歧义。与此相反，我们设计了场景条件超网络（scene-conditioned hypernetworks），以同时学习场景不可知的嵌入，利用来自各种 3D 场景的可共享抽象，以及特定于场景的知识，该知识在测试时将 3D 检测器适应给定场景。结果，对象表示中的低层次歧义可以通过场景先验中的层次上下文来解决。然而，由于 HyperDet3D 中的上游超网络以原始场景作为输入，这些场景包含噪声和冗余，导致在下游检测损失的约束下为 3D 检测器产生次优参数。基于下游 3D 检测任务可以分解为对象级语义分类和边界框回归的事实，我们进一步提出了 HyperFormer3D，通过相应地设计它们在上游超网络中的场景级先验任务，即 SemanticOccurrence 和 ObjectnessLocalization。为此，我们设计了一个基于 Transformer 的超网络，将任务导向的场景先验转化为下游检测器的参数，避免了场景的噪声和冗余。在 ScanNet、SUN RGB-D 和 MatterPort3D 数据集上的广泛实验结果证明了所提方法的有效性。

# 关键词

- 3D 对象检测
- 动态超网络
- 点云
- 先验知识
- 场景条件学习


# I. 引言

3D 对象检测近年来受到了广泛关注，它是自动驾驶、机器人导航和增强现实等应用的基础。早期的工作采用滑动窗口[2]或2D先验[3]从RGB-D数据中定位对象。然而，点云的无序性和稀疏性使得直接采用2D对象检测的最新进展变得困难。为了解决这个问题，基于视图的方法[4]将点投影到多个2D平面并应用标准的2D检测器。体积卷积基于方法[5], [6]将点分割成规则网格，这对3D卷积是可行的。

与上述基于视图和体积卷积的方法不同，PointNet++ [7]专注于局部几何形状，同时消耗原始点云，因此在3D检测器中被广泛用作骨干网络。基于[7]，VoteNet [8]通过从种子坐标和局部特征回归到对象中心的偏移投票，取得了优异的结果。随后的工作结合概率投票[9]、多级上下文学习[10], [11]和自注意力基于Transformer[12], [13], [14]，进一步提高了局部表示。这些方法强调了局部元素的基于对象和基于关系的表示，例如给定点、检测候选和不规则局部几何形状。

然而，如果我们只关注它们自身或关系，类似对象的属性就会变得模糊不清。在本文中，我们发现场景级信息提供了先验知识，以消除这种歧义。如图1所示，没有场景条件知识，推理对象级特征或它们的关系对于检测对象候选是不够的，导致在场景级理解方面出现反直觉的检测结果。据我们所知，3D检测器通过各种场景获取此类场景级信息的方法尚未得到充分研究。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/3a9937eaa1584de7b6ec4ac85abc79ad.png#pic_ center" width="70%" />
</div>

为此，我们首先提出了一个名为 HyperDet3D 的基于点云的场景条件3D对象检测器，它利用基于超网络的结构。与现有方法相比，这些方法专注于更低级别的逐点或对象级表示，我们的 HyperDet3D 学习场景条件信息作为层次上下文先验，并将其融入网络参数中，以便我们的3D对象检测器根据不同的输入场景动态调整。具体来说，场景条件知识可以划分为两个层次：场景不可知和场景特定信息。对于场景不可知知识，我们维护一个可学习的嵌入，它由超网络消耗，并在训练期间随着解析各种输入场景而迭代更新。这种可共享的场景不可知知识通常抽象了训练场景的特征，并可在测试时被检测器利用。此外，由于传统的检测器在不同场景中识别对象时保持相同的参数集，我们提出学习场景特定信息，以便在测试时适应给定场景的检测器。为此，我们旨在有意识地使用特定输入数据作为查询，以获取通用场景条件知识表示。我们通过提出多头场景条件注意（MSA）模块，同时学习两个层次的场景条件知识。学到的先验知识被嵌入到对象候选特征中，作为输入和动态权重参数的聚合，从而为检测对象提供有意义的指导。这样，HyperDet3D 不仅从各种3D场景中探索可共享的抽象，而且在测试时适应给定场景的检测器。

虽然学习到的场景条件知识赋予了对象特征表示的多样性和灵活性，但 HyperDet3D 中的上游超网络以原始场景作为输入，这些场景不可避免地包含噪声和冗余。结果，给定某个原始输入场景，动态超网络仅在下游检测头的损失约束下为初级网络产生次优参数。为了解决这个问题，我们进一步提出了基于 HyperDet3D 的属性动态场景条件3D对象检测器，名为 HyperFormer3D。由于下游3D检测可以分解为两个对象级子任务，即语义分类和边界框回归，我们相应地在上游超网络中为它们设计了场景级预文本任务，即 Semantic Occurrence 和 Objectness Localization。通过学习这些特定设计的预文本任务，HyperFormer3D 能够提取面向任务的场景先验，以避免噪声和冗余。具体来说，与从原始场景中学习场景先验的抽象以用于主网络中的对象表示不同，HyperFormer3D 相应地学习一组权重标记，以预测这两种类型的场景级先验。我们设计了一个基于 Transformer 的超网络，它同时将权重标记转换为参数空间，并将它们与两种类型的场景先验结合起来。这些场景级预文本任务不需要额外的监督，并且为下游3D对象检测提供了重要的先验知识。此外，为了更好地将输入场景与学习到的场景条件知识关联起来，我们提出了多层次标记融合（MLTF），在输入（MLTF-I）和网络（MLTF-N）层面上通过几何和语义感知特征对嵌入标记进行编码。在 ScanNet [15]、SUN RGB-D [16] 和 Matterport3D [17] 等具有挑战性的数据集上的广泛实验表明，我们的方法以明显的优势超越了现有技术。此外，通过跨数据集评估，我们展示了面对领域差距时获取的场景条件先验知识仍然有效。我们还进行了彻底的消融研究，以验证所提出的 HyperDet3D 和 HyperFormer3D 的必要性和设计选择。

本文是关于我们会议论文[1]的扩展版本，我们在其中做了以下新的贡献：

1)我们进一步提出了一种新的HyperFormer3D方法，基于HyperDet3D来学习面向任务的场景先验，从而避免原始输入场景中的噪声和冗余的干扰。

2)我们提出通过基于Transformer的超网络架构，学习一组权重标记，并将其转换为参数空间，与学习到的场景先验结合。我们还提出了一种多层标记融合(MLTF)方法，通过两个预文本任务和自注意力层中的密集交互，用语义感知特征对权重标记进行编码，以便更好地将输入场景与学习到的场景条件知识关联起来。

3)除了在会议版本中使用的ScanNet和SUN RGB-D数据集上进行评估外，我们还在具有更多房间类型和数量、更具挑战性的MatterPort3D数据集上进行了基准测试。结果表明，我们的方法具有有效性。此外，我们展示了我们的技术可以应用于其他方法中的解码器，以增强基线模型。

# III. 方法


我们首先介绍一个名为 HyperDet3D 的 3D 对象检测器，它隐式地学习场景条件知识，然后详细介绍 HyperFormer3D，以更精确地为检测器学习场景先验。

## A. HyperDet3D

在这一小节中，我们首先简要介绍整体架构和一些预备知识。接下来，我们详细阐述我们提出的 HyperDet3D。最后，我们提供所提出方法的实现细节。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/492d353c180c4bf4b9101d1c88e78049.png#pic_ center" width="70%" />
</div>

1)概述和预备知识：图 2 展示了我们提出的 HyperDet3D 中的 3 个关键组件，它们是骨干编码器、对象解码层和检测头部。给定一个输入点云 P，骨干首先将密集点云下采样成初始对象候选，并通过对层次结构进行粗糙特征提取。为了公平比较，我们考虑使用 PointNet++ [7] 作为骨干网络，与之前的作品 [8], [13], [29] 类似，它使用最远点采样 (FPS) 来均匀覆盖 3D 空间。然后，对象解码层通过将场景条件先验知识整合到对象级表示中来细化候选特征（在第 III-A2 节中详细说明）。最后，检测头部从这些对象候选的位置和细化特征中回归边界框（在第 III-A3 节中详细说明）。

为了使 HyperDet3D 能够感知场景级元信息，我们采用了超网络 [42]，这是一种神经网络，用于为另一个网络（称为主网络）参数化可学习参数。对于主网络中的目标层，其可学习参数 W 通常是通过将可学习的嵌入 z 或中间特征 x 馈送到超网络 H 中生成的：

$$
W = H(z) \quad \text{或} \quad W = H(x)
$$

与在测试时保持层固定的传统深度神经网络不同，超网络通过修改输入来实现可学习参数的灵活性。

在 HyperDet3D 中，我们提出使用场景条件超网络向 Transformer 解码器中的层参数注入先验知识，从而根据不同的输入场景动态调整检测网络。

2)场景条件超网络：对于由骨干编码器生成的对象候选的特征表示 o，我们场景条件超网络的目标是赋予它由 {W, b} 参数化的先验知识：

$$
\hat{o} = W o + b
$$

其中 W ∈ RCout×Cin 和 b ∈ RCout 是主检测网络中的权重和偏置参数。这些参数由我们的场景条件超网络生成，可以分为场景不可知和场景特定超网络。

场景不可知超网络：以主网络的权重参数 W 为例。对于场景不可知知识，我们首先维护一组新的场景不可知嵌入向量  $Z_ a = \{ z_ {aj} \in R^{C_ a} \}^{n_ e}_ {j=1}$ 。然后，这些向量由场景不可知超网络  $h_ {\theta}$  消耗，将  $z_ {aj}$  投影到另一个  $R^{C_ {ui}}$  空间，输出  $W_ a$  参数化我们的场景不可知知识：

$$
W_ a := \{ w_ {aj} \in R^{C_ {ui}} \}^{n_ e}_ {j=1}, \quad w_ {aj} = h_ {\theta}(z_ {aj})
$$

其中  $C_ {ui}$  是单元输入通道大小，满足：

$$
\text{mod}(C_ {out}, n) \equiv 0, \quad \text{mod}(C_ {in}, C_ {ui}) \equiv 0
$$

当对象特征通过一系列解码器层 [12], [13] 迭代细化时，它们可以与场景不可知超网络的输出一致地结合，后者抽象了各种 3D 场景的先验知识。通过这种方式，我们不仅在解码器层中保持了一般场景条件知识，还通过与丰富的特征层次结构共享知识来节省计算成本。

场景特定超网络：对于场景特定知识，我们也学习一组嵌入向量  $Z_ s = \{ z_ {sk} \in R^{C_ s} \}^{n_ e}_ {k=1}$ ，与  $Z_ a$  类似。不同之处在于，为了使  $Z_ s$  适应输入场景，我们的场景特定超网络  $h_ {\theta}$  使用输入场景 P 作为场景特定查询。受到语言模型中对齐 [57] 的启发，我们通过注意力机制在嵌入空间中测量  $z_ {sw}$  与输入场景（或它们的差异）的匹配程度：

$$
W_ s := \{ w_ {sk} \in R^{C_ {ui}} \}^{n_ e}_ {k=1}, \quad w_ {sk} = h_ {\theta}(z_ {sk}, P_ d) = W_ f(z_ {sk} \| W_ p P_ d)
$$

其中  $P_ d \in R^{N_ d \times 3}$ ,  $W_ p \in R^{C_ n \times Nd}$  是当前输入场景的一个子集，以及将  $P_ d$  投影到  $Z_ s$  的嵌入空间的变换矩阵。 $W_ f$  表示带有 Tanh 激活函数的权重矩阵。由于我们打算从潜在嵌入空间中获得响应，我们使用连接（·||·）作为查询点和嵌入向量的编码，类似于 SDF 查询 [58]。我们采用下采样表示  $P_ d$  而不是 P，因为正如先前研究 [43] 所建议的，超网络无法完全捕获高分辨率信息。

从场景特定注意力分数  $W_ s$  和场景条件知识  $W_ a$  中，我们现在可以得到 W 的单元块：

$$
W_ u = W_ s \otimes W_ a
$$

其中 ⊗ 表示逐元素乘法。

多头场景条件注意力：对于输入场景 P，上述过程可以封装成两个场景条件注意力操作：

$$
W_ u = \text{Att2}(\{z_ {aj}\}, \text{Att1}(\{z_ {sk}\}, P))
$$

其中 Att1 和 Att2 分别对应于 (5) 和 (6) 中的注意力。为了适应主网络目标权重 W ∈ RCout×Cin 的形状，一个简单的解决方案是将  $W_ u$  重复  $C_ {out} \times \frac{n_ e \times C_ {in}}{C_ {ui}}$  次，并沿着其两个维度平铺。(4) 中的可行性得到了保证。由于  $Z_ a$  和  $Z_ s$  只由超网络初始化和消耗一次，我们将其命名为单头场景条件注意力（SSA）。

为了允许主检测器在各种子空间中联合关注场景条件知识，我们进一步提出了基于 SSA 的多头场景条件注意力（MSA）。[35] 中的多头注意力通过并行注意力模块消耗同一组输入。然而，由于在我们的情况下目标权重 W 是基于超网络的输入进行条件化的，我们通过多次重新初始化  $Z_ a$  和  $Z_ s$  来实现 MSA。因此，我们的 MSA 可以表述为：

$$
W = \text{Concat}\{W_ u^{(1)}, W_ u^{(2)}, ..., W_ u^{(C_ {out} \times \frac{n_ e \times C_ {in}}{C_ {ui}})}\}
$$

其中  $W_ u^{(l)}$  表示由  $Z_ a$  和  $Z_ s$  第 l 次初始化在 (7) 中产生的结果。Concat 操作类似于 SSA，沿着两个维度平铺矩阵。

在图 3 中，我们展示了 [35] 中的原始多头注意力、我们提出的多头场景条件注意力（MSA）和单头场景条件注意力（SSA）之间的比较。在 [35] 中，单个输入样本的计算开销与定义注意力子空间的并行注意力模块的数量成正比。相反，MSA 网络在我们 HyperDet3D 的所有训练样本之间共享。此外，由于我们通过超网络结构挖掘子空间，MSA 通过修改 (1) 中的输入来利用场景条件知识的灵活性。相比之下，SSA 消耗同一组嵌入向量，在表达能力方面不如 MSA，我们在消融实验中验证了这一点。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/49fdd6022aa74c98a405d4c5f178f505.png#pic_ center" width="70%" />
</div>

对于偏置 b，我们维护与获得 W 时相同的  $Z_ a$  和  $Z_ s$ 。场景特定和场景不可知超网络都包含一个线性层。在第二次注意力之前，我们沿着特征维度平均场景特定和场景不可知向量的结果。W 和 b 与对象特征结合，如 (2) 中所述。更新后的特征表示  $\hat{o}$  随后被检测头部消耗以生成检测结果。

3)解耦检测头部：按照 [8] 的做法，现有工作通过检测头部参数化 Wc 直接回归候选位置 qi 的偏移量 (Δqi) 来定位对象中心 ci：

$$
c_ i = q_ i + \Delta q_ i, \quad \Delta q_ i = W_ c \hat{o}_ i
$$

我们在这里使用了解耦检测头部（DDH）的变体，它将偏移量回归分解为两个分支。给定预测的 $\Delta q_ i$ ，一个分支回归一个标量 $r \in \mathbb{R}^1$ 来调节其长度，另一个分支回归一个向量 $R \in \mathbb{R}^4$ 来调节其方向。每个分支都包含一个轻量级的回归头部。 $R$ 被视为四元数的实部，可以转换成旋转矩阵以调节 $\Delta q_ i$ 的方向。因此，最终的偏移 $\Delta q'_ i$ 计算如下：

$$
\Delta q'_ i = f_ T(R) \cdot (r \Delta q_ i)
$$
 
其中 $\cdot$ 表示点积。 $f_ T$ 是将四元数转换为 $3 \times 3$ 旋转矩阵的转换函数。

## B. HyperFormer3D

在这一小节中，我们首先概述提出的 HyperFormer3D。然后，我们展示如何通过学习一组权重标记，并将它们通过基于 Transformer 的架构（HyperFormer）整合到主检测器网络中，从而更精确地学习场景先验。我们同样介绍多级标记融合（MLTF）机制，以便更好地将场景条件嵌入与几何和语义感知的标记结合。

1)为精确场景先验设计的属性场景条件 3D 对象检测器：尽管 HyperDet3D 中的场景条件知识提供了表示的多样性和灵活性，但 HyperDet3D 的上游超网络以包含噪声和冗余的原始场景作为输入。因此，给定某个特定的原始输入场景，动态超网络仅在下游检测头部的损失约束下为初级网络产生次优参数。为此，我们考虑将下游 3D 对象检测任务分解为两个场景级预文本任务，即语义分类和边界框回归。相应地，我们在 HyperFormer3D 的上游超网络中为对象表示设计场景级预文本任务，即 Semantic Occurrence 和 Objectness Localization，如图 4 所示的玩具示例。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/7cd9f69ee0c244ce8ff9f514737ddf62.png#pic_ center" width="70%" />
</div>

通过学习这些特别设计的预文本任务，HyperFormer3D 能够提取面向任务的场景先验，以消除噪声和冗余。研究 [60], [61] 也发现，当上游超网络 H 在下游检测网络 G 及其损失 L 的嵌套函数下优化时，即 L ◦ G，由于局部最小值的普遍性，H 的参数难以优化。因此，HyperFormer3D 也旨在通过设计 H 中的上游预文本任务，更有效地学习检测网络中的对象表示的场景先验。

HyperFormer3D 给定场景查询 Pd 并学习一组可学习的权重标记 $Z_ s = \{z_ {sk} \in \mathbb{R}^{C_ s}\}^{n_ e}_ {k=1}$ 。我们将超网络的场景查询表示为点状特征，因为它们在语义上和几何上具有更高的信息容量，为了方便起见，仍记为 Pd。此外，我们使用 Pd 和 $Z_ s$ 的组合表示来执行预文本任务。对于语义出现，我们预测一个 C 维向量，其中 C 是数据集中注释的语义类别的数量。例如，如果场景中椅子的出现次数大于预定义的阈值超参数，其对应的语义出现布尔标签就设置为 TRUE。对于对象性定位，我们预测一组坐标，指示任何感兴趣对象最有可能的位置。因此，在训练阶段，我们设计了并行预测头 $f_ {\theta_ e}$ 和 $f_ {\theta_ o}$ 来在上游超网络中执行二元回归：

$$
f_ {\theta_ e}(\cdot) = \sigma(W_ e \text{Pool}\{MLP_ e(\cdot)\})
$$
 
$$
f_ {\theta_ o}(\cdot) = \sigma(W_ o \text{Pool}\{MLPo(\cdot)\})
$$

其中 $\sigma$ 表示 Sigmoid 函数，用于将预测归一化到 (0,1) 区间。MLP_ e 和 MLPo 是全连接层，用于维度扩展。We 和 Wo 将高维特征映射到表示语义出现和对象性的标量场。由于我们的目标是从预文本任务中学习场景级先验，所以采用均值池化以消除除批量大小和特征通道之外的所有维度。

2)基于 Transformer 的超网络（HyperFormer）：为了为主网络生成权重参数，HyperFormer3D 学习了一个基于 Transformer 的超网络（HyperFormer），它同时将权重标记转换为参数空间，并将其与学习到的场景先验中的层次上下文结合起来。为了更好地将学习到的场景先验与随机初始化的权重标记结合，我们在 HyperFormer3D 的输入级和网络级提出了多级标记融合（MLTF）。在 HyperFormer3D 的输入级（MLTF-I）， $f_ {\theta_ s}$ 和 $f_ {\theta_ o}$ 都消耗 $Z_ s$ 和 $W_ pPd$ 的组合结果，分别预测语义类别的出现（pe）和对象性的位置（po）：

$$
p_ e = f_ {\theta_ e}(Z_ s \oplus W_ pP_ d)
$$
$$
p_ o = f_ {\theta_ o}(Z_ s \oplus W_ pP_ d)
$$

其中（·⊕·）表示随机初始化的权重标记（ $z_ {sk}$ ）和场景表示（ $W_ pPd$ ）的融合函数。我们按照（5）使用连接作为融合技术。注意，如果我们在（11）中使用最大池化，那么 Pool{MLP(·)} 在（11）中的输出可能完全从随机初始化的 $z_ {sk}$ 中检索向量，这由于信息不足而无法执行预文本任务。我们将在实验部分展示与替代实现的比较。通过输入级融合输入场景并执行为 3D 对象检测量身定制的上游任务，权重标记在显式语义和几何感知的指导下进行迭代优化。在 HyperFormer3D 的网络级（MLTF-N），我们旨在利用 Transformer [35] 编码器层中的密集成对交互机制。具体来说，我们再次使用权重标记和输入数据的组合作为输入，并执行自注意力以自适应地将它们与可学习的权重融合：

$$
\{q_ w, q_ d\} = \text{Transblock}(t_ w \| t_ d, \{t_ w \| t_ d\})
$$

其中 $t_ d$ 是通过 MLP 从 Pd 映射的数据标记，权重标记 $t_ w = Z_ s$ 。Transblock 包含查询-键-值的投影层以及两对自注意力层和前馈层，如图 5 中的 MLTF-N 部分所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b6ad38061e434db2aed92ef2e3a82dc8.png#pic_ center" width="70%" />
</div>

自注意力和前馈层采用残差连接实现。我们对嵌入和数据标记都省略了位置编码。我们采用 MLTF-N 的输出与场景不可知先验融合，生成最终的输出权重，如 HyperDet3D 中所述。

3)损失函数：由于我们工作中的超网络输出解码器的线性层权重，并且检测头部保持不变，因此传统的 3D 对象检测损失 $L_ d$ [8], [10], [13] 由所有解码器层对 HyperDet3D 和 HyperFormer3D 计算，其中 k 表示解码器层的索引。对于涉及上游预文本任务学习的 HyperFormer3D，额外的损失函数 $L_ h$ 在超网络中进行优化：

$$
L_ h = \beta_ 1 L_ {\text{sem}} + \beta_ 2 L_ {\text{obj}}
$$

其中 $L_ {\text{sem}}$ 和 $L_ {\text{obj}}$ 分别是语义出现预测的 C 维逻辑预测的二元交叉熵损失和对象性定位预测的 $N_ p \times 3$ 维逻辑预测的二元交叉熵损失。 $N_ p$ 是表示预测对象性定位数量的超参数。HyperFormer3D 的最终损失公式为：

$$
L = \sum_ {k=1}^{N} L_ {\text{d}}^{(k)} + \left\lfloor \frac{n}{n_ {\text{g}}} \right\rfloor \sum_ {l=1}^{n_ {\text{g}}} L_ {\text{h}}^{(l)}
$$

其中 N 是解码器层的总数，n 是嵌入场景条件知识的解码器层的数量。在实践中，我们设置 N = 2n，其中场景条件知识每两个解码器层学习一次。 $n_ {\text{g}}$ 是一个 HyperFormer3D 网络负责的解码器层的数量，满足 $n \mod n_ {\text{g}} \equiv 0$ 。l 是 HyperFormer 网络的索引。总共有 $\left\lfloor \frac{n}{n_ {\text{g}}} \right\rfloor$ 个 HyperFormers 并行学习。如图 6 所示，权重标记顺序地转换为解码器参数空间。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c2b11a8f3ed849078a6f4e58d4a8c1da.png#pic_ center" width="70%" />
</div>

对于 HyperDet3D [1]，最终损失仅由所有检测头部中的检测损失组成：

$$
L = \sum_ {k=1}^{N} L_ {\text{d}}^{(k)}
$$

图 6 中，我们展示了 HyperDet3D（左）和 HyperFormer3D（右）之间的比较。除了超网络结构的差异外，权重标记在 HyperFormer3D 中作为场景特定嵌入，并且通过预文本任务头部提供的额外损失进行引导学习。注意，场景条件嵌入包含场景特定（SS）和场景不可知（SA）嵌入。（最佳视图以彩色显示）。

## C. 实现细节

对于 HyperDet3D 和 HyperFormer3D，骨干网络 PointNet++ [7] 包含 4 个集合抽象层，它们连续地将输入点云下采样到 {2048, 1024, 512, 256} 点。球查询的半径为 {0.2 m, 0.4 m, 0.8 m, 1.2m}。然后，2 个特征传播层将它们恢复到 1024 点，并产生逐点特征。我们使用 [13] 中提出的 KPS 来从原始 1024 点生成普通对象表示，因为它在 FPS [7] 的 kNN 搜索空间中节省了计算成本。对于每个解码器层中的 o，我们遵循 [12], [13]，采用标准的多头注意力层来计算对象候选的自注意力，然后是对象候选和骨干产生的下采样点之间的交叉注意力。至于检测头部，每个轻量级回归头部主要包含一个全连接 (FC) 层，将 f_ i 映射到 r 或 R。在 r 头部，FC 层的输出通过 Sigmoid 函数处理，并进一步归一化到 [0.9,1.1] 以控制调整的程度。在 R 头部，将 R 转换前加上单位四元数，这可以同时保持身份旋转的可能性并控制旋转程度。

对于 HyperDet3D，(3) 中的场景不可知超网络包含 2 个线性层。(5) 中的场景特定超网络包含 1 个线性层，后跟 Tanh 激活函数。每个线性层由 Xavier [62] 初始化的权重矩阵和零初始化的偏置向量参数化。对于场景特定超网络的场景查询 P_ id，我们使用现成的 KPS 下采样结果。按照 [13]，ScanNet [15] 和 SUN RGB-D [16] 的解码器层数分别为 12 和 6。我们将 Ca 设置为 256，n 设置为 256 对于两个数据集。我们分别为 ScanNet 和 SUN RGB-D 设置 Cs 为 253 和 285。

对于 HyperFormer3D，高维 Pd 由骨干编码器产生。在图 5 中的每个自注意力或前馈块之后应用 Layer Normalization [63]。为了定义语义出现的真值标签，我们一致地将阈值设置为 1 作为所有类别的默认值。例如，只要椅子在场景中至少出现一次，其在 HyperFormer3D 中的语义出现的布尔标签就设置为 TRUE。对于对象性定位的预文本任务，我们一致地将预测位置 (Np) 的数量设置为 256。

# IV. 实验

在实验部分，我们首先介绍用于3D对象检测的基准数据集和评估指标（第IV-A节）。然后，我们通过定量（第IV-B节）和定性（第IV-C节）的实验结果，将HyperDet3D和HyperFormer3D与最先进的基于点的方法进行比较。我们通过消融研究和跨数据集评估（第IV-D节），分析了所提出方法的设计选择和有效性。

## A. 数据集和设置

ScanNet V2：ScanNet V2数据集[15]包括1,513个扫描和重建的室内场景，有18个对象类别的轴对齐边界框标签。点云数据由重建的网格转换而来。按照[8]的做法，我们使用1,201个场景作为训练集，其余312个验证场景作为测试集。

SUN RGB-D V1：SUN RGB-D V1数据集[16]包含10,000个单视图室内RGB-D图像，其中5,285个用于训练，5,050个用于测试。它密集地注释了64,000个定向的3D边界框。整个数据集分为37个室内对象类别。为了公平比较，我们遵循[8]中的评估协议，选择了10个最常见的类别。

MatterPort3D：MatterPort3D数据集[17]包含超过194,000个RGB-D图像的90个建筑规模场景。它涵盖了ScanNet三倍的房间数量和更多的房间类型。按照[75]的做法，我们选择了MatterPort3D和ModelNet40[76]共有的13个常见类别，每个类别在MatterPort3D训练集中至少包含80个对象实例。

对于所有3个数据集，我们采用点云数据作为输入。按照[8]，我们通过计算3D IoU阈值为0.25（mAP@0.25）和0.5（mAP@0.5）的平均精度均值（mAP）在验证集上报告结果。显示了个别类别的检测性能及其平均结果。我们为ScanNet, SUN RGB-D和MatterPort3D分别设置{β1, β2}为{0.1,0.05}, {0.02,0.05}和{0.03,0.05}。

为了稳定性，在训练期间，标准的线性层在前100个周期内替换了(2)中的超网络，以用于ScanNet和SUN RGB-D数据集。我们在其余的训练周期中微调了网络模型。在测试时使用微调后的网络进行推理。MatterPort3D上的训练从零开始。

## B. 定量结果

我们定量地将我们的方法与许多参考方法进行比较，这些方法可以分为3类：需要2D引导来定位3D对象的早期方法[2],[3], [65], [66], [67], [72], [73]，探索最优局部表示以提供信息线索的基于投票的方法[8], [29], [30], [31], [32], [56], [68], [69], [74]，以及探索对象或点簇等局部元素之间交互的基于关系的方法[9], [10], [12], [13], [14], [34], [36]。我们主要与基于点的方法进行比较，以与我们的HyperDet3D和HyperFormer3D保持一致。实验结果展示在表I和表II中。加粗的表示在只有点云作为输入模态时的最佳结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/590807b6933349fc9aba36f364f16fbc.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a2dcba8a045f43e4a500a0932c5f6be9.png#pic_ center" width="70%" />
</div>

从表I中的比较结果可以看出，HyperDet3D或HyperFormer3D在ScanNet V2和SUN RGB-D V1验证集的两个阈值下都取得了领先的AP。请注意，与SUN RGB-D相比，ScanNet用1.8倍的类别进行了3D检测任务的注释。因此，HyperDet3D和HyperFormer3D学习的场景级先验在ScanNet上比SUN RGB-D更为丰富，并且在前一个数据集上获得了更显著的mAP增益。

然后我们查看表II和表III中ScanNet和SUN RGB-D的每个类别结果。最佳和第二佳结果分别加粗和下划线。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c9d96a6d4fca4c989295f9bb7062f629.png#pic_ center" width="70%" />
</div>

对于那些在很大程度上依赖于场景先验的类别（例如卧室中的dresser/nightstand，kitchen/canteen中的fridge，浴室中的shower curtain/toilet/sink/bathtub），HyperDet3D或HyperFormer3D与基线方法相比获得了显著的AP增益。这表明了HyperDet3D和HyperFormer3D学习到的场景条件知识的效力。HyperDet3D在bookshelf类别上的性能有所下降，部分原因是两个数据集中library场景的稀缺性。尽管如此，HyperFormer3D通过学习设计好的预文本任务中面向任务的场景先验，提高了bookshelf的检测性能。

在表IV中，我们进一步将我们的方法与最先进的GF3D [13]进行比较。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/26b89940a7ab44d8864d2cc1ab179ac7.png#pic_ center" width="70%" />
</div>

可以看出，在正常或轻量级网络配置中，HyperDet3D在两个指标上都优于GF3D，同时包含更少的可学习参数。因此，HyperDet3D可能有效地吸收外部数据，这是由于场景条件超网络的机制和不同层之间的知识共享。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ad9bf2843fc24841a320f8fce3ad9f2f.png#pic_ center" width="70%" />
</div>

在表VI中，我们彻底比较了HyperDet3D和HyperFormer3D的参数数量，GPU内存消耗和推理速度。我们可以看到，尽管MLTF-N中的自注意力层贡献了额外的可学习参数(+5.2 M)，但HyperFormer在保持边际内存占用和推理时间的同时，仍然取得了显著更好的检测性能。

我们在更具挑战性的MatterPort3D [17]数据集上进一步进行了实验。由于很少有工作在这个基准上进行完全监督设置，我们实现了3种流行的方法，即VoteNet [8], MLCVNet [10]和H3DNet [29]。比较结果还包括GF3D [13]，其结果取自[75]。在表V中展示了mAP@0.25, mAP@0.5和在阈值为0.25下的每个类别结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ebea834712fa478a80111d443562464e.png#pic_ center" width="70%" />
</div>

AP数字波动很大，因为与ScanNet和SUN RGB-D相比，数据集更具挑战性。具体来说，提出的方法在一些困难的类别上取得了有利的结果，如bench, curtain和sofa，但在bed和door上表现较差。然而，HyperDet3D和HyperFormer在整体mAP@0.25和mAP@0.5指标上，与基线方法相比，始终展现出最先进的结果。HyperFormer在mAP@0.25和mAP@0.5上分别进一步提高了0.9%和2.0%。

## C. 定性结果

在图7中，我们首先比较了ScanNet中4个扫描的检测结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/830ec6eaa554456389284497fde49418.png#pic_ center" width="70%" />
</div>

以真实标注（GT）和真实图像作为参考，我们比较了HyperDet3D与涉及对象候选之间密集交互的基线方法[13]。前3个扫描突出显示了大量相交边界框方面的歧义，其中[13]将冰箱或洗衣机误认为是橱柜，或在办公室中检测到水槽。借助场景条件先验知识的帮助，我们的HyperDet3D可以在这些对象上获得更好的检测结果。歧义还包括错误的检测。例如，在最后一个扫描中，基线方法将卧室中的橱柜误认为是柜台。然后，我们展示了一些HyperDet3D仍然混淆的对象，这些对象不太依赖于场景类型（例如，橱柜，沙发和桌子）。此外，它仍然在长尾分布中稀缺场景相关联的对象上存在困难（例如，图书馆中的书架）。

在图8中，我们比较了HyperFormer3D与HyperDet3D在更具挑战性的SUN RGB-D数据集上的表现。我们可以看到，HyperFormer3D进一步改善了nightstand, bookshelf, dresser（在前3个示例中）的检测，并消除了bookshelf（在最后一个示例中）的错误检测，这得益于面向任务的场景先验。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/dc6bd5073f3c4dca96069b44fc608796.png#pic_ center" width="70%" />
</div>

## D. 消融研究和讨论

为了分析HyperDet3D中学习到的场景条件知识的重要性，我们首先对各种设计选择的组合进行了消融实验。定量结果展示在表VII中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e7f2cc467b7a40649a72f884f0231d66.png#pic_ center" width="70%" />
</div>

基线模型仅使用了解耦的检测头部，我们为了清晰的比较将其对应的行置灰。应用SSA来学习场景条件知识，使得mAP@0.25提高了1.2%，mAP@0.5提高了1.2%。多头变体（MSA）进一步带来了+1.1% mAP@0.25和+2.5% mAP@0.5。正如预期的那样，仅学习场景不可知或场景特定先验知识是不充分的，对于彻底的场景条件理解。对于具有挑战性的mAP@0.5指标，仅学习场景不可知或场景特定知识会导致性能下降-1.8%和-3.4%。移除中心偏移的解耦回归导致了-0.6% mAP@0.25和-0.4% mAP@0.5的性能下降，这表明精细的目标回归有助于利用学习到的场景条件知识。

跨数据集评估：由于HyperDet3D将场景条件知识作为先验学习，我们推断当面临领域差异时，通过检测器学习到的场景条件知识仍然有效。为了验证这一点，我们进行了跨数据集评估，并与VoteNet [8]和GF3D [13]作为基线检测器进行了比较。我们首先在SUN RGB-D上预训练了基线检测器和HyperDet3D，然后在ScanNet上进行了微调。在所有3种方法中，骨干网络和我们的场景条件超网络在微调期间被冻结。

在表VIII中，我们展示了SUN RGB-D和ScanNet之间共享的8个类别的mAP，以及ScanNet中所有18个（mAP8）类别或共享的8个（mAP18）类别的平均mAP。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a2d3aa2df63d45efaf8021a99c565a3f.png#pic_ center" width="70%" />
</div>

观察结果是两方面的。我们的HyperDet3D在mAP8和mAP18上都超过了基线方法，特别是在两个数据集之间共享的类别上。这表明通过我们的方法在源数据集上学到的场景条件知识可以很好地转移到目标数据集。另一方面，在这8个共享类别中，更多依赖于场景语义的类别与表I中的结果类似，有显著的改进幅度。例外是书架类别，部分原因是SUN-RGBD [16]中图书馆场景的稀缺性（1.9%）。此外，像冰箱和水槽这样的新类别分别提高了+14.9%和+11.1%。

附加场景标签的整合：一个有趣的问题是，如果我们使用真实的场景标签作为额外的监督信号会怎样？为此，我们在GF3D的瓶颈处添加了一个分类（Cls.）分支，并基于GF3D预训练模型对整个网络进行了100个周期的微调。表IX中的ScanNet结果表明，带有场景类型标签的额外分支提高了检测性能，但仍然不如HyperDet3D。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/8d518a60aae543f598e2e11747c76522.png#pic_ center" width="70%" />
</div>

注意，HyperDet3D在没有场景类型分类监督的情况下取得了更好的结果。我们期望通过为每种场景类型训练一个独特的检测器来获得更好的检测性能。然而，这可能会限制方法的通用性，并且对于现实世界的应用来说，在计算上不够友好。

我们也在HyperFormer3D的超网络中尝试采用场景类型预测作为我们的预文本任务。从表IX的下半部分结果来看，可以发现显式预测场景类型作为唯一的先验知识对下游3D对象检测是有益的（None → T, 70.5→71.5 on mAP@0.25）。然而，结合预文本任务（O+S → O+S+T）的改进是可以忽略不计的。场景类型先验在语义出现（S → S+T）上表现出其效力，但几乎不改进对象性定位先验（O → O+T）。我们推断，虽然场景类型先验可以明确地帮助推导对象表示，但场景类型和对象类别之间的联系在整个数据集中并不是确定性的。从适度细粒度的预文本任务（如对象性位置和语义出现）中学习提高了灵活性，从而容忍一些脱离上下文的情况。

语义出现的阈值：HyperFormer3D的语义出现默认阈值设置为1。换句话说，如果给定场景中至少存在一把椅子，HyperFormer3D中的语义出现布尔标签就设置为 TRUE。为了进一步测试语义出现阈值对检测结果的影响，我们首先统计了ScanNet和MatterPort3D数据集中每个语义类别的平均出现频率，如图9所示。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/72bdabc6fce045c7be9b12915cf6fe3d.png#pic_ center" width="70%" />
</div>

请注意，出现频率是按出现场景的数量平均的，而不是按所有可用场景平均的，因此数值总是≥ 1。然后，我们一致地将语义出现的阈值在[1, 5]内进行消融，并在图10中总结了在ScanNet和MatterPort3D上的实验结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/57dabb7dd33143478b00ca8371263b3a.png#pic_ center" width="70%" />
</div>

我们观察到阈值1已经可以很好地定义语义出现。此外，在图9中，MatterPort3D中平均出现频率≥ 2的类别比例小于ScanNet（3/13 < 5/18）。因此，当增加阈值时，MatterPort3D的性能下降得更剧烈。由于语义类别的数量通常不少于10个，我们没有为每个类别设置独特的阈值，这可能是未来工作的内容。

输入级MLTF后的池化函数：默认情况下，我们在(11)中使用均值池化来消耗MLTF-I处融合的结果。我们在上表X的上半部分展示了池化函数的消融结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/099e76f9538b4b4193bfbe7f29731582.png#pic_ center" width="70%" />
</div>

我们可以看到，在均值池化和最大池化中，检测性能都明显下降，其中汇聚的结果可以完全从权重标记中检索出来。相反，均值池化明确地在相同索引的权重标记和输入表示中聚合元素。加权求和池化仅实现了可比或更差的结果，同时带来了额外的计算成本。此外，这个消融研究表明了两种类型的标记之间潜在的信息差异。均值池化技术作为后处理，增强了信息聚合质量，同时弥合了这种差异。

预文本任务的位置：由于HyperFormer的输入和输出由于基于Transformer的架构而对齐（如图5所示），我们可以轻松地将输入级标记融合移动到输出级（MLTF-O）。上表X中的数字表明，与默认设计选择相比，MLTF-O是一个次优的实现，其中在执行MLTF-I中的上游预文本任务后，进行了密集的MLTF-N计算。我们推断，在HyperFormer中（MLTF-N）的计算更好地将权重标记与上游预文本任务中学习到的场景先验结合起来。

为了测试预文本任务在其他方法（如GF3D [13]和HyperDet3D [1]）中的有效性，我们将对象性和语义出现的学习整合到检测头部，即GF3D、HyperDet3D和HyperFormer3D的下游位置，以及超网络部分，即HyperDet3D的上游位置。如表XI所示，实验结果的观察结果是两方面的。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/1dd313ad1a264ee59b00fb1b7d2a5f12.png#pic_ center" width="70%" />
</div>

首先，场景先验上下文在上游超网络中比在下游检测头部学得更好。这表明了自然使用上游场景先验指导下游3D对象检测任务的有效性。其次，没有场景先验上下文，HyperFormer3D中的MLTF在更严格的度量下仍然有效（57.2→57.7 on mAP@0.5），但在mAP@0.25上从70.9下降到70.5。相比之下，通过在HyperFormer3D中明确地从上游预文本任务中学习分层场景先验，带来了更显著的性能提升（70.5→73.6 on mAP@0.25, 57.7→58.7 on mAP@0.5）。

HyperFormer3D中场景先验的有效性：如表X的下半部分所示，我们通过将语义出现（w./o. S）或对象性定位（w./o. O）的预测移除，并设置相应的β值为0，测试了场景先验的有效性。我们可以观察到在两种试验中都有明显的性能下降，这表明了两种显式分层上下文在场景先验中的有效性。通过MLTF-I学习到的组合场景先验为HyperFormer3D带来了最佳结果。

HyperFormer3D的位置编码：默认情况下，我们省略了HyperFormer3D中权重和数据标记的位置编码，如图5所示。我们还测试了添加目标解码器索引和数据标记的xyz坐标的位置编码。具体来说，3D坐标或单热层索引通过另一个全连接层映射到标记特征空间，然后添加到相应的标记中。然而，在试验中没有观察到显著的性能提升或下降。这可以归因于高维数据标记已经嵌入了其位置信息，因为它们实际上是从骨干网络中的低维坐标映射而来。此外，层索引在下游检测网络中很难为对象表示提供信息性的先验。

在3DETR上的场景条件学习：我们进一步测试了所提出方法在3DETR [12]上的有效性，它也包含顺序解码器层。具体来说，我们将HyperDet3D实现在3DETR的Transformer解码器层中。从表XII中可以看出，虽然我们在0.25的阈值下取得了略低的mAP（-0.3%），但在更难的指标mAP@0.5上性能显著提高（+3.9%）。GPU内存消耗与原始网络相比略有增加，这是在训练阶段用默认批量大小和查询测量的。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9421e20d197a4202a59e2ebe86e12233.png#pic_ center" width="70%" />
</div>


户外场景的泛化能力：为了测试场景条件学习在户外场景等分布外数据上的泛化能力，我们进行了以下实验（1）从零开始训练HyperDet3D在户外KITTI [77]上，以及（2）在室内SUN RGBD（+SR）上预训练HyperDet3D并在户外KITTI上微调。在表XIII中报告了通过0.25和0.5阈值的检测mAP。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/805c1802925e4300b8989b9936dfbbec.png#pic_ center" width="70%" />
</div>

从表XIII中可以看出，户外场景的检测可以从室内场景的预训练中受益。然而，由于领域差距更大，将室内先验转移到户外的改进不如表VIII中的SUN RGBD → ScanNet案例那样显著。尽管如此，如图11所示，在微调的早期阶段，目标损失被更急剧地最小化。我们推断，虽然场景先验本身在室内和室外场景中可能不同，但场景条件学习机制事先已经很好地建立，因此在微调到KITTI时提供了网络参数的适当初始化。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/7fc9e2ac700947369b671551307e51c1.png#pic_ center" width="70%" />
</div>

## E. 限制

像大多数先前的方法[8], [10], [13]一样，我们的方法主要在室内场景上进行了实验。虽然在将学习到的室内先验转移到分布外的室外场景时可以观察到改进，但我们方法的最终检测性能仍然不如为室外场景设计的最先进方法。此外，场景先验可能会受到对象类别和场景类型之间相关性的影响，以及数据集中场景类型分布的影响。因此，并非所有语义类别都能从学习到的场景先验中受益，特别是那些与特定场景类型相关性较小的类别，或者与长尾分布中稀缺场景相关联的类别。

# V. 结论

在本文中，我们介绍了HyperDet3D：一个动态探索场景条件先验作为3D对象检测层次上下文的框架。我们的HyperDet3D同时学习了场景不可知知识，它从各种3D场景中探索可共享的抽象，以及场景特定知识，它使检测器适应给定的场景。我们进一步提出了基于HyperDet3D的HyperFormer3D，它通过上游预文本任务学习面向任务的场景先验，并更好地将超网络与给定场景结合。我们通过基准比较、跨数据集微调、消融研究和泛化实验，全面展示了所提出方法的有效性。


