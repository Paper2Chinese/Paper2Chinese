# 题目：[Sheared Epipolar Focus Spectrum for Dense Light Field Reconstruction](https://ieeexplore.ieee.org/document/10334032/)  
## 用于稠密光场重建的剪切焦谱
**作者：Yaning Li; Xue Wang; Guoqing Zhou; Hao Zhu; Qing Wang** 


****

# 摘要

本文提出了一种从稀疏输入视图进行光场（LFs）密集重建的新技术。我们的方法利用了Epipolar Focus Spectrum (EFS) 表示法，它通过变换的空间-焦点域来建模光场，避免了对场景深度的依赖，为密集光场重建提供了高质量的基础。先前的基于EFS的光场重建方法同时学习跨视图、遮挡、深度和剪切项，这使得训练过程因稳定性和收敛问题而变得困难，并进一步导致在具有挑战性的场景中的重建性能受限。为了解决这个问题，我们对从具有稀疏和密集角采样的LF导出的EFSs之间的转换进行了理论研究，并提出密集EFS可以明确地分解为稀疏输入EFS、剪切EFS和高阶遮挡项的线性组合。我们设计的基于学习的框架输入欠采样EFS及其剪切版本，提供了高质量的重建结果，特别是在大视差区域。全面的实验评估表明，我们的方法在重建包含细结构的场景时，与现有最先进方法相比，最多实现了> 4 dB的优势。

# 关键词

- 表观焦谱（EFS）
-  剪切表观焦谱
- 焦距堆栈
- 光场重建

# I. 引言

光场(LF)成像技术作为一种强大的工具，已经出现，它可以捕获具有新颖和独特效果的图像，包括自由视点成像[1]、全聚焦成像[3]和4D编辑[4]、[5]。由于其卓越的能力，光场成像在计算摄影社区[6]中获得了相当大的关注。然而，光场成像的一个基本挑战是“空间-角度分辨率权衡”[7]、[8]、[9]，这限制了光场系统的获取分辨率。具体来说，一个维度分辨率的增加会导致另一个维度分辨率的减少。因此，从稀疏采样的采集中重建一个密集采样的光场是至关重要的。

近年来，为了重建高角度分辨率的光场(LFs)，文献中提出了几种方法。尽管取得了显著进展，但仍然存在几个挑战。在空间域中，现有方法，如[10]、[11]、[12]、[13]、[14]、[15]、[16]中的那些，涉及估计深度、变形输入视图和细化新视图的纹理。尽管研究工作大量投入以提高深度精度[10]、场景表示[17]、[18]和纹理一致性[19]、[20]，但由于这些方法中采用的独立视图优化的固有性质，保持重建光场的视图一致性仍然具有挑战性。此外，大视差区域中的遮挡累积会导致显著的伪影。在频率域中，一些现有工作，如[21]、[22]、[23]、[24]、[25]中的那些，专注于使用3D焦点堆栈和4D LF之间的维度差距[22]、连续傅里叶域中的稀疏性[23]或剪切域中的频带一致性[24]进行傅里叶谱补全。然而，这些方法只对非重叠和连续的像平面图像(EPI)线建模，并且不能对有交叉和离散EPI点/线段建模，即遮挡和大视差。这些模型的基本不完整性导致重建的遮挡边界出现伪影。

最近引入了一种新的表示法，称为表观焦谱(EFS)，用于4D LF的抗锯齿重聚焦[26]和密集重建[27]。EFS具有对场景深度不变的语义属性。渲染一个完整的EFS字面上有助于视图一致性光场重建的任务。然而，现有的文献[26]、[27]主要关注EFS内幅度谱的分布，对从稀疏和密集LF数据导出的EFS之间的转换分析有限。

在本文中，我们提出了EFS理论的增强公式，并推广了其在光场重建领域的适用性。我们的分析显示，密集采样的LF的EFS可以表示为三个组成项的线性组合，即对应于稀疏采样LF的EFS、剪切稀疏EFS和交叉项。基于这一洞见，我们引入了一个新颖的剪切模块，有助于预处理输入的稀疏EFS，以生成密集EFS的粗略近似。随后，我们提出了一种特别设计的Occlusion-aware Dual-stream U-Net (ODU-Net)架构，专门用于细化粗略近似。最后，通过将逆傅里叶切片摄影技术[28]应用于细化的EFS，获得重建的LF。与先前的方法相比，我们的方法中采用的剪切操作降低了网络训练期间频谱补全的难度，从而提高了准确性。我们通过在合成和真实世界的LF数据集上进行广泛的实验评估，证明了我们提出的方法的有效性（详见第V节）。

本文做出了几个重要贡献，包括：1) 对基于稀疏采样EFS的EFS进行了正式分解。分析了稀疏和密集LF的EFS之间的关系，并得出结论，为高质量、视图一致性的LF重建提供了坚实的理论基础。2) 特别设计的ODU-Net，用于密集LF重建。当应用于剪切稀疏EFS时，ODU-Net不仅减轻了频谱补全的训练难度，还提高了视图重建的准确性。

# III. 理论分析

为了更好地分析稀疏EFS和密集EFS之间的关系，我们首先介绍背景和整篇论文中使用的符号。然后，我们将推导出剪切EFS的完整理论分析。

## A. 符号

给定一个4D光场  $L(u, v, x, y)$ ， $E_ n(u, x)$  是通过固定  $(v, y) = (v^\*, y^\*)$  得到的像平面图像（EPI），其中  $n$  是EPI中的视图数量。 $E_ {kn}(u, x)$  指的是  $k$  倍密集采样的EPI。注意，对于  $E_ n(u, x)$  和  $E_ {kn}(u, x)$  的建模，我们采用相同的单位，即  $E_ n(u, x)$  中的视图索引集合是  $1, k+1, 2k+1, ..., (n-1)k+1$ ，而  $E_ {kn}(u, x)$  中的视图索引集合是  $1, 2, ..., nk$ 。 $E_ n(\omega_ u, \omega_ x)$  表示  $E_ n(u, x)$  的傅里叶谱， $F_ n(f, \omega_ x)$  是在切片后  $E_ n(\omega_ u, \omega_ x)$  的重新排列。

 $F_ n(f, x)$  是由  $E_ n(u, x)$  形成的焦点堆栈，其中  $f$  指的是相对焦点深度。需要注意的是， $F_ n(f, x)$  是以  $\Delta\alpha$  的重聚焦步骤构建的，参考视图  $u_ {ref}$  在整篇论文中被设置为中心视图。 $\Delta F(f, x)$  指的是由额外的  $(k-1)n$  个视图形成的焦点堆栈，即视图集合  $\{2, 3, ..., k, k+2, k+3, ..., 2k, ..., (n-1)k+2, ..., nk\}$ 。 $EFS_ n(\omega_ f, \omega_ x)$  是  $E_ n(u, x)$  的表观焦谱，或者说，是  $F_ n(f, x)$  的傅里叶谱。 $\Delta EFS(\omega_ f, \omega_ x)$  是  $\Delta F(f, x)$  的傅里叶谱。

 $FT_ {1D}(\cdot)$  和  $FT_ {2D}(\cdot)$  分别是对信号  $\cdot$  的一维和二维傅里叶变换。表I列出了整篇论文中使用的所有符号。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/73d512226e6940239afdad3cff53d15c.png#pic_ center" width="70%" />
</div>

## B. 背景

给定一个2D EPI  $E_ n(u, x)$ ，其EFS可以通过两种方式构建[27]。在空间域中，首先构建焦点堆栈  $F_ n(f, x)$ ，然后通过二维傅里叶变换处理，得到

$$
F_ n(f, x) = \frac{1}{n} \sum_ {u=1}^{n} E_ n(u, x + f(u - u_ {ref})), \quad (1a)
$$
 
$$
EFS_ n(\omega_ f, \omega_ x) = FT_ {2D}(F_ n(f, x)). \quad (1b)
$$

在傅里叶域中，首先获得谱  $E_ n(\omega_ u, \omega_ x)$ ，然后以切片方式重新排列。最后，对重新排列的EPI谱应用一维傅里叶变换，

$$
F_ n(f, \omega_ x) = E_ n(-f\omega_ x, \omega_ x), \quad (2a)
$$
 
$$
EFS_ n(\omega_ f, \omega_ x) = FT_ {1D}(F_ n(f, \omega_ x)). \quad (2b)
$$

请参考原始EFS论文[27]和傅里叶切片摄影理论，了解上述方程的等价性。

根据[26]、[27]，EFS的模式与场景深度无关，并且每个EFS线与光场中的视图有一一对应关系。所有线都通过原点。每条线的斜率由重聚焦步长  $\Delta\alpha$  和当前视图与参考视图之间的间隔决定。

## C. 从稀疏焦堆表示稠密焦堆

根据视图和EFS线之间的一一对应关系，从稀疏的光场  $E_ n$  重建密集的光场  $E_ {kn}$  的任务可以被建模为在  $EFS_ n$  中“插入”其他  $(k-1)n$  个视图的能量带，然后调制相位。关键是通过从输入的  $n$  个视图中表示其他  $(k-1)n$  个视图的信息来实现“插入”过程。

让我们从焦点堆栈的表示开始。根据从EPI生成焦点堆栈的线性加权组合特性，焦点堆栈  $F_ {kn}$  可以被分解为  $F_ n$  和  $\Delta F$  的和，

$$
F_ {kn} = \frac{1}{k} F_ n + \frac{k-1}{k} \Delta F. \quad (3)
$$

相应地，(3) 可以根据傅里叶变换的线性属性在傅里叶域中重写，

$$
EFS_ {kn} = \frac{1}{k} EFS_ n + \frac{k-1}{k} \Delta EFS. \quad (4)
$$

在接下来的段落中，我们将分别在单点和多点场景中分析(3)和(4)的扩展。

1)具有单点的场景: 为了简化从  $F_ n$  推导  $\Delta F$  的过程，我们首先关注一个只存在一个点 P 的简单场景。

Spatial Domain: 图1直观地展示了 k=2 和 n=3 采样的一个点的情况。图1(a)显示了焦点堆栈  $F_ n$ 。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/b59226c0f2e64005b3d06f5af28130d2.png#pic_ center" width="70%" />
</div>

像素 p 位于焦点深度  $f^\*_ p$ 。p1、p3 和 p5 分别来自视图 u1、u3 和 u5，并且位于散焦深度 f。根据朗伯假设，p1、p3 和 p5 的强度可以被建模为 p 的强度和背景的混合。考虑到空间中只存在一个点，即背景的强度是一个常数值 C。因此，pj(f, xj) 的强度可以被建模为

$$
\forall j \in \{1, 3, 5\}, F_ n(f, x_ j) = \frac{1}{3} F_ n(f^\*_ p, x) + \frac{2}{3}C. \quad (5)
$$

图1(b)显示了由其他3个视图形成的焦点堆栈  $\Delta F$ ，我们也有

$$
\forall j \in \{2, 4, 6\}, \Delta F(f, x_ j) = \frac{1}{3}\Delta F(f^\*_ p, x) + \frac{2}{3}C. \quad (6)
$$

根据朗伯假设，我们有  $F_ n(f^\*_ p, x) = \Delta F(f^\*_ p, x)$ 。通过结合(5)和(6)，以下等式成立，

$$
\forall i \in \{1, 3, 5\}, j \in \{2, 4, 6\}, F_ n(f, x_ i) = \Delta F(f, x_ j). \quad (7)
$$

由于  $F_ n$  中相邻视图之间的基线等于  $\Delta F$  中的基线（对于任何 i ∈ {2, 4, 6}，ui - ui-1 = ui+1 - ui）， $\Delta F$  可以通过剪切  $F_ n$  来表示，

$$
\Delta F(f, x) = F_ n(f, x + \Delta x), \quad (8)
$$

其中  $\Delta x$  模拟了沿着焦点线 f 的线段 pp1 和 pp2 之间的间隔，在图1(c)中（例如，当扫描线 f^\*_ p 时  $\Delta x = 0$ ）。根据焦点堆栈构造中的散焦模糊半径，我们有  $\Delta x = \Delta u(f^\*_ p - f)$ ，其中  $\Delta u = u_ 2 - u_ 1$  描述了第一组视图 {u1, u3, u5} 和第二组视图 {u2, u4, u6} 之间的距离。因此，(8) 可以扩展为

$$
\Delta F(f, x) = F_ n(f, x + \Delta u(f^\*_ p - f)). \quad (9)
$$

此外，通过将简单示例 (k = 2, n = 3) 扩展到一般情况，(9) 被重新表述为

$$
\Delta F(f, x) = \frac{1}{k-1} \sum_ {j=1}^{k-1} F_ n(f, x + \Delta u_ j(f^\*_ p - f)), \quad (10)
$$

其中  $\Delta u_ j = u_ {1+j} - u_ 1$ ，指的是原始视图集合 {u1, uk+1, u2k+1, ..., unk+1} 和第 j 组重建视图 {u1+j, uk+1+j, ..., unk+1+j} 之间的距离。因此，(3) 可以重写为

$$
F_ {kn}(f, x) = \frac{1}{k} [F_ n(f, x) + \sum_ {j=1}^{k-1} F_ n(f, x + \Delta u_ j(f^\*_ p - f))]. \quad (11)
$$

Fourier Domain: 在(11)中描述的  $\Delta F$  的剪切过程可以被表述为仿射变换

$$
\begin{bmatrix} f' \\x' \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ -\Delta u_ j & 1 \end{bmatrix} \begin{bmatrix} f \\ x \end{bmatrix} + \begin{bmatrix} 0 \\ f^\*_ p \Delta u_ j \end{bmatrix}, \quad (12)
$$

根据傅里叶变换的仿射定理[49]，空间域中的仿射变换等同于傅里叶域中的仿射变换、幅度缩放和相位调制的组合。简而言之， $\omega = (\omega_ f, \omega_ x)^T$ ，因此，

$$
EFS_ {kn} = \frac{1}{k} [EFS_ n + \sum_ {j=1}^{k-1} \frac{1}{|\det(A)|}e^{2\pi i x_ p^0 A^{-T} \omega} EFS_ n(A^{-T} \omega)], \quad (13)
$$

其中 i 代表满足  $i^2 = -1$  的虚数。

比较(11)和(13)，发现剪切过程只能在已知点的焦点深度时在空间域中实现，然而在傅里叶域中，剪切过程是可分离的，并且功率谱中的剪切与点的焦点深度无关。

2)从单点到多点场景: 上述分析集中在只有一个点的案例上。当场景中存在许多点时，(11)不再适用，因为每个点的焦点深度都不相同。为了分析多点情况下的情况，引入额外的掩模集合  $\{M_ p\}_ {p=1}^{N_ p}$  将  $F_ n$  分解为多个焦点堆栈  $\{F_ {p_ n}\}_ {p=1}^{N_ p}$ ，其中每个  $F_ {p_ n}$  只包含一个点， $N_ p$  表示场景中的点数。除此之外，还会出现跨视图现象（见图2中的蓝色框）。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/bab252603e11402a979ca16368cb5c3c.png#pic_ center" width="70%" />
</div>

为了模拟跨视图效应，必须添加第三个项  $F_ {cross_ n}$ 。因此，(11) 可以修改为

$$
F_ {kn}(f, x) = \frac{1}{k} [F_ n(f, x) + \sum_ {j=1}^{k-1} \sum_ {p=1}^{N_ p} M_ p F_ n(f, x + \Delta u_ j(f^\*_ p - f))] + F_ {cross_ n}, \quad (14)
$$

其中矩阵 Mp 只包含 {0, 1} 的值，并且  $\sum_ {p=1}^{N_ p} M_ p = 1$ 。这里的 1 表示完全用值 1 填充的矩阵。图2说明了将具有多个点的焦点堆栈分解为每个包含一个点的多个焦点堆栈的概念。结合傅里叶变换的仿射定理[49]和卷积定理[50]，公式可以写成

$$
EFS_ {kn} = \frac{1}{k} [EFS_ n + \sum_ {j=1}^{k-1} \sum_ {p=1}^{N_ p} FT_ {2D}(M_ p) \* e^{2\pi i x_ p^0 A^{-T} \omega} EFS_ n(A^{-T} \omega)] + FT_ {2D}(F_ {cross_ n})], \quad (15)
$$

其中 \* 表示卷积运算符。

共享剪切操作: 值得注意的是，由于剪切操作与深度耦合，在空间域中实现(14)中的剪切过程是具有挑战性的。幸运的是，在傅里叶域中，深度和剪切操作是解耦的。由于所有点共享相同的仿射变换矩阵 A，剪切过程可以通过对EFS应用相同的仿射变换  $A^{-T}$  来实现。然后可以在EFS的相位分量中补偿深度。

深度作为相位调制: 根据(15)，每个点 p 的深度信息  $x_ p^0$  不影响 EFS 的功率谱，而只作用于相位谱（即，深度项）。

遮挡分析: 除了深度和剪切项之外，掩模 Mp 的傅里叶变换指示了每个点的遮挡信息。图3说明了焦点堆栈中遮挡的产生和外观。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/df26f2e7237e43528ef6a9c8f34077b8.png#pic_ center" width="70%" />
</div>

在图3(a)中，三个输入视图（实线相机和线条）中没有遮挡，然而，当另外三个视图（虚线相机和线条）被合成时，遮挡出现了，即橙色矩形在顶视图中被绿色矩形遮挡。图3(b)显示了相应的焦点堆栈。由于点 P 被 O 遮挡，通过 p 和 o 的线是绿色的，与点 o 的颜色相匹配。基于这些观察，图3(c)和(d)分别说明了点 p 和 o 的掩模。值得注意的是，点 p 的掩模只包含5个视图，而 o 显示了6个视图，表明 p 和 o 之间存在遮挡。因此，遮挡可以从掩模集合  $\{M_ p\}_ {p=1}^{N_ p}$  中获得。

交叉视图效果: 另外，在(15)中引入了一个交叉项。图2展示了焦点堆栈中的跨视图现象。与(15)中的原始项和剪切项相比，交叉视图点的数量远小于总像素数。我们进行了一系列实验来评估 Fcross_ n 的数量级，发现 Fcross_ n 占据了整个焦点堆栈的2%到8%。有关更详细的信息，请参考补充材料，可在线获取。

总之，我们得出结论，命题 1: 给定由稀疏和密集采样 LF 分别形成的两个焦点堆栈 Fkn 和 Fn，Fkn 可以用 Fn 和傅里叶域中相位调制的剪切 Fn 来表示，即原始和剪切的 EFSn。

# IV. 基于剪切函数的稠密重建

基于对稀疏  $EFS_ n$  与只有  $n$  个视图的密集  $EFS_ {kn}$  之间转换模型的理论分析，我们提出了一个两步学习框架来实现密集光场重建。第一步中，对输入的  $EFS_ n$  应用剪切操作以获得(15)中的剪切项。第二步中，将原始的  $EFS_ n$  与其剪切版本累积，得到粗略的  $Coarse-EFS_ {kn}$ 。然后，将  $Coarse-EFS_ {kn}$  输入到神经网络中以补偿频谱。所提出的密集光场重建框架如图4所示。先前的工作[26]，[27] 采用原始的  $EFS_ n$  作为输入，网络专注于同时学习(15)中的所有四项，即交叉项、遮挡项、深度项和剪切项。与[26]，[27] 不同，所提出的ODU-Net不需要通过网络学习(15)中的剪切项，这减轻了训练的难度并允许更快的收敛。因此，所提出的方法有效地且高效地实现了更好的频谱补偿。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/fefc62018497446cb7df2ebacbee37fe.png#pic_ center" width="70%" />
</div>

## A. 欠采样EFS上的剪切

给定一个欠采样的EPI，通过应用(1a)中的剪切操作获得一个混叠的焦点堆栈，然后应用(1b)中的二维傅里叶变换操作，使用(1b)得到欠采样的  $EFS_ n$ 。为了从输入的  $EFS_ n$  重建具有  $k$  倍视图的  $EFS_ {kn}$ ，根据(15)中的剪切项，剪切操作被应用  $k-1$  次。然后，将原始输入的  $EFS_ n$  与其剪切版本结合起来，产生粗略的  $Coarse-EFS_ {kn}$ 。

## B. EFS重建

在这一部分，我们设计了一个名为Occlusion-aware-Dual stream U-Net (ODU-Net)的网络来优化  $Coarse-EFS_ {kn}$ 。根据(15)，深度作为相位调制  $e^{2\pi i x_ p^0 A^{-T} \omega}$  对剪切的  $EFS_ n$  起作用。基于这一观察，首先引入了一个相位调制模块（如图5中的蓝色虚线框所示，以及图5(c)中的详细视图）来优化从剪切操作中获得的相位谱。另外，为了加速网络训练速度，加入了额外的快捷路径（如图5(c)中的蓝色箭头所示）[51]，[52]。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/ea0d73814f184bbfb20a6da4f05650a7.png#pic_ center" width="70%" />
</div>

然后，考虑到第III-C2节中讨论的不确定的遮挡和跨视图效应，我们采用CNN自适应地学习用于导出掩模 Mp 和(15)中显示的交叉项的滤波器。然后，EFS重建可以被表述为以下无约束问题：

$$
\hat{EFS}_ {kn} = \Phi_ {\theta^\*} \* \left( \frac{1}{\sqrt{k}} \Psi_ {\theta^\*} \left( \sum_ {j=0}^{k-1} EFS_ n(A^{-T} \omega) \right) \right), \quad (16)
$$

$$
(\theta_ 1^\*, \theta_ 2^\*) = \arg\min_ {\theta_ 1,\theta_ 2} \left\| EFS_ {kn} - \hat{EFS}_ {kn} \right\|^2 + \lambda loss_ s, \quad (17)
$$

其中  $\Psi(\cdot)$  指的是网络中的相位调制模块， $\Phi(\cdot)$  模拟了(15)中的遮挡项和交叉项。参数  $\theta_ 1$  和  $\theta_ 2$  分别对应于  $\Phi$  和  $\Psi$  的优化目标。标量  $\lambda$  设置为1.5，用于平衡两个损失项的贡献。

第一项  $|EFS_ {kn} - \hat{EFS}_ {kn}|$  量化了重建的EFS与真实密集EFS之间的平均绝对误差（MAE）。第二项  $loss_ s$  强制执行在重建的EFS中保持共轭对称性，

$$
loss_ s = \frac{1}{N_ f W N_ f - 1} \sum_ {i=0}^{W - 1} \sum_ {j=0}^{N_ f - 1} |EFS(\omega_ i, \omega_ j) - EFS(-\omega_ i, -\omega_ j)|. \quad (18)
$$

请参考[26]，[27]了解EFS的共轭对称性。

所使用的神经网络的架构如图5所示。与[26]中采用的方法类似，这个网络采用双流U-Net分别从功率谱和相位谱中提取特征。然而，在从相位谱中提取特征之前，引入了相位调制模块来优化剪切操作获得的相位谱。在整个特征提取过程中，滤波器被自适应地训练以获取不同的场景点掩模 Mp，从而分离场景遮挡。随后，实部和虚部通过欧拉公式结合起来生成实部和虚部。最后，实部和虚部被连接起来并前向传递到卷积神经网络层进行优化（有关U-Net和CNN层的详细信息，请参考[27]的补充材料，可在[27]的网站在线获取）。

## C. EPI细化

给定ODU-Net优化的  $\hat{EFS}_ {kn}$ ，相应的EPI谱  $E_ {kn}(\omega_ u, \omega_ x)$  可以通过逆转(2a)和(2b)中的操作直接获得。通过对  $E_ {kn}(\omega_ u, \omega_ x)$  应用二维逆傅里叶变换（IFT），可以获得密集采样的  $\hat{E}_ {kn}(u, x)$ 。

然而，由于在从  $\hat{F}_ {kn}(f, \omega_ x)$  构造  $E_ {kn}(\omega_ u, \omega_ x)$  时进行了插值操作，特别是在远离参考视图的边缘视图中，会出现“拖尾”效应（如图4(e)中的红框所示）和颜色失真（如图4(e)中的蓝框所示）。因此，使用一个额外的U-Net和感知损失来细化  $\hat{E}_ {kn}(u, x)$ （有关U-Net和CNN层的详细信息，请参考[27]的补充材料，可在[27]的网站在线获取）。

完整的基于剪切EFS的密集光场重建算法见算法1。H代表子孔径图像的高度。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/4fe9b26f2aca4e608a514c3406728a2c.png#pic_ center" width="70%" />
</div>

# V. 评估

为了评估我们提出的基于剪切EFS的密集光场重建方法，我们在合成和真实世界的光场数据集上进行了实验[53]。真实世界的光场数据集是通过相机阵列和 plenoptic 相机（Lytro）捕获的。

我们主要将我们的方法与五种最先进的基于学习的方法进行比较：Wu 2019 [55]、Wu 2021 [56]（没有显式深度估计）、LLFF [57]（基于多平面图像MPI）、Guo 2023 [16]和没有剪切的EFS [27]（EFS w/o shear）。可以观察到，Wu 2021 [56]、LLFF [57]、Guo 2023 [16]和EFS w/o shear [27]是在我们的训练日期上使用发布的训练代码重新训练的，以实现公平比较。对于没有发布原始代码的Wu 2019 [55]，我们使用了作者提供的预训练模型。

定量评估是通过测量合成视图的亮度通道上的平均PSNR和SSIM指标来执行的。在消融实验中，我们分别分析了剪切操作的数量、下采样的界限以及相位调制模块的影响。

## A. 数据集和实现细节

在训练过程中，使用了合成光场（由POV [58]渲染）和真实世界的光场。合成数据集包括12个光场，包含复杂纹理结构，使用自动光场生成器[36]、[58]渲染。其中，7个光场用于训练，另外5个用于测试。真实世界数据集来自Guo等人的高分辨率光场数据集[38]，包含26个光场。其中，20个用于训练，其余6个用于测试。为了说明视图和EFS线之间的关系，并探索剪切操作数量的影响，我们使用前200个视图进行实验。此外，我们还在虚拟相机阵列（由Blender软件[59]生成）和真实相机阵列（Disney [60]）捕获的以前未见过的场景上评估了所提出的方法的性能，以评估其泛化能力。表II列出了所有数据集的参数。在密集光场中，大多数场景的两个相邻视图之间的视差小于一个像素，而在几种场景中，视差达到两个像素。我们的训练数据包括在不同的下采样率下获得的EPI样本。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/98789b6e49614cb1ac3ab3107271cbc2.png#pic_ center" width="70%" />
</div>

## B. 消融实验

本节通过执行以下消融实验，实证验证了剪切操作的数量、重建EPI的采样率以及相位调制模块的影响。

剪切操作分析：在这个实验中，我们使用包含200个视图的合成“Pot”光场场景（POV-Synthetic LFs [58]）进行测试。我们在10倍和15倍的下采样水平下分别进行了5次、10次、15次和20次的剪切操作。值得注意的是，在15倍下采样率下，两个相邻视图之间的最大视差可以达到15像素。

如图6(b)所示，剪切操作不足导致重建视图由于在EFS中大量丢失而显示出显著的颜色偏差。比较图6(d)和(e)，重要的是要注意，重建性能的改进并不总是与剪切操作的数量增加相一致。当剪切操作的数量等于下采样率（导致相邻视图之间的视差小于±1）时，进一步增加剪切操作的数量对重建质量几乎没有影响，但会增加算法的时间复杂度。表III展示了图6的定量结果。因此，在后续实验中，我们在处理k倍下采样时执行k-1次剪切操作。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e9a05ea6009c495f9e316bb0e84a1fcf.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/38ca0f3f2dc7407d86f84e95b092586d.png#pic_ center" width="70%" />
</div>

界限分析：为了评估我们方法的鲁棒性，我们在10倍、15倍、20倍和25倍的下采样设置下，分别对“自行车”真实光场[38]进行了实验。如图7(b)和(c)所示，对于10倍和15倍的下采样设置，所提出的方法可以重建具有清晰边界的视图。尽管在20倍下采样时PSNR/SSIM值有所下降，重建结果在视觉上仍然显示出明显的遮挡边缘。在25倍下采样时，重建视图和误差图的质量显著下降。图8(a)和(b)显示了在不同下采样率下所有重构视图的定量比较（PSNR/SSIM）。请注意，图8中显示的准周期性“山谷”是由于光场捕获过程中的不完美对齐造成的[38]。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/03e6e82640314a59ba77ad451cdd6ec1.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9eafeb1340f34f969861f9542c36fd4a.png#pic_ center" width="70%" />
</div>

相位调制分析：在这里我们验证了图5中描述的相位调制的有效性。图9展示了在“pot-cube”场景上，有无相位调制模块的视图重建结果的定量比较，展示了相位调制模块的影响（15倍下采样和14次剪切操作）。比较图9(b)和(c)，我们观察到在添加相位调制模块后，重建质量显著提高。特别是在EPI中，保持重建的亮度和视图间的一致性是具有挑战性的，没有相位调制模块时，如图9(b)中的红框所示的伪影。图10展示了有无相位调制在每个视图上的“pot-cube”场景的重建结果的定量评估曲线。结果表明，当缺少相位调制模块时，重建视图的PSNR和SSIM值都有明显的降低。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0572e247f6fc424a946256048bffb61b.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/062c8ff77ff44fdfa9fa3805d1a563d1.png#pic_ center" width="70%" />
</div>

最大视差：为了评估我们方法处理视差上限的能力，我们将“雕像”场景的Disney数据集[60]进行了10倍到70倍的下采样。此时，最大视差从20像素以上变化到140像素以上（输入视图也从15个视图变化到3个视图）。图11提供了在不同视差范围内“雕像”场景[60]的重建结果。如图11所示，随着视差的增加，视图重建的质量逐渐下降。当视差超过100像素时，图像质量明显恶化，特征是增加了伪影。然而，如图11(b)到(f)所示的EPI结构，所提出的方法即使在面对大视差时，也在很大程度上保持了视图的一致性。表IV显示了在不同视差范围内“雕像”场景[60]的平均PSNR和SSIM测量值，证明了我们方法在不同视差场景中的鲁棒性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5bc401905c4f446ca0962554b8751a0c.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/39dc75793b42434cbd7a322956b20cf6.png#pic_ center" width="70%" />
</div>

EPI细化中的损失函数：我们的EPI细化U-Net的损失函数由三部分组成：MAE损失、SSIM损失和感知损失。为了评估这三个项对实验结果的影响，我们分别进行了不同组合的训练。然后在POV-Syn. LFs [53]、Blend-Syn. LFs [53]和Real LFs [38]数据集上进行了测试，下采样15倍，剪切操作14次。损失函数的不同组合和测试结果列在表V中。从表V的最后一行可以看出，感知损失显著提高了重建性能。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/0698d9327aa7431aa98effb7d7971161.png#pic_ center" width="70%" />
</div>

## C. 与SOTA的比较

我们的方法与以下最先进的学习方法进行了比较：Wu 2019 [55]、Wu 2021 [56]、LLFF [57]、Guo 2023 [16]和没有剪切的EFS方法 [27]。表VI展示了在不同下采样率下，对合成和真实光场的平均PSNR/SSIM/LPIPS [61]测量值。不同方法在几个测试场景上的定性比较分别显示在图12、图14和图16中。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/bc2a39ed57994de5a653ec02c42c5488.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/a768cf27a03e4a9e87dcc4f0e5a6389c.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/d1d3cf247c434a579c7859ddd3d8623e.png#pic_ center" width="70%" />
</div>

1）用全光纤摄像机拍摄的真实LFs：我们使用真实世界光场数据集 [38] 在15倍下采样下评估了所提出的方法，这些数据集包含了现实世界中的大量静态场景。图12展示了在“basket”场景下15倍下采样的定性结果，该场景包含多个细结构，如篮柄。注意，在15倍下采样率下，剪切操作的数量是14。

在图12(b)中，Wu 2019 [55]的重建结果在篮柄周围出现了明显的重影伪影，这是由于他们网络的有限感知场所致。此外，高斯卷积核仅对小视差有效。Wu 2021 [56]由于在后续融合网络中输入了不同剪切量生成的多个“合理”结果，导致在这些输出之间出现模糊性。这种模糊性导致无法重建具有细薄和重复模式的区域，如图12(c)中的绿色框所示。基于MPI的LLFF [57]倾向于在具有模糊或重复纹理的区域，或在输入图像之间有移动内容的区域，为错误的层分配高不透明度。这种行为导致了如图12(d)中的绿色框所示的细长结构周围的浮动或模糊区域。Guo 2023 [16]的方法需要额外的光流估计，此过程中的错误可能会影响视图一致性。这种影响在图12(e)中的绿色框和EPI中是明显的。图12(f)展示了没有剪切操作的EFS方法 [27]的结果，由于剪切操作前的频谱重建导致频谱能量损失，导致重建视图中的颜色偏差。相比之下，所提出的基于剪切EFS的重建方法在重复纹理区域实现了更清晰的边界，并减少了颜色失真（如图12(g)所示）。图13(a)和(b)分别展示了在“basket”场景下15倍下采样的每个重建视图的PSNR和SSIM测量值。总体而言，我们的方法优于现有最先进方法。表VI的第5列进一步展示了包括PSNR、SSIM和LPIPS在内的定量比较，进一步证实了所提出方法的优越性。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/c89cd0cda9e14de99d02d39384fcb25e.png#pic_ center" width="70%" />
</div>

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/9674b67cefbb40f0878f81559b47f659.png#pic_ center" width="70%" />
</div>

2）Blender生成的合成LF数据集：我们还使用Blender合成光场数据集在15倍下采样下评估了所提出的方法（剪切操作的数量为14），这些数据集具有较大的视差（最大视差达到15像素）。

图14展示了在两个合成光场下15倍下采样的定性结果。我们可以看到，在Wu 2019 [55]的结果中，出现了严重的重影伪影，并且重建视图不一致（见图14(b)中的EPI）。类似地，在LLFF [57]的结果中，台灯和水槽边缘（图14(d)中的红色框）出现了模糊现象。Wu 2021 [56]在细薄结构和大视差情况下无法很好地保持视差一致性（见图14(c)中的黄色框上的EPI）。Guo 2023 [16]方法中的光流估计错误导致物体边缘的错误变形，从而在场景中物体边缘的重建中出现错误（参见图14(e)中放大的红色框）。尽管EFS w/o shear方法 [27]在频域中重建了密集光场，但由于频谱能量误差，它仍然遭受颜色偏差的困扰（见图14(f)）。我们的方法不仅对空间内容的敏感性降低，而且在网络中集成了遮挡感知模块，能够产生高质量和视图一致的重建（见图14(g)）。如图15所示，我们方法在每个重建视图上实现的PSNR/SSIM测量值高于现有最先进方法。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/cfad958f52c64a10947b186230474803.png#pic_ center" width="70%" />
</div>

3）使用相机阵列拍摄的真实世界LFs：为了验证所提出方法在宽基线下的效用，我们进一步使用Disney LFs [60]（由相机阵列捕获）来评估所提出的方法。

图16展示了在“church”场景下10倍下采样（14次剪切操作）的结果，这些场景具有宽基线和复杂的遮挡（最大视差达到20像素）。由于Wu 2019 [55]网络的感知场有限，重建视图显示出严重的伪影（见图16(b)）。在Wu 2021 [56]中，使用不同的视差对EPI进行剪切。然而，剪切操作可能在大视差下引入错误，导致重建结果中的视图一致性丧失（见图16(c)中的红色框和EPI）。LLFF [57]需要大量内存来构建MPI，这在图像分辨率和使用的MPI层数之间造成了权衡。这种权衡导致性能下降，尤其是在高分辨率输入区域和大视差下，如图16(d)中的放大矩形框和EPI所示。我们重新训练了Guo 2023 [16]方法，但它在光场数据集中任意位置的视图重建方面仍然面临挑战。值得注意的是，在大视差场景中，不同视图之间的内容一致性显著不同（见图16(e)中的EPI）。此外，在Guo 2023 [16]方法的预处理阶段中光流估计的错误阻碍了场景中细长纹理结构的准确重建，导致描绘电线时出现明显错误，如图16(e)所示。由于大视差导致的频谱能量损失，没有剪切操作的EFS [27]重建的视图也出现了颜色偏差。相比之下，我们提出的方法在大视差下表现出优越的性能，并与其他方法相比展现出更好的视图一致性。这归因于我们方法中集成的EFS剪切操作和遮挡分析。图17展示了在“church”场景（10倍下采样）上每个重建视图的定量比较曲线（PSNR/SSIM），我们发现我们方法的定量比较曲线显著高于现有最先进方法。表VI的右四列列出了四个Disney LFs的定量结果。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/f5e2a95b2e8442e7b24e3573f52c7771.png#pic_ center" width="70%" />
</div>

4）4D光场中输入视图的最小数量：为了分析所提出方法对输入视图数量的要求及其对4D光场重建的可行性，图18展示了从2×2视图重建9×9视图的4D光场的结果。我们遵循“先水平后垂直”[27]的策略来重建4D光场。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/e6f82b5b5b52429c84a6790d6248c1c9.png#pic_ center" width="70%" />
</div>

通过空间重聚焦构建EFS意味着即使输入视图数量有限，只要可以执行重聚焦（至少使用两个视图），所提出的基于剪切EFS的方法仍然是可行的。在图18中，前景区域，特别是折纸鹤中，明显存在显著的视差变化。与其他最先进方法相比，我们的方法仍然可以在具有如此显著视差变化的区域（如图18中放大框所示）实现优越的重建结果。图18(f)和(g)分别提供了使用不同方法重建的9×9视图的PSNR和SSIM值。显然，我们的方法提供了比其他方法更高的PSNR/SSIM值。

## D. 局限性

本文中的理论分析基于朗伯假设（Lambertian assumption），该假设认为在不同视图下同一空间点的纹理是一致的，从而在场景中形成直线的像平面图像（EPI）线。然而，当场景中存在非朗伯材料，并且它们的表面粗糙时，EPI线不再是直线。因此，使用(1)构建的焦点堆栈及其对应的EFS将不再展示图1中描述的中心对称结构。结果，我们的方法在处理具有这些特征的非朗伯场景时会遇到挑战。

图19展示了在两个不同的非朗伯场景中使用我们的方法构建焦点堆栈和EPI重建的结果。在图19(a)中，我们观察到一个包含曲面物体的非朗伯场景。具体来说，陶瓷盘子的表面不均匀，导致从不同视图反射到盘子表面的纹理不一致（左侧）。这种不一致性破坏了绿色线位置（顶右）的EPI线的线性。由这个15倍下采样的EPI（中间右）构建的焦点堆栈及其对应的EFS不再展示[26]，[27]中描述的特征（参见图19(a)中的黄色框）。因此，第III-C节中详细描述的剪切操作在这种情况下变得不适用。在这种情况下，我们的方法在重建的EPI的结构和颜色方面都产生了错误（底部右）。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/5c3f385519ad46289a1f567633957eac.png#pic_ center" width="70%" />
</div>

图19(b)展示了一个具有平滑表面的非朗伯场景。具体来说，在这个场景的中心，有两个镜子，它们的表面都很平滑。因此，从不同视图反射到镜子表面的纹理保持一致（左侧），导致绿色线位置的EPI线形成一条连续的直线（顶右）。由这个15倍下采样的EPI（中间右）构建的焦点堆栈及其对应的EFS展示了[26]，[27]中描述的特征，并且与我们的方法一致。因此，重建的EPI保持了其结构特征（底部右）。

尽管所提出的方法在视图重建质量和跨视图一致性保持方面优于现有最先进方法，但剪切操作可能会引入额外的时间成本。这个问题可以通过将剪切操作嵌入到网络[56]中来缓解。此外，所提出的方法依赖于2D EPI，缺乏对子孔径图像中行与行之间一致性的显式约束，如图20中放大的框所示。一个可能的解决方案可能涉及在f-x-y空间内使用3D EFS。

<div align=center>
  <img src="https://img-blog.csdnimg.cn/direct/bff640d8dac74bc0ad4c70844e8fb634.png#pic_ center" width="70%" />
</div>

# VI. 结论

基于傅里叶仿射和卷积定理，我们分析了稀疏和密集光场的EFS之间的关系，并在稀疏采样的EFS上对EFS进行了正式的分解。我们还分析了焦点堆栈中的遮挡以及EFS剪切操作，并提供了遮挡模型。基于理论分析，我们为从欠采样光场重建密集光场设计了一个特别调制相位的ODU-Net。所提出的方法在具有挑战性的条件下表现出色，例如大视差和复杂的遮挡，并保持了跨视图的一致性。实验结果验证了剪切策略不仅提高了EFS补全的准确性，还降低了网络学习的复杂性。

# 声明
本文内容为论文学习收获分享，受限于知识能力，本文队员问的理解可能存在偏差，最终内容以原论文为准。本文信息旨在传播和学术交流，其内容由作者负责，不代表本号观点。文中作品文字、图片等如涉及内容、版权和其他问题，请及时与我们联系，我们将在第一时间回复并处理。

